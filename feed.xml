<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: PodSecurityPolicy Deprecation: Past, Present, and Future</title><link>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Tabitha Sable (Kubernetes SIG Security)&lt;/p>
&lt;p>PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week. This starts the countdown to its removal, but doesn’t change anything else. PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely. In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.&lt;/p>
&lt;p>What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next? How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP, so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.&lt;/p>
&lt;h2 id="what-does-deprecation-mean-in-kubernetes">What does deprecation mean in Kubernetes?&lt;/h2>
&lt;p>Whenever a Kubernetes feature is set to go away, our &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.&lt;/p>
&lt;p>Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations, PodSecurityPolicy will continue to be fully functional for several more releases. The current plan is to remove PSP from Kubernetes in the 1.25 release.&lt;/p>
&lt;p>Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.&lt;/p>
&lt;h2 id="what-is-podsecuritypolicy">What is PodSecurityPolicy?&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a> is a built-in &lt;a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/">admission controller&lt;/a> that allows a cluster administrator to control security-sensitive aspects of the Pod specification.&lt;/p>
&lt;p>First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet. Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and cannot run.&lt;/p>
&lt;p>One more important thing to know about PodSecurityPolicy: it’s not the same as &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context">PodSecurityContext&lt;/a>.&lt;/p>
&lt;p>A part of the Pod specification, PodSecurityContext (and its per-container counterpart &lt;code>SecurityContext&lt;/code>) is the collection of fields that specify many of the security-relevant settings for a Pod. The security context dictates to the kubelet and container runtime how the Pod should actually be run. In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.&lt;/p>
&lt;p>The deprecation of PSP does not affect PodSecurityContext in any way.&lt;/p>
&lt;h2 id="why-did-we-need-podsecuritypolicy">Why did we need PodSecurityPolicy?&lt;/h2>
&lt;p>In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that represent the building blocks of software applications. The various controllers inside a Kubernetes cluster react to these resources, creating further Kubernetes resources or configuring some software or hardware to accomplish our goals.&lt;/p>
&lt;p>In most Kubernetes clusters, RBAC (Role-Based Access Control) &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">rules&lt;/a> control access to these resources. &lt;code>list&lt;/code>, &lt;code>get&lt;/code>, &lt;code>create&lt;/code>, &lt;code>edit&lt;/code>, and &lt;code>delete&lt;/code> are the sorts of API operations that RBAC cares about, but &lt;em>RBAC does not consider what settings are being put into the resources it controls&lt;/em>. For example, a Pod can be almost anything from a simple webserver to a privileged command prompt offering full access to the underlying server node and all the data. It’s all the same to RBAC: a Pod is a Pod is a Pod.&lt;/p>
&lt;p>To control what sorts of settings are allowed in the resources defined in your cluster, you need Admission Control in addition to RBAC. Since Kubernetes 1.3, PodSecurityPolicy has been the built-in way to do that for security-related Pod fields. Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,” without needing to deploy additional external admission controllers.&lt;/p>
&lt;h2 id="why-is-podsecuritypolicy-going-away">Why is PodSecurityPolicy going away?&lt;/h2>
&lt;p>In the years since PodSecurityPolicy was first introduced, we have realized that PSP has some serious usability problems that can’t be addressed without making breaking changes.&lt;/p>
&lt;p>The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them. It is easy to accidentally grant broader permissions than intended, and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy, but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod. Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely, and it’s impossible for PSP to ever be enabled by default.&lt;/p>
&lt;p>For more information about these and other PSP difficulties, check out SIG Auth’s KubeCon NA 2019 Maintainer Track session video:
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>Today, you’re not limited only to deploying PSP or writing your own custom admission controller. Several external admission controllers are available that incorporate lessons learned from PSP to provide a better user experience. &lt;a href="https://github.com/cruise-automation/k-rail">K-Rail&lt;/a>, &lt;a href="https://github.com/kyverno/kyverno/">Kyverno&lt;/a>, and &lt;a href="https://github.com/open-policy-agent/gatekeeper/">OPA/Gatekeeper&lt;/a> are all well-known, and each has its fans.&lt;/p>
&lt;p>Although there are other good options available now, we believe there is still value in having a built-in admission controller available as a choice for users. With this in mind, we turn toward building what’s next, inspired by the lessons learned from PSP.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members have been working together for months to ensure that what’s coming next is going to be awesome. We have developed a Kubernetes Enhancement Proposal (&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">KEP 2579&lt;/a>) and a prototype for a new feature, currently being called by the temporary name &amp;quot;PSP Replacement Policy.&amp;quot; We are targeting an Alpha release in Kubernetes 1.22.&lt;/p>
&lt;p>PSP Replacement Policy starts with the realization that since there is a robust ecosystem of external admission controllers already available, PSP’s replacement doesn’t need to be all things to all people. Simplicity of deployment and adoption is the key advantage a built-in admission controller has compared to an external webhook, so we have focused on how to best utilize that advantage.&lt;/p>
&lt;p>PSP Replacement Policy is designed to be as simple as practically possible while providing enough flexibility to really be useful in production at scale. It has soft rollout features to enable retrofitting it to existing clusters, and is configurable enough that it can eventually be active by default. It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.&lt;/p>
&lt;h2 id="what-does-this-mean-for-you">What does this mean for you?&lt;/h2>
&lt;p>What this all means for you depends on your current PSP situation. If you’re already using PSP, there’s plenty of time to plan your next move. Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.&lt;/p>
&lt;p>If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules, you will likely find the simplicity of PSP Replacement Policy too limiting. Use the next year to evaluate the other admission controller choices in the ecosystem. There are resources available to ease this transition, such as the &lt;a href="https://github.com/open-policy-agent/gatekeeper-library">Gatekeeper Policy Library&lt;/a>.&lt;/p>
&lt;p>If your use of PSP is relatively simple, with a few policies and straightforward binding to service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs. Evaluate your PSPs compared to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies. Please follow along with or contribute to the KEP and subsequent development, and try out the Alpha release of PSP Replacement Policy when it becomes available.&lt;/p>
&lt;p>If you’re just beginning your PSP journey, you will save time and effort by keeping it simple. You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs. If you set the cluster default by binding a Baseline or Restricted policy to the &lt;code>system:serviceaccounts&lt;/code> group, and then make a more-permissive policy available as needed in certain Namespaces &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod">using ServiceAccount bindings&lt;/a>, you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy. If your needs are much more complex than this, your effort is probably better spent adopting one of the more fully-featured external admission controllers mentioned above.&lt;/p>
&lt;p>We’re dedicated to making Kubernetes the best container orchestration tool we can, and sometimes that means we need to remove longstanding features to make space for better things to come. When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move. In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases. Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!&lt;/p>
&lt;p>&lt;strong>Acknowledgment:&lt;/strong> It takes a wonderful group to make wonderful software. Thanks are due to everyone who has contributed to the PSP replacement effort, especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt. It’s been a joy to work with y’all on this.&lt;/p></description></item><item><title>Blog: The Evolution of Kubernetes Dashboard</title><link>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</guid><description>
&lt;p>Authors: Marcin Maciaszczyk, Kubermatic &amp;amp; Sebastian Florek, Kubermatic&lt;/p>
&lt;p>In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.&lt;/p>
&lt;h2 id="how-it-all-began">How It All Began&lt;/h2>
&lt;p>The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.&lt;/p>
&lt;p>The very &lt;a href="https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c">first commit&lt;/a> to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (&lt;a href="https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd">Sebastian committed on 16 November 2015&lt;/a>; &lt;a href="https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55">Marcin committed on 23 November 2015&lt;/a>). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png"
alt="The First Version of the User Interface"/> &lt;figcaption>
&lt;p>The First Version of the User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png"
alt="Prototype of the New User Interface"/> &lt;figcaption>
&lt;p>Prototype of the New User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png"
alt="The Current User Interface"/> &lt;figcaption>
&lt;p>The Current User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.&lt;/p>
&lt;h2 id="growing-up-the-big-migration">Growing Up - The Big Migration&lt;/h2>
&lt;p>At &lt;a href="https://github.com/kubernetes/dashboard/pull/2727">the beginning of 2018&lt;/a>, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.&lt;/p>
&lt;p>The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1">first beta version&lt;/a> in July 2019, followed by the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0">2.0 release&lt;/a> in April 2020 — our baby had grown up.&lt;/p>
&lt;h2 id="where-are-we-standing-in-2021">Where Are We Standing in 2021?&lt;/h2>
&lt;p>Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.2.0">Dashboard v2.2.0&lt;/a> provides support for Kubernetes v1.20.&lt;/p>
&lt;p>On top of that, we put in a great deal of effort into &lt;a href="https://github.com/kubernetes/dashboard/issues/5232">improving resource support&lt;/a>. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.&lt;/p>
&lt;p>Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)&lt;/p>
&lt;h2 id="what-s-next">What’s Next&lt;/h2>
&lt;p>The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:&lt;/p>
&lt;ul>
&lt;li>Keep providing support for the new Kubernetes versions&lt;/li>
&lt;li>Keep improving the support for the existing resources&lt;/li>
&lt;li>Keep working on auth system improvements&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/pull/5449">Rewrite the API to use gRPC and shared informers&lt;/a>: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.&lt;/li>
&lt;li>Split the application into two containers, one with the UI and the second with the API running inside.&lt;/li>
&lt;/ul>
&lt;h2 id="the-kubernetes-dashboard-in-numbers">The Kubernetes Dashboard in Numbers&lt;/h2>
&lt;ul>
&lt;li>Initial commit made on October 16, 2015&lt;/li>
&lt;li>Over 100 million pulls from Dockerhub since the v2 release&lt;/li>
&lt;li>8 supported languages and the next 2 in progress&lt;/li>
&lt;li>Over 3360 closed PRs&lt;/li>
&lt;li>Over 2260 closed issues&lt;/li>
&lt;li>100% coverage of the supported core Kubernetes resources&lt;/li>
&lt;li>Over 9000 stars on GitHub&lt;/li>
&lt;li>Over 237 000 lines of code&lt;/li>
&lt;/ul>
&lt;h2 id="join-us">Join Us&lt;/h2>
&lt;p>As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., &lt;a href="https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">issues with help wanted label&lt;/a>. Please feel free to reach out via GitHub or the #sig-ui channel in the &lt;a href="https://slack.k8s.io/">Kubernetes Slack&lt;/a>.&lt;/p></description></item><item><title>Blog: A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</title><link>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Chris Seto (Cockroach Labs)&lt;/p>
&lt;p>As long as you're willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will &amp;quot;just work&amp;quot;. However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one's own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.&lt;/p>
&lt;p>Scaling any system has two main components:&lt;/p>
&lt;ol>
&lt;li>Adding or removing infrastructure that the system will run on, and&lt;/li>
&lt;li>Ensuring that the system knows how to handle additional instances of itself being added and removed.&lt;/li>
&lt;/ol>
&lt;p>Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).&lt;/p>
&lt;p>One of Kubernetes' responsibilities is to place &amp;quot;resources&amp;quot; (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: &amp;quot;I must be in availability zone &lt;em>A&lt;/em>&amp;quot; (see &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#nodes-are-labeled">Running in multiple zones&lt;/a>), or &amp;quot;I can't be placed onto the same node as this other Pod&amp;quot; (see &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Affinity and anti-affinity&lt;/a>).&lt;/p>
&lt;p>As an addition to those constraints, Kubernetes offers &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Statefulsets&lt;/a> that provide identity to Pods as well as persistent storage that &amp;quot;follows&amp;quot; these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod's name. It's important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.&lt;/p>
&lt;p>Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see &lt;a href="https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html">Orchestrate CockroachDB in a Single Kubernetes Cluster&lt;/a>.
In this article, I'll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.&lt;/p>
&lt;p>A three-node CockroachCloud cluster would look something like this:&lt;/p>
&lt;p>&lt;img src="image01.png" alt="3-node, multi-zone cockroachdb cluster">&lt;/p>
&lt;p>When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.&lt;/p>
&lt;p>&lt;img src="image02.png" alt="illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster">&lt;/p>
&lt;p>Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.&lt;/p>
&lt;p>To remove resources from a cluster, we perform these operations in reverse order.&lt;/p>
&lt;p>We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.&lt;/p>
&lt;p>&lt;img src="image03.png" alt="illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes">&lt;/p>
&lt;p>Now, remember that pods in a StatefulSet of size &lt;em>n&lt;/em> must have ids in the range &lt;code>[0,n)&lt;/code>. When scaling down a StatefulSet by &lt;em>m&lt;/em>, Kubernetes removes &lt;em>m&lt;/em> pods, starting from the highest ordinals and moving towards the lowest, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">the reverse in which they were added&lt;/a>.
Consider the cluster topology below:&lt;/p>
&lt;p>&lt;img src="image04.png" alt="illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones">&lt;/p>
&lt;p>As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.&lt;/p>
&lt;p>&lt;img src="image05.png" alt="illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster">&lt;/p>
&lt;p>However, Kubernetes' scheduler doesn't &lt;em>guarantee&lt;/em> the placement above as we expected at first.&lt;/p>
&lt;p>Our combined knowledge of the following is what lead to this misconception.&lt;/p>
&lt;ul>
&lt;li>Kubernetes' ability to &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones">automatically spread Pods across zone&lt;/a>&lt;/li>
&lt;li>The behavior that a StatefulSet with &lt;em>n&lt;/em> replicas, when Pods are being deployed, they are created sequentially, in order from &lt;code>{0..n-1}&lt;/code>. See &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">StatefulSet&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;p>Consider the following topology:&lt;/p>
&lt;p>&lt;img src="image06.png" alt="illustration: 6-node cockroachdb cluster distributed across 3 availability zones">&lt;/p>
&lt;p>These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!&lt;/p>
&lt;p>&lt;img src="image07.png" alt="illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ">&lt;/p>
&lt;p>Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.&lt;/p>
&lt;p>To correct the latter issue, we now employ a &amp;quot;hunt and peck&amp;quot; approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.&lt;/p>
&lt;h2 id="a-session-of-brainstorming-left-us-with-3-options">A session of brainstorming left us with 3 options:&lt;/h2>
&lt;h3 id="1-upgrade-to-kubernetes-1-18-and-make-use-of-pod-topology-spread-constraints">1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints&lt;/h3>
&lt;p>While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">pod topology spread constraints&lt;/a> were still a &lt;a href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">beta feature in 1.18&lt;/a> which meant that it &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices">wasn't guaranteed to be available in managed clusters&lt;/a> even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking &lt;a href="https://caniuse.com/">caniuse.com&lt;/a> when Internet Explorer 8 was still around.&lt;/p>
&lt;h3 id="2-deploy-a-statefulset-per-zone">2. Deploy a statefulset &lt;em>per zone&lt;/em>.&lt;/h3>
&lt;p>Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.&lt;/p>
&lt;h3 id="3-write-a-custom-kubernetes-scheduler">3. Write a custom Kubernetes scheduler.&lt;/h3>
&lt;p>Thanks to an example from &lt;a href="https://github.com/kelseyhightower/scheduler">Kelsey Hightower&lt;/a> and a blog post from &lt;a href="https://banzaicloud.com/blog/k8s-custom-scheduler/">Banzai Cloud&lt;/a>, we decided to dive in head first and write our own &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">custom Kubernetes scheduler&lt;/a>.
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers">&lt;code>kubectl get events&lt;/code>&lt;/a> had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">kube-scheduler plugin system&lt;/a>. Our next POC was a &lt;code>Filter&lt;/code> plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!&lt;/p>
&lt;p>Our &lt;a href="https://github.com/cockroachlabs/crl-scheduler">custom scheduler plugin&lt;/a> is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin's implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the &lt;code>schedulerName&lt;/code> field in our StatefulSet definitions.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/_ostriches">Chris Seto&lt;/a> is a software engineer at Cockroach Labs and works on their Kubernetes automation for &lt;a href="https://cockroachlabs.cloud">CockroachCloud&lt;/a>, CockroachDB.&lt;/em>&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</title><link>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Shihang Zhang (Google)&lt;/p>
&lt;p>Typically when a &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.&lt;/p>
&lt;p>Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.&lt;/p>
&lt;p>Both of them exhibit the following drawbacks:&lt;/p>
&lt;ul>
&lt;li>Violating the principle of least privilege&lt;/li>
&lt;li>Every CSI driver needs to re-implement the logic of getting the pod’s service account token&lt;/li>
&lt;/ul>
&lt;p>The second approach is more problematic due to:&lt;/p>
&lt;ul>
&lt;li>The audience of the token defaults to the kube-apiserver&lt;/li>
&lt;li>The token is not guaranteed to be available (e.g. &lt;code>AutomountServiceAccountToken=false&lt;/code>)&lt;/li>
&lt;li>The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See &lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">file permission section for service account token&lt;/a>&lt;/li>
&lt;li>The token might be legacy Kubernetes service account token which doesn’t expire if &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Kubernetes 1.20 introduces an alpha feature, &lt;code>CSIServiceAccountToken&lt;/code>, to improve the security posture. The new feature allows CSI drivers to receive pods' &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">bound service account tokens&lt;/a>.&lt;/p>
&lt;p>This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.&lt;/p>
&lt;h2 id="pod-impersonation">Pod Impersonation&lt;/h2>
&lt;h3 id="using-gcp-apis">Using GCP APIs&lt;/h3>
&lt;p>Using &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>, a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to &lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">exchange for GCP tokens&lt;/a>. The pod's service account token is plumbed through the volume context in &lt;code>NodePublishVolume&lt;/code> RPC calls when the feature &lt;code>CSIServiceAccountToken&lt;/code> is enabled. For example: accessing &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a> via a &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">secret store CSI driver&lt;/a>.&lt;/p>
&lt;h3 id="using-vault">Using Vault&lt;/h3>
&lt;p>If users configure &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes as an auth method&lt;/a>, Vault uses the &lt;code>TokenReview&lt;/code> API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, &lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">secrets store CSI driver&lt;/a> and &lt;a href="https://github.com/jetstack/cert-manager-csi">cert manager CSI driver&lt;/a>.&lt;/p>
&lt;h2 id="short-lived-volumes">Short-lived Volumes&lt;/h2>
&lt;p>To keep short-lived volumes such as certificates effective, CSI drivers can specify &lt;code>RequiresRepublish=true&lt;/code> in their&lt;code>CSIDriver&lt;/code> object to have the kubelet periodically call &lt;code>NodePublishVolume&lt;/code> on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: Service Account Token for CSI Driver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">Token Requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is always welcome!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack and the mailing list&lt;/a>&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and the mailing list&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Third Party Device Metrics Reaches GA</title><link>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Renaud Gaubert (NVIDIA), David Ashpole (Google), and Pramod Ramarao (NVIDIA)&lt;/p>
&lt;p>With Kubernetes 1.20, infrastructure teams who manage large scale Kubernetes clusters, are seeing the graduation of two exciting and long awaited features:&lt;/p>
&lt;ul>
&lt;li>The Pod Resources API (introduced in 1.13) is finally graduating to GA. This allows Kubernetes plugins to obtain information about the node’s resource usage and assignment; for example: which pod/container consumes which device.&lt;/li>
&lt;li>The &lt;code>DisableAcceleratorMetrics&lt;/code> feature (introduced in 1.19) is graduating to beta and will be enabled by default. This removes device metrics reported by the kubelet in favor of the new plugin architecture.&lt;/li>
&lt;/ul>
&lt;p>Many of the features related to fundamental device support (device discovery, plugin, and monitoring) are reaching a strong level of stability.
Kubernetes users should see these features as stepping stones to enable more complex use cases (networking, scheduling, storage, etc.)!&lt;/p>
&lt;p>One such example is Non Uniform Memory Access (NUMA) placement where, when selecting a device, an application typically wants to ensure that data transfer between CPU Memory and Device Memory is as fast as possible. In some cases, incorrect NUMA placement can nullify the benefit of offloading compute to an external device.&lt;/p>
&lt;p>If these are topics of interest to you, consider joining the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node Special Insterest Group&lt;/a> (SIG) for all topics related to the Kubernetes node, the COD (container orchestrated device) workgroup for topics related to runtimes, or the resource management forum for topics related to resource management!&lt;/p>
&lt;h2 id="the-pod-resources-api-why-does-it-need-to-exist">The Pod Resources API - Why does it need to exist?&lt;/h2>
&lt;p>Kubernetes is a vendor neutral platform. If we want it to support device monitoring, adding vendor-specific code in the Kubernetes code base is not an ideal solution. Ultimately, devices are a domain where deep expertise is needed and the best people to add and maintain code in that area are the device vendors themselves.&lt;/p>
&lt;p>The Pod Resources API was built as a solution to this issue. Each vendor can build and maintain their own out-of-tree monitoring plugin. This monitoring plugin, often deployed as a separate pod within a cluster, can then associate the metrics a device emits with the associated pod that's using it.&lt;/p>
&lt;p>For example, use the NVIDIA GPU dcgm-exporter to scrape metrics in Prometheus format:&lt;/p>
&lt;pre>&lt;code>$ curl -sL http://127.0.01:8080/metrics
# HELP DCGM_FI_DEV_SM_CLOCK SM clock frequency (in MHz).
# TYPE DCGM_FI_DEV_SM_CLOCK gauge
# HELP DCGM_FI_DEV_MEM_CLOCK Memory clock frequency (in MHz).
# TYPE DCGM_FI_DEV_MEM_CLOCK gauge
# HELP DCGM_FI_DEV_MEMORY_TEMP Memory temperature (in C).
# TYPE DCGM_FI_DEV_MEMORY_TEMP gauge
...
DCGM_FI_DEV_SM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 139
DCGM_FI_DEV_MEM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 405
DCGM_FI_DEV_MEMORY_TEMP{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 9223372036854775794
&lt;/code>&lt;/pre>&lt;p>Each agent is expected to adhere to the node monitoring guidelines. In other words, plugins are expected to generate metrics in Prometheus format, and new metrics should not have any dependency on the Kubernetes base directly.&lt;/p>
&lt;p>This allows consumers of the metrics to use a compatible monitoring pipeline to collect and analyze metrics from a variety of agents, even if they are maintained by different vendors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/metrics-chart.png" alt="Device metrics flowchart">&lt;/p>
&lt;h2 id="nvidia-gpu-metrics-deprecated">Disabling the NVIDIA GPU metrics - Warning&lt;/h2>
&lt;p>With the graduation of the plugin monitoring system, Kubernetes is deprecating the NVIDIA GPU metrics that are being reported by the kubelet.&lt;/p>
&lt;p>With the &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#disable-accelerator-metrics">DisableAcceleratorMetrics&lt;/a> feature being enabled by default in Kubernetes 1.20, NVIDIA GPUs are no longer special citizens in Kubernetes. This is a good thing in the spirit of being vendor-neutral, and enables the most suited people to maintain their plugin on their own release schedule!&lt;/p>
&lt;p>Users will now need to either install the &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools">NVIDIA GDGM exporter&lt;/a> or use &lt;a href="https://github.com/nvidia/go-nvml">bindings&lt;/a> to gather more accurate and complete metrics about NVIDIA GPUs. This deprecation means that you can no longer rely on metrics that were reported by kubelet, such as &lt;code>container_accelerator_duty_cycle&lt;/code> or &lt;code>container_accelerator_memory_used_bytes&lt;/code> which were used to gather NVIDIA GPU memory utilization.&lt;/p>
&lt;p>This means that users who used to rely on the NVIDIA GPU metrics reported by the kubelet, will need to update their reference and deploy the NVIDIA plugin. Namely the different metrics reported by Kubernetes map to the following metrics:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Kubernetes Metrics&lt;/th>
&lt;th>NVIDIA dcgm-exporter metric&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>container_accelerator_duty_cycle&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_GPU_UTIL&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_used_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_total_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_FREE + DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You might also be interested in other metrics such as &lt;code>DCGM_FI_DEV_GPU_TEMP&lt;/code> (the GPU temperature) or DCGM_FI_DEV_POWER_USAGE (the power usage). The &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools/blob/d5c9bb55b4d1529ca07068b7f81e690921ce2b59/etc/dcgm-exporter/default-counters.csv">default set&lt;/a> is available in Nvidia's &lt;a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/group__dcgmFieldIdentifiers.html">Data Center GPU Manager documentation&lt;/a>.&lt;/p>
&lt;p>Note that for this release you can still set the &lt;code>DisableAcceleratorMetrics&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> to &lt;em>false&lt;/em>, effectively re-enabling the ability for the kubelet to report NVIDIA GPU metrics.&lt;/p>
&lt;p>Paired with the graduation of the Pod Resources API, these tools can be used to generate GPU telemetry &lt;a href="https://grafana.com/grafana/dashboards/12239">that can be used in visualization dashboards&lt;/a>, below is an example:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/grafana.png" alt="Grafana visualization of device metrics">&lt;/p>
&lt;h2 id="the-pod-resources-api-what-can-i-go-on-to-do-with-this">The Pod Resources API - What can I go on to do with this?&lt;/h2>
&lt;p>As soon as this interface was introduced, many vendors started using it for widely different use cases! To list a few examples:&lt;/p>
&lt;p>The &lt;a href="https://github.com/openstack/kuryr-kubernetes">kuryr-kubernetes&lt;/a> CNI plugin in tandem with &lt;a href="https://github.com/intel/sriov-network-device-plugin">intel-sriov-device-plugin&lt;/a>. This allowed the CNI plugin to know which allocation of SR-IOV Virtual Functions (VFs) the kubelet made and use that information to correctly setup the container network namespace and use a device with the appropriate NUMA node. We also expect this interface to be used to track the allocated and available resources with information about the NUMA topology of the worker node.&lt;/p>
&lt;p>Another use-case is GPU telemetry, where GPU metrics can be associated with the containers and pods that the GPU is assigned to. One such example is the NVIDIA &lt;code>dcgm-exporter&lt;/code>, but others can be easily built in the same paradigm.&lt;/p>
&lt;p>The Pod Resources API is a simple gRPC service which informs clients of the pods the kubelet knows. The information concerns the devices assignment the kubelet made and the assignment of CPUs. This information is obtained from the internal state of the kubelet's Device Manager and CPU Manager respectively.&lt;/p>
&lt;p>You can see below a sample example of the API and how a go client could use that information in a few lines:&lt;/p>
&lt;pre>&lt;code>service PodResourcesLister {
rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
// Kubernetes 1.21
rpc Watch(WatchPodResourcesRequest) returns (stream WatchPodResourcesResponse) {}
}
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
ctx, cancel &lt;span style="color:#666">:=&lt;/span> context.&lt;span style="color:#00a000">WithTimeout&lt;/span>(context.&lt;span style="color:#00a000">Background&lt;/span>(), connectionTimeout)
&lt;span style="color:#a2f;font-weight:bold">defer&lt;/span> &lt;span style="color:#00a000">cancel&lt;/span>()
socket &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#b44">&amp;#34;/var/lib/kubelet/pod-resources/kubelet.sock&amp;#34;&lt;/span>
conn, err &lt;span style="color:#666">:=&lt;/span> grpc.&lt;span style="color:#00a000">DialContext&lt;/span>(ctx, socket, grpc.&lt;span style="color:#00a000">WithInsecure&lt;/span>(), grpc.&lt;span style="color:#00a000">WithBlock&lt;/span>(),
grpc.&lt;span style="color:#00a000">WithDialer&lt;/span>(&lt;span style="color:#a2f;font-weight:bold">func&lt;/span>(addr &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>, timeout time.Duration) (net.Conn, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> net.&lt;span style="color:#00a000">DialTimeout&lt;/span>(&lt;span style="color:#b44">&amp;#34;unix&amp;#34;&lt;/span>, addr, timeout)
}),
)
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
client &lt;span style="color:#666">:=&lt;/span> podresourcesapi.&lt;span style="color:#00a000">NewPodResourcesListerClient&lt;/span>(conn)
resp, err &lt;span style="color:#666">:=&lt;/span> client.&lt;span style="color:#00a000">List&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>podresourcesapi.ListPodResourcesRequest{})
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
net.&lt;span style="color:#00a000">Printf&lt;/span>(&lt;span style="color:#b44">&amp;#34;%+v\n&amp;#34;&lt;/span>, resp)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, note that you can watch the number of requests made to the Pod Resources endpoint by watching the new kubelet metric called &lt;code>pod_resources_endpoint_requests_total&lt;/code> on the kubelet's &lt;code>/metrics&lt;/code> endpoint.&lt;/p>
&lt;h2 id="is-device-monitoring-suitable-for-production-can-i-extend-it-can-i-contribute">Is device monitoring suitable for production? Can I extend it? Can I contribute?&lt;/h2>
&lt;p>Yes! This feature released in 1.13, almost 2 years ago, has seen broad adoption, is already used by different cloud managed services, and with its graduation to G.A in Kubernetes 1.20 is production ready!&lt;/p>
&lt;p>If you are a device vendor, you can start using it today! If you just want to monitor the devices in your cluster, go get the latest version of your monitoring plugin!&lt;/p>
&lt;p>If you feel passionate about that area, join the kubernetes community, help improve the API or contribute the device monitoring plugins!&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management, SIG-Node and the Resource management forum!&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Granular Control of Volume Permission Changes</title><link>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Hemant Kumar, Red Hat &amp;amp; Christian Huffman, Red Hat&lt;/p>
&lt;p>Kubernetes 1.20 brings two important beta features, allowing Kubernetes admins and users alike to have more adequate control over how volume permissions are applied when a volume is mounted inside a Pod.&lt;/p>
&lt;h3 id="allow-users-to-skip-recursive-permission-changes-on-mount">Allow users to skip recursive permission changes on mount&lt;/h3>
&lt;p>Traditionally if your pod is running as a non-root user (&lt;a href="https://twitter.com/thockin/status/1333892204490735617">which you should&lt;/a>), you must specify a &lt;code>fsGroup&lt;/code> inside the pod’s security context so that the volume can be readable and writable by the Pod. This requirement is covered in more detail in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">here&lt;/a>.&lt;/p>
&lt;p>But one side-effect of setting &lt;code>fsGroup&lt;/code> is that, each time a volume is mounted, Kubernetes must recursively &lt;code>chown()&lt;/code> and &lt;code>chmod()&lt;/code> all the files and directories inside the volume - with a few exceptions noted below. This happens even if group ownership of the volume already matches the requested &lt;code>fsGroup&lt;/code>, and can be pretty expensive for larger volumes with lots of small files, which causes pod startup to take a long time. This scenario has been a &lt;a href="https://github.com/kubernetes/kubernetes/issues/69699">known problem&lt;/a> for a while, and in Kubernetes 1.20 we are providing knobs to opt-out of recursive permission changes if the volume already has the correct permissions.&lt;/p>
&lt;p>When configuring a pod’s security context, set &lt;code>fsGroupChangePolicy&lt;/code> to &amp;quot;OnRootMismatch&amp;quot; so if the root of the volume already has the correct permissions, the recursive permission change can be skipped. Kubernetes ensures that permissions of the top-level directory are changed last the first time it applies permissions.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroupChangePolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;OnRootMismatch&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can learn more about this in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods&lt;/a>.&lt;/p>
&lt;h3 id="allow-csi-drivers-to-declare-support-for-fsgroup-based-permissions">Allow CSI Drivers to declare support for fsGroup based permissions&lt;/h3>
&lt;p>Although the previous section implied that Kubernetes &lt;em>always&lt;/em> recursively changes permissions of a volume if a Pod has a &lt;code>fsGroup&lt;/code>, this is not strictly true. For certain multi-writer volume types, such as NFS or Gluster, the cluster doesn’t perform recursive permission changes even if the pod has a &lt;code>fsGroup&lt;/code>. Other volume types may not even support &lt;code>chown()&lt;/code>/&lt;code>chmod()&lt;/code>, which rely on Unix-style permission control primitives.&lt;/p>
&lt;p>So how do we know when to apply recursive permission changes and when we shouldn't? For in-tree storage drivers, this was relatively simple. For &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html#introduction">CSI&lt;/a> drivers that could span a multitude of platforms and storage types, this problem can be a bigger challenge.&lt;/p>
&lt;p>Previously, whenever a CSI volume was mounted to a Pod, Kubernetes would attempt to automatically determine if the permissions and ownership should be modified. These methods were imprecise and could cause issues as we already mentioned, depending on the storage type.&lt;/p>
&lt;p>The CSIDriver custom resource now has a &lt;code>.spec.fsGroupPolicy&lt;/code> field, allowing storage drivers to explicitly opt in or out of these recursive modifications. By having the CSI driver specify a policy for the backing volumes, Kubernetes can avoid needless modification attempts. This optimization helps to reduce volume mount time and also cuts own reporting errors about modifications that would never succeed.&lt;/p>
&lt;h4 id="csidriver-fsgrouppolicy-api">CSIDriver FSGroupPolicy API&lt;/h4>
&lt;p>Three FSGroupPolicy values are available as of Kubernetes 1.20, with more planned for future releases.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ReadWriteOnceWithFSType&lt;/strong> - This is the default policy, applied if no &lt;code>fsGroupPolicy&lt;/code> is defined; this preserves the behavior from previous Kubernetes releases. Each volume is examined at mount time to determine if permissions should be recursively applied.&lt;/li>
&lt;li>&lt;strong>File&lt;/strong> - Always attempt to apply permission modifications, regardless of the filesystem type or PersistentVolumeClaim’s access mode.&lt;/li>
&lt;li>&lt;strong>None&lt;/strong> - Never apply permission modifications.&lt;/li>
&lt;/ul>
&lt;h4 id="how-do-i-use-it">How do I use it?&lt;/h4>
&lt;p>The only configuration needed is defining &lt;code>fsGroupPolicy&lt;/code> inside of the &lt;code>.spec&lt;/code> for a CSIDriver. Once that element is defined, any subsequently mounted volumes will automatically use the defined policy. There’s no additional deployment required!&lt;/p>
&lt;h4 id="what-s-next">What’s next?&lt;/h4>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push these implementations to GA in either 1.21 or 1.22.&lt;/p>
&lt;h3 id="how-can-i-learn-more">How can I learn more?&lt;/h3>
&lt;p>This feature is explained in more detail in Kubernetes project documentation: &lt;a href="https://kubernetes-csi.github.io/docs/support-fsgroup.html">CSI Driver fsGroup Support&lt;/a> and &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods &lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack channel #csi&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</title><link>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p>
&lt;p>The Kubernetes Volume Snapshot feature is now GA in Kubernetes v1.20. It was introduced as &lt;a href="https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/">alpha&lt;/a> in Kubernetes v1.12, followed by a &lt;a href="https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/">second alpha&lt;/a> with breaking changes in Kubernetes v1.13, and promotion to &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">beta&lt;/a> in Kubernetes 1.17. This blog post summarizes the changes releasing the feature from beta to GA.&lt;/p>
&lt;h2 id="what-is-a-volume-snapshot">What is a volume snapshot?&lt;/h2>
&lt;p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to rehydrate a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p>
&lt;h2 id="why-add-volume-snapshots-to-kubernetes">Why add volume snapshots to Kubernetes?&lt;/h2>
&lt;p>Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster-specific” knowledge.&lt;/p>
&lt;p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database’s volumes before starting a database operation.&lt;/p>
&lt;p>By providing a standard way to trigger volume snapshot operations in Kubernetes, this feature allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of the underlying storage.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced enterprise-grade storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since beta?&lt;/h2>
&lt;p>With the promotion of Volume Snapshot to GA, the feature is enabled by default on standard Kubernetes deployments and cannot be turned off.&lt;/p>
&lt;p>Many enhancements have been made to improve the quality of this feature and to make it production-grade.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The Volume Snapshot APIs and client library were moved to a separate Go module.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A snapshot validation webhook has been added to perform necessary validation on volume snapshot objects. More details can be found in the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1900-volume-snapshot-validation-webhook">Volume Snapshot Validation Webhook Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Along with the validation webhook, the volume snapshot controller will start labeling invalid snapshot objects that already existed. This allows users to identify, remove any invalid objects, and correct their workflows. Once the API is switched to the v1 type, those invalid objects will not be deletable from the system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To provide better insights into how the snapshot feature is performing, an initial set of operation metrics has been added to the volume snapshot controller.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are more end-to-end tests, running on GCP, that validate the feature in a real Kubernetes cluster. Stress tests (based on Google Persistent Disk and &lt;code>hostPath&lt;/code> CSI Drivers) have been introduced to test the robustness of the system.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Other than introducing tightening validation, there is no difference between the v1beta1 and v1 Kubernetes volume snapshot API. In this release (with Kubernetes 1.20), both v1 and v1beta1 are served while the stored API version is still v1beta1. Future releases will switch the stored version to v1 and gradually remove v1beta1 support.&lt;/p>
&lt;h2 id="which-csi-drivers-support-volume-snapshots">Which CSI drivers support volume snapshots?&lt;/h2>
&lt;p>Snapshots are only supported for CSI drivers, not for in-tree or FlexVolume drivers. Ensure the deployed CSI driver on your cluster has implemented the snapshot interfaces. For more information, see &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI) for Kubernetes GA&lt;/a>.&lt;/p>
&lt;p>Currently more than &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">50 CSI drivers&lt;/a> support the Volume Snapshot feature. The &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE Persistent Disk CSI Driver&lt;/a> has gone through the tests for upgrading from volume snapshots beta to GA. GA level support for other CSI drivers should be available soon.&lt;/p>
&lt;h2 id="who-builds-products-using-volume-snapshots">Who builds products using volume snapshots?&lt;/h2>
&lt;p>As of the publishing of this blog, the following participants from the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-data-protection">Kubernetes Data Protection Working Group&lt;/a> are building products or have already built products using Kubernetes volume snapshots.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.delltechnologies.com/en-us/data-protection/powerprotect-data-manager.htm">Dell-EMC: PowerProtect&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.druva.com/">Druva&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kasten.io/">Kasten K10&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cloud.netapp.com/project-astra">NetApp: Project Astra&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://portworx.com/products/px-backup/">Portworx (PX-Backup)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/purestorage/pso-csi">Pure Storage (Pure Service Orchestrator)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage">Red Hat OpenShift Container Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://robin.io/storage/">Robin Cloud Native Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.trilio.io/kubernetes/">TrilioVault for Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-csi">Velero plugin for CSI&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-deploy-volume-snapshots">How to deploy volume snapshots?&lt;/h2>
&lt;p>Volume Snapshot feature contains the following components:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd">Kubernetes Volume Snapshot CRDs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller">Volume snapshot controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/validation-webhook">Snapshot validation webhook&lt;/a>&lt;/li>
&lt;li>CSI Driver along with &lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/sidecar-controller">CSI Snapshotter sidecar&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>It is strongly recommended that Kubernetes distributors bundle and deploy the volume snapshot controller, CRDs, and validation webhook as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p>
&lt;blockquote class="warning callout">
&lt;div>&lt;strong>Warning:&lt;/strong> The snapshot validation webhook serves as a critical component to transition smoothly from using v1beta1 to v1 API. Not installing the snapshot validation webhook makes prevention of invalid volume snapshot objects from creation/updating impossible, which in turn will block deletion of invalid volume snapshot objects in coming upgrades.&lt;/div>
&lt;/blockquote>
&lt;p>If your cluster does not come pre-installed with the correct components, you may manually install them. See the &lt;a href="https://github.com/kubernetes-csi/external-snapshotter#readme">CSI Snapshotter&lt;/a> README for details.&lt;/p>
&lt;h2 id="how-to-use-volume-snapshots">How to use volume snapshots?&lt;/h2>
&lt;p>Assuming all the required components (including CSI driver) have been already deployed and running on your cluster, you can create volume snapshots using the &lt;code>VolumeSnapshot&lt;/code> API object, or use an existing &lt;code>VolumeSnapshot&lt;/code> to restore a PVC by specifying the VolumeSnapshot data source on it. For more details, see the &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">volume snapshot documentation&lt;/a>.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> The Kubernetes Snapshot API does not provide any application consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using higher level APIs/controllers.&lt;/div>
&lt;/blockquote>
&lt;h3 id="dynamically-provision-a-volume-snapshot">Dynamically provision a volume snapshot&lt;/h3>
&lt;p>To dynamically provision a volume snapshot, create a &lt;code>VolumeSnapshotClass&lt;/code> API object first.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecret&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecretnamespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> API object from a PVC by specifying the volume snapshot class.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="importing-an-existing-volume-snapshot-with-kubernetes">Importing an existing volume snapshot with Kubernetes&lt;/h3>
&lt;p>To import a pre-existing volume snapshot into Kubernetes, manually create a &lt;code>VolumeSnapshotContent&lt;/code> object first.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-xxx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> object pointing to the &lt;code>VolumeSnapshotContent&lt;/code> object.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="rehydrate-volume-from-snapshot">Rehydrate volume from snapshot&lt;/h3>
&lt;p>A bound and ready &lt;code>VolumeSnapshot&lt;/code> object can be used to rehydrate a new volume with data pre-populated from snapshotted data as shown here:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-restore&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>demo-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-storageclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-to-add-support-for-snapshots-in-a-csi-driver">How to add support for snapshots in a CSI driver?&lt;/h2>
&lt;p>See the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html">Kubernetes-CSI Driver Developer Guide&lt;/a> for more details on how to implement the snapshot feature in a CSI driver.&lt;/p>
&lt;h2 id="what-are-the-limitations">What are the limitations?&lt;/h2>
&lt;p>The GA implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p>
&lt;ul>
&lt;li>Does not support reverting an existing PVC to an earlier state represented by a snapshot (only supports provisioning a new volume from a snapshot).&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-learn-more">How to learn more?&lt;/h3>
&lt;p>The code repository for snapshot APIs and controller is here: &lt;a href="https://github.com/kubernetes-csi/external-snapshotter">https://github.com/kubernetes-csi/external-snapshotter&lt;/a>&lt;/p>
&lt;p>Check out additional documentation on the snapshot feature here: &lt;a href="http://k8s.io/docs/concepts/storage/volume-snapshots">http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a> and &lt;a href="https://kubernetes-csi.github.io/docs/">https://kubernetes-csi.github.io/docs/&lt;/a>&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach GA. We want to thank Saad Ali, Michelle Au, Tim Hockin, and Jordan Liggitt for their insightful reviews and thorough consideration with the design, thank Andi Li for his work on adding the support of the snapshot validation webhook, thank Grant Griffiths on implementing metrics support in the snapshot controller and handling password rotation in the validation webhook, thank Chris Henzie, Raunak Shah, and Manohar Reddy for writing critical e2e tests to meet the scalability and stability requirements for graduation, thank Kartik Sharma for moving snapshot APIs and client lib to a separate go module, and thank Raunak Shah and Prafull Ladha for their help with upgrade testing from beta to GA.&lt;/p>
&lt;p>There are many more people who have helped to move the snapshot feature from beta to GA. We want to thank everyone who has contributed to this effort:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/AndiLi99">Andi Li&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/bswartz">Ben Swartzlander&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/chrishenzie">Chris Henzie&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/huffmanca">Christian Huffman&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ggriffiths">Grant Griffiths&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/humblec">Humble Devassy Chirammal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jsafrane">Jan Šafránek&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Jiawei0227">Jiawei Wang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jingxu97">Jing Xu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/liggitt">Jordan Liggitt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Kartik494">Kartik Sharma&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Madhu-1">Madhu Rajanna&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/boddumanohar">Manohar Reddy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/msau42">Michelle Au&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/pohly">Patrick Ohly&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prafull01">Prafull Ladha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prateekpandey14">Prateek Pandey&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/RaunakShah">Raunak Shah&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saad-ali">Saad Ali&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saikat-royc">Saikat Roychowdhury&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/thockin">Tim Hockin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/yuxiangqian">Xiangqian Yu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/xing-yang">Xing Yang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/zhucan">Zhu Can&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>We also hold regular &lt;a href="https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#">Data Protection Working Group meetings&lt;/a>. New attendees are welcome to join in discussions.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: The Raddest Release</title><link>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha.&lt;/p>
&lt;p>The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume Snapshot Operations Goes Stable&lt;/h3>
&lt;p>This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;p>Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster.&lt;/p>
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug Graduates to Beta&lt;/h3>
&lt;p>The &lt;code>kubectl alpha debug&lt;/code> features graduates to beta in 1.20, becoming &lt;code>kubectl debug&lt;/code>. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include:&lt;/p>
&lt;ul>
&lt;li>Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.&lt;/li>
&lt;li>Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)&lt;/li>
&lt;li>Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.&lt;/li>
&lt;/ul>
&lt;p>Note that as a new built-in command, &lt;code>kubectl debug&lt;/code> takes priority over any kubectl plugin named “debug”. You must rename the affected plugin.&lt;/p>
&lt;p>Invocations using &lt;code>kubectl alpha debug&lt;/code> are now deprecated and will be removed in a subsequent release. Update your scripts to use &lt;code>kubectl debug&lt;/code>. For more information about &lt;code>kubectl debug&lt;/code>, see &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">Debugging Running Pods&lt;/a>.&lt;/p>
&lt;h3 id="beta-api-priority-and-fairness">Beta: API Priority and Fairness&lt;/h3>
&lt;p>Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows &lt;code>kube-apiserver&lt;/code> to categorize incoming requests by priority levels.&lt;/p>
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha with updates: IPV4/IPV6&lt;/h3>
&lt;p>The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa.&lt;/p>
&lt;h3 id="ga-process-pid-limiting-for-stability">GA: Process PID Limiting for Stability&lt;/h3>
&lt;p>Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine.&lt;/p>
&lt;p>Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node.
After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both &lt;code>SupportNodePidsLimit&lt;/code> (node-to-pod PID isolation) and &lt;code>SupportPodPidsLimit&lt;/code> (ability to limit PIDs per pod).&lt;/p>
&lt;h3 id="alpha-graceful-node-shutdown">Alpha: Graceful node shutdown&lt;/h3>
&lt;p>Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The &lt;code>GracefulNodeShutdown&lt;/code> feature is now in Alpha. &lt;code>GracefulNodeShutdown&lt;/code> makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="dockershim-deprecation">Dockershim Deprecation&lt;/h3>
&lt;p>Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a &lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">detailed blog post about deprecation&lt;/a> with &lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">a dedicated FAQ page for it&lt;/a>.&lt;/p>
&lt;h3 id="exec-probe-timeout-handling">Exec Probe Timeout Handling&lt;/h3>
&lt;p>A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field &lt;code>timeoutSeconds&lt;/code> was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of &lt;code>1 second&lt;/code> will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called &lt;code>ExecProbeTimeout&lt;/code>, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to &lt;code>false&lt;/code>.&lt;/p>
&lt;p>Please review the updated documentation regarding &lt;a href="docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">configuring probes&lt;/a> for more details.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">Built-in API Types Defaults&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">Add Pod-Startup Liveness-Probe Holdoff&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">Support CRI-ContainerD On Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP Support for Services&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">Adding AppProtocol To Services And Endpoints&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.20 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.20 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">download on GitHub&lt;/a>. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;blockquote>
&lt;p>The Kubernetes 1.20 Release has been the raddest release yet.&lt;/p>
&lt;/blockquote>
&lt;p>2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> with a &amp;quot;rad&amp;quot; cat named Humphrey.&lt;/p>
&lt;p>Humphrey is the release lead's cat and has a permanent &lt;a href="https://www.inverse.com/article/42316-why-do-cats-blep-science-explains">&lt;code>blep&lt;/code>&lt;/a>. &lt;em>Rad&lt;/em> was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his &lt;em>blep&lt;/em> bring you a little joy at the end of 2020!&lt;/p>
&lt;p>The release logo was created by &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a>.&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokharchyk's KubeCon NA Keynote&lt;/a> to learn more about their cloud native journey.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 individuals&lt;/a> (&lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 of whom&lt;/a> made their first Kubernetes contribution) from &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 countries&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">now available to all on-demand&lt;/a> for anyone still needing to catch up!&lt;/li>
&lt;li>In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given &lt;a href="https://sched.co/eukp">at KubeCon 2020 North America&lt;/a>, and the initial impact of this labor &lt;a href="https://github.com/kubernetes/enhancements/issues/2067">can actually be seen in the v1.20 release&lt;/a>.&lt;/li>
&lt;li>Previously announced this summer, &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">The Certified Kubernetes Security Specialist (CKS) Certification&lt;/a> was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?).&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;p>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">here&lt;/a>. Remember that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">the CFP&lt;/a> closes on Sunday, December 13, 11:59pm PST!&lt;/p>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Stay tuned for the upcoming release webinar happening this January.&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: GSoD 2020: Improving the API Reference Experience</title><link>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://github.com/feloy">Philippe Martin&lt;/a>&lt;/p>
&lt;p>&lt;em>Editor's note: Better API references have been my goal since I joined Kubernetes docs three and a half years ago. Philippe has succeeded fantastically. More than a better API reference, though, Philippe embodied the best of the Kubernetes community in this project: excellence through collaboration, and a process that made the community itself better. Thanks, Google Season of Docs, for making Philippe's work possible. —Zach Corleissen&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> project brings open source organizations and technical writers together to work closely on a specific documentation project.&lt;/p>
&lt;p>I was selected by the CNCF to work on Kubernetes documentation, specifically to make the API Reference documentation more accessible.&lt;/p>
&lt;p>I'm a software developer with a great interest in documentation systems. In the late 90's I started translating Linux-HOWTO documents into French. From one thing to another, I learned about documentation systems. Eventually, I wrote a Linux-HOWTO to help documentarians learn the language used at that time for writing documents, LinuxDoc/SGML.&lt;/p>
&lt;p>Shortly afterward, Linux documentation adopted the DocBook language. I helped some writers rewrite their documents in this format; for example, the Advanced Bash-Scripting Guide. I also worked on the GNU &lt;code>makeinfo&lt;/code> program to add DocBook output, making it possible to transform &lt;em>GNU Info&lt;/em> documentation into Docbook format.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.io/docs/home/">Kubernetes website&lt;/a> is built with Hugo from documentation written in Markdown format in the &lt;a href="https://github.com/kubernetes/website">website repository&lt;/a>, using the &lt;a href="https://www.docsy.dev/about/">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The existing API reference documentation is a large HTML file generated from the Kubernetes OpenAPI specification.&lt;/p>
&lt;p>On my side, I wanted for some time to make the API Reference more accessible, by:&lt;/p>
&lt;ul>
&lt;li>building individual and autonomous pages for each Kubernetes resource&lt;/li>
&lt;li>adapting the format to mobile reading&lt;/li>
&lt;li>reusing the website's assets and theme to build, integrate, and display the reference pages&lt;/li>
&lt;li>allowing the search engines to reference the content of the pages&lt;/li>
&lt;/ul>
&lt;p>Around one year ago, I started to work on the generator building the current unique HTML page, to add a DocBook output, so the API Reference could be generated first in DocBook format, and after that in PDF or other formats supported by DocBook processors. The first result has been some &lt;a href="https://github.com/feloy/kubernetes-resources-reference/releases">Ebook files for the API Reference&lt;/a> and an auto-edited paper book.&lt;/p>
&lt;p>I decided later to add another output to this generator, to generate Markdown files and create &lt;a href="https://web.archive.org/web/20201022201911/https://www.k8sref.io/docs/workloads/">a website with the API Reference&lt;/a>.&lt;/p>
&lt;p>When the CNCF proposed a project for the Google Season of Docs to work on the API Reference, I applied, and the match occurred.&lt;/p>
&lt;h2 id="the-project">The Project&lt;/h2>
&lt;h3 id="swagger-ui">swagger-ui&lt;/h3>
&lt;p>The first idea of the CNCF members that proposed this project was to test the &lt;a href="https://swagger.io/tools/swagger-ui/">&lt;code>swagger-ui&lt;/code> tool&lt;/a>, to try and document the Kubernetes API Reference with this standard tool.&lt;/p>
&lt;p>Because the Kubernetes API is much larger than many other APIs, it has been necessary to write a tool to split the complete API Reference by API Groups, and insert in the Documentation website several &lt;code>swagger-ui&lt;/code> components, one for each API Group.&lt;/p>
&lt;p>Generally, APIs are used by developers by calling endpoints with a specific HTTP verb, with specific parameters and waiting for a response. The &lt;code>swagger-ui&lt;/code> interface is built for this usage: the interface displays a list of endpoints and their associated verbs, and for each the parameters and responses formats.&lt;/p>
&lt;p>The Kubernetes API is most of the time used differently: users create manifest files containing resources definitions in YAML format, and use the &lt;code>kubectl&lt;/code> CLI to &lt;em>apply&lt;/em> these manifests to the cluster. In this case, the most important information is the description of the structures used as parameters and responses (the Kubernetes Resources).&lt;/p>
&lt;p>Because of this specificity, we realized that it would be difficult to adapt the &lt;code>swagger-ui&lt;/code> interface to satisfy the users of the Kubernetes API and this direction has been abandoned.&lt;/p>
&lt;h3 id="markdown-pages">Markdown pages&lt;/h3>
&lt;p>The second stage of the project has been to adapt the work I had done to create the k8sref.io website, to include it in the official documentation website.&lt;/p>
&lt;p>The main changes have been to:&lt;/p>
&lt;ul>
&lt;li>use go-templates to represent the output pages, so non-developers can adapt the generated pages without having to edit the generator code&lt;/li>
&lt;li>create a new custom &lt;a href="https://gohugo.io/content-management/shortcodes/">shortcode&lt;/a>, to easily create links from inside the website to specific pages of the API reference&lt;/li>
&lt;li>improve the navigation between the sections of the API reference&lt;/li>
&lt;li>add the code of the generator to the Kubernetes GitHub repository containing the different reference generators&lt;/li>
&lt;/ul>
&lt;p>All the discussions and work done can be found in website &lt;a href="https://github.com/kubernetes/website/pull/23294">pull request #23294&lt;/a>.&lt;/p>
&lt;p>Adding the generator code to the Kubernetes project happened in &lt;a href="https://github.com/kubernetes-sigs/reference-docs/pull/179">kubernetes-sigs/reference-docs#179&lt;/a>.&lt;/p>
&lt;p>Here are the features of the new API Reference to be included in the official documentation website:&lt;/p>
&lt;ul>
&lt;li>the resources are categorized, in the categories Workloads, Services, Config &amp;amp; Storage, Authentication, Authorization, Policies, Extend, Cluster. This structure is configurable with a simple &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/toc.yaml">&lt;code>toc.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>each page displays associated resources at the first level ; for example: Pod, PodSpec, PodStatus, PodList&lt;/li>
&lt;li>most resource pages inline relevant definitions ; the exceptions are when those definitions are common to several resources, or are too complex to be displayed inline. With the old approach, you had to follow a hyperlink to read each extra detail.&lt;/li>
&lt;li>some widely used definitions, such as &lt;code>ObjectMeta&lt;/code>, are documented in a specific page&lt;/li>
&lt;li>required fields are indicated, and placed first&lt;/li>
&lt;li>fields of a resource can be categorized and ordered, with the help of a &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/fields.yaml">&lt;code>fields.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>&lt;code>map&lt;/code> fields are indicated. For example the &lt;code>.spec.nodeSelector&lt;/code> for a &lt;code>Pod&lt;/code> is &lt;code>map[string]string&lt;/code>, instead of &lt;code>object&lt;/code>, using the value of &lt;code>x-kubernetes-list-type&lt;/code>&lt;/li>
&lt;li>patch strategies are indicated&lt;/li>
&lt;li>&lt;code>apiVersion&lt;/code> and &lt;code>kind&lt;/code> display the value, not the &lt;code>string&lt;/code> type&lt;/li>
&lt;li>At the top of a reference page, the page displays the Go import necessary to use these resources from a Go program.&lt;/li>
&lt;/ul>
&lt;p>The work is currently on hold pending the 1.20 release. When the release finishes and the work is integrated, the API reference will be available at &lt;a href="https://kubernetes.io/docs/reference/">https://kubernetes.io/docs/reference/&lt;/a>.&lt;/p>
&lt;h3 id="future-work">Future Work&lt;/h3>
&lt;p>There are points to improve, particularly:&lt;/p>
&lt;ul>
&lt;li>Some Kubernetes resources are deeply nested. Inlining the definition of these resources makes them difficult to understand.&lt;/li>
&lt;li>The created &lt;code>shortcode&lt;/code> uses the URL of the page to reference a Resource page. It would be easier for documentarians if they could reference a Resource by its group and name.&lt;/li>
&lt;/ul>
&lt;h2 id="appreciation">Appreciation&lt;/h2>
&lt;p>I would like to thank my mentor &lt;a href="https://github.com/zacharysarah">Zach Corleissen&lt;/a> and the lead writers &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a>, &lt;a href="https://github.com/celestehorgan">Celeste Horgan&lt;/a>, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/tengqm">Qiming Teng&lt;/a> who supervised me during all the season. They all have been very encouraging and gave me tons of great advice.&lt;/p></description></item><item><title>Blog: Dockershim Deprecation FAQ</title><link>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;p>This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>.&lt;/p>
&lt;h3 id="why-is-dockershim-being-deprecated">Why is dockershim being deprecated?&lt;/h3>
&lt;p>Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.&lt;/p>
&lt;p>Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1985-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.&lt;/p>
&lt;h3 id="can-i-still-use-docker-in-kubernetes-1-20">Can I still use Docker in Kubernetes 1.20?&lt;/h3>
&lt;p>Yes, the only thing changing in 1.20 is a single warning log printed at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
startup if using Docker as the runtime.&lt;/p>
&lt;h3 id="when-will-dockershim-be-removed">When will dockershim be removed?&lt;/h3>
&lt;p>Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021. We will be working closely with vendors
and other ecosystem groups to ensure a smooth transition and will evaluate things
as the situation evolves.&lt;/p>
&lt;h3 id="will-my-existing-docker-images-still-work">Will my existing Docker images still work?&lt;/h3>
&lt;p>Yes, the images produced from &lt;code>docker build&lt;/code> will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p>
&lt;h3 id="what-about-private-images">What about private images?&lt;/h3>
&lt;p>Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p>
&lt;h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?&lt;/h3>
&lt;p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p>
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?&lt;/h3>
&lt;p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p>
&lt;p>Additionally, the &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href="https://cri-o.io/">CRI-O&lt;/a> runtime in production since June 2019.&lt;/p>
&lt;p>For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (&lt;a href="https://cncf.io">CNCF&lt;/a>).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?&lt;/h3>
&lt;p>OCI stands for the &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>, which is the underlying default runtime for both
&lt;a href="https://containerd.io/">containerd&lt;/a> and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p>
&lt;h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?&lt;/h3>
&lt;p>That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a> in case another would be an
even better fit for your environment.&lt;/p>
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?&lt;/h3>
&lt;p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p>
&lt;ul>
&lt;li>Logging configuration&lt;/li>
&lt;li>Runtime resource limitations&lt;/li>
&lt;li>Node provisioning scripts that call docker or use docker via it's control socket&lt;/li>
&lt;li>Kubectl plugins that require docker CLI or the control socket&lt;/li>
&lt;li>Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)&lt;/li>
&lt;li>Configuration of functionality like &lt;code>registry-mirrors&lt;/code> and insecure registries&lt;/li>
&lt;li>Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)&lt;/li>
&lt;li>GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li>
&lt;/ul>
&lt;p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p>
&lt;p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> tool as a drop-in replacement (see &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl">mapping from docker cli to crictl&lt;/a>) and for the
latter you can use newer container build options like &lt;a href="https://github.com/genuinetools/img">img&lt;/a>, &lt;a href="https://github.com/containers/buildah">buildah&lt;/a>,
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>, or &lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a> that don’t require Docker.&lt;/p>
&lt;p>For containerd, you can start with their &lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation&lt;/a> to see what configuration
options are available as you migrate things over.&lt;/p>
&lt;p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes">Container Runtimes&lt;/a>&lt;/p>
&lt;h3 id="what-if-i-have-more-questions">What if I have more questions?&lt;/h3>
&lt;p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/&lt;/a>.&lt;/p>
&lt;p>You can also check out the excellent blog post
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?&lt;/a> a more in-depth technical
discussion of the changes.&lt;/p>
&lt;h3 id="can-i-have-a-hug">Can I have a hug?&lt;/h3>
&lt;p>Always and whenever you want! 🤗🤗&lt;/p></description></item><item><title>Blog: Don't Panic: Kubernetes and Docker</title><link>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p>
&lt;p>Kubernetes is &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">deprecating
Docker&lt;/a>
as a container runtime after v1.20.&lt;/p>
&lt;p>&lt;strong>You do not need to panic. It’s not as dramatic as it sounds.&lt;/strong>&lt;/p>
&lt;p>TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)&lt;/a>
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.&lt;/p>
&lt;p>If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running &lt;code>docker build&lt;/code> can still run in your Kubernetes cluster.&lt;/p>
&lt;p>If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which &lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">defaults to containerd&lt;/a>) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.&lt;/p>
&lt;p>If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).&lt;/p>
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">So why the confusion and what is everyone freaking out about?&lt;/h2>
&lt;p>We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.&lt;/p>
&lt;p>You see, the thing we call “Docker” isn’t actually one thing—it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.&lt;/p>
&lt;p>As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?&lt;/p>
&lt;p>Docker isn’t compliant with CRI, the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a>.
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic—you just need to change
your container runtime from Docker to another supported container runtime.&lt;/p>
&lt;p>One thing to note: If you are relying on the underlying docker socket
(&lt;code>/var/run/docker.sock&lt;/code>) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>,
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>, and
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>.&lt;/p>
&lt;h2 id="what-does-this-change-mean-for-developers-though-do-we-still-write-dockerfiles-do-we-still-build-things-with-docker">What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?&lt;/h2>
&lt;p>This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image—it’s an OCI (&lt;a href="https://opencontainers.org/">Open Container Initiative&lt;/a>) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both &lt;a href="https://containerd.io/">containerd&lt;/a> and
&lt;a href="https://cri-o.io/">CRI-O&lt;/a> know how to pull those images and run them. This is
why we have a standard for what containers should look like.&lt;/p>
&lt;p>So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay—there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️&lt;/p>
&lt;p>Looking for more answers? Check out our accompanying &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a>.&lt;/p></description></item><item><title>Blog: Cloud native security for your clusters</title><link>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/pudijoglekar">Pushkar Joglekar&lt;/a>&lt;/p>
&lt;p>Over the last few years a small, security focused community has been working diligently to deepen our understanding of security, given the evolving cloud native infrastructure and corresponding iterative deployment practices. To enable sharing of this knowledge with the rest of the community, members of &lt;a href="https://github.com/cncf/sig-security">CNCF SIG Security&lt;/a> (a group which reports into &lt;a href="https://github.com/cncf/toc#sigs">CNCF TOC&lt;/a> and who are friends with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security">Kubernetes SIG Security&lt;/a>) led by Emily Fox, collaborated on a whitepaper outlining holistic cloud native security concerns and best practices. After over 1200 comments, changes, and discussions from 35 members across the world, we are proud to share &lt;a href="https://www.cncf.io/blog/2020/11/18/announcing-the-cloud-native-security-white-paper">cloud native security whitepaper v1.0&lt;/a> that serves as essential reading for security leadership in enterprises, financial and healthcare industries, academia, government, and non-profit organizations.&lt;/p>
&lt;p>The paper attempts to &lt;em>not&lt;/em> focus on any specific &lt;a href="https://www.cncf.io/projects/">cloud native project&lt;/a>. Instead, the intent is to model and inject security into four logical phases of cloud native application lifecycle: &lt;em>Develop, Distribute, Deploy, and Runtime&lt;/em>.&lt;/p>
&lt;p>&lt;img alt="Cloud native application lifecycle phases"
src="cloud-native-app-lifecycle-phases.svg"
style="width:60em;max-width:100%;">&lt;/p>
&lt;h2 id="kubernetes-native-security-controls">Kubernetes native security controls&lt;/h2>
&lt;p>When using Kubernetes as a workload orchestrator, some of the security controls this version of the whitepaper recommends are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies&lt;/a>: Implement a single source of truth for “least privilege” workloads across the entire cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Resource requests and limits&lt;/a>: Apply requests (soft constraint) and limits (hard constraint) for shared resources such as memory and CPU&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Audit log analysis&lt;/a>: Enable Kubernetes API auditing and filtering for security relevant events&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">Control plane authentication and certificate root of trust&lt;/a>: Enable mutual TLS authentication with a trusted CA for communication within the cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets management&lt;/a>: Integrate with a built-in or external secrets store&lt;/li>
&lt;/ul>
&lt;h2 id="cloud-native-complementary-security-controls">Cloud native complementary security controls&lt;/h2>
&lt;p>Kubernetes has direct involvement in the &lt;em>deploy&lt;/em> phase and to a lesser extent in the &lt;em>runtime&lt;/em> phase. Ensuring the artifacts are securely &lt;em>developed&lt;/em> and &lt;em>distributed&lt;/em> is necessary for, enabling workloads in Kubernetes to run “secure by default”. Throughout all phases of the Cloud native application life cycle, several complementary security controls exist for Kubernetes orchestrated workloads, which includes but are not limited to:&lt;/p>
&lt;ul>
&lt;li>Develop:
&lt;ul>
&lt;li>Image signing and verification&lt;/li>
&lt;li>Image vulnerability scanners&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Distribute:
&lt;ul>
&lt;li>Pre-deployment checks for detecting excessive privileges&lt;/li>
&lt;li>Enabling observability and logging&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deploy:
&lt;ul>
&lt;li>Using a service mesh for workload authentication and authorization&lt;/li>
&lt;li>Enforcing “default deny” network policies for inter-workload communication via &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugins&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Runtime:
&lt;ul>
&lt;li>Deploying security monitoring agents for workloads&lt;/li>
&lt;li>Isolating applications that run on the same node using SELinux, AppArmor, etc.&lt;/li>
&lt;li>Scanning configuration against recognized secure baselines for node, workload and orchestrator&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="understand-first-secure-next">Understand first, secure next&lt;/h2>
&lt;p>The cloud native way, including containers, provides great security benefits for its users: immutability, modularity, faster upgrades and consistent state across the environment. Realizing this fundamental change in “the way things are done”, motivates us to look at security with a cloud native lens. One of the things that was evident for all the authors of the paper was the fact that it’s tough to make smarter decisions on how and what to secure in a cloud native ecosystem if you do not understand the tools, patterns, and frameworks at hand (in addition to knowing your own critical assets). Hence, for all the security practitioners out there who want to be partners rather than a gatekeeper for your friends in Operations, Product Development, and Compliance, let’s make an attempt to &lt;em>learn more so we can secure better&lt;/em>.&lt;/p>
&lt;p>We recommend following this &lt;strong>7 step R.U.N.T.I.M.E. path&lt;/strong> to get started on cloud native security:&lt;/p>
&lt;ol>
&lt;li>&lt;b>R&lt;/b>ead the paper and any linked material in it&lt;/li>
&lt;li>&lt;b>U&lt;/b>nderstand challenges and constraints for your environment&lt;/li>
&lt;li>&lt;b>N&lt;/b>ote the content and controls that apply to your environment&lt;/li>
&lt;li>&lt;b>T&lt;/b>alk about your observations with your peers&lt;/li>
&lt;li>&lt;b>I&lt;/b>nvolve your leadership and ask for help&lt;/li>
&lt;li>&lt;b>M&lt;/b>ake a risk profile based on existing and missing security controls&lt;/li>
&lt;li>&lt;b>E&lt;/b>xpend time, money, and resources that improve security posture and reduce risk where appropriate.&lt;/li>
&lt;/ol>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Huge shout out to &lt;em>Emily Fox, Tim Bannister (The Scale Factory), Chase Pettet (Mirantis), and Wayne Haber (GitLab)&lt;/em> for contributing with their wonderful suggestions for this blog post.&lt;/p></description></item><item><title>Blog: Remembering Dan Kohn</title><link>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: The Kubernetes Steering Committee&lt;/p>
&lt;p>Dan Kohn was instrumental in getting Kubernetes and CNCF community to where it is today. He shared our values, motivations, enthusiasm, community spirit, and helped the Kubernetes community to become the best that it could be. Dan loved getting people together to solve problems big and small. He enabled people to grow their individual scope in the community which often helped launch their career in open source software.&lt;/p>
&lt;p>Dan built a coalition around the nascent Kubernetes project and turned that into a cornerstone to build the larger cloud native space. He loved challenges, especially ones where the payoff was great like building worldwide communities, spreading the love of open source, and helping diverse, underprivileged communities and students to get a head start in technology.&lt;/p>
&lt;p>Our heart goes out to his family. Thank you, Dan, for bringing your boys to events in India and elsewhere as we got to know how great you were as a father. Dan, your thoughts and ideas will help us make progress in our journey as a community. Thank you for your life's work!&lt;/p>
&lt;p>If Dan has made an impact on you in some way, please consider adding a memory of him in his &lt;a href="https://github.com/cncf/memorials/blob/master/dan-kohn.md">CNCF memorial&lt;/a>.&lt;/p></description></item><item><title>Blog: Announcing the 2020 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Kaslin Fields&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2020">2020 Steering Committee Election&lt;/a> is now complete. In 2019, the committee arrived at its final allocation of 7 seats, 3 of which were up for election in 2020. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p>
&lt;p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">charter&lt;/a>.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt (&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen (&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join continuing members Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat; Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat; Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), VMware; and Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>), Apple. Davanum Srinivas is returning for his second term on the committee.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks!&lt;/h2>
&lt;ul>
&lt;li>Thank you and congratulations on a successful election to this round’s election officers:
&lt;ul>
&lt;li>Jaice Singer DuMars (&lt;a href="https://github.com/jdumars">@jdumars&lt;/a>), Apple&lt;/li>
&lt;li>Ihor Dvoretskyi (&lt;a href="https://github.com/idvoretskyi">@idvoretskyi&lt;/a>), CNCF&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
&lt;ul>
&lt;li>Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>), Google&lt;/li>
&lt;li>and Lachlan Evenson(&lt;a href="https://github.com/lachie8e">@lachie8e)&lt;/a>), Microsoft&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>And thank you to all the candidates who came forward to run for election. As &lt;a href="https://twitter.com/castrojo/status/1315718627639820288?s=20">Jorge Castro put it&lt;/a>: we are spoiled with capable, kind, and selfless volunteers who put the needs of the project first.&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They have an open meeting on &lt;a href="https://github.com/kubernetes/steering">the first Monday of the month at 6pm UTC&lt;/a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>This post was written by the &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">Upstream Marketing Working Group&lt;/a>. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em>&lt;/p></description></item><item><title>Blog: Contributing to the Development Guide</title><link>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</guid><description>
&lt;p>When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of &amp;quot;client.&amp;quot; I just didn't know what to expect when Google asked my compatriots and me at
&lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> to make much-needed updates to the Kubernetes Development Guide.&lt;/p>
&lt;p>&lt;em>This article originally appeared on the &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="the-delights-of-working-with-a-community">The Delights of Working With a Community&lt;/h2>
&lt;p>As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.&lt;/p>
&lt;p>One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a>
handles most of the client contact.&lt;/p>
&lt;p>I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.&lt;/p>
&lt;p>&amp;quot;How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?&amp;quot; These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the &lt;code>#sig-contribex&lt;/code> channel on the Kubernetes
Slack and announced that I would be working on the
&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">Development Guide&lt;/a>.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes Code of Conduct&lt;/a> was in
effect, and that we should, like Bill and Ted, be excellent to each other.&lt;/p>
&lt;h2 id="this-doesn-t-mean-it-s-all-easy">This Doesn't Mean It's All Easy&lt;/h2>
&lt;p>The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
&lt;a href="https://github.com/kubernetes/community">Community repo&lt;/a>: 267 additions and 88 deletions.&lt;/p>
&lt;p>The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, &lt;a href="https://github.com/kubernetes/community/pull/5003">it was successful&lt;/a>.&lt;/p>
&lt;p>Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the &lt;a href="https://github.com/amwat">labyrinthine mind of
a brilliant engineer&lt;/a>, and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.&lt;/p>
&lt;p>This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
&lt;a href="https://github.com/kubernetes-sigs/kubetest2">&lt;code>kubetest2&lt;/code> framework&lt;/a> to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
&lt;a href="https://github.com/kubernetes/community/pull/5045">completed pull request&lt;/a>.&lt;/p>
&lt;h2 id="nobody-is-the-boss-and-everybody-gives-feedback">Nobody Is the Boss, and Everybody Gives Feedback&lt;/h2>
&lt;p>But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was &lt;em>enjoyable&lt;/em>.&lt;/p>
&lt;p>With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.&lt;/p>
&lt;p>In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.&lt;/p>
&lt;p>Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.&lt;/p></description></item><item><title>Blog: GSoC 2020 - Building operators for cluster addons</title><link>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Somtochi Onyekwere&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>&lt;a href="https://summerofcode.withgoogle.com/">Google Summer of Code&lt;/a> is a global program that is geared towards introducing students to open source. Students are matched with open-source organizations to work with them for three months during the summer.&lt;/p>
&lt;p>My name is Somtochi Onyekwere from the Federal University of Technology, Owerri (Nigeria) and this year, I was given the opportunity to work with Kubernetes (under the CNCF organization) and this led to an amazing summer spent learning, contributing and interacting with the community.&lt;/p>
&lt;p>Specifically, I worked on the &lt;em>Cluster Addons: Package all the things!&lt;/em> project. The project focused on building operators for better management of various cluster addons, extending the tooling for building these operators and making the creation of these operators a smooth process.&lt;/p>
&lt;h1 id="background">Background&lt;/h1>
&lt;p>Kubernetes has progressed greatly in the past few years with a flourishing community and a large number of contributors. The codebase is gradually moving away from the monolith structure where all the code resides in the &lt;a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes&lt;/a> repository to being split into multiple sub-projects. Part of the focus of cluster-addons is to make some of these sub-projects work together in an easy to assemble, self-monitoring, self-healing and Kubernetes-native way. It enables them to work seamlessly without human intervention.&lt;/p>
&lt;p>The community is exploring the use of operators as a mechanism to monitor various resources in the cluster and properly manage these resources. In addition to this, it provides self-healing and it is a kubernetes-native pattern that can encode how best these addons work and manage them properly.&lt;/p>
&lt;p>What are cluster addons? Cluster addons are a collection of resources (like Services and deployment) that are used to give a Kubernetes cluster additional functionalities. They range from things as simple as the Kubernetes dashboards (for visualization) to more complex ones like Calico (for networking). These addons are essential to different applications running in the cluster and the cluster itself. The addon operator provides a nicer way of managing these addons and understanding the health and status of the various resources that comprise the addon. You can get a deeper overview in this &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#addons">article&lt;/a>.&lt;/p>
&lt;p>Operators are custom controllers with custom resource definitions that encode application-specific knowledge and are used for managing complex stateful applications. It is a widely accepted pattern. Managing addons via operators, with these operators encoding knowledge of how best the addons work, introduces a lot of advantages while setting standards that will be easy to follow and scale. This &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator">article&lt;/a> does a good job of explaining operators.&lt;/p>
&lt;p>The addon operators can solve a lot of problems, but they have their challenges. Those under the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons project&lt;/a> had missing pieces and were still a proof of concept. Generating the RBAC configuration for the operators was a pain and sometimes the operators were given too much privilege. The operators weren’t very extensible as it only pulled manifests from local filesystems or HTTP(s) servers and a lot of simple addons were generating the same code.
I spent the summer working on these issues, looking at them with fresh eyes and coming up with solutions for both the known and unknown issues.&lt;/p>
&lt;h1 id="various-additions-to-kubebuilder-declarative-pattern">Various additions to kubebuilder-declarative-pattern&lt;/h1>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern">kubebuilder-declarative-pattern&lt;/a> (from here on referred to as KDP) repo is an extra layer of addon specific tooling on top of the &lt;a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder&lt;/a> SDK that is enabled by passing the experimental &lt;code>--pattern=addon&lt;/code> flag to &lt;code>kubebuilder create&lt;/code> command. Together, they create the base code for the addon operator. During the internship, I worked on a couple of features in KDP and cluster-addons.&lt;/p>
&lt;h2 id="operator-version-checking">Operator version checking&lt;/h2>
&lt;p>Enabling version checks for operators helped in making upgrades/downgrades safer to different versions of the addon, even though the operator had complex logic. It is a way of matching the version of an addon to the version of the operator that knows how to manage it well. Most addons have different versions and these versions might need to be managed differently. This feature checks the custom resource for the &lt;code>addons.k8s.io/min-operator-version&lt;/code> annotation which states the minimum operator version that is needed to manage the version against the version of the operator. If the operator version is below the minimum version required, the operator pauses with an error telling the user that the version of the operator is too low. This helps to ensure that the correct operator is being used for the addon.&lt;/p>
&lt;h2 id="git-repository-for-storing-the-manifests">Git repository for storing the manifests&lt;/h2>
&lt;p>Previously, there was support for only local file directories and HTTPS repositories for storing manifests. Giving creators of addon operators the ability to store manifest in GitHub repository enables faster development and version control. When starting the controller, you can pass a flag to specify the location of your channels directory. The channels directory contains the manifests for different versions, the controller pulls the manifest from this directory and applies it to the cluster. During the internship period, I extended it to include Git repositories.&lt;/p>
&lt;h2 id="annotations-to-temporarily-disable-reconciliation">Annotations to temporarily disable reconciliation&lt;/h2>
&lt;p>The reconciliation loop that ensures that the desired state matches the actual state prevents modification of objects in the cluster. This makes it hard to experiment or investigate what might be wrong in the cluster as any changes made are promptly reverted. I resolved this by allowing users to place an &lt;code>addons.k8s.io/ignore&lt;/code> annotation on the resource that they don’t want the controller to reconcile. The controller checks for this annotation and doesn’t reconcile that object. To resume reconciliation, the annotation can be removed from the resource.&lt;/p>
&lt;h2 id="unstructured-support-in-kubebuilder-declarative-pattern">Unstructured support in kubebuilder-declarative-pattern&lt;/h2>
&lt;p>One of the operators that I worked on is a generic controller that could manage more than one cluster addon that did not require extra configuration. To do this, the operator couldn’t use a particular type and needed the kubebuilder-declarative-repo to support using the &lt;a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured#Unstructured">unstructured.Unstructured&lt;/a> type. There were various functions in the kubebuilder-declarative-pattern that couldn’t handle this type and returned an error if the object passed in was not of type &lt;code>addonsv1alpha1.CommonObject&lt;/code>. The functions were modified to handle both &lt;code>unstructured.Unstructured&lt;/code> and &lt;code>addonsv1alpha.CommonObject&lt;/code>.&lt;/p>
&lt;h1 id="tools-and-cli-programs">Tools and CLI programs&lt;/h1>
&lt;p>There were also some command-line programs I wrote that could be used to make working with addon operators easier. Most of them have uses outside the addon operators as they try to solve a specific problem that could surface anywhere while working with Kubernetes. I encourage you to &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools">check them out&lt;/a> when you have the chance!&lt;/p>
&lt;h2 id="rbac-generator">RBAC Generator&lt;/h2>
&lt;p>One of the biggest concerns with the operator was RBAC. You had to manually look through the manifest and add the RBAC rule for each resource as it needs to have RBAC permissions to create, get, update and delete the resources in the manifest when running in-cluster. Building the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">RBAC generator&lt;/a> automated the process of writing the RBAC roles and role bindings. The function of the RBAC generator is simple. It accepts the file name of the manifest as a flag. Then, it parses the manifest and gets the API group and resource name of the resources and adds it to a role. It outputs the role and role binding to stdout or a file if the &lt;code>--out&lt;/code> flag is parsed.&lt;/p>
&lt;p>Additionally, the tool enables you to split the RBAC by separating the cluster roles in the manifest. This lessened the security concern of an operator being over-privileged as it needed to have all the permissions that the clusterrole has. If you want to apply the clusterrole yourself and not give the operator these permissions, you can pass in a &lt;code>--supervisory&lt;/code> boolean flag so that the generator does not add these permissions to the role. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">here&lt;/a>.&lt;/p>
&lt;h2 id="kubectl-ownerref">Kubectl Ownerref&lt;/h2>
&lt;p>It is hard to find out at a glance which objects were created by an addon custom resource. This kubectl plugin alleviates that pain by displaying all the objects in the cluster that a resource has ownerrefs on. You simply pass the kind and the name of the resource as arguments to the program and it checks the cluster for the objects and gives the kind, name, the namespace of such an object. It could be useful to get a general overview of all the objects that the controller is reconciling by passing in the name and kind of custom resource. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools/kubectl-ownerref">here&lt;/a>.&lt;/p>
&lt;h1 id="addon-operators">Addon Operators&lt;/h1>
&lt;p>To fully understand addons operators and make changes to how they are being created, you have to try creating and using them. Part of the summer was spent building operators for some popular addons like the Kubernetes dashboard, flannel, NodeLocalDNS and so on. Please check the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons&lt;/a> repository for the different addon operators. In this section, I will just highlight one that is a little different from the others.&lt;/p>
&lt;h2 id="generic-controller">Generic Controller&lt;/h2>
&lt;p>The generic controller can be shared between addons that don’t require much configuration. This minimizes resource consumption on the cluster as it reduces the number of controllers that need to be run. Also instead of building your own operator, you can just use the generic controller and whenever you feel that your needs have grown and you need a more complex operator, you can always scaffold the code with kubebuilder and continue from where the generic operator stopped. To use the generic controller, you can generate the CustomResourceDefinition(CRD) using this tool (&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">generic-addon&lt;/a>). You pass in the kind, group, and the location of your channels directory (it could be a Git repository too!). The tool generates the - CRD, RBAC manifest and two custom resources for you.&lt;/p>
&lt;p>The process is as follows:&lt;/p>
&lt;ul>
&lt;li>Create the Generic CRD&lt;/li>
&lt;li>Generate all the manifests needed with the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">&lt;code>generic-addon tool&lt;/code>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This tool creates:&lt;/p>
&lt;ol>
&lt;li>The CRD for your addon&lt;/li>
&lt;li>The RBAC rules for the CustomResourceDefinitions&lt;/li>
&lt;li>The RBAC rules for applying the manifests&lt;/li>
&lt;li>The custom resource for your addon&lt;/li>
&lt;li>A Generic custom resource&lt;/li>
&lt;/ol>
&lt;p>The Generic custom resource looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Generic&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>generic-sample&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">objectKind&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NodeLocalDNS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1alpha1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">channel&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;../nodelocaldns/channels&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Apply these manifests but ensure to apply the CRD before the CR.
Then, run the Generic controller, either on your machine or in-cluster.&lt;/p>
&lt;p>If you are interested in building an operator, Please check out &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">this guide&lt;/a>.&lt;/p>
&lt;h1 id="relevant-links">Relevant Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s">Detailed breakdown of work done during the internship&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/addons/0035-20190128-addons-via-operators.md">Addon Operator (KEP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/issues/39">Original GSoC Issue&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s/blob/master/GSoC%202020%20PROPOSAL%20-%20PACKAGE%20ALL%20THINGS.pdf">Proposal Submitted for GSoC&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/commits?author=SomtochiAma">All commits to kubernetes-sigs/cluster-addons&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern/commits?author=SomtochiAma">All commits to kubernetes-sigs/kubebuidler-declarative-pattern&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="further-work">Further Work&lt;/h1>
&lt;p>A lot of work was definitely done on the cluster addons during the GSoC period. But we need more people building operators and using them in the cluster. We need wider adoption in the community. Build operators for your favourite addons and tell us how it went and if you had any issues. Check out this &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">README.md&lt;/a> to get started.&lt;/p>
&lt;h1 id="appreciation">Appreciation&lt;/h1>
&lt;p>I really want to appreciate my mentors &lt;a href="https://github.com/justinsb">Justin Santa Barbara&lt;/a> (Google) and &lt;a href="https://github.com/stealthybox">Leigh Capili&lt;/a> (Weaveworks). My internship was awesome because they were awesome. They set a golden standard for what mentorship should be. They were accessible and always available to clear any confusion. I think what I liked best was that they didn’t just dish out tasks, instead, we had open discussions about what was wrong and what could be improved. They are really the best and I hope I get to work with them again!
Also, I want to say a huge thanks to &lt;a href="https://github.com/neolit123">Lubomir I. Ivanov&lt;/a> for reviewing this blog post!&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>So far I have learnt a lot about Go, the internals of Kubernetes, and operators. I want to conclude by encouraging people to contribute to open-source (especially Kubernetes :)) regardless of your level of experience. It has been a well-rounded experience for me and I have come to love the community. It is a great initiative and it is a great way to learn and meet awesome people. Special shoutout to Google for organizing this program.&lt;/p>
&lt;p>If you are interested in cluster addons and finding out more on addon operators, you are welcome to join our slack channel on the Kubernetes &lt;a href="https://kubernetes.slack.com/messages/cluster-addons">#cluster-addons&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/SomtochiAma">Somtochi Onyekwere&lt;/a> is a software engineer that loves contributing to open-source and exploring cloud native solutions.&lt;/em>&lt;/p></description></item><item><title>Blog: Introducing Structured Logs</title><link>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Marek Siarkowicz (Google), Nathan Beach (Google)&lt;/p>
&lt;p>Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.&lt;/p>
&lt;p>In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.&lt;/p>
&lt;p>To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those &amp;quot;key&amp;quot;=&amp;quot;value&amp;quot; pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the &lt;code>--logging-format=json&lt;/code> flag.&lt;/p>
&lt;h2 id="using-structured-logs">Using Structured Logs&lt;/h2>
&lt;p>We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>will result in this log:&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Or, if the --logging-format=json flag is set, it will result in this output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.&lt;/p>
&lt;p>With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.&lt;/p>
&lt;p>Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.&lt;/p>
&lt;p>Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.&lt;/p>
&lt;p>Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">migrate existing log calls to use structured logs&lt;/a>. It's a great and easy way to make your first contribution to Kubernetes!&lt;/p></description></item><item><title>Blog: Warning: Helpful Warnings Ahead</title><link>https://kubernetes.io/blog/2020/09/03/warnings/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/03/warnings/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Jordan Liggitt (Google)&lt;/p>
&lt;p>As Kubernetes maintainers, we're always looking for ways to improve usability while preserving compatibility.
As we develop features, triage bugs, and answer support questions, we accumulate information that would be helpful for Kubernetes users to know.
In the past, sharing that information was limited to out-of-band methods like release notes, announcement emails, documentation, and blog posts.
Unless someone knew to seek out that information and managed to find it, they would not benefit from it.&lt;/p>
&lt;p>In Kubernetes v1.19, we added a feature that allows the Kubernetes API server to
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings">send warnings to API clients&lt;/a>.
The warning is sent using a &lt;a href="https://tools.ietf.org/html/rfc7234#section-5.5">standard &lt;code>Warning&lt;/code> response header&lt;/a>,
so it does not change the status code or response body in any way.
This allows the server to send warnings easily readable by any API client, while remaining compatible with previous client versions.&lt;/p>
&lt;p>Warnings are surfaced by &lt;code>kubectl&lt;/code> v1.19+ in &lt;code>stderr&lt;/code> output, and by the &lt;code>k8s.io/client-go&lt;/code> client library v0.19.0+ in log output.
The &lt;code>k8s.io/client-go&lt;/code> behavior can be &lt;a href="#customize-client-handling">overridden per-process or per-client&lt;/a>.&lt;/p>
&lt;h2 id="deprecation-warnings">Deprecation Warnings&lt;/h2>
&lt;p>The first way we are using this new capability is to send warnings for use of deprecated APIs.&lt;/p>
&lt;p>Kubernetes is a &lt;a href="https://www.cncf.io/cncf-kubernetes-project-journey/#development-velocity">big, fast-moving project&lt;/a>.
Keeping up with the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1180">changes&lt;/a>
in each release can be daunting, even for people who work on the project full-time. One important type of change is API deprecations.
As APIs in Kubernetes graduate to GA versions, pre-release API versions are deprecated and eventually removed.&lt;/p>
&lt;p>Even though there is an &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">extended deprecation period&lt;/a>,
and deprecations are &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecation">included in release notes&lt;/a>,
they can still be hard to track. During the deprecation period, the pre-release API remains functional,
allowing several releases to transition to the stable API version. However, we have found that users often don't even realize
they are depending on a deprecated API version until they upgrade to the release that stops serving it.&lt;/p>
&lt;p>Starting in v1.19, whenever a request is made to a deprecated REST API, a warning is returned along with the API response.
This warning includes details about the release in which the API will no longer be available, and the replacement API version.&lt;/p>
&lt;p>Because the warning originates at the server, and is intercepted at the client level, it works for all kubectl commands,
including high-level commands like &lt;code>kubectl apply&lt;/code>, and low-level commands like &lt;code>kubectl get --raw&lt;/code>:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file, then displaying a warning message 'networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress'."
src="kubectl-warnings.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This helps people affected by the deprecation to know the request they are making is deprecated,
how long they have to address the issue, and what API they should use instead.
This is especially helpful when the user is applying a manifest they didn't create,
so they have time to reach out to the authors to ask for an updated version.&lt;/p>
&lt;p>We also realized that the person &lt;em>using&lt;/em> a deprecated API is often not the same person responsible for upgrading the cluster,
so we added two administrator-facing tools to help track use of deprecated APIs and determine when upgrades are safe.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>Starting in Kubernetes v1.19, when a request is made to a deprecated REST API endpoint,
an &lt;code>apiserver_requested_deprecated_apis&lt;/code> gauge metric is set to &lt;code>1&lt;/code> in the kube-apiserver process.
This metric has labels for the API &lt;code>group&lt;/code>, &lt;code>version&lt;/code>, &lt;code>resource&lt;/code>, and &lt;code>subresource&lt;/code>,
and a &lt;code>removed_version&lt;/code> label that indicates the Kubernetes release in which the API will no longer be served.&lt;/p>
&lt;p>This is an example query using &lt;code>kubectl&lt;/code>, &lt;a href="https://github.com/prometheus/prom2json">prom2json&lt;/a>,
and &lt;a href="https://stedolan.github.io/jq/">jq&lt;/a> to determine which deprecated APIs have been requested
from the current instance of the API server:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This shows the deprecated &lt;code>extensions/v1beta1&lt;/code> Ingress and &lt;code>rbac.authorization.k8s.io/v1beta1&lt;/code> ClusterRole APIs
have been requested on this server, and will be removed in v1.22.&lt;/p>
&lt;p>We can join that information with the &lt;code>apiserver_request_total&lt;/code> metrics to get more details about the requests being made to these APIs:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> # set $deprecated to a list of deprecated APIs
&lt;/span>&lt;span style="color:#b44"> [
&lt;/span>&lt;span style="color:#b44"> .[] |
&lt;/span>&lt;span style="color:#b44"> select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels |
&lt;/span>&lt;span style="color:#b44"> {group,version,resource}
&lt;/span>&lt;span style="color:#b44"> ] as $deprecated
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> |
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> # select apiserver_request_total metrics which are deprecated
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_request_total&amp;#34;).metrics[] |
&lt;/span>&lt;span style="color:#b44"> select(.labels | {group,version,resource} as $key | $deprecated | index($key))
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;0&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf;stream=watch&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;WATCH&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;21&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/json&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output shows that only read requests are being made to these APIs, and the most requests have been made to watch the deprecated Ingress API.&lt;/p>
&lt;p>You can also find that information through the following Prometheus query,
which returns information about requests made to deprecated APIs which will be removed in v1.22:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-promql" data-lang="promql">&lt;span style="color:#b8860b">apiserver_requested_deprecated_apis&lt;/span>{&lt;span style="color:#a0a000">removed_version&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">1.22&lt;/span>&amp;#34;}&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">*&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">on&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">group&lt;/span>,&lt;span style="color:#b8860b">version&lt;/span>,&lt;span style="color:#b8860b">resource&lt;/span>,&lt;span style="color:#b8860b">subresource&lt;/span>&lt;span style="color:#666">)&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">group_right&lt;/span>&lt;span style="color:#666">()&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b8860b">apiserver_request_total&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="audit-annotations">Audit Annotations&lt;/h3>
&lt;p>Metrics are a fast way to check whether deprecated APIs are being used, and at what rate,
but they don't include enough information to identify particular clients or API objects.
Starting in Kubernetes v1.19, &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">audit events&lt;/a>
for requests to deprecated APIs include an audit annotation of &lt;code>&amp;quot;k8s.io/deprecated&amp;quot;:&amp;quot;true&amp;quot;&lt;/code>.
Administrators can use those audit events to identify specific clients or objects that need to be updated.&lt;/p>
&lt;h2 id="custom-resource-definitions">Custom Resource Definitions&lt;/h2>
&lt;p>Along with the API server ability to warn about deprecated API use, starting in v1.19, a CustomResourceDefinition can indicate a
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-deprecation">particular version of the resource it defines is deprecated&lt;/a>.
When API requests to a deprecated version of a custom resource are made, a warning message is returned, matching the behavior of built-in APIs.&lt;/p>
&lt;p>The author of the CustomResourceDefinition can also customize the warning for each version if they want to.
This allows them to give a pointer to a migration guide or other information if needed.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiextensions.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CustomResourceDefinition&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>crontabs.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">versions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1alpha1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This overrides the default warning returned to clients making v1alpha1 API requests.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecationWarning&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/v1alpha1 CronTab is deprecated; use example.com/v1 CronTab (see http://example.com/v1alpha1-v1)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1beta1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># A default warning message is returned for this version.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="admission-webhooks">Admission Webhooks&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers">Admission webhooks&lt;/a>
are the primary way to integrate custom policies or validation with Kubernetes.
Starting in v1.19, admission webhooks can &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#response">return warning messages&lt;/a>
that are passed along to the requesting API client. Warnings can be returned with allowed or rejected admission responses.&lt;/p>
&lt;p>As an example, to allow a request but warn about a configuration known not to work well, an admission webhook could send this response:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;admission.k8s.io/v1&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;AdmissionReview&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;response&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;lt;value from request.uid&amp;gt;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;allowed&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;warnings&amp;#34;&lt;/span>: [
&lt;span style="color:#b44">&amp;#34;.spec.memory: requests &amp;gt;1GB do not work on Fridays&amp;#34;&lt;/span>
]
}
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are implementing a webhook that returns a warning message, here are some tips:&lt;/p>
&lt;ul>
&lt;li>Don't include a &amp;quot;Warning:&amp;quot; prefix in the message (that is added by clients on output)&lt;/li>
&lt;li>Use warning messages to describe problems the client making the API request should correct or be aware of&lt;/li>
&lt;li>Be brief; limit warnings to 120 characters if possible&lt;/li>
&lt;/ul>
&lt;p>There are many ways admission webhooks could use this new feature, and I'm looking forward to seeing what people come up with.
Here are a couple ideas to get you started:&lt;/p>
&lt;ul>
&lt;li>webhook implementations adding a &amp;quot;complain&amp;quot; mode, where they return warnings instead of rejections,
to allow trying out a policy to verify it is working as expected before starting to enforce it&lt;/li>
&lt;li>&amp;quot;lint&amp;quot; or &amp;quot;vet&amp;quot;-style webhooks, inspecting objects and surfacing warnings when best practices are not followed&lt;/li>
&lt;/ul>
&lt;h2 id="customize-client-handling">Customize Client Handling&lt;/h2>
&lt;p>Applications that use the &lt;code>k8s.io/client-go&lt;/code> library to make API requests can customize
how warnings returned from the server are handled. By default, warnings are logged to
stderr as they are received, but this behavior can be customized
&lt;a href="https://godoc.org/k8s.io/client-go/rest#SetDefaultWarningHandler">per-process&lt;/a>
or &lt;a href="https://godoc.org/k8s.io/client-go/rest#Config">per-client&lt;/a>.&lt;/p>
&lt;p>This example shows how to make your application behave like &lt;code>kubectl&lt;/code>,
overriding message handling process-wide to deduplicate warnings
and highlighting messages using colored output where supported:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;os&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/kubectl/pkg/util/term&amp;#34;&lt;/span>
&lt;span style="color:#666">...&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
rest.&lt;span style="color:#00a000">SetDefaultWarningHandler&lt;/span>(
rest.&lt;span style="color:#00a000">NewWarningWriter&lt;/span>(os.Stderr, rest.WarningWriterOptions{
&lt;span style="color:#080;font-style:italic">// only print a given warning the first time we receive it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Deduplicate: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#080;font-style:italic">// highlight the output with color when the output supports it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Color: term.&lt;span style="color:#00a000">AllowsColorOutput&lt;/span>(os.Stderr),
},
),
)
&lt;span style="color:#666">...&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next example shows how to construct a client that ignores warnings.
This is useful for clients that operate on metadata for all resource types
(found dynamically at runtime using the discovery API)
and do not benefit from warnings about a particular resource being deprecated.
Suppressing deprecation warnings is not recommended for clients that require use of particular APIs.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">getClientWithoutWarnings&lt;/span>(config &lt;span style="color:#666">*&lt;/span>rest.Config) (kubernetes.Interface, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#080;font-style:italic">// copy to avoid mutating the passed-in config
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config = rest.&lt;span style="color:#00a000">CopyConfig&lt;/span>(config)
&lt;span style="color:#080;font-style:italic">// set the warning handler for this client to ignore warnings
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config.WarningHandler = rest.NoWarnings{}
&lt;span style="color:#080;font-style:italic">// construct and return the client
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> kubernetes.&lt;span style="color:#00a000">NewForConfig&lt;/span>(config)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="kubectl-strict-mode">Kubectl Strict Mode&lt;/h2>
&lt;p>If you want to be sure you notice deprecations as soon as possible and get a jump start on addressing them,
&lt;code>kubectl&lt;/code> added a &lt;code>--warnings-as-errors&lt;/code> option in v1.19. When invoked with this option,
&lt;code>kubectl&lt;/code> treats any warnings it receives from the server as errors and exits with a non-zero exit code:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file with a --warnings-as-errors flag, displaying a warning message and exiting with a non-zero exit code."
src="kubectl-warnings-as-errors.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This could be used in a CI job to apply manifests to a current server,
and required to pass with a zero exit code in order for the CI job to succeed.&lt;/p>
&lt;h2 id="future-possibilities">Future Possibilities&lt;/h2>
&lt;p>Now that we have a way to communicate helpful information to users in context,
we're already considering other ways we can use this to improve people's experience with Kubernetes.
A couple areas we're looking at next are warning about &lt;a href="http://issue.k8s.io/64841#issuecomment-395141013">known problematic values&lt;/a>
we cannot reject outright for compatibility reasons, and warning about use of deprecated fields or field values
(like selectors using beta os/arch node labels, &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#beta-kubernetes-io-arch-deprecated">deprecated in v1.14&lt;/a>).
I'm excited to see progress in this area, continuing to make it easier to use Kubernetes.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/liggitt">Jordan Liggitt&lt;/a> is a software engineer at Google, and helps lead Kubernetes authentication, authorization, and API efforts.&lt;/em>&lt;/p></description></item><item><title>Blog: Scaling Kubernetes Networking With EndpointSlices</title><link>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rob Scott (Google)&lt;/p>
&lt;p>EndpointSlices are an exciting new API that provides a scalable and extensible alternative to the Endpoints API. EndpointSlices track IP addresses, ports, readiness, and topology information for Pods backing a Service.&lt;/p>
&lt;p>In Kubernetes 1.19 this feature is enabled by default with kube-proxy reading from &lt;a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">EndpointSlices&lt;/a> instead of Endpoints. Although this will mostly be an invisible change, it should result in noticeable scalability improvements in large clusters. It also enables significant new features in future Kubernetes releases like &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/">Topology Aware Routing&lt;/a>.&lt;/p>
&lt;h2 id="scalability-limitations-of-the-endpoints-api">Scalability Limitations of the Endpoints API&lt;/h2>
&lt;p>With the Endpoints API, there was only one Endpoints resource for a Service. That meant that it needed to be able to store IP addresses and ports (network endpoints) for every Pod that was backing the corresponding Service. This resulted in huge API resources. To compound this problem, kube-proxy was running on every node and watching for any updates to Endpoints resources. If even a single network endpoint changed in an Endpoints resource, the whole object would have to be sent to each of those instances of kube-proxy.&lt;/p>
&lt;p>A further limitation of the Endpoints API is that it limits the number of network endpoints that can be tracked for a Service. The default size limit for an object stored in etcd is 1.5MB. In some cases that can limit an Endpoints resource to 5,000 Pod IPs. This is not an issue for most users, but it becomes a significant problem for users with Services approaching this size.&lt;/p>
&lt;p>To show just how significant these issues become at scale it helps to have a simple example. Think about a Service which has 5,000 Pods, it might end up with a 1.5MB Endpoints resource. If even a single network endpoint in that list changes, the full Endpoints resource will need to be distributed to each Node in the cluster. This becomes quite an issue in a large cluster with 3,000 Nodes. Each update would involve sending 4.5GB of data (1.5MB Endpoints * 3,000 Nodes) across the cluster. That's nearly enough to fill up a DVD, and it would happen for each Endpoints change. Imagine a rolling update that results in all 5,000 Pods being replaced - that's more than 22TB (or 5,000 DVDs) worth of data transferred.&lt;/p>
&lt;h2 id="splitting-endpoints-up-with-the-endpointslice-api">Splitting endpoints up with the EndpointSlice API&lt;/h2>
&lt;p>The EndpointSlice API was designed to address this issue with an approach similar to sharding. Instead of tracking all Pod IPs for a Service with a single Endpoints resource, we split them into multiple smaller EndpointSlices.&lt;/p>
&lt;p>Consider an example where a Service is backed by 15 pods. We'd end up with a single Endpoints resource that tracked all of them. If EndpointSlices were configured to store 5 endpoints each, we'd end up with 3 different EndpointSlices:
&lt;img src="https://kubernetes.io/images/blog/2020-09-02-scaling-kubernetes-networking-endpointslices/endpoint-slices.png" alt="EndpointSlices">&lt;/p>
&lt;p>By default, EndpointSlices store as many as 100 endpoints each, though this can be configured with the &lt;code>--max-endpoints-per-slice&lt;/code> flag on kube-controller-manager.&lt;/p>
&lt;h2 id="endpointslices-provide-10x-scalability-improvements">EndpointSlices provide 10x scalability improvements&lt;/h2>
&lt;p>This API dramatically improves networking scalability. Now when a Pod is added or removed, only 1 small EndpointSlice needs to be updated. This difference becomes quite noticeable when hundreds or thousands of Pods are backing a single Service.&lt;/p>
&lt;p>Potentially more significant, now that all Pod IPs for a Service don't need to be stored in a single resource, we don't have to worry about the size limit for objects stored in etcd. EndpointSlices have already been used to scale Services beyond 100,000 network endpoints.&lt;/p>
&lt;p>All of this is brought together with some significant performance improvements that have been made in kube-proxy. When using EndpointSlices at scale, significantly less data will be transferred for endpoints updates and kube-proxy should be faster to update iptables or ipvs rules. Beyond that, Services can now scale to at least 10 times beyond any previous limitations.&lt;/p>
&lt;h2 id="endpointslices-enable-new-functionality">EndpointSlices enable new functionality&lt;/h2>
&lt;p>Introduced as an alpha feature in Kubernetes v1.16, EndpointSlices were built to enable some exciting new functionality in future Kubernetes releases. This could include dual-stack Services, topology aware routing, and endpoint subsetting.&lt;/p>
&lt;p>Dual-Stack Services are an exciting new feature that has been in development alongside EndpointSlices. They will utilize both IPv4 and IPv6 addresses for Services and rely on the addressType field on EndpointSlices to track these addresses by IP family.&lt;/p>
&lt;p>Topology aware routing will update kube-proxy to prefer routing requests within the same zone or region. This makes use of the topology fields stored for each endpoint in an EndpointSlice. As a further refinement of that, we're exploring the potential of endpoint subsetting. This would allow kube-proxy to only watch a subset of EndpointSlices. For example, this might be combined with topology aware routing so that kube-proxy would only need to watch EndpointSlices containing endpoints within the same zone. This would provide another very significant scalability improvement.&lt;/p>
&lt;h2 id="what-does-this-mean-for-the-endpoints-api">What does this mean for the Endpoints API?&lt;/h2>
&lt;p>Although the EndpointSlice API is providing a newer and more scalable alternative to the Endpoints API, the Endpoints API will continue to be considered generally available and stable. The most significant change planned for the Endpoints API will involve beginning to truncate Endpoints that would otherwise run into scalability issues.&lt;/p>
&lt;p>The Endpoints API is not going away, but many new features will rely on the EndpointSlice API. To take advantage of the new scalability and functionality that EndpointSlices provide, applications that currently consume Endpoints will likely want to consider supporting EndpointSlices in the future.&lt;/p></description></item><item><title>Blog: Ephemeral volumes with storage capacity tracking: EmptyDir on steroids</title><link>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance. Other applications expect some read-only input
data to be present in files, like configuration data or secret keys.&lt;/p>
&lt;p>Kubernetes already supports several kinds of such &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes">ephemeral
volumes&lt;/a>, but the
functionality of those is limited to what is implemented inside
Kubernetes.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">CSI ephemeral volumes&lt;/a>
made it possible to extend Kubernetes with CSI
drivers that provide light-weight, local volumes. These &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md#motivation">&lt;em>inject
arbitrary states, such as configuration, secrets, identity, variables
or similar
information&lt;/em>&lt;/a>.
CSI drivers must be modified to support this Kubernetes feature,
i.e. normal, standard-compliant CSI drivers will not work, and
by design such volumes are supposed to be usable on whatever node
is chosen for a pod.&lt;/p>
&lt;p>This is problematic for volumes which consume significant resources on
a node or for special storage that is only available on some nodes.
Therefore, Kubernetes 1.19 introduces two new alpha features for
volumes that are conceptually more like the &lt;code>EmptyDir&lt;/code> volumes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#generic-ephemeral-volumes">&lt;em>generic&lt;/em> ephemeral volumes&lt;/a> and&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity">CSI storage capacity tracking&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The advantages of the new approach are:&lt;/p>
&lt;ul>
&lt;li>Storage can be local or network-attached.&lt;/li>
&lt;li>Volumes can have a fixed size that applications are never able to exceed.&lt;/li>
&lt;li>Works with any CSI driver that supports provisioning of persistent
volumes and (for capacity tracking) implements the CSI &lt;code>GetCapacity&lt;/code> call.&lt;/li>
&lt;li>Volumes may have some initial data, depending on the driver and
parameters.&lt;/li>
&lt;li>All of the typical volume operations (snapshotting,
resizing, the future storage capacity tracking, etc.)
are supported.&lt;/li>
&lt;li>The volumes are usable with any app controller that accepts
a Pod or volume specification.&lt;/li>
&lt;li>The Kubernetes scheduler itself picks suitable nodes, i.e. there is
no need anymore to implement and configure scheduler extenders and
mutating webhooks.&lt;/li>
&lt;/ul>
&lt;p>This makes generic ephemeral volumes a suitable solution for several
use cases:&lt;/p>
&lt;h1 id="use-cases">Use cases&lt;/h1>
&lt;h2 id="persistent-memory-as-dram-replacement-for-memcached">Persistent Memory as DRAM replacement for memcached&lt;/h2>
&lt;p>Recent releases of memcached added &lt;a href="https://memcached.org/blog/persistent-memory/">support for using Persistent
Memory&lt;/a> (PMEM) instead
of standard DRAM. When deploying memcached through one of the app
controllers, generic ephemeral volumes make it possible to request a PMEM volume
of a certain size from a CSI driver like
&lt;a href="https://intel.github.io/pmem-csi/">PMEM-CSI&lt;/a>.&lt;/p>
&lt;h2 id="local-lvm-storage-as-scratch-space">Local LVM storage as scratch space&lt;/h2>
&lt;p>Applications working with data sets that exceed the RAM size can
request local storage with performance characteristics or size that is
not met by the normal Kubernetes &lt;code>EmptyDir&lt;/code> volumes. For example,
&lt;a href="https://github.com/cybozu-go/topolvm">TopoLVM&lt;/a> was written for that
purpose.&lt;/p>
&lt;h2 id="read-only-access-to-volumes-with-data">Read-only access to volumes with data&lt;/h2>
&lt;p>Provisioning a volume might result in a non-empty volume:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">restore a snapshot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource">cloning a volume&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20200120-generic-data-populators.md">generic data populators&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Such volumes can be mounted read-only.&lt;/p>
&lt;h1 id="how-it-works">How it works&lt;/h1>
&lt;h2 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h2>
&lt;p>The key idea behind generic ephemeral volumes is that a new volume
source, the so-called
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/#ephemeralvolumesource-v1alpha1-core">&lt;code>EphemeralVolumeSource&lt;/code>&lt;/a>
contains all fields that are needed to created a volume claim
(historically called persistent volume claim, PVC). A new controller
in the &lt;code>kube-controller-manager&lt;/code> waits for Pods which embed such a
volume source and then creates a PVC for that pod. To a CSI driver
deployment, that PVC looks like any other, so no special support is
needed.&lt;/p>
&lt;p>As long as these PVCs exist, they can be used like any other volume claim. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.&lt;/p>
&lt;p>Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (&lt;code>-&lt;/code>) in the
middle. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known. The downside is that the name might
be in use already. This is detected by Kubernetes and then blocks Pod
startup.&lt;/p>
&lt;p>To ensure that the volume gets deleted together with the pod, the
controller makes the Pod the owner of the volume claim. When the Pod
gets deleted, the normal garbage-collection mechanism also removes the
claim and thus the volume.&lt;/p>
&lt;p>Claims select the storage driver through the normal storage class
mechanism. Although storage classes with both immediate and late
binding (aka &lt;code>WaitForFirstConsumer&lt;/code>) are supported, for ephemeral
volumes it makes more sense to use &lt;code>WaitForFirstConsumer&lt;/code>: then Pod
scheduling can take into account both node utilization and
availability of storage when choosing a node. This is where the other
new feature comes in.&lt;/p>
&lt;h2 id="storage-capacity-tracking">Storage capacity tracking&lt;/h2>
&lt;p>Normally, the Kubernetes scheduler has no information about where a
CSI driver might be able to create a volume. It also has no way of
talking directly to a CSI driver to retrieve that information. It
therefore tries different nodes until it finds one where all volumes
can be made available (late binding) or leaves it entirely to the
driver to choose a location (immediate binding).&lt;/p>
&lt;p>The new &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io">&lt;code>CSIStorageCapacity&lt;/code> alpha
API&lt;/a>
allows storing the necessary information in etcd where it is available to the
scheduler. In contrast to support for generic ephemeral volumes,
storage capacity tracking must be &lt;a href="https://github.com/kubernetes-csi/external-provisioner/blob/master/README.md#capacity-support">enabled when deploying a CSI
driver&lt;/a>:
the &lt;code>external-provisioner&lt;/code> must be told to publish capacity
information that it then retrieves from the CSI driver through the normal
&lt;code>GetCapacity&lt;/code> call.&lt;/p>
&lt;!-- TODO: update the link with a revision once https://github.com/kubernetes-csi/external-provisioner/pull/450 is merged -->
&lt;p>When the Kubernetes scheduler needs to choose a node for a Pod with an
unbound volume that uses late binding and the CSI driver deployment
has opted into the feature by setting the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csidriver-v1beta1-storage-k8s-io">&lt;code>CSIDriver.storageCapacity&lt;/code>
flag&lt;/a>
flag, the scheduler automatically filters out nodes that do not have
access to enough storage capacity. This works for generic ephemeral
and persistent volumes but &lt;em>not&lt;/em> for CSI ephemeral volumes because the
parameters of those are opaque for Kubernetes.&lt;/p>
&lt;p>As usual, volumes with immediate binding get created before scheduling
pods, with their location chosen by the storage driver. Therefore, the
external-provisioner's default configuration skips storage
classes with immediate binding as the information wouldn't be used anyway.&lt;/p>
&lt;p>Because the Kubernetes scheduler must act on potentially outdated
information, it cannot be ensured that the capacity is still available
when a volume is to be created. Still, the chances that it can be created
without retries should be higher.&lt;/p>
&lt;h1 id="security">Security&lt;/h1>
&lt;h2 id="csistoragecapacity">CSIStorageCapacity&lt;/h2>
&lt;p>CSIStorageCapacity objects are namespaced. When deploying each CSI
drivers in its own namespace and, as recommended, limiting the RBAC
permissions for CSIStorageCapacity to that namespace, it is
always obvious where the data came from. However, Kubernetes does
not check that and typically drivers get installed in the same
namespace anyway, so ultimately drivers are &lt;em>expected to behave&lt;/em> and
not publish incorrect data.&lt;/p>
&lt;h2 id="generic-ephemeral-volumes-1">Generic ephemeral volumes&lt;/h2>
&lt;p>If users have permission to create a Pod (directly or indirectly),
then they can also create generic ephemeral volumes even when they do
not have permission to create a volume claim. That's because RBAC
permission checks are applied to the controller which creates the
PVC, not the original user. This is a fundamental change that must be
&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#security">taken into
account&lt;/a> before
enabling the feature in clusters where untrusted users are not
supposed to have permission to create volumes.&lt;/p>
&lt;h1 id="example">Example&lt;/h1>
&lt;p>A &lt;a href="https://github.com/intel/pmem-csi/commits/kubernetes-1-19-blog-post">special branch&lt;/a>
in PMEM-CSI contains all the necessary changes to bring up a
Kubernetes 1.19 cluster inside QEMU VMs with both alpha features
enabled. The PMEM-CSI driver code is used unchanged, only the
deployment was updated.&lt;/p>
&lt;p>On a suitable machine (Linux, non-root user can use Docker - see the
&lt;a href="https://intel.github.io/pmem-csi/0.7/docs/autotest.html#qemu-and-kubernetes">QEMU and
Kubernetes&lt;/a>
section in the PMEM-CSI documentation), the following commands bring
up a cluster and install the PMEM-CSI driver:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">git clone --branch=kubernetes-1-19-blog-post https://github.com/intel/pmem-csi.git
cd pmem-csi
export TEST_KUBERNETES_VERSION=1.19 TEST_FEATURE_GATES=CSIStorageCapacity=true,GenericEphemeralVolume=true TEST_PMEM_REGISTRY=intel
make start &amp;amp;&amp;amp; echo &amp;amp;&amp;amp; test/setup-deployment.sh
&lt;/code>&lt;/pre>&lt;p>If all goes well, the output contains the following usage
instructions:&lt;/p>
&lt;pre>&lt;code>The test cluster is ready. Log in with [...]/pmem-csi/_work/pmem-govm/ssh.0, run
kubectl once logged in. Alternatively, use kubectl directly with the
following env variable:
KUBECONFIG=[...]/pmem-csi/_work/pmem-govm/kube.config
secret/pmem-csi-registry-secrets created
secret/pmem-csi-node-secrets created
serviceaccount/pmem-csi-controller created
...
To try out the pmem-csi driver ephemeral volumes:
cat deploy/kubernetes-1.19/pmem-app-ephemeral.yaml |
[...]/pmem-csi/_work/pmem-govm/ssh.0 kubectl create -f -
&lt;/code>&lt;/pre>&lt;p>The CSIStorageCapacity objects are not meant to be human-readable, so
some post-processing is needed. The following Golang template filters
all objects by the storage class that the example uses and prints the
name, topology and capacity:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get \
-o go-template='{{range .items}}{{if eq .storageClassName &amp;quot;pmem-csi-sc-late-binding&amp;quot;}}{{.metadata.name}} {{.nodeTopology.matchLabels}} {{.capacity}}
{{end}}{{end}}' \
csistoragecapacities
&lt;/code>&lt;/pre>&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 30716Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>One individual object has the following content:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl describe csistoragecapacities/csisc-6cw8j
&lt;/code>&lt;/pre>&lt;pre>&lt;code>Name: csisc-sqdnt
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
API Version: storage.k8s.io/v1alpha1
Capacity: 30716Mi
Kind: CSIStorageCapacity
Metadata:
Creation Timestamp: 2020-08-11T15:41:03Z
Generate Name: csisc-
Managed Fields:
...
Owner References:
API Version: apps/v1
Controller: true
Kind: StatefulSet
Name: pmem-csi-controller
UID: 590237f9-1eb4-4208-b37b-5f7eab4597d1
Resource Version: 2994
Self Link: /apis/storage.k8s.io/v1alpha1/namespaces/default/csistoragecapacities/csisc-sqdnt
UID: da36215b-3b9d-404a-a4c7-3f1c3502ab13
Node Topology:
Match Labels:
pmem-csi.intel.com/node: pmem-csi-pmem-govm-worker1
Storage Class Name: pmem-csi-sc-late-binding
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Now let's create the example app with one generic ephemeral
volume. The &lt;code>pmem-app-ephemeral.yaml&lt;/code> file contains:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#080;font-style:italic"># This example Pod definition demonstrates&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># how to use generic ephemeral inline volumes&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># with a PMEM-CSI storage class.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-app-inline-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-frontend&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>intel/pmem-csi-driver-test:v0.7.14&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;100000&amp;#34;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/data&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeClaimTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>4Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pmem-csi-sc-late-binding&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After creating that as shown in the usage instructions above, we have one additional Pod and PVC:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pods/my-csi-app-inline-volume -o wide
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
my-csi-app-inline-volume 1/1 Running 0 6m58s 10.36.0.2 pmem-csi-pmem-govm-worker1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
my-csi-app-inline-volume-my-csi-volume Bound pvc-c11eb7ab-a4fa-46fe-b515-b366be908823 4Gi RWO pmem-csi-sc-late-binding 9m21s
&lt;/code>&lt;/pre>&lt;p>That PVC is owned by the Pod:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get -o yaml pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
annotations:
pv.kubernetes.io/bind-completed: &amp;quot;yes&amp;quot;
pv.kubernetes.io/bound-by-controller: &amp;quot;yes&amp;quot;
volume.beta.kubernetes.io/storage-provisioner: pmem-csi.intel.com
volume.kubernetes.io/selected-node: pmem-csi-pmem-govm-worker1
creationTimestamp: &amp;quot;2020-08-11T15:44:57Z&amp;quot;
finalizers:
- kubernetes.io/pvc-protection
managedFields:
...
name: my-csi-app-inline-volume-my-csi-volume
namespace: default
ownerReferences:
- apiVersion: v1
blockOwnerDeletion: true
controller: true
kind: Pod
name: my-csi-app-inline-volume
uid: 75c925bf-ca8e-441a-ac67-f190b7a2265f
...
&lt;/code>&lt;/pre>&lt;p>Eventually, the storage capacity information for &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> also gets updated:&lt;/p>
&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 26620Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>If another app needs more than 26620Mi, the Kubernetes
scheduler will not pick &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> anymore.&lt;/p>
&lt;h1 id="next-steps">Next steps&lt;/h1>
&lt;p>Both features are under development. Several open questions were
already raised during the alpha review process. The two enhancement
proposals document the work that will be needed for migration to beta and what
alternatives were already considered and rejected:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/9d7a75d/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">KEP-1698: generic ephemeral inline
volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/9d7a75d/keps/sig-storage/1472-storage-capacity-tracking">KEP-1472: Storage Capacity
Tracking&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is crucial for driving that development. SIG-Storage
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets
regularly&lt;/a>
and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and a mailing
list&lt;/a>.&lt;/p></description></item><item><title>Blog: Increasing the Kubernetes Support Window to One Year</title><link>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Pepper (VMware), Nick Young (VMware)&lt;/p>
&lt;p>Starting with Kubernetes 1.19, the support window for Kubernetes versions &lt;a href="https://github.com/kubernetes/enhancements/issues/1498">will increase from 9 months to one year&lt;/a>. The longer support window is intended to allow organizations to perform major upgrades at a time of the year that works the best for them.&lt;/p>
&lt;p>This is a big change. For many years, the Kubernetes project has delivered a new minor release (e.g.: 1.13 or 1.14) every 3 months. The project provides bugfix support via patch releases (e.g.: 1.13.Y) for three parallel branches of the codebase. Combined, this led to each minor release (e.g.: 1.13) having a patch release stream of support for approximately 9 months. In the end, a cluster operator had to upgrade at least every 9 months to remain supported.&lt;/p>
&lt;p>A survey conducted in early 2019 by the WG LTS showed that a significant subset of Kubernetes end-users fail to upgrade within the 9-month support period.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/versions-in-production-text-2.png" alt="Versions in Production">&lt;/p>
&lt;p>This, and other responses from the survey, suggest that a considerable portion of our community would better be able to manage their deployments on supported versions if the patch support period were extended to 12-14 months. It appears to be true regardless of whether the users are on DIY builds or commercially vendored distributions. An extension in the patch support length of time would thus lead to a larger percentage of our user base running supported versions compared to what we have now.&lt;/p>
&lt;p>A yearly support period provides the cushion end-users appear to desire, and is more aligned with familiar annual planning cycles.
There are many unknowns about changing the support windows for a project with as many moving parts as Kubernetes. Keeping the change relatively small (relatively being the important word), gives us the chance to find out what those unknowns are in detail and address them.
From Kubernetes version 1.19 on, the support window will be extended to one year. For Kubernetes versions 1.16, 1.17, and 1.18, the story is more complicated.&lt;/p>
&lt;p>All of these versions still fall under the older “three releases support” model, and will drop out of support when 1.19, 1.20 and 1.21 are respectively released. However, because the 1.19 release has been delayed due to the events of 2020, they will end up with close to a year of support (depending on their exact release dates).&lt;/p>
&lt;p>For example, 1.19 was released on the 26th of August 2020, which is 11 months since the release of 1.16. Since 1.16 is still under the old release policy, this means that it is now out of support.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/support-timeline.png" alt="Support Timeline">&lt;/p>
&lt;p>If you’ve got thoughts or feedback, we’d love to hear them. Please contact us on &lt;a href="https://kubernetes.slack.com/messages/wg-lts/">#wg-lts&lt;/a> on the Kubernetes Slack, or to the &lt;a href="https://groups.google.com/g/kubernetes-wg-lts">kubernetes-wg-lts mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.19: Accentuate the Paw-sitive</title><link>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</link><pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">Kubernetes 1.19 Release Team&lt;/a>&lt;/p>
&lt;p>Finally, we have arrived with Kubernetes 1.19, the second release for 2020, and by far the longest release cycle lasting 20 weeks in total. It consists of 34 enhancements: 10 enhancements are moving to stable, 15 enhancements in beta, and 9 enhancements in alpha.&lt;/p>
&lt;p>The 1.19 release was quite different from a regular release due to COVID-19, the George Floyd protests, and several other global events that we experienced as a release team. Due to these events, we made the decision to adjust our timeline and allow the SIGs, Working Groups, and contributors more time to get things done. The extra time also allowed for people to take time to focus on their lives outside of the Kubernetes project, and ensure their mental wellbeing was in a good place.&lt;/p>
&lt;p>Contributors are the heart of Kubernetes, not the other way around. The Kubernetes code of conduct asks that people be excellent to one another and despite the unrest in our world, we saw nothing but greatness and humility from the community.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="increase-kubernetes-support-window-to-one-year">Increase Kubernetes support window to one year&lt;/h3>
&lt;p>A survey conducted in early 2019 by the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-lts#readme">Long Term Support (LTS) working group&lt;/a> showed that a significant subset of Kubernetes end-users fail to upgrade within the current 9-month support period.
This, and other responses from the survey, suggest that 30% of users would be able to keep their deployments on supported versions if the patch support period were extended to 12-14 months. This appears to be true regardless of whether the users are on self build or commercially vendored distributions. An extension would thus lead to more than 80% of users being on supported versions, instead of the 50-60% we have now.
A yearly support period provides the cushion end-users appear to desire, and is more in harmony with familiar annual planning cycles.
From Kubernetes version 1.19 on, the support window will be extended to one year.&lt;/p>
&lt;h3 id="storage-capacity-tracking">Storage capacity tracking&lt;/h3>
&lt;p>Traditionally, the Kubernetes scheduler was based on the assumptions that additional persistent storage is available everywhere in the cluster and has infinite capacity. Topology constraints addressed the first point, but up to now pod scheduling was still done without considering that the remaining storage capacity may not be enough to start a new pod. &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">Storage capacity tracking&lt;/a>, a new alpha feature, addresses that by adding an API for a CSI driver to report storage capacity and uses that information in the Kubernetes scheduler when choosing a node for a pod. This feature serves as a stepping stone for supporting dynamic provisioning for local volumes and other volume types that are more capacity constrained.&lt;/p>
&lt;h4 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h4>
&lt;p>Kubernetes provides volume plugins whose lifecycle is tied to a pod and can be used as scratch space (e.g. the builtin &lt;code>emptydir&lt;/code> volume type) or to load some data in to a pod (e.g. the builtin &lt;code>configmap&lt;/code> and &lt;code>secret&lt;/code> volume types, or “CSI inline volumes”). The new &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">generic ephemeral volumes&lt;/a> alpha feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume’s lifecycle bound to the Pod.
It can be used to provide scratch storage that is different from the root disk, for example persistent memory, or a separate local disk on that node.
All StorageClass parameters for volume provisioning are supported.
All features supported with PersistentVolumeClaims are supported, such as storage capacity tracking, snapshots and restore, and volume resizing.&lt;/p>
&lt;h4 id="csi-volume-health-monitoring">CSI Volume Health Monitoring&lt;/h4>
&lt;p>The alpha version of CSI health monitoring is being released with Kubernetes 1.19. This feature enables CSI Drivers to share abnormal volume conditions from the underlying storage systems with Kubernetes so that they can be reported as events on PVCs or Pods. This feature serves as a stepping stone towards programmatic detection and resolution of individual volume health issues by Kubernetes.&lt;/p>
&lt;h3 id="ingress-graduates-to-general-availability">Ingress graduates to General Availability&lt;/h3>
&lt;p>In terms of moving the Ingress API towards GA, the API itself has been available in beta for so long that it has attained de facto GA status through usage and adoption (both by users and by load balancer / ingress controller providers). Abandoning it without a full replacement is not a viable approach. It is clearly a useful API and captures a non-trivial set of use cases. At this point, it seems more prudent to declare the current API as something the community will support as a V1, codifying its status, while working on either a V2 Ingress API or an entirely different API with a superset of features.&lt;/p>
&lt;h3 id="structured-logging">Structured logging&lt;/h3>
&lt;p>Before v1.19, logging in the Kubernetes control plane couldn't guarantee any uniform structure for log messages and references to Kubernetes objects in those logs. This makes parsing, processing, storing, querying and analyzing logs hard and forces administrators and developers to rely on ad-hoc solutions in most cases based on some regular expressions. Due to those problems any analytical solution based on those logs is hard to implement and maintain.&lt;/p>
&lt;h4 id="new-klog-methods">New klog methods&lt;/h4>
&lt;p>This Kubernetes release introduces new methods to the &lt;em>klog&lt;/em> library that provide a more structured interface for formatting log messages. Each existing formatted log method (&lt;code>Infof&lt;/code>, &lt;code>Errorf&lt;/code>) is now matched by a structured method (&lt;code>InfoS&lt;/code>, &lt;code>ErrorS&lt;/code>). The new logging methods accept log messages as a first argument and a list of key-values pairs as a variadic second argument. This approach allows incremental adoption of structured logging without converting &lt;strong>all&lt;/strong> of Kubernetes to a new API at one time.&lt;/p>
&lt;h3 id="client-tls-certificate-rotation-for-kubelet">Client TLS certificate rotation for kubelet&lt;/h3>
&lt;p>A kubelet authenticates the kubelet to the kube-apiserver using a private key and certificate. The certificate is supplied to the kubelet when it is first booted, via an out-of-cluster mechanism. Since Kubernetes v1.8, clusters have included a (beta) process for obtaining the initial cert/key pair and rotating it as expiration of the certificate approaches. In Kubernetes v1.19 this graduates to stable.&lt;/p>
&lt;p>During the kubelet start-up sequence, the filesystem is scanned for an existing cert/key pair, which is managed by the certificate manager. In the case that a cert/key is available it will be loaded. If not, the kubelet checks its config file for an encoded certificate value or a file reference in the kubeconfig. If the certificate is a bootstrap certificate, this will be used to generate a key, create a certificate signing request and request a signed certificate from the API server.&lt;/p>
&lt;p>When an expiration approaches the cert manager takes care of providing the correct certificate, generating new private keys and requesting new certificates. With the kubelet requesting certificates be signed as part of its boot sequence, and on an ongoing basis, certificate signing requests from the kubelet need to be auto approved to make cluster administration manageable.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/135">Seccomp&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/266">Kubelet client TLS certificate rotation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/279">Limit node access to API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/383">Redesign Event API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1453">Graduate Ingress to V1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1547">Building Kubelet without Docker&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/693">Node Topology Manager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">New Endpoint API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1498">Increase Kubernetes support window to one year&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="other-notable-features">Other Notable Features&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1451">Run multiple Scheduling Profiles&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1412">Immutable Secrets and ConfigMaps&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="release-notes">Release Notes&lt;/h2>
&lt;p>Check out the full details of the Kubernetes 1.19 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md">release notes&lt;/a>.&lt;/p>
&lt;h2 id="availability">Availability&lt;/h2>
&lt;p>Kubernetes 1.19 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.19.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> (Kubernetes in Docker). You can also easily install 1.19 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h2 id="release-team">Release Team&lt;/h2>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">release team&lt;/a> led by Taylor Dolezal, Senior Developer Advocate at HashiCorp. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">49,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h2 id="release-logo">Release Logo&lt;/h2>
&lt;p>All of you inspired this Kubernetes 1.19 release logo! This release was a bit more of a marathon and a testament to when the world is a wild place, we can come together and do unbelievable things.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-26-kubernetes-1.19-release-announcement/accentuate.png" alt="Kubernetes 1.19 Release Logo">&lt;/p>
&lt;p>&amp;quot;Accentuate the Paw-sitive&amp;quot; was chosen as the release theme because it captures the positive outlook that the release team had, despite the state of the world. The characters pictured in the 1.19 logo represent everyone's personalities on our release team, from emo to peppy, and beyond!&lt;/p>
&lt;p>About the designer: Hannabeth Lagerlof is a Visual Designer based in Los Angeles, California, and she has an extensive background in Environments and Graphic Design. Hannabeth creates art and user experiences that inspire connection. You can find Hannabeth on Twitter as @emanate_design.&lt;/p>
&lt;h2 id="the-long-run">The Long Run&lt;/h2>
&lt;p>The release was also different from the enhancements side of things. Traditionally, we have had 3-4 weeks between the call for enhancements and Enhancements Freeze, which ends the phase in which contributors can acknowledge whether a particular feature will be part of the cycle. This release cycle, being unique, we had five weeks for the same milestone. The extended duration gave the contributors more time to plan and decide about the graduation of their respective features.&lt;/p>
&lt;p>The milestone until which contributors implement the features was extended from the usual five weeks to 7 weeks. Contributors were provided with 40% more time to work on their features, resulting in reduced fatigue and more to think through about the implementation. We also noticed a considerable reduction in last-minute hustles. There were also a lesser number of exception requests this cycle - 6 compared to 14 the previous release cycle.&lt;/p>
&lt;h2 id="user-highlights">User Highlights&lt;/h2>
&lt;ul>
&lt;li>The CNCF grants Zalando, Europe’s leading online platform for fashion and lifestyle, the &lt;a href="https://www.cncf.io/announcement/2020/08/20/cloud-native-computing-foundation-grants-zalando-the-top-end-user-award/">Top End User Award&lt;/a>. Zalando leverages numerous CNCF projects and open sourced multiple of their own development.&lt;/li>
&lt;/ul>
&lt;h2 id="ecosystem-updates">Ecosystem Updates&lt;/h2>
&lt;ul>
&lt;li>The CNCF just concluded its very first Virtual KubeCon. All talks are &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">on-demand&lt;/a> for anyone registered, it's not too late!&lt;/li>
&lt;li>The &lt;a href="https://www.cncf.io/blog/2020/07/15/certified-kubernetes-security-specialist-cks-coming-in-november/">Certified Kubernetes Security Specialist&lt;/a> (CKS) coming in November! CKS focuses on cluster &amp;amp; system hardening, minimizing microservice vulnerabilities and the security of the supply chain.&lt;/li>
&lt;li>CNCF published the second &lt;a href="https://www.cncf.io/blog/2020/08/14/state-of-cloud-native-development/">State of Cloud Native Development&lt;/a>, showing the massively growing number of cloud native developer using container and serverless technology.&lt;/li>
&lt;li>&lt;a href="https://www.kubernetes.dev">Kubernetes.dev&lt;/a>, a Kubernetes contributor focused website has been launched. It brings the contributor documentation, resources and project event information into one central location.&lt;/li>
&lt;/ul>
&lt;h2 id="project-velocity">Project Velocity&lt;/h2>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">Kubernetes DevStats dashboard&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. If you want to gather numbers, facts and figures from Kubernetes and the CNCF community it is the best place to start.&lt;/p>
&lt;p>During this release cycle from April till August, 382 different companies and over 2,464 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&amp;amp;from=1585692000000&amp;amp;to=1598392799000">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming release webinar&lt;/h2>
&lt;p>Join the members of the Kubernetes 1.19 release team on September 25th, 2020 to learn about the major features in this release including storage capacity tracking, structured logging, Ingress V1 GA, and many more. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-19/">https://www.cncf.io/webinars/kubernetes-1-19/&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our monthly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Moving Forward From Beta</title><link>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Tim Bannister, The Scale Factory&lt;/p>
&lt;p>In Kubernetes, features follow a defined
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">lifecycle&lt;/a>.
First, as the twinkle of an eye in an interested developer. Maybe, then,
sketched in online discussions, drawn on the online equivalent of a cafe
napkin. This rough work typically becomes a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#kubernetes-enhancement-proposal-process">Kubernetes Enhancement Proposal&lt;/a> (KEP), and
from there it usually turns into code.&lt;/p>
&lt;p>For Kubernetes v1.20 and onwards, we're focusing on helping that code
graduate into stable features.&lt;/p>
&lt;p>That lifecycle I mentioned runs as follows:&lt;/p>
&lt;p>&lt;img src="feature_stages.svg" alt="Alpha → Beta → General Availability">&lt;/p>
&lt;p>Usually, alpha features aren't enabled by default. You turn them on by setting a feature
gate; usually, by setting a command line flag on each of the components that use the
feature.&lt;/p>
&lt;p>(If you use Kubernetes through a managed service offering such as AKS, EKS, GKE, etc then
the vendor who runs that service may have decided what feature gates are enabled for you).&lt;/p>
&lt;p>There's a defined process for graduating an existing, alpha feature into the beta phase.
This is important because &lt;strong>beta features are enabled by default&lt;/strong>, with the feature flag still
there so cluster operators can opt out if they want.&lt;/p>
&lt;p>A similar but more thorough set of graduation criteria govern the transition to general
availability (GA), also known as &amp;quot;stable&amp;quot;. GA features are part of Kubernetes, with a
commitment that they are staying in place throughout the current major version.&lt;/p>
&lt;p>Having beta features on by default lets Kubernetes and its contributors get valuable
real-world feedback. However, there's a mismatch of incentives. Once a feature is enabled
by default, people will use it. Even if there might be a few details to shake out,
the way Kubernetes' REST APIs and conventions work mean that any future stable API is going
to be compatible with the most recent beta API: your API objects won't stop working when
a beta feature graduates to GA.&lt;/p>
&lt;p>For the API and its resources in particular, there's a much less strong incentive to move
features from beta to GA than from alpha to beta. Vendors who want a particular feature
have had good reason to help get code to the point where features are enabled by default,
and beyond that the journey has been less clear.&lt;/p>
&lt;p>KEPs track more than code improvements. Essentially, anything that would need
communicating to the wider community merits a KEP. That said, most KEPs cover
Kubernetes features (and the code to implement them).&lt;/p>
&lt;p>You might know that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
has been in Kubernetes for a while, but did you realize that it actually went beta in 2015? To help
drive things forward, Kubernetes' Architecture Special Interest Group (SIG) have a new approach in
mind.&lt;/p>
&lt;h2 id="avoiding-permanent-beta">Avoiding permanent beta&lt;/h2>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts a countdown.
The beta-quality API now has &lt;strong>three releases&lt;/strong> (about nine calendar months) to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (&lt;em>and deprecate the previous beta&lt;/em>).&lt;/li>
&lt;/ul>
&lt;p>To be clear, at this point &lt;strong>only REST APIs are affected&lt;/strong>. For example, &lt;em>APIListChunking&lt;/em> is
a beta feature but isn't itself a REST API. Right now there are no plans to automatically
deprecate &lt;em>APIListChunking&lt;/em> nor any other features that aren't REST APIs.&lt;/p>
&lt;p>If a beta API has not graduated to GA after three Kubernetes releases, then the
next Kubernetes release will deprecate that API version. There's no option for
the REST API to stay at the same beta version beyond the first Kubernetes
release to come out after the release window.&lt;/p>
&lt;h3 id="what-this-means-for-you">What this means for you&lt;/h3>
&lt;p>If you're using Kubernetes, there's a good chance that you're using a beta feature. Like
I said, there are lots of them about.
As well as Ingress, you might be using &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a>,
or &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a>, or others.
There's an even bigger chance that you're running on a control plane with at least one beta
feature enabled.&lt;/p>
&lt;p>If you're using or generating Kubernetes manifests that use beta APIs like Ingress, you'll
need to plan to revise those. The current APIs are going to be deprecated following a
schedule (the 9 months I mentioned earlier) and after a further 9 months those deprecated
APIs will be removed. At that point, to stay current with Kubernetes, you should already
have migrated.&lt;/p>
&lt;h3 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h3>
&lt;p>The motivation here seems pretty clear: get features stable. Guaranteeing that beta
features will be deprecated adds a pretty big incentive so that people who want the
feature continue their effort until the code, documentation and tests are ready for this
feature to graduate to stable, backed by several Kubernetes' releases of evidence in
real-world use.&lt;/p>
&lt;h3 id="what-this-means-for-the-ecosystem">What this means for the ecosystem&lt;/h3>
&lt;p>In my opinion, these harsh-seeming measures make a lot of sense, and are going to be
good for Kubernetes. Deprecating existing APIs, through a rule that applies across all
the different Special Interest Groups (SIGs), helps avoid stagnation and encourages
fixes.&lt;/p>
&lt;p>Let's say that an API goes to beta and then real-world experience shows that it
just isn't right - that, fundamentally, the API has shortcomings. With that 9 month
countdown ticking, the people involved have the means and the justification to revise
and release an API that deals with the problem cases. Anyone who wants to live with
the deprecated API is welcome to - Kubernetes is open source - but their needs do not
have to hold up progress on the feature.&lt;/p></description></item><item><title>Blog: Introducing Hierarchical Namespaces</title><link>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adrian Ludwin (Google)&lt;/p>
&lt;p>Safely hosting large numbers of users on a single Kubernetes cluster has always
been a troublesome task. One key reason for this is that different organizations
use Kubernetes in different ways, and so no one tenancy model is likely to suit
everyone. Instead, Kubernetes offers you building blocks to create your own
tenancy solution, such as Role Based Access Control (RBAC) and NetworkPolicies;
the better these building blocks, the easier it is to safely build a multitenant
cluster.&lt;/p>
&lt;h1 id="namespaces-for-tenancy">Namespaces for tenancy&lt;/h1>
&lt;p>By far the most important of these building blocks is the namespace, which forms
the backbone of almost all Kubernetes control plane security and sharing
policies. For example, RBAC, NetworkPolicies and ResourceQuotas all respect
namespaces by default, and objects such as Secrets, ServiceAccounts and
Ingresses are freely usable &lt;em>within&lt;/em> any one namespace, but fully segregated
from &lt;em>other&lt;/em> namespaces.&lt;/p>
&lt;p>Namespaces have two key properties that make them ideal for policy enforcement.
Firstly, they can be used to &lt;strong>represent ownership&lt;/strong>. Most Kubernetes objects
&lt;em>must&lt;/em> be in a namespace, so if you use namespaces to represent ownership, you
can always count on there being an owner.&lt;/p>
&lt;p>Secondly, namespaces have &lt;strong>authorized creation and use&lt;/strong>. Only
highly-privileged users can create namespaces, and other users require explicit
permission to use those namespaces - that is, create, view or modify objects in
those namespaces. This allows them to be carefully created with appropriate
policies, before unprivileged users can create “regular” objects like pods and
services.&lt;/p>
&lt;h1 id="the-limits-of-namespaces">The limits of namespaces&lt;/h1>
&lt;p>However, in practice, namespaces are not flexible enough to meet some common use
cases. For example, let’s say that one team owns several microservices with
different secrets and quotas. Ideally, they should place these services into
different namespaces in order to isolate them from each other, but this presents
two problems.&lt;/p>
&lt;p>Firstly, these namespaces have no common concept of ownership, even though
they’re both owned by the same team. This means that if the team controls
multiple namespaces, not only does Kubernetes not have any record of their
common owner, but namespaced-scoped policies cannot be applied uniformly across
them.&lt;/p>
&lt;p>Secondly, teams generally work best if they can operate autonomously, but since
namespace creation is highly privileged, it’s unlikely that any member of the
dev team is allowed to create namespaces. This means that whenever a team wants
a new namespace, they must raise a ticket to the cluster administrator. While
this is probably acceptable for small organizations, it generates unnecessary
toil as the organization grows.&lt;/p>
&lt;h1 id="introducing-hierarchical-namespaces">Introducing hierarchical namespaces&lt;/h1>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic">Hierarchical
namespaces&lt;/a>
are a new concept developed by the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Working Group for Multi-Tenancy
(wg-multitenancy)&lt;/a> in order to
solve these problems. In its simplest form, a hierarchical namespace is a
regular Kubernetes namespace that contains a small custom resource that
identifies a single, optional, parent namespace. This establishes the concept of
ownership &lt;em>across&lt;/em> namespaces, not just &lt;em>within&lt;/em> them.&lt;/p>
&lt;p>This concept of ownership enables two additional types of behaviours:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Policy inheritance:&lt;/strong> if one namespace is a child of another, policy objects
such as RBAC RoleBindings are &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-propagation">copied from the parent to the
child&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Delegated creation:&lt;/strong> you usually need cluster-level privileges to create a
namespace, but hierarchical namespaces adds an alternative:
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-subns">&lt;em>subnamespaces&lt;/em>&lt;/a>,
which can be manipulated using only limited permissions in the parent
namespace.&lt;/li>
&lt;/ul>
&lt;p>This solves both of the problems for our dev team. The cluster administrator can
create a single “root” namespace for the team, along with all necessary
policies, and then delegate permission to create subnamespaces to members of
that team. Those team members can then create subnamespaces for their own use,
without violating the policies that were imposed by the cluster administrators.&lt;/p>
&lt;h1 id="hands-on-with-hierarchical-namespaces">Hands-on with hierarchical namespaces&lt;/h1>
&lt;p>Hierarchical namespaces are provided by a Kubernetes extension known as the
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">&lt;strong>Hierarchical Namespace
Controller&lt;/strong>&lt;/a>,
or &lt;strong>HNC&lt;/strong>. The HNC consists of two components:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>manager&lt;/strong> runs on your cluster, manages subnamespaces, propagates policy
objects, ensures that your hierarchies are legal and manages extension points.&lt;/li>
&lt;li>The &lt;strong>kubectl plugin&lt;/strong>, called &lt;code>kubectl-hns&lt;/code>, makes it easy for users to
interact with the manager.&lt;/li>
&lt;/ul>
&lt;p>Both can be easily installed from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases">releases page of our
repo&lt;/a>.&lt;/p>
&lt;p>Let’s see HNC in action. Imagine that I do not have namespace creation
privileges, but I can view the namespace &lt;code>team-a&lt;/code> and create subnamespaces
within it&lt;sup>&lt;a href="#note-1">1&lt;/a>&lt;/sup>. Using the plugin, I can now say:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns create svc1-team-a -n team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This creates a subnamespace called &lt;code>svc1-team-a&lt;/code>. Note that since subnamespaces
are just regular Kubernetes namespaces, all subnamespace names must still be
unique.&lt;/p>
&lt;p>I can view the structure of these namespaces by asking for a tree view:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns tree team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
team-a
└── svc1-team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And if there were any policies in the parent namespace, these now appear in the
child as well&lt;sup>&lt;a href="#note-2">2&lt;/a>&lt;/sup>. For example, let’s say that &lt;code>team-a&lt;/code> had
an RBAC RoleBinding called &lt;code>sres&lt;/code>. This rolebinding will also be present in the
subnamespace:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe rolebinding sres -n svc1-team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
Name: sres
Labels: hnc.x-k8s.io/inheritedFrom&lt;span style="color:#666">=&lt;/span>team-a &lt;span style="color:#080;font-style:italic"># inserted by HNC&lt;/span>
Annotations: &amp;lt;none&amp;gt;
Role:
Kind: ClusterRole
Name: admin
Subjects: ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, HNC adds labels to these namespaces with useful information about the
hierarchy which you can use to apply other policies. For example, you can create
the following NetworkPolicy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;team-a.tree.hnc.x-k8s.io/depth&amp;#39;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Label created by HNC&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Exists&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This policy will both be propagated to all descendants of &lt;code>team-a&lt;/code>, and will
&lt;em>also&lt;/em> allow ingress traffic between all of those namespaces. The “tree” label
can only be applied by HNC, and is guaranteed to reflect the latest hierarchy.&lt;/p>
&lt;p>You can learn all about the features of HNC from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc/docs/user-guide">user
guide&lt;/a>.&lt;/p>
&lt;h1 id="next-steps-and-getting-involved">Next steps and getting involved&lt;/h1>
&lt;p>If you think that hierarchical namespaces can work for your organization, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases/tag/hnc-v0.5.1">HNC
v0.5.1 is available on
GitHub&lt;/a>.
We’d love to know what you think of it, what problems you’re using it to solve
and what features you’d most like to see added. As with all early software, you
should be cautious about using HNC in production environments, but the more
feedback we get, the sooner we’ll be able to drive to HNC 1.0.&lt;/p>
&lt;p>We’re also open to additional contributors, whether it’s to fix or report bugs,
or help prototype new features such as exceptions, improved monitoring,
hierarchical resource quotas or fine-grained configuration.&lt;/p>
&lt;p>Please get in touch with us via our
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">repo&lt;/a>, &lt;a href="https://groups.google.com/g/kubernetes-wg-multitenancy">mailing
list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-multitenancy">Slack&lt;/a> - we look forward
to hearing from you!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/aludwin">Adrian Ludwin&lt;/a> is a software engineer and the
tech lead for the Hierarchical Namespace Controller.&lt;/em>&lt;/p>
&lt;a name="note-1"/>
&lt;p>&lt;em>Note 1: technically, you create a small object called a &amp;quot;subnamespace anchor&amp;quot;
in the parent namespace, and then HNC creates the subnamespace for you.&lt;/em>&lt;/p>
&lt;a name="note-2"/>
&lt;p>&lt;em>Note 2: By default, only RBAC Roles and RoleBindings are propagated, but you
can configure HNC to propagate any namespaced Kubernetes object.&lt;/em>&lt;/p></description></item><item><title>Blog: Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</title><link>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The start of the COVID-19 pandemic couldn't delay the release of Kubernetes 1.18, but unfortunately &lt;a href="https://github.com/kubernetes/utils/issues/141">a small bug&lt;/a> could — thankfully only by a day. This was the last cat that needed to be herded by 1.18 release lead &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> before the &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">release on March 25&lt;/a>.&lt;/p>
&lt;p>One of the best parts about co-hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is the conversations we have with the people who help bring Kubernetes releases together. &lt;a href="https://kubernetespodcast.com/episode/096-kubernetes-1.18/">Jorge was our guest on episode 96&lt;/a> back in March, and &lt;a href="https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/">just like last week&lt;/a> we are delighted to bring you the transcript of this interview.&lt;/p>
&lt;p>If you'd rather enjoy the &amp;quot;audiobook version&amp;quot;, including another interview when 1.19 is released later this month, &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe to the show&lt;/a> wherever you get your podcasts.&lt;/p>
&lt;p>In the last few weeks, we've talked to long-time Kubernetes contributors and SIG leads &lt;a href="https://kubernetespodcast.com/episode/114-scheduling/">David Oppenheimer&lt;/a>, &lt;a href="https://kubernetespodcast.com/episode/113-instrumentation-and-cadvisor/">David Ashpole&lt;/a> and &lt;a href="https://kubernetespodcast.com/episode/111-scalability/">Wojciech Tyczynski&lt;/a>. All are worth taking the dog for a longer walk to listen to!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You're a former physicist. I have to ask, what kind of physics did you work on?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Back in my days of math and all that, I used to work in &lt;a href="https://en.wikipedia.org/wiki/Computational_biology">computational biology&lt;/a> and a little bit of high energy physics. Computational biology was, for the most part, what I spent most of my time on. And it was essentially exploring the big idea of we have the structure of proteins. We know what they're made of. Now, based on that structure, we want to be able to predict &lt;a href="https://en.wikipedia.org/wiki/Protein_folding">how they're going to fold&lt;/a> and how they're going to behave, which essentially translates into the whole idea of designing pharmaceuticals, designing vaccines, or anything that you can possibly think of that has any connection whatsoever to a living organism.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That would seem to ladder itself well into maybe going to something like bioinformatics. Did you take a tour into that, or did you decide to go elsewhere directly?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It is related, and I worked a little bit with some people that did focus on bioinformatics on the field specifically, but I never took a detour into it. Really, my big idea with computational biology, to be honest, it wasn't even the biology. That's usually what sells it, what people are really interested in, because protein engineering, all the cool and amazing things that you can do.&lt;/p>
&lt;p>Which is definitely good, and I don't want to take away from it. But my big thing is because biology is such a real thing, it is amazingly complicated. And the math— the models that you have to design to study those systems, to be able to predict something that people can actually experiment and measure, it just captivated me. The level of complexity, the beauty, the mechanisms, all the structures that you see once you got through the math and look at things, it just kind of got to me.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How did you go from that world into the world of Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: That's both a really boring story and an interesting one.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>I did my thing with physics, and it was good. It was fun. But at some point, I wanted— working in academia— at least my feeling for it is that generally all the people that you're surrounded with are usually academics. Just another bunch of physics, a bunch of mathematicians.&lt;/p>
&lt;p>But very seldom do you actually get the opportunity to take what you're working on and give it to someone else to use. Even with the mathematicians and physicists, the things that we're working on are super specialized, and you can probably find three, four, five people that can actually understand everything that you're saying. A lot of people are going to get the gist of it, but understanding the details, it's somewhat rare.&lt;/p>
&lt;p>One of the things that I absolutely love about tech, about software engineering, coding, all that, is how open and transparent everything is. You can write your library in Python, you can publish it, and suddenly the world is going to actually use it, actually consume it. And because normally, I've seen that it has a large avenue where you can work in something really complicated, you can communicate it, and people can actually go ahead and take it and run with it in their given direction. And that is kind of what happened.&lt;/p>
&lt;p>At some point, by pure accident and chance, I came across this group of people on the internet, and they were in the stages of making up this new group that's called &lt;a href="https://datafordemocracy.org/">Data for Democracy&lt;/a>, a non-profit. And the whole idea was the internet, especially Twitter— that's how we congregated— Twitter, the internet. We have a ton of data scientists, people who work as software engineers, and the like. What if we all come together and try to solve some issues that actually affect the daily lives of people. And there were a ton of projects. Helping the ACLU gather data for something interesting that they were doing, gather data and analyze it for local governments— where do you have potholes, how much water is being consumed.&lt;/p>
&lt;p>Try to apply all the science that we knew, combined with all the code that we could write, and offer a good and digestible idea for people to say, OK, this makes sense, let's do something about it— policy, action, whatever. And I started working with this group, Data for Democracy— wonderful set of people. And the person who I believe we can blame for Data for Democracy— the one who got the idea and got it up and running, his name is Jonathan Morgan. And eventually, we got to work together. He started a startup, and I went to work with the startup. And that was essentially the thing that took me away from physics and into the world of software engineering— Data for Democracy, definitely.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Were you using Kubernetes as part of that work there?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: No, it was simple as it gets. You just try to get some data. You create a couple &lt;a href="https://ipython.org/">IPython notebooks&lt;/a>, some setting up of really simple MySQL databases, and that was it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Where did you get started using Kubernetes? And was it before you started contributing to it and being a part, or did you decide to jump right in?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: When I first started using Kubernetes, it was also on my first job. So there wasn't a lot of specific training in regards to software engineering or anything of the sort that I did before I actually started working as a software engineer. I just went from physicist to engineer. And in my days of physics, at least on the computer side, I was completely trained in the super old school system administrator, where you have your 10, 20 computers. You know physically where they are, and you have to connect the cables.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: All pets— all pets all the time.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] You have to have your huge Python, bash scripts, three, five major versions, all because doing an upgrade will break something really important and you have no idea how to work on it. And that was my training. That was the way that I learned how to do things. Those were the kind of things that I knew how to do.&lt;/p>
&lt;p>And when I got to this company— startup— we were pretty much starting from scratch. We were building a couple applications. We work testing them, we were deploying them on a couple of managed instances. But like everything, there was a lot of toil that we wanted to automate. The whole issue of, OK, after days of work, we finally managed to get this version of the application up and running in these machines.&lt;/p>
&lt;p>It's open to the internet. People can test it out. But it turns out that it is now two weeks behind the latest on all the master branches for this repo, so now we want to update. And we have to go through the process of bringing it back up, creating new machines, do that whole thing. And I had no idea what Kubernetes was, to be honest. My boss at the moment mentioned it to me like, hey, we should use Kubernetes because apparently, Kubernetes is something that might be able to help us here. And we did some— I want to call it research and development.&lt;/p>
&lt;p>It was actually just making— again, startup, small company, small team, so really me just playing around with Kubernetes trying to get it to work, trying to get it to run. I was so lost. I had no idea what I was doing— not enough. I didn't have an idea of how Kubernetes was supposed to help me. And at that point, I did the best Googling that I could manage. Didn't really find a lot of examples. Didn't find a lot of blog posts. It was early.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What time frame was this?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Three, four years ago, so definitely not 1.13. That's the best guesstimate that I can give at this point. But I wasn't able to find any good examples, any tutorials. The only book that I was able to get my hands on was the one written by Joe Beda, Kelsey Hightower, and I forget the other author. But what is it? &amp;quot;&lt;a href="%5D(http://shop.oreilly.com/product/0636920223788.do)">Kubernetes— Up and Running&lt;/a>&amp;quot;?&lt;/p>
&lt;p>And in general, right now I use it as reference— it's really good. But as a beginner, I still was lost. They give all these amazing examples, they provide the applications, but I had no idea why someone might need a Pod, why someone might need a Deployment. So my last resort was to try and find someone who actually knew Kubernetes.&lt;/p>
&lt;p>By accident, during my eternal Googling, I actually found a link to the &lt;a href="http://slack.kubernetes.io/">Kubernetes Slack&lt;/a>. I jumped into the Kubernetes Slack hoping that someone might be able to help me out. And that was my entry point into the Kubernetes community. I just kept on exploring the Slack, tried to see what people were talking about, what they were asking to try to make sense of it, and just kept on iterating. And at some point, I think I got the hang of it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What made you decide to be a release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The answer to this is my answer to why I have been contributing to Kubernetes. I really just want to be able to help out the community. Kubernetes is something that I absolutely adore.&lt;/p>
&lt;p>Comparing Kubernetes to old school system administration, a handful of years ago, it took me like a week to create a node for an application to run. It took me months to get something that vaguely looked like an Ingress resource— just setting up the Nginx, and allowing someone else to actually use my application. And the fact that I could do all of that in five minutes, it really captivated me. Plus I've got to blame it on the physics. The whole idea with physics, I really like the patterns, and I really like the design of Kubernetes.&lt;/p>
&lt;p>Once I actually got the hang of it, I loved the idea of how everything was designed, and I just wanted to learn a lot more about it. And I wanted to help the contributors. I wanted to help the people who actually build it. I wanted to help maintain it, and help provide the information for new contributors or new users. So instead of taking months for them to be up and running, let's just chat about what your issue is, and let's try to get a fix within the next hour or so.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: You work for a stealth startup right now. Is it fair to assume that they're using Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yes—&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>—for everything.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Are you able to say what &lt;a href="https://www.searchable.ai/">Searchable&lt;/a> does?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The thing that we are trying to build is kind of like a search engine for your documents. Usually, if people have a question, they jump on Google. And for the most part, you're going to be able to get a good answer. You can ask something really random, like 'what is the weight of an elephant?'&lt;/p>
&lt;p>Which, if you think about it, it's kind of random, but Google is going to give you an answer. And the thing that we are trying to build is something similar to that, but for files. So essentially, a search engine for your files. And most people, you have your local machine loaded up with— at least mine, I have a couple tens of gigabytes of different files.&lt;/p>
&lt;p>I have Google Drive. I have a lot of documents that live in my email and the like. So the idea is to kind of build a search engine that is going to be able to connect all of those pieces. And besides doing simple word searches— for example, 'Kubernetes interview', and bring me the documents that we're looking at with all the questions— I can also ask things like what issue did I find last week while testing Prometheus. And it's going to be able to read my files, like through natural language processing, understand it, and be able to give me an answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: It is a Google for your personal and non-public information, essentially?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Hopefully.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is the work that you do with Kubernetes as the release lead— is that part of your day job, or is that something that you're doing kind of nights and weekends separate from your day job?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Both. Strictly speaking, my day job is just keep working on the application, build the things that it needs, maintain the infrastructure, and all that. When I started working at the company— which by the way, the person who brought me into the company was also someone that I met from my days in Data for Democracy— we started talking about the work.&lt;/p>
&lt;p>I mentioned that I do a lot of work with the Kubernetes community and if it was OK that I continue doing it. And to my surprise, the answer was not only a yes, but yeah, you can do it during your day work. And at least for the time being, I just balance— I try to keep things organized.&lt;/p>
&lt;p>Some days I just focus on Kubernetes. Some mornings I do Kubernetes. And then afternoon, I do Searchable, vice-versa, or just go back and forth, and try to balance the work as much as possible. But being release lead, definitely, it is a lot, so nights and weekends.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How much time does it take to be the release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It varies, but probably, if I had to give an estimate, at the very least you have to be able to dedicate four hours most days.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Four hours a day?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, most days. It varies a lot. For example, at the beginning of the release cycle, you don't need to put in that much work because essentially, you're just waiting and helping people get set up, and people are writing their &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposals&lt;/a>, they are implementing it, and you can answer some questions. It's relatively easy, but for the most part, a lot of the time the four hours go into talking with people, just making sure that, hey, are people actually writing their enhancements, do we have all the enhancements that we want. And most of those fours hours, going around, chatting with people, and making sure that things are being done. And if, for some reason, someone needs help, just directing them to the right place to get their answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What does Searchable get out of you doing this work?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Physically, nothing. The thing that we're striving for is to give back to the community. My manager/boss/homeslice— I told him I was going to call him my homeslice— both of us have experience working in open source. At some point, he was also working on a project that I'm probably going to mispronounce, but Mahout with Apache.&lt;/p>
&lt;p>And he also has had this experience. And both of us have this general idea and strive to build something for Searchable that's going to be useful for people, but also build knowledge, build guides, build applications that are going to be useful for the community. And at least one of the things that I was able to do right now is be the lead for the Kubernetes team. And this is a way of giving back to the community. We're using Kubernetes to run our things, so let's try to balance how things work.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Lachlan Evenson was the release lead on 1.16 as well as &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">our guest back in episode 72&lt;/a>, and he's returned on this release as the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks/emeritus-adviser">emeritus advisor&lt;/a>. What did you learn from him?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Oh, everything. And it actually all started back on 1.16. So like you said, an amazing person— he's an amazing individual. And it's truly an opportunity to be able to work with him. During 1.16, I was the CI Signal lead, and Lachie is very hands on.&lt;/p>
&lt;p>He's not the kind of person to just give you a list of things and say, do them. He actually comes to you, has a conversation, and he works with you more than anything. And when we were working together on 1.16, I got to learn a lot from him in terms of CI Signal. And especially because we talked about everything just to make sure that 1.16 was ready to go, I also got to pick up a couple of things that a release lead has to know, has to be able to do, has to work on to get a release out the door.&lt;/p>
&lt;p>And now, during this release, there is a lot of information that's really useful, and there's a lot of advice and general wisdom that comes in handy. For most of the things that impact a lot of things, we are always in communication. Like, I'm doing this, you're doing that, advice. And essentially, every single thing that we do is pretty much a code review. You do it, and then you wait for someone else to give you comments. And that's been a strong part of our relationship working.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would you say the theme for this release is?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I think one of the themes is &amp;quot;fit and finish&amp;quot;. There are a lot of features that we are bumping from alpha to beta, from beta to stable. And we want to make sure that people have a good user experience. Operators and developers alike just want to get rid of as many bugs as possible, improve the flow of things.&lt;/p>
&lt;p>But the other really cool thing is we have about an equal distribution between alpha, beta, and stable. We are also bringing up a lot of new features. So besides making Kubernetes more stable for all the users that are already using it, we are working on bringing up new things that people can try out for the next release and see how it goes in the future.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did you have a release team mascot?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Kind of.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Who/what was it?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] I say kind of because I'm using the mascot in the &lt;a href="https://twitter.com/KubernetesPod/status/1242953121380392963">logo&lt;/a>, and the logo is inspired by the Large Hadron Collider.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, fantastic.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Being the release lead, I really had to take a chance on this opportunity to use the LHC as the mascot.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We've had &lt;a href="https://kubernetespodcast.com/episode/062-cern/">some of the folks from the LHC on the show&lt;/a>, and I know they listen, and they will be thrilled with that.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] Hopefully, they like the logo.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you look at this release, what part of this release, what thing that has been added to it are you personally most excited about?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Like a parent can't choose which child is his or her favorite, you really can't choose a specific thing.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We have been following online and in the issues an enhancement that's called &lt;a href="https://github.com/kubernetes/enhancements/issues/753">sidecar containers&lt;/a>. You'd be able to mark the order of containers starting in a pod. Tim Hockin posted &lt;a href="https://github.com/kubernetes/enhancements/issues/753#issuecomment-597372056">a long comment on behalf of a number of SIG Node contributors&lt;/a> citing social, procedural, and technical concerns about what's going on with that— in particular, that it moved out of 1.18 and is now moving to 1.19. Did you have any thoughts on that?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The sidecar enhancement has definitely been an interesting one. First off, thank you very much to Joseph Irving, the author of the KEP. And thank you very much to Tim Hockin, who voiced out the point of view of the approvers, maintainers of SIG Node. And I guess a little bit of context before we move on is, in the Kubernetes community, we have contributors, we have reviewers, and we have approvers.&lt;/p>
&lt;p>Contributors are people who write PRs, who file issues, who troubleshoot issues. Reviewers are contributors who focus on one or multiple specific areas within the project, and then approvers are maintainers for the specific area, for one or multiple specific areas, of the project. So you can think of approvers as people who have write access in a repo or someplace within a repo.&lt;/p>
&lt;p>The issue with the sidecar enhancement is that it has been deferred for multiple releases now, and that's been because there hasn't been a lot of collaboration between the KEP authors and the approvers for specific parts of the project. Something worthwhile to mention— and this was brought up during the original discussion— is this can obviously be frustrating for both contributors and for approvers. From the contributor's side of things, you are working on something. You are doing your best to make sure that it works.&lt;/p>
&lt;p>And to build something that's going to be used by people, both from the approver side of things and, I think, for the most part, every single person in the Kubernetes community, we are all really excited to see this project grow. We want to help improve it, and we love when new people come in and work on new enhancements, bug fixes, and the like.&lt;/p>
&lt;p>But one of the limitations is the day only has so many hours, and there are only so many things that we can work on at a time. So people prioritize in whatever way works best, and some things just fall behind. And a lot of the time, the things that fall behind are not because people don't want them to continue moving forward, but it's just a limited amount of resources, a limited amount of people.&lt;/p>
&lt;p>And I think this discussion around the sidecar enhancement proposal has been very useful, and it points us to the need for more standardized mentoring programs. This is something that multiple SIGs are working on. For example, SIG Contribex, SIG Cluster Lifecycle, SIG Release. The idea is to standardize some sort of mentoring experience so that we can better prepare new contributors to become reviewers and ultimately approvers.&lt;/p>
&lt;p>Because ultimately at the end of the day, if we have more people who are knowledgeable about Kubernetes, or even some specific area of Kubernetes, we can better distribute the load, and we can better collaborate on whatever new things come up. I think the sidecar enhancement has shown us mentoring is something worthwhile, and we need a lot more of it. Because as much work as we do, more things are going to continue popping in throughout the project. And the more people we have who are comfortable working in these really complicated areas of Kubernetes, the better off that we are going to be.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Was there any talk of delaying 1.18 due to the current worldwide health situation?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: We thought about it, and the plan was to just wait and see how people felt. Tried make sure that people were comfortable continuing to work and all the people were landing in new enhancements, or fixing tests, or members of the release team who were making sure that things were happening. We wanted to see that people were comfortable, that they could continue doing their job. And for a moment, I actually thought about delaying just outright— we're going to give it more time, and hopefully at some point, things are going to work out.&lt;/p>
&lt;p>But people just continue doing their amazing work. There was no delay. There was no hitch throughout the process. So at some point, I just figured we stay with the current timeline and see how we went. And at this point, things are more or less set.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Amazing power of a distributed team.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, definitely.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: &lt;a href="https://twitter.com/alejandrox135/status/1239629281766096898">Taylor Dolezal was announced as the 1.19 release lead&lt;/a>. Do you know how that choice was made, and by whom?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I actually got to choose the lead. The practice is the current lead for the release team is going to look at people and see, first off, who's interested and out of the people interested, who can do the job, who's comfortable enough with the release team, with the Kubernetes community at large who can actually commit the amount of hours throughout the next, hopefully, three months.&lt;/p>
&lt;p>And for one, I think Taylor has been part of my team. So there is the release team. Then the release team has multiple subgroups. One of those subgroups is actually just for me and my shadows. So for this release, it was mrbobbytables and Taylor. And Taylor volunteered to take over 1.19, and I'm sure that he will do an amazing job.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I am as well. What advice will you give Taylor?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Over-communicate as much as possible. Normally, if you made it to the point that you are the lead for a release, or even the shadow for a release, you more or less are familiar with a lot of the work— CI Signal, enhancements, documentation, and the like. And a lot of people, if they know how to do their job, they might tell themselves, yeah, I could do it— no need to worry about it. I'm just going to go ahead and sign this PR, debug this test, whatever.&lt;/p>
&lt;p>But one of the interesting aspects is whenever we are actually working in a release, 50% of the work has to go into actually making the release happen. The other 50% of the work has to go into mentoring people, and making sure the newcomers, new members are able to learn everything that they need to learn to do your job, you being in the lead for a subgroup or the entire team. And whenever you actually see that things need to happen, just over-communicate.&lt;/p>
&lt;p>Try to provide the opportunity for someone else to do the work, and over-communicate with them as much as possible to make sure that they are learning whatever it is that they need to learn. If neither you or the other person knows what's going on, then I can over-communicate, so someone hopefully will see your messages and come to the rescue. That happens a lot. There's a lot of really nice and kind people who will come out and tell you how something works, help you fix it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you were to sum up your experience running this release, what would it be?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It's been super fun and a little bit stressing, to be honest. Being the release lead is definitely amazing. You're kind of sitting at the center of Kubernetes.&lt;/p>
&lt;p>You not only see the people who are working on things— the things that are broken, and the users filling out issues, and saying what broke, and the like. But you also get the opportunity to work with a lot of people who do a lot of non-code related work. Docs is one of the most obvious things. There's a lot of work that goes into communications, contributor experience, public relations.&lt;/p>
&lt;p>And being connected, getting to talk with those people mostly every other day, it's really fun. It's a really good experience in terms of becoming a better contributor to the community, but also taking some of that knowledge home with you and applying it somewhere else. If you are a software engineer, if you are a project manager, whatever, it's amazing how much you can learn.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I know the community likes to rotate around who are the release leads. But if you were given the opportunity to be a release lead for a future release of Kubernetes, would you do it again?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, it's a fun job. To be honest, it can be really stressing. Especially, as I mentioned, at some point, most of that work is just going to be talking with people, and talking requires a lot more thought and effort than just sitting down and thinking about things sometimes. And some of that can be really stressful.&lt;/p>
&lt;p>But the job itself, it is definitely fun. And at some distant point in the future, if for some reason it was a possibility, I will think about it. But definitely, as you mentioned, one thing that we try to do is cycle out, because I can have fun in it, and that's all good and nice. And hopefully I can help another release go out the door. But providing the opportunity for other people to learn I think is a lot more important than just being the lead itself.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> is a site reliability engineer with Searchable AI and served as the Kubernetes 1.18 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Music and math: the Kubernetes 1.17 release interview</title><link>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adam Glick (Google)&lt;/p>
&lt;p>Every time the Kubernetes release train stops at the station, we like to ask the release lead to take a moment to reflect on their experience. That takes the form of an interview on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> that I co-host with &lt;a href="https://twitter.com/craigbox">Craig Box&lt;/a>. If you're not familiar with the show, every week we summarise the new in the Cloud Native ecosystem, and have an insightful discussion with an interesting guest from the broader Kubernetes community.&lt;/p>
&lt;p>At the time of the 1.17 release in December, we &lt;a href="https://kubernetespodcast.com/episode/083-kubernetes-1.17/">talked to release team lead Guinevere Saenger&lt;/a>. We have &lt;a href="https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/">shared&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/">the&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/">transcripts&lt;/a> of previous interviews on the Kubernetes blog, and we're very happy to share another today.&lt;/p>
&lt;p>Next week we will bring you up to date with the story of Kubernetes 1.18, as we gear up for the release of 1.19 next month. &lt;a href="https://kubernetespodcast.com/subscribe/">Subscribe to the show&lt;/a> wherever you get your podcasts to make sure you don't miss that chat!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You have a nontraditional background for someone who works as a software engineer. Can you explain that background?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: My first career was as a &lt;a href="https://en.wikipedia.org/wiki/Collaborative_piano">collaborative pianist&lt;/a>, which is an academic way of saying &amp;quot;piano accompanist&amp;quot;. I was a classically trained pianist who spends most of her time onstage, accompanying other people and making them sound great.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that the piano equivalent of pair-programming?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: No one has said it to me like that before, but all sorts of things are starting to make sense in my head right now. I think that's a really great way of putting it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That's a really interesting background, as someone who also has a background with music. What made you decide to get into software development?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I found myself in a life situation where I needed more stable source of income, and teaching music, and performing for various gig opportunities, was really just not cutting it anymore. And I found myself to be working really, really hard with not much to show for it. I had a lot of friends who were software engineers. I live in Seattle. That's sort of a thing that happens to you when you live in Seattle — you get to know a bunch of software engineers, one way or the other.&lt;/p>
&lt;p>The ones I met were all lovely people, and they said, hey, I'm happy to show you how to program in Python. And so I did that for a bit, and then I heard about this program called &lt;a href="https://adadevelopersacademy.org/">Ada Developers Academy&lt;/a>. That's a year long coding school, targeted at women and non-binary folks that are looking for a second career in tech. And so I applied for that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What can you tell us about that program?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's incredibly selective, for starters. It's really popular in Seattle and has gotten quite a good reputation. It took me three tries to get in. They do two classes a year, and so it was a while before I got my response saying 'congratulations, we are happy to welcome you into Cohort 6'. I think what sets Ada Developers Academy apart from other bootcamp style coding programs are three things, I think? The main important one is that if you get in, you pay no tuition. The entire program is funded by company sponsors.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The other thing that really convinced me is that five months of the 11-month program are an industry internship, which means you get both practical experience, mentorship, and potential job leads at the end of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So very much like a condensed version of the University of Waterloo degree, where you do co-op terms.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Interesting. I didn't know about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having lived in Waterloo for a while, I knew a lot of people who did that. But what would you say the advantages were of going through such a condensed schooling process in computer science?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure that the condensed process is necessarily an advantage. I think it's a necessity, though. People have to quit their jobs to go do this program. It's not an evening school type of thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: And your internship is basically a full-time job when you do it. One thing that Ada was really, really good at is giving us practical experience that directly relates to the workplace. We learned how to use Git. We learned how to design websites using &lt;a href="https://rubyonrails.org/">Rails&lt;/a>. And we also learned how to collaborate, how to pair-program. We had a weekly retrospective, so we sort of got a soft introduction to workflows at a real workplace. Adding to that, the internship, and I think the overall experience is a little bit more 'practical workplace oriented' and a little bit less academic.&lt;/p>
&lt;p>When you're done with it, you don't have to relearn how to be an adult in a working relationship with other people. You come with a set of previous skills. There are Ada graduates who have previously been campaign lawyers, and veterinarians, and nannies, cooks, all sorts of people. And it turns out these skills tend to translate, and they tend to matter.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: With your background in music, what do you think that that allows you to bring to software development that could be missing from, say, standard software development training that people go through?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People tend to really connect the dots when I tell them I used to be a musician. Of course, I still consider myself a musician, because you don't really ever stop being a musician. But they say, 'oh, yeah, music and math', and that's just a similar sort of brain. And that makes so much sense. And I think there's a little bit of a point to that. When you learn a piece of music, you have to start recognizing patterns incredibly quickly, almost intuitively.&lt;/p>
&lt;p>And I think that is the main skill that translates into programming— recognizing patterns, finding the things that work, finding the things that don't work. And for me, especially as a collaborative pianist, it's the communicating with people, the finding out what people really want, where something is going, how to figure out what the general direction is that we want to take, before we start writing the first line of code.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In your experience at Ada or with other experiences you've had, have you been able to identify patterns in other backgrounds for people that you'd recommend, 'hey, you're good at music, so therefore you might want to consider doing something like a course in computer science'?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Overall, I think ultimately writing code is just giving a set of instructions to a computer. And we do that in daily life all the time. We give instructions to our kids, we give instructions to our students. We do math, we write textbooks. We give instructions to a room full of people when you're in court as a lawyer.&lt;/p>
&lt;p>Actually, the entrance exam to Ada Developers Academy used to have questions from the &lt;a href="https://en.wikipedia.org/wiki/Law_School_Admission_Test">LSAT&lt;/a> on it to see if you were qualified to join the program. They changed that when I applied, but I think that's a thing that happened at one point. So, overall, I think software engineering is a much more varied field than we give it credit for, and that there are so many ways in which you can apply your so-called other skills and bring them under the umbrella of software engineering.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do think that programming is effectively half art and half science. There's creativity to be applied. There is perhaps one way to solve a problem most efficiently. But there are many different ways that you can choose to express how you compiled something down to that way.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah, I mean, that's definitely true. I think one way that you could probably prove that is that if you write code at work and you're working on something with other people, you can probably tell which one of your co-workers wrote which package, just by the way it's written, or how it is documented, or how it is styled, or any of those things. I really do think that the human character shines through.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What got you interested in Kubernetes and open source?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The honest answer is absolutely nothing. Going back to my programming school— and remember that I had to do a five-month internship as part of my training— the way that the internship works is that sponsor companies for the program get interns in according to how much they sponsored a specific cohort of students.&lt;/p>
&lt;p>So at the time, Samsung and SDS offered to host two interns for five months on their &lt;a href="https://samsung-cnct.github.io/">Cloud Native Computing team&lt;/a> and have that be their practical experience. So I go out of a Ruby on Rails full stack web development bootcamp and show up at my internship, and they said, &amp;quot;Welcome to Kubernetes. Try to bring up a cluster.&amp;quot; And I said, &amp;quot;Kuber what?&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've all said that on occasion.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Trial by fire, wow.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I will say that that entire team was absolutely wonderful, delightful to work with, incredibly helpful. And I will forever be grateful for all of the help and support that I got in that environment. It was a great place to learn.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You now work on GitHub's Kubernetes infrastructure. Obviously, there was GitHub before there was a Kubernetes, so a migration happened. What can you tell us about the transition that GitHub made to running on Kubernetes?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A disclaimer here— I was not at GitHub at the time that the transition to Kubernetes was made. However, to the best of my knowledge, the decision to transition to Kubernetes was made and people decided, yes, we want to try Kubernetes. We want to use Kubernetes. And mostly, the only decision left was, which one of our applications should we move over to Kubernetes?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I thought GitHub was written on Rails, so there was only one application.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: [LAUGHING] We have a lot of supplementary stuff under the covers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But yes, GitHub is written in Rails. It is still written in Rails. And most of the supplementary things are currently running on Kubernetes. We have a fair bit of stuff that currently does not run on Kubernetes. Mainly, that is GitHub Enterprise related things. I would know less about that because I am on the platform team that helps people use the Kubernetes infrastructure. But back to your question, leadership at the time decided that it would be a good idea to start with GitHub the Rails website as the first project to move to Kubernetes.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: High stakes!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The reason for this was that they decided if they were going to not start big, it really wasn't going to transition ever. It was really not going to happen. So they just decided to go all out, and it was successful, for which I think the lesson would probably be commit early, commit big.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other lessons that you would take away or that you've learned kind of from the transition that the company made, and might be applicable to other people who are looking at moving their companies from a traditional infrastructure to a Kubernetes infrastructure?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure this is a lesson specifically, but I was on support recently, and it turned out that, due to unforeseen circumstances and a mix of human error, a bunch of the namespaces on one of our Kubernetes clusters got deleted.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, my.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It should not have affected any customers, I should mention, at this point. But all in all, it took a few of us a few hours to almost completely recover from this event. I think that, without Kubernetes, this would not have been possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Generally, deleting something like that is quite catastrophic. We've seen a number of other vendors suffer large outages when someone's done something to that effect, which is why we get &lt;a href="https://twitter.com/hashtag/hugops">#hugops&lt;/a> on Twitter all the time.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People did send me #hugops, that is a thing that happened. But overall, something like this was an interesting stress test and sort of proved that it wasn't nearly as catastrophic as a worst case scenario.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: GitHub &lt;a href="https://githubengineering.com/githubs-metal-cloud/">runs its own data centers&lt;/a>. Kubernetes was largely built for running on the cloud, but a lot of people do choose to run it on their own, bare metal. How do you manage clusters and provisioning of the machinery you run?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I started, my onboarding project was to deprovision an old cluster, make sure all the traffic got moved to somewhere where it would keep running, provision a new cluster, and then move website traffic onto the new cluster. That was a really exciting onboarding project. At the time, we provisioned bare metal machines using Puppet. We still do that to a degree, but I believe the team that now runs our computing resources actually inserts virtual machines as an extra layer between the bare metal and the Kubernetes nodes.&lt;/p>
&lt;p>Again, I was not intrinsically part of that decision, but my understanding is that it just makes for a greater reliability and reproducibility across the board. We've had some interesting hardware dependency issues come up, and the virtual machines basically avoid those.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been working with Kubernetes for a couple of years now. How did you get involved in the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I first started in the project, I started at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience#readme">special interest group for contributor experience&lt;/a>, namely because one of my co-workers at the time, Aaron Crickenberger, was a big Kubernetes community person. Still is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've &lt;a href="https://kubernetespodcast.com/episode/046-kubernetes-1.14/">had him on the show&lt;/a> for one of these very release interviews!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: In fact, this is true! So Aaron and I actually go way back to Samsung SDS. Anyway, Aaron suggested that I should write up a contribution to the Kubernetes project, and I said, me? And he said, yes, of course. You will be &lt;a href="https://www.youtube.com/watch?v=TkCDUFR6xqw">speaking at KubeCon&lt;/a>, so you should probably get started with a PR or something. So I tried, and it was really, really hard. And I complained about it &lt;a href="https://github.com/kubernetes/community/issues/141">in a public GitHub issue&lt;/a>, and people said, yeah. Yeah, we know it's hard. Do you want to help with that?&lt;/p>
&lt;p>And so I started getting really involved with the &lt;a href="https://github.com/kubernetes/community/tree/master/contributors/guide">process for new contributors to get started&lt;/a> and have successes, kind of getting a foothold into a project that's as large and varied as Kubernetes. From there on, I began to talk to people, get to know people. The great thing about the Kubernetes community is that there is so much mentorship to go around.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: There are so many friendly people willing to help. It's really funny when I talk to other people about it. They say, what do you mean, your coworker? And I said, well, he's really a colleague. He really works for another company.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: He's sort-of officially a competitor.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But we're friends.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But he totally helped me when I didn't know how to git patch my borked pull request. So that happened. And eventually, somebody just suggested that I start following along in the release process and shadow someone on their release team role. And that, at the time, was Tim Pepper, who was bug triage lead, and I shadowed him for that role.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another &lt;a href="https://kubernetespodcast.com/episode/010-kubernetes-1.11/">podcast guest&lt;/a> on the interview train.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This is a pattern that probably will make more sense once I explain to you about the shadow process of the release team.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Well, let's turn to the Kubernetes release and the release process. First up, what's new in this release of 1.17?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: We have only a very few new things. The one that I'm most excited about is that we have moved &lt;a href="https://github.com/kubernetes/enhancements/issues/563">IPv4 and IPv6 dual stack&lt;/a> support to alpha. That is the most major change, and it has been, I think, a year and a half in coming. So this is the very first cut of that feature, and I'm super excited about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The people who have been promised IPv6 for many, many years and still don't really see it, what will this mean for them?&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: And most importantly, why did we skip IPv5 support?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I don't know!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Please see &lt;a href="https://softwareengineering.stackexchange.com/questions/185380/ipv4-to-ipv6-where-is-ipv5">the appendix to this podcast&lt;/a> for technical explanations.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Having a dual stack configuration obviously enables people to have a much more flexible infrastructure and not have to worry so much about making decisions that will become outdated or that may be over-complicated. This basically means that pods can have dual stack addresses, and nodes can have dual stack addresses. And that basically just makes communication a lot easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What about features that didn't make it into the release? We had a conversation with Lachie in the &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">1.16 interview&lt;/a>, where he mentioned &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md">sidecar containers&lt;/a>. They unfortunately didn't make it into that release. And I see now that they haven't made this one either.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: They have not, and we are actually currently undergoing an effort of tracking features that flip multiple releases.&lt;/p>
&lt;p>As a community, we need everyone's help. There are a lot of features that people want. There is also a lot of cleanup that needs to happen. And we have started talking at previous KubeCons repeatedly about problems with maintainer burnout, reviewer burnout, have a hard time finding reviews for your particular contributions, especially if you are not an entrenched member of the community. And it has become very clear that this is an area where the entire community needs to improve.&lt;/p>
&lt;p>So the unfortunate reality is that sometimes life happens, and people are busy. This is an open source project. This is not something that has company mandated OKRs. Particularly during the fourth quarter of the year in North America, but around the world, we have a lot of holidays. It is the end of the year. Kubecon North America happened as well. This makes it often hard to find a reviewer in time or to rally the support that you need for your enhancement proposal. Unfortunately, slipping releases is fairly common and, at this point, expected. We started out with having 42 enhancements and &lt;a href="https://docs.google.com/spreadsheets/d/1ebKGsYB1TmMnkx86bR2ZDOibm5KWWCs_UjV3Ys71WIs/edit#gid=0">landed with roughly half of that&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was going to ask about the truncated schedule due to the fourth quarter of the year, where there are holidays in large parts of the world. Do you find that the Q4 release on the whole is smaller than others, if not for the fact that it's some week shorter?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Q4 releases are shorter by necessity because we are trying to finish the final release of the year before the end of the year holidays. Often, releases are under pressure of KubeCons, during which finding reviewers or even finding the time to do work can be hard to do, if you are attending. And even if you're not attending, your reviewers might be attending.&lt;/p>
&lt;p>It has been brought up last year to make the final release more of a stability release, meaning no new alpha features. In practice, for this release, this is actually quite close to the truth. We have four features graduating to beta and most of our features are graduating to stable. I am hoping to use this as a precedent to change our process to make the final release a stability release from here on out. The timeline fits. The past experience fits this model.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: On top of all of the release work that was going on, there was also KubeCon that happened. And you were involved in the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/11-contributor-summit">contributor summit&lt;/a>. How was the summit?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This was the first contributor summit where we had an organized events team with events organizing leads, and handbooks, and processes. And I have heard from multiple people— this is just word of mouth— that it was their favorite contributor summit ever.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was someone allocated to hat production? &lt;a href="https://flickr.com/photos/143247548@N03/49093218951/">Everyone had sailor hats&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yes, the entire event staff had sailor hats with their GitHub handle on them, and it was pretty fantastic. You can probably see me wearing one in some of the pictures from the contributor summit. That literally was something that was pulled out of a box the morning of the contributor summit, and no one had any idea. But at first, I was a little skeptical, but then I put it on and looked at myself in the mirror. And I was like, yes. Yes, this is accurate. We should all wear these.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did getting everyone together for the contributor summit help with the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It did not. It did quite the opposite, really. Well, that's too strong.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that just a matter of the time taken up?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's just a completely different focus. Honestly, it helped getting to know people face-to-face that I had currently only interacted with on video. But we did have to cancel the release team meeting the day of the contributor summit because there was kind of no sense in having it happen. We moved it to the Tuesday, I believe.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The role of the release team leader has been described as servant leadership. Do you consider the position proactive or reactive?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Honestly, I think that depends on who's the release team lead, right? There are some people who are very watchful and look for trends, trying to detect problems before they happen. I tend to be in that camp, but I also know that sometimes it's not possible to predict things. There will be last minute bugs sometimes, sometimes not. If there is a last minute bug, you have to be ready to be on top of that. So for me, the approach has been I want to make sure that I have my priorities in order and also that I have backups in case I can't be available.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What was the most interesting part of the release process for you?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A release lead has to have served in other roles on the release team prior to being release team lead. To me, it was very interesting to see what other roles were responsible for, ones that I hadn't seen from the inside before, such as docs, CI signal. I had helped out with CI signal for a bit, but I want to give a big shout out to CI signal lead, Alena Varkockova, who was able to communicate effectively and kindly with everyone who was running into broken tests, failing tests. And she was very effective in getting all of our tests up and running.&lt;/p>
&lt;p>So that was actually really cool to see. And yeah, just getting to see more of the workings of the team, for me, it was exciting. The other big exciting thing, of course, was to see all the changes that were going in and all the efforts that were being made.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release lead for 1.18 has just been announced as &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcon&lt;/a>. What are you going to put in the proverbial envelope as advice for him?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I would want Jorge to be really on top of making sure that every Special Interest Group that enters a change, that has an enhancement for 1.18, is on top of the timelines and is responsive. Communication tends to be a problem. And I had hinted at this earlier, but some enhancements slipped simply because there wasn't enough reviewer bandwidth.&lt;/p>
&lt;p>Greater communication of timelines and just giving people more time and space to be able to get in their changes, or at least, seemingly give them more time and space by sending early warnings, is going to be helpful. Of course, he's going to have a slightly longer release, too, than I did. This might be related to a unique Q4 challenge. Overall, I would encourage him to take more breaks, to rely more on his release shadows, and split out the work in a fashion that allows everyone to have a turn and everyone to have a break as well.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would your advice be to someone who is hearing your experience and is inspired to get involved with the Kubernetes release or contributer process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Those are two separate questions. So let me tackle the Kubernetes release question first. Kubernetes &lt;a href="https://github.com/kubernetes/sig-release/#readme">SIG Release&lt;/a> has, in my opinion, a really excellent onboarding program for new members. We have what is called the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">Release Team Shadow Program&lt;/a>. We also have the Release Engineering Shadow Program, or the Release Management Shadow Program. Those are two separate subprojects within SIG Release. And each subproject has a team of roles, and each role can have two to four shadows that are basically people who are part of that role team, and they are learning that role as they are doing it.&lt;/p>
&lt;p>So for example, if I am the lead for bug triage on the release team, I may have two, three or four people that I closely work with on the bug triage tasks. These people are my shadows. And once they have served one release cycle as a shadow, they are now eligible to be lead in that role. We have an application form for this process, and it should probably be going up in January. It usually happens the first week of the release once all the release leads are put together.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think being a member of the release team is something that is a good first contribution to the Kubernetes project overall?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It depends on what your goals are, right? I believe so. I believe, for me, personally, it has been incredibly helpful looking into corners of the project that I don't know very much about at all, like API machinery, storage. It's been really exciting to look over all the areas of code that I normally never touch.&lt;/p>
&lt;p>It depends on what you want to get out of it. In general, I think that being a release team shadow is a really, really great on-ramp to being a part of the community because it has a paved path solution to contributing. All you have to do is show up to the meetings, ask questions of your lead, who is required to answer those questions.&lt;/p>
&lt;p>And you also do real work. You really help, you really contribute. If you go across the issues and pull requests in the repo, you will see, 'Hi, my name is so-and-so. I am shadowing the CI signal lead for the current release. Can you help me out here?' And that's a valuable contribution, and it introduces people to others. And then people will recognize your name. They'll see a pull request by you, and they're like oh yeah, I know this person. They're legit.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/guincodes">Guinevere Saenger&lt;/a> is a software engineer for GitHub and served as the Kubernetes 1.17 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: SIG-Windows Spotlight</title><link>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</guid><description>
&lt;p>&lt;em>This post tells the story of how Kubernetes contributors work together to provide a container orchestrator that works for both Linux and Windows.&lt;/em>&lt;/p>
&lt;img alt="Image of a computer with Kubernetes logo" width="30%" src="KubernetesComputer_transparent.png">
&lt;p>Most people who are familiar with Kubernetes are probably used to associating it with Linux. The connection makes sense, since Kubernetes ran on Linux from its very beginning. However, many teams and organizations working on adopting Kubernetes need the ability to orchestrate containers on Windows. Since the release of Docker and rise to popularity of containers, there have been efforts both from the community and from Microsoft itself to make container technology as accessible in Windows systems as it is in Linux systems.&lt;/p>
&lt;p>Within the Kubernetes community, those who are passionate about making Kubernetes accessible to the Windows community can find a home in the Windows Special Interest Group. To learn more about SIG-Windows and the future of Kubernetes on Windows, I spoke to co-chairs &lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a> and &lt;a href="https://github.com/michmike">Michael Michael&lt;/a> about the SIG's goals and how others can contribute.&lt;/p>
&lt;h2 id="intro-to-windows-containers-kubernetes">Intro to Windows Containers &amp;amp; Kubernetes&lt;/h2>
&lt;p>Kubernetes is the most popular tool for orchestrating container workloads, so to understand the Windows Special Interest Group (SIG) within the Kubernetes project, it's important to first understand what we mean when we talk about running containers on Windows.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;When looking at Windows support in Kubernetes,&amp;quot; says SIG (Special Interest Group) Co-chairs Mark Rossetti and Michael Michael, &amp;quot;many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between operational limitations and differences between the Windows and Linux operating systems. Windows containers run the Windows operating system and Linux containers run Linux.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In essence, any &amp;quot;container&amp;quot; is simply a process being run on its host operating system, with some key tooling in place to isolate that process and its dependencies from the rest of the environment. The goal is to make that running process safely isolated, while taking up minimal resources from the system to perform that isolation. On Linux, the tooling used to isolate processes to create &amp;quot;containers&amp;quot; commonly boils down to cgroups and namespaces (among a few others), which are themselves tools built in to the Linux Kernel.&lt;/p>
&lt;img alt="A visual analogy using dogs to explain Linux cgroups and namespaces." width="40%" src="cgroupsNamespacesComboPic.png">
&lt;h4 id="if-dogs-were-processes-containerization-would-be-like-giving-each-dog-their-own-resources-like-toys-and-food-using-cgroups-and-isolating-troublesome-dogs-using-namespaces">&lt;em>If dogs were processes: containerization would be like giving each dog their own resources like toys and food using cgroups, and isolating troublesome dogs using namespaces.&lt;/em>&lt;/h4>
&lt;p>Native Windows processes are processes that are or must be run on a Windows operating system. This makes them fundamentally different from a process running on a Linux operating system. Since Linux containers are Linux processes being isolated by the Linux kernel tools known as cgroups and namespaces, containerizing native Windows processes meant implementing similar isolation tools within the Windows kernel itself. Thus, &amp;quot;Windows Containers&amp;quot; and &amp;quot;Linux Containers&amp;quot; are fundamentally different technologies, even though they have the same goals (isolating processes) and in some ways work similarly (using kernel level containerization).&lt;/p>
&lt;p>So when it comes to running containers on Windows, there are actually two very important concepts to consider:&lt;/p>
&lt;ul>
&lt;li>Native Windows processes running as native Windows Server style containers,&lt;/li>
&lt;li>and traditional Linux containers running on a Linux Kernel, generally hosted on a lightweight Hyper-V Virtual Machine.&lt;/li>
&lt;/ul>
&lt;p>You can learn more about Linux and Windows containers in this &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers">tutorial&lt;/a> from Microsoft.&lt;/p>
&lt;h3 id="kubernetes-on-windows">Kubernetes on Windows&lt;/h3>
&lt;p>Kubernetes was initially designed with Linux containers in mind and was itself designed to run on Linux systems. Because of that, much of the functionality of Kubernetes involves unique Linux functionality. The Linux-specific work is intentional--we all want Kubernetes to run optimally on Linux--but there is a growing demand for similar optimization for Windows servers. For cases where users need container orchestration on Windows, the Kubernetes contributor community of SIG-Windows has incorporated functionality for Windows-specific use cases.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;A common question we get is, will I be able to have a Windows-only cluster. The answer is NO. Kubernetes control plane components will continue to be based on Linux, while SIG-Windows is concentrating on the experience of having Windows worker nodes in a Kubernetes cluster.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Rather than separating out the concepts of &amp;quot;Windows Kubernetes,&amp;quot; and &amp;quot;Linux Kubernetes,&amp;quot; the community of SIG-Windows works toward adding functionality to the main Kubernetes project which allows it to handle use cases for Windows. These Windows capabilities mirror, and in some cases add unique functionality to, the Linux use cases Kubernetes has served since its release in 2014 (want to learn more history? Scroll through this &lt;a href="https://github.com/kubernetes/kubernetes/blob/e2b948dbfbba62b8cb681189377157deee93bb43/DESIGN.md">original design document&lt;/a>.&lt;/p>
&lt;h2 id="what-does-sig-windows-do">What Does SIG-Windows Do?&lt;/h2>
&lt;hr>
&lt;p>&lt;em>&amp;quot;SIG-Windows is really the center for all things Windows in Kubernetes,&amp;quot;&lt;/em> SIG chairs Mark and Michael said, &lt;em>&amp;quot;We mainly focus on the compute side of things, but really anything related to running Kubernetes on Windows is in scope for SIG-Windows.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In order to best serve users, SIG-Windows works to make the Kubernetes user experience as consistent as possible for users of Windows and Linux. However some use cases simply only apply to one Operating System, and as such, the SIG-Windows group also works to create functionality that is unique to Windows-only workloads.&lt;/p>
&lt;p>Many SIGs, or &amp;quot;Special Interest Groups&amp;quot; within Kubernetes have a narrow focus, allowing members to dive deep on a certain facet of the technology. While specific expertise is welcome, those interested in SIG-Windows will find it to be a great community to build broad understanding across many focus areas of Kubernetes. &amp;quot;Members from our SIG interface with storage, network, testing, cluster-lifecycle and others groups in Kubernetes.&amp;quot;&lt;/p>
&lt;h3 id="who-are-sig-windows-users">Who are SIG-Windows' Users?&lt;/h3>
&lt;p>The best way to understand the technology a group makes, is often to understand who their customers or users are.&lt;/p>
&lt;h4 id="a-majority-of-the-users-we-ve-interacted-with-have-business-critical-infrastructure-running-on-windows-developed-over-many-years-and-can-t-move-those-workloads-to-linux-for-various-reasons-cost-time-compliance-etc-the-sig-chairs-shared-by-transporting-those-workloads-into-windows-containers-and-running-them-in-kubernetes-they-are-able-to-quickly-modernize-their-infrastructure-and-help-migrate-it-to-the-cloud">&amp;quot;A majority of the users we've interacted with have business-critical infrastructure running on Windows developed over many years and can't move those workloads to Linux for various reasons (cost, time, compliance, etc),&amp;quot; the SIG chairs shared. &amp;quot;By transporting those workloads into Windows containers and running them in Kubernetes they are able to quickly modernize their infrastructure and help migrate it to the cloud.&amp;quot;&lt;/h4>
&lt;p>As anyone in the Kubernetes space can attest, companies around the world, in many different industries, see Kubernetes as their path to modernizing their infrastructure. Often this involves re-architecting or event totally re-inventing many of the ways they've been doing business. With the goal being to make their systems more scalable, more robust, and more ready for anything the future may bring. But not every application or workload can or should change the core operating system it runs on, so many teams need the ability to run containers at scale on Windows, or Linux, or both.&lt;/p>
&lt;p>&amp;quot;Sometimes the driver to Windows containers is a modernization effort and sometimes it’s because of expiring hardware warranties or end-of-support cycles for the current operating system. Our efforts in SIG-Windows enable Windows developers to take advantage of cloud native tools and Kubernetes to build and deploy distributed applications faster. That’s exciting! In essence, users can retain the benefits of application availability while decreasing costs.&amp;quot;&lt;/p>
&lt;h2 id="who-are-sig-windows">Who are SIG-Windows?&lt;/h2>
&lt;p>Who are these contributors working on enabling Windows workloads for Kubernetes? It could be you!&lt;/p>
&lt;p>Like with other Kubernetes SIGs, contributors to SIG-Windows can be anyone from independent hobbyists to professionals who work at many different companies. They come from many different parts of the world and bring to the table many different skill sets.&lt;/p>
&lt;img alt="Image of several people chatting pleasantly" width="30%" src="PeopleDoodle_transparent.png">
&lt;p>&lt;em>&amp;quot;Like most other Kubernetes SIGs, we are a very welcome and open community,&amp;quot; explained the SIG co-chairs Michael Michael and Mark Rosetti.&lt;/em>&lt;/p>
&lt;h3 id="becoming-a-contributor">Becoming a contributor&lt;/h3>
&lt;p>For anyone interested in getting started, the co-chairs added, &amp;quot;New contributors can view old community meetings on GitHub (we record every single meeting going back three years), read our documentation, attend new community meetings, ask questions in person or on Slack, and file some issues on Github. We also attend all KubeCon conferences and host 1-2 sessions, a contributor session, and meet-the-maintainer office hours.&amp;quot;&lt;/p>
&lt;p>The co-chairs also shared a glimpse into what the path looks like to becoming a member of the SIG-Windows community:&lt;/p>
&lt;p>&amp;quot;We encourage new contributors to initially just join our community and listen, then start asking some questions and get educated on Windows in Kubernetes. As they feel comfortable, they could graduate to improving our documentation, file some bugs/issues, and eventually they can be a code contributor by fixing some bugs. If they have long-term and sustained substantial contributions to Windows, they could become a technical lead or a chair of SIG-Windows. You won't know if you love this area unless you get started :) To get started, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">visit this getting-started page&lt;/a>. It's a one stop shop with links to everything related to SIG-Windows in Kubernetes.&amp;quot;&lt;/p>
&lt;p>When asked if there were any useful skills for new contributors, the co-chairs said,&lt;/p>
&lt;p>&amp;quot;We are always looking for expertise in Go and Networking and Storage, along with a passion for Windows. Those are huge skills to have. However, we don’t require such skills, and we welcome any and all contributors, with varying skill sets. If you don’t know something, we will help you acquire it.&amp;quot;&lt;/p>
&lt;p>You can get in touch with the folks at SIG-Windows in their &lt;a href="https://kubernetes.slack.com/archives/C0SJ4AFB7">Slack channel&lt;/a> or attend one of their regular meetings - currently 30min long on Tuesdays at 12:30PM EST! You can find links to their regular meetings as well as past meeting notes and recordings from the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows#readme">SIG-Windows README&lt;/a> on GitHub.&lt;/p>
&lt;p>As a closing message from SIG-Windows:&lt;/p>
&lt;hr>
&lt;h4 id="we-welcome-you-to-get-involved-and-join-our-community-to-share-feedback-and-deployment-stories-and-contribute-to-code-docs-and-improvements-of-any-kind">&lt;em>&amp;quot;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&amp;quot;&lt;/em>&lt;/h4>
&lt;hr></description></item><item><title>Blog: Working with Terraform and Kubernetes</title><link>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</link><pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://twitter.com/pst418">Philipp Strube&lt;/a>, Kubestack&lt;/p>
&lt;p>Maintaining Kubestack, an open-source &lt;a href="https://www.kubestack.com/lp/terraform-gitops-framework">Terraform GitOps Framework&lt;/a> for Kubernetes, I unsurprisingly spend a lot of time working with Terraform and Kubernetes. Kubestack provisions managed Kubernetes services like AKS, EKS and GKE using Terraform but also integrates cluster services from Kustomize bases into the GitOps workflow. Think of cluster services as everything that's required on your Kubernetes cluster, before you can deploy application workloads.&lt;/p>
&lt;p>Hashicorp recently announced &lt;a href="https://www.hashicorp.com/blog/deploy-any-resource-with-the-new-kubernetes-provider-for-hashicorp-terraform/">better integration between Terraform and Kubernetes&lt;/a>. I took this as an opportunity to give an overview of how Terraform can be used with Kubernetes today and what to be aware of.&lt;/p>
&lt;p>In this post I will however focus only on using Terraform to provision Kubernetes API resources, not Kubernetes clusters.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/intro/index.html">Terraform&lt;/a> is a popular infrastructure as code solution, so I will only introduce it very briefly here. In a nutshell, Terraform allows declaring a desired state for resources as code, and will determine and execute a plan to take the infrastructure from its current state, to the desired state.&lt;/p>
&lt;p>To be able to support different resources, Terraform requires providers that integrate the respective API. So, to create Kubernetes resources we need a Kubernetes provider. Here are our options:&lt;/p>
&lt;h2 id="terraform-kubernetes-provider-official">Terraform &lt;code>kubernetes&lt;/code> provider (official)&lt;/h2>
&lt;p>First, the &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes">official Kubernetes provider&lt;/a>. This provider is undoubtedly the most mature of the three. However, it comes with a big caveat that's probably the main reason why using Terraform to maintain Kubernetes resources is not a popular choice.&lt;/p>
&lt;p>Terraform requires a schema for each resource and this means the maintainers have to translate the schema of each Kubernetes resource into a Terraform schema. This is a lot of effort and was the reason why for a long time the supported resources where pretty limited. While this has improved over time, still not everything is supported. And especially &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> are not possible to support this way.&lt;/p>
&lt;p>This schema translation also results in some edge cases to be aware of. For example, &lt;code>metadata&lt;/code> in the Terraform schema is a list of maps. Which means you have to refer to the &lt;code>metadata.name&lt;/code> of a Kubernetes resource like this in Terraform: &lt;code>kubernetes_secret.example.metadata.0.name&lt;/code>.&lt;/p>
&lt;p>On the plus side however, having a Terraform schema means full integration between Kubernetes and other Terraform resources. Like for &lt;a href="https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/aws/_modules/eks/ingress.tf#L37">example&lt;/a>, using Terraform to create a Kubernetes service of type &lt;code>LoadBalancer&lt;/code> and then use the returned ELB hostname in a Route53 record to configure DNS.&lt;/p>
&lt;p>The biggest benefit when using Terraform to maintain Kubernetes resources is integration into the Terraform plan/apply life-cycle. So you can review planned changes before applying them. Also, using &lt;code>kubectl&lt;/code>, purging of resources from the cluster is not trivial without manual intervention. Terraform does this reliably.&lt;/p>
&lt;h2 id="terraform-kubernetes-alpha-provider">Terraform &lt;code>kubernetes-alpha&lt;/code> provider&lt;/h2>
&lt;p>Second, the new &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha">alpha Kubernetes provider&lt;/a>. As a response to the limitations of the current Kubernetes provider the Hashicorp team recently released an alpha version of a new provider.&lt;/p>
&lt;p>This provider uses dynamic resource types and server-side-apply to support all Kubernetes resources. I personally think this provider has the potential to be a game changer - even if &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha#moving-from-yaml-to-hcl">managing Kubernetes resources in HCL&lt;/a> may still not be for everyone. Maybe the Kustomize provider below will help with that.&lt;/p>
&lt;p>The only downside really is, that it's explicitly discouraged to use it for anything but testing. But the more people test it, the sooner it should be ready for prime time. So I encourage everyone to give it a try.&lt;/p>
&lt;h2 id="terraform-kustomize-provider">Terraform &lt;code>kustomize&lt;/code> provider&lt;/h2>
&lt;p>Last, we have the &lt;a href="https://github.com/kbst/terraform-provider-kustomize">&lt;code>kustomize&lt;/code> provider&lt;/a>. Kustomize provides a way to do customizations of Kubernetes resources using inheritance instead of templating. It is designed to output the result to &lt;code>stdout&lt;/code>, from where you can apply the changes using &lt;code>kubectl&lt;/code>. This approach means that &lt;code>kubectl&lt;/code> edge cases like no purging or changes to immutable attributes still make full automation difficult.&lt;/p>
&lt;p>Kustomize is a popular way to handle customizations. But I was looking for a more reliable way to automate applying changes. Since this is exactly what Terraform is great at the Kustomize provider was born.&lt;/p>
&lt;p>Not going into too much detail here, but from Terraform's perspective, this provider treats every Kubernetes resource as a JSON string. This way it can handle any Kubernetes resource resulting from the Kustomize build. But it has the big disadvantage that Kubernetes resources can not easily be integrated with other Terraform resources. Remember the load balancer example from above.&lt;/p>
&lt;p>Under the hood, similarly to the new Kubernetes alpha provider, the Kustomize provider also uses the dynamic Kubernetes client and server-side-apply. Going forward, I plan to deprecate this part of the Kustomize provider that overlaps with the new Kubernetes provider and only keep the Kustomize integration.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>For teams that are already invested into Terraform, or teams that are looking for ways to replace &lt;code>kubectl&lt;/code> in automation, Terraform's plan/apply life-cycle has always been a promising option to automate changes to Kubernetes resources. However, the limitations of the official Kubernetes provider resulted in this not seeing significant adoption.&lt;/p>
&lt;p>The new alpha provider removes the limitations and has the potential to make Terraform a prime option to automate changes to Kubernetes resources.&lt;/p>
&lt;p>Teams that have already adopted Kustomize, may find integrating Kustomize and Terraform using the Kustomize provider beneficial over &lt;code>kubectl&lt;/code> because it avoids common edge cases. Even if in this set up, Terraform can only easily be used to plan and apply the changes, not to adapt the Kubernetes resources. In the future, this issue may be resolved by combining the Kustomize provider with the new Kubernetes provider.&lt;/p>
&lt;p>If you have any questions regarding these three options, feel free to reach out to me on the Kubernetes Slack in either the &lt;a href="https://app.slack.com/client/T09NY5SBT/CMBCT7XRQ">#kubestack&lt;/a> or the &lt;a href="https://app.slack.com/client/T09NY5SBT/C9A5ALABG">#kustomize&lt;/a> channel. If you happen to give any of the providers a try and encounter a problem, please file a GitHub issue to help the maintainers fix it.&lt;/p></description></item><item><title>Blog: A Better Docs UX With Docsy</title><link>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>I'm pleased to announce that the &lt;a href="https://kubernetes.io">Kubernetes website&lt;/a> now features the &lt;a href="https://docsy.dev">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The Docsy theme improves the site's organization and navigability, and opens a path to improved API references. After over 4 years with few meaningful UX improvements, Docsy implements some best practices for technical content. The theme makes the Kubernetes site easier to read and makes individual pages easier to navigate. It gives the site a much-needed facelift.&lt;/p>
&lt;p>For example: adding a right-hand rail for navigating topics on the page. No more scrolling up to navigate!&lt;/p>
&lt;p>The theme opens a path for future improvements to the website. The Docsy functionality I'm most excited about is the theme's &lt;a href="https://www.docsy.dev/docs/adding-content/shortcodes/#swaggerui">&lt;code>swaggerui&lt;/code> shortcode&lt;/a>, which provides native support for generating API references from an OpenAPI spec. The CNCF is partnering with &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> (GSoD) for staffing to make better API references a reality in Q4 this year. We're hopeful to be chosen, and we're looking forward to Google's list of announced projects on August 16th. Better API references have been a personal goal since I first started working with SIG Docs in 2017. It's exciting to see the goal within reach.&lt;/p>
&lt;p>One of SIG Docs' tech leads, &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a> did a lot of heavy lifting to fix a wide range of site compatibility issues, including a fix to the last of our &lt;a href="https://github.com/kubernetes/website/pull/21359">legacy pieces&lt;/a> when we &lt;a href="2018-05-05-hugo-migration/">migrated from Jekyll to Hugo&lt;/a> in 2018. Our other tech leads, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/onlydole">Taylor Dolezal&lt;/a> provided extensive reviews.&lt;/p>
&lt;p>Thanks also to &lt;a href="https://bep.is/">Björn-Erik Pedersen&lt;/a>, who provided invaluable advice about how to navigate a Hugo upgrade beyond &lt;a href="https://gohugo.io/news/0.60.0-relnotes/">version 0.60.0&lt;/a>.&lt;/p>
&lt;p>The CNCF contracted with &lt;a href="https://gearboxbuilt.com/">Gearbox&lt;/a> in Victoria, BC to apply the theme to the site. Thanks to Aidan, Troy, and the rest of the team for all their work!&lt;/p></description></item><item><title>Blog: Supporting the Evolving Ingress Specification in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Gervais (Datawire.io)&lt;/p>
&lt;p>Earlier this year, the Kubernetes team released &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">Kubernetes 1.18&lt;/a>, which extended Ingress. In this blog post, we’ll walk through what’s new in the new Ingress specification, what it means for your applications, and how to upgrade to an ingress controller that supports this new specification.&lt;/p>
&lt;h3 id="what-is-kubernetes-ingress">What is Kubernetes Ingress&lt;/h3>
&lt;p>When deploying your applications in Kubernetes, one of the first challenges many people encounter is how to get traffic into their cluster. &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Kubernetes ingress&lt;/a> is a collection of routing rules that govern how external users access services running in a Kubernetes cluster. There are &lt;a href="https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d">three general approaches&lt;/a> for exposing your application:&lt;/p>
&lt;ul>
&lt;li>Using a &lt;code>NodePort&lt;/code> to expose your application on a port across each of your nodes&lt;/li>
&lt;li>Using a &lt;code>LoadBalancer&lt;/code> service to create an external load balancer that points to a Kubernetes service in your cluster&lt;/li>
&lt;li>Using a Kubernetes Ingress resource&lt;/li>
&lt;/ul>
&lt;h3 id="what-s-new-in-kubernetes-1-18-ingress">What’s new in Kubernetes 1.18 Ingress&lt;/h3>
&lt;p>There are three significant additions to the Ingress API in Kubernetes 1.18:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource&lt;/li>
&lt;li>Support for wildcards in hostnames&lt;/li>
&lt;/ul>
&lt;p>The new &lt;code>pathType&lt;/code> field allows you to specify how Ingress paths should match.
The field supports three types: &lt;code>ImplementationSpecific&lt;/code> (default), &lt;code>exact&lt;/code>, and &lt;code>prefix&lt;/code>. Explicitly defining the expected behavior of path matching will allow every ingress-controller to support a user’s needs and will increase portability between ingress-controller implementation solutions.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource specifies how Ingresses should be implemented by controllers. This was added to formalize the commonly used but never standardized &lt;code>kubernetes.io/ingress.class&lt;/code> annotation and allow for implementation-specific extensions and configuration.&lt;/p>
&lt;p>You can read more about these changes, as well as the support for wildcards in hostnames in more detail in &lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">a previous blog post&lt;/a>.&lt;/p>
&lt;h2 id="supporting-kubernetes-ingress">Supporting Kubernetes ingress&lt;/h2>
&lt;p>&lt;a href="https://www.getambassador.io">Ambassador&lt;/a> is an open-source Envoy-based ingress controller. We believe strongly in supporting common standards such as Kubernetes ingress, which we adopted and &lt;a href="https://blog.getambassador.io/ambassador-ingress-controller-better-config-reporting-updated-envoy-proxy-99dc9139e28f">announced our initial support for back in 2019&lt;/a>.&lt;/p>
&lt;p>Every Ambassador release goes through rigorous testing. Therefore, we also contributed an &lt;a href="https://github.com/kubernetes-sigs/ingress-controller-conformance">open conformance test suite&lt;/a>, supporting Kubernetes ingress. We wrote the initial bits of test code and will keep iterating over the newly added features and different versions of the Ingress specification as it evolves to a stable v1 GA release. Documentation and usage samples, is one of our top priorities. We understand how complex usage can be, especially when transitioning from a previous version of an API.&lt;/p>
&lt;p>Following a test-driven development approach, the first step we took in supporting Ingress improvements in Ambassador was to translate the revised specification -- both in terms of API and behavior -- into a comprehensible test suite. The test suite, although still under heavy development and going through multiple iterations, was rapidly added to the Ambassador CI infrastructure and acceptance criteria. This means every change to the Ambassador codebase going forward will be compliant with the Ingress API and be tested end-to-end in a lightweight &lt;a href="https://kind.sigs.k8s.io/">KIND cluster&lt;/a>. Using KIND allowed us to make rapid improvements while limiting our cloud provider infrastructure bill and testing out unreleased Kubernetes features with pre-release builds.&lt;/p>
&lt;h3 id="adopting-a-new-specification">Adopting a new specification&lt;/h3>
&lt;p>With a global comprehension of additions to Ingress introduced in Kubernetes 1.18 and a test suite on hand, we tackled the task of adapting the Ambassador code so that it would support translating the high-level Ingress API resources into Envoy configurations and constructs. Luckily Ambassador already supported previous versions of ingress functionalities so the development effort was incremental.&lt;/p>
&lt;p>We settled on a controller name of &lt;code>getambassador.io/ingress-controller&lt;/code>. This value, consistent with Ambassador's domain and CRD versions, must be used to tie in an IngressClass &lt;code>spec.controller&lt;/code> with an Ambassador deployment. The new IngressClass resource allows for extensibility by setting a &lt;code>spec.parameters&lt;/code> field. At the moment Ambassador makes no use of this field and its usage is reserved for future development.&lt;/p>
&lt;p>Paths can now define different matching behaviors using the &lt;code>pathType&lt;/code> field. The field will default to a value of &lt;code>ImplementationSpecific&lt;/code>, which uses the same matching rules as the &lt;a href="https://www.getambassador.io/docs/latest/topics/using/mappings/">Ambassador Mappings&lt;/a> prefix field and previous Ingress specification for backward compatibility reasons.&lt;/p>
&lt;h3 id="kubernetes-ingress-controllers">Kubernetes Ingress Controllers&lt;/h3>
&lt;p>A comprehensive &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">list of Kubernetes ingress controllers&lt;/a> is available in the Kubernetes documentation. Currently, Ambassador is the only ingress controller that supports these new additions to the ingress specification. Powered by the &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a>, Ambassador is the fastest way for you to try out the new ingress specification today.&lt;/p>
&lt;p>Check out the following resources:&lt;/p>
&lt;ul>
&lt;li>Ambassador on &lt;a href="https://www.github.com/datawire/ambassador">GitHub&lt;/a>&lt;/li>
&lt;li>The Ambassador &lt;a href="https://www.getambassador.io/docs">documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">Improvements to the Ingress API&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Or join the community on &lt;a href="http://d6e.co/slack">Slack&lt;/a>!&lt;/p></description></item><item><title>Blog: K8s KPIs with Kuberhealthy</title><link>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joshulyne Park (Comcast), Eric Greer (Comcast)&lt;/p>
&lt;h3 id="building-onward-from-kuberhealthy-v2-0-0">Building Onward from Kuberhealthy v2.0.0&lt;/h3>
&lt;p>Last November at KubeCon San Diego 2019, we announced the release of
&lt;a href="https://www.youtube.com/watch?v=aAJlWhBtzqY">Kuberhealthy 2.0.0&lt;/a> - transforming Kuberhealthy into a Kubernetes operator
for synthetic monitoring. This new ability granted developers the means to create their own Kuberhealthy check
containers to synthetically monitor their applications and clusters. The community was quick to adopt this new feature and we're grateful for everyone who implemented and tested Kuberhealthy 2.0.0 in their clusters. Thanks to all of you who reported
issues and contributed to discussions on the #kuberhealthy Slack channel. We quickly set to work to address all your feedback
with a newer version of Kuberhealthy. Additionally, we created a guide on how to easily install and use Kuberhealthy in order to capture some helpful synthetic &lt;a href="https://kpi.org/KPI-Basics">KPIs&lt;/a>.&lt;/p>
&lt;h3 id="deploying-kuberhealthy">Deploying Kuberhealthy&lt;/h3>
&lt;p>To install Kuberhealthy, make sure you have &lt;a href="https://helm.sh/docs/intro/install/">Helm 3&lt;/a> installed. If not, you can use the generated flat spec files located
in this &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/deploy">deploy folder&lt;/a>. You should use &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus.yaml">kuberhealthy-prometheus.yaml&lt;/a> if you don't use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>, and &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus-operator.yaml">kuberhealthy-prometheus-operator.yaml&lt;/a> if you do. If you don't use Prometheus at all, you can still use Kuberhealthy with a JSON status page and/or InfluxDB integration using &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy.yaml">this spec&lt;/a>.&lt;/p>
&lt;h4 id="to-install-using-helm-3">To install using Helm 3:&lt;/h4>
&lt;h5 id="1-create-namespace-kuberhealthy-in-the-desired-kubernetes-cluster-context">1. Create namespace &amp;quot;kuberhealthy&amp;quot; in the desired Kubernetes cluster/context:&lt;/h5>
&lt;pre>&lt;code>kubectl create namespace kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="2-set-your-current-namespace-to-kuberhealthy">2. Set your current namespace to &amp;quot;kuberhealthy&amp;quot;:&lt;/h5>
&lt;pre>&lt;code>kubectl config set-context --current --namespace=kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="3-add-the-kuberhealthy-repo-to-helm">3. Add the kuberhealthy repo to Helm:&lt;/h5>
&lt;pre>&lt;code>helm repo add kuberhealthy https://comcast.github.io/kuberhealthy/helm-repos
&lt;/code>&lt;/pre>&lt;h5 id="4-depending-on-your-prometheus-implementation-install-kuberhealthy-using-the-appropriate-command-for-your-cluster">4. Depending on your Prometheus implementation, install Kuberhealthy using the appropriate command for your cluster:&lt;/h5>
&lt;ul>
&lt;li>If you use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true,prometheus.serviceMonitor=true
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>If you use Prometheus, but NOT Prometheus Operator:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true
&lt;/code>&lt;/pre>&lt;p>See additional details about configuring the appropriate scrape annotations in the section &lt;a href="#prometheus-integration-details">Prometheus Integration Details&lt;/a> below.&lt;/p>
&lt;ul>
&lt;li>Finally, if you don't use Prometheus:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy
&lt;/code>&lt;/pre>&lt;p>Running the Helm command should automatically install the newest version of Kuberhealthy (v2.2.0) along with a few basic checks. If you run &lt;code>kubectl get pods&lt;/code>, you should see two Kuberhealthy pods. These are the pods that create, coordinate, and track test pods. These two Kuberhealthy pods also serve a JSON status page as well as a &lt;code>/metrics&lt;/code> endpoint. Every other pod you see created is a checker pod designed to execute and shut down when done.&lt;/p>
&lt;h3 id="configuring-additional-checks">Configuring Additional Checks&lt;/h3>
&lt;p>Next, you can run &lt;code>kubectl get khchecks&lt;/code>. You should see three Kuberhealthy checks installed by default:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/daemonset-check">daemonset&lt;/a>: Deploys and tears down a daemonset to ensure all nodes in the cluster are functional.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment&lt;/a>: Creates a deployment and then triggers a rolling update. Tests that the deployment is reachable via a service and then deletes everything. Any problem in this process will cause this check to report a failure.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/dns-resolution-check">dns-status-internal&lt;/a>: Validates that internal cluster DNS is functioning as expected.&lt;/li>
&lt;/ul>
&lt;p>To view other available external checks, check out the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECKS_REGISTRY.md">external checks registry&lt;/a> where you can find other yaml files you can apply to your cluster to enable various checks.&lt;/p>
&lt;p>Kuberhealthy check pods should start running shortly after Kuberhealthy starts running (1-2 minutes). Additionally, the check-reaper cronjob runs every few minutes to ensure there are no more than 5 completed checker pods left lying around at a time.&lt;/p>
&lt;p>To get status page view of these checks, you'll need to either expose the &lt;code>kuberhealthy&lt;/code> service externally by editing the service &lt;code>kuberhealthy&lt;/code> and setting &lt;code>Type: LoadBalancer&lt;/code> or use &lt;code>kubectl port-forward service/kuberhealthy 8080:80&lt;/code>. When viewed, the service endpoint will display a JSON status page that looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CheckDetails&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/daemonset&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;22.512278967s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;9abd3ec0-b82f-44f0-b8a7-fa6709f759cd&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/deployment&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;29.142295647s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;5f0d2765-60c9-47e8-b2c9-8bc6e61727b2&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/dns-status-internal&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2.43940936s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:44.6294547Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;c85f95cb-87e2-4ff5-b513-e02b3d25973a&amp;#34;&lt;/span>
}
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CurrentMaster&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-7cf79bdc86-m78qr&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This JSON page displays all Kuberhealthy checks running in your cluster. If you have Kuberhealthy checks running in different namespaces, you can filter them by adding the &lt;code>GET&lt;/code> variable &lt;code>namespace&lt;/code> parameter: &lt;code>?namespace=kuberhealthy,kube-system&lt;/code> onto the status page URL.&lt;/p>
&lt;h3 id="writing-your-own-checks">Writing Your Own Checks&lt;/h3>
&lt;p>Kuberhealthy is designed to be extended with custom check containers that can be written by anyone to check anything. These checks can be written in any language as long as they are packaged in a container. This makes Kuberhealthy an excellent platform for creating your own synthetic checks!&lt;/p>
&lt;p>Creating your own check is a great way to validate your client library, simulate real user workflow, and create a high level of confidence in your service or system uptime.&lt;/p>
&lt;p>To learn more about writing your own checks, along with simple examples, check the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECK_CREATION.md">custom check creation&lt;/a> documentation.&lt;/p>
&lt;h3 id="prometheus-integration-details">Prometheus Integration Details&lt;/h3>
&lt;p>When enabling Prometheus (not the operator), the Kuberhealthy service gets the following annotations added:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-.env" data-lang=".env">prometheus.io/path: /metrics
prometheus.io/port: &lt;span style="color:#b44">&amp;#34;80&amp;#34;&lt;/span>
prometheus.io/scrape: &lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In your prometheus configuration, add the following example scrape_config that scrapes the Kuberhealthy service given the added prometheus annotation:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#008000;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;kuberhealthy&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes_sd_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">role&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaces&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">relabel_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">source_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[__meta_kubernetes_service_annotation_prometheus_io_scrape]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">action&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>keep&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">regex&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also specify the target endpoint to be scraped using this example job:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#008000;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">static_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy.kuberhealthy.svc.cluster.local:80&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the appropriate prometheus configurations are applied, you should be able to see the following Kuberhealthy metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kuberhealthy_check&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_check_duration_seconds&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_cluster_states&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_running&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="creating-key-performance-indicators">Creating Key Performance Indicators&lt;/h3>
&lt;p>Using these Kuberhealthy metrics, our team has been able to collect KPIs based on the following definitions, calculations, and PromQL queries.&lt;/p>
&lt;p>&lt;em>Availability&lt;/em>&lt;/p>
&lt;p>We define availability as the K8s cluster control plane being up and functioning as expected. This is measured by our ability to create a deployment, do a rolling update, and delete the deployment within a set period of time.&lt;/p>
&lt;p>We calculate this by measuring Kuberhealthy's &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> successes and failures.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Availability = Uptime / (Uptime * Downtime)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Uptime = Number of Deployment Check Passes * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Downtime = Number of Deployment Check Fails * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check Run Interval = how often the check runs (&lt;code>runInterval&lt;/code> set in your KuberhealthyCheck Spec)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PromQL Query (Availability % over the past 30 days):&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-promql" data-lang="promql">&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">-&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">sum&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">count_over_time&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b8860b">kuberhealthy_check&lt;/span>{&lt;span style="color:#a0a000">check&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">kuberhealthy/deployment&lt;/span>&amp;#34;,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a0a000">status&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">0&lt;/span>&amp;#34;}[&lt;span style="color:#b44">30d&lt;/span>]&lt;span style="color:#666">))&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b8860b">OR&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">vector&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#666">))&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">/&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">sum&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">count_over_time&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b8860b">kuberhealthy_check&lt;/span>{&lt;span style="color:#a0a000">check&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">kuberhealthy/deployment&lt;/span>&amp;#34;,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a0a000">status&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">1&lt;/span>&amp;#34;}[&lt;span style="color:#b44">30d&lt;/span>]&lt;span style="color:#666">))&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Utilization&lt;/em>&lt;/p>
&lt;p>We define utilization as user uptake of product (k8s) and its resources (pods, services, etc.). This is measured by how many nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs are being utilized by our customers.
We calculate this by counting the total number of nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs.&lt;/p>
&lt;p>&lt;em>Duration (Latency)&lt;/em>&lt;/p>
&lt;p>We define duration as the control plane's capacity and utilization of throughput. We calculate this by capturing the average run duration of a Kuberhealthy &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> run.&lt;/p>
&lt;ul>
&lt;li>PromQL Query (Deployment check average run duration):
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-promql" data-lang="promql">&lt;span style="color:#a2f;font-weight:bold">avg&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b8860b">kuberhealthy_check_duration_seconds&lt;/span>{&lt;span style="color:#a0a000">check&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">kuberhealthy/deployment&lt;/span>&amp;#34;}&lt;span style="color:#666">)&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Errors / Alerts&lt;/em>&lt;/p>
&lt;p>We define errors as all k8s cluster and Kuberhealthy related alerts. Every time one of our Kuberhealthy check fails, we are alerted of this failure.&lt;/p>
&lt;h3 id="thank-you">Thank You!&lt;/h3>
&lt;p>Thanks again to everyone in the community for all of your contributions and help! We are excited to see what you build. As always, if you find an issue, have a feature request, or need to open a pull request, please &lt;a href="https://github.com/Comcast/kuberhealthy/issues">open an issue&lt;/a> on the Github project.&lt;/p></description></item><item><title>Blog: My exciting journey into Kubernetes’ history</title><link>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert, SUSE Software Solutions&lt;/p>
&lt;p>&lt;em>Editor's note: Sascha is part of &lt;a href="https://github.com/kubernetes/sig-release">SIG Release&lt;/a> and is working on many other
different container runtime related topics. Feel free to reach him out on
Twitter &lt;a href="https://twitter.com/saschagrunert">@saschagrunert&lt;/a>.&lt;/em>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>A story of data science-ing 90,000 GitHub issues and pull requests by using
Kubeflow, TensorFlow, Prow and a fully automated CI/CD pipeline.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#getting-the-data">Getting the Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exploring-the-data">Exploring the Data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#labels-labels-labels">Labels, Labels, Labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#building-the-machine-learning-model">Building the Machine Learning Model&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#training-the-model">Training the Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-first-prediction">A first Prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#automate-everything">Automate Everything&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Choosing the right steps when working in the field of data science is truly no
silver bullet. Most data scientists might have their custom workflow, which
could be more or less automated, depending on their area of work. Using
&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> can be a tremendous enhancement when trying to automate
workflows on a large scale. In this blog post, I would like to take you on my
journey of doing data science while integrating the overall workflow into
Kubernetes.&lt;/p>
&lt;p>The target of the research I did in the past few months was to find any
useful information about all those thousands of GitHub issues and pull requests
(PRs) we have in the &lt;a href="https://github.com/kubernetes/kubernetes">Kubernetes repository&lt;/a>. What I ended up with was a
fully automated, in Kubernetes running Continuous Integration (CI) and
Deployment (CD) data science workflow powered by &lt;a href="https://www.kubeflow.org">Kubeflow&lt;/a> and &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>.
You may not know both of them, but we get to the point where I explain what
they’re doing in detail. The source code of my work can be found in the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis">kubernetes-analysis GitHub repository&lt;/a>, which contains everything source
code-related as well as the raw data. But how to retrieve this data I’m talking
about? Well, this is where the story begins.&lt;/p>
&lt;h1 id="getting-the-data">Getting the Data&lt;/h1>
&lt;p>The foundation for my experiments is the raw GitHub API data in plain &lt;a href="https://en.wikipedia.org/wiki/JSON">JSON&lt;/a>
format. The necessary data can be retrieved via the &lt;a href="https://developer.github.com/v3/issues">GitHub issues
endpoint&lt;/a>, which returns all pull requests as well as regular issues in the
&lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST&lt;/a> API. I exported roughly &lt;strong>91000&lt;/strong> issues and pull requests in
the first iteration into a massive &lt;strong>650 MiB&lt;/strong> data blob. This took me about &lt;strong>8
hours&lt;/strong> of data retrieval time because for sure, the GitHub API is &lt;a href="https://developer.github.com/apps/building-github-apps/understanding-rate-limits-for-github-apps/">rate
limited&lt;/a>. To be able to put this data into a GitHub repository, I’d chosen
to compress it via &lt;a href="https://linux.die.net/man/1/xz">&lt;code>xz(1)&lt;/code>&lt;/a>. The result was a roundabout &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/data/api.tar.xz">25 MiB sized
tarball&lt;/a>, which fits well into the repository.&lt;/p>
&lt;p>I had to find a way to regularly update the dataset because the Kubernetes
issues and pull requests are updated by the users over time as well as new ones
are created. To achieve the continuous update without having to wait 8 hours
over and over again, I now fetch the delta GitHub API data between the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/.update">last update&lt;/a> and the current time. This way, a Continuous Integration job
can update the data on a regular basis, whereas I can continue my research with
the latest available set of data.&lt;/p>
&lt;p>From a tooling perspective, I’ve written an &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/main">all-in-one Python executable&lt;/a>,
which allows us to trigger the different steps during the data science
experiments separately via dedicated subcommands. For example, to run an export
of the whole data set, we can call:&lt;/p>
&lt;pre>&lt;code>&amp;gt; export GITHUB_TOKEN=&amp;lt;MY-SECRET-TOKEN&amp;gt;
&amp;gt; ./main export
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Dumping all issues
INFO | Pulling 90929 items
INFO | 1: Unit test coverage in Kubelet is lousy. (~30%)
INFO | 2: Better error messages if go isn't installed, or if gcloud is old.
INFO | 3: Need real cluster integration tests
INFO | 4: kubelet should know which containers it is managing
… [just wait 8 hours] …
&lt;/code>&lt;/pre>&lt;p>To update the data between the last time stamp stored in the repository we can
run:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main export --update-api
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Retrieving issues and PRs
INFO | Updating API
INFO | Got update timestamp: 2020-05-09T10:57:40.854151
INFO | 90786: Automated cherry pick of #90749: fix: azure disk dangling attach issue
INFO | 90674: Switch core master base images from debian to distroless
INFO | 90086: Handling error returned by request.Request.ParseForm()
INFO | 90544: configurable weight on the CPU and memory
INFO | 87746: Support compiling Kubelet w/o docker/docker
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Updating data
INFO | Updating issue 90786 (updated at 2020-05-09T10:59:43Z)
INFO | Updating issue 90674 (updated at 2020-05-09T10:58:27Z)
INFO | Updating issue 90086 (updated at 2020-05-09T10:58:26Z)
INFO | Updating issue 90544 (updated at 2020-05-09T10:57:51Z)
INFO | Updating issue 87746 (updated at 2020-05-09T11:01:51Z)
INFO | Saving data
&lt;/code>&lt;/pre>&lt;p>This gives us an idea of how fast the project is actually moving: On a Saturday
at noon (European time), 5 issues and pull requests got updated within literally 5
minutes!&lt;/p>
&lt;p>Funnily enough, &lt;a href="https://github.com/jbeda">Joe Beda&lt;/a>, one of the founders of Kubernetes, created the
first GitHub issue &lt;a href="https://github.com/kubernetes/kubernetes/issues/1">mentioning that the unit test coverage is too low&lt;/a>. The
issue has no further description than the title, and no enhanced labeling
applied, like we know from more recent issues and pull requests. But now we have
to explore the exported data more deeply to do something useful with it.&lt;/p>
&lt;h1 id="exploring-the-data">Exploring the Data&lt;/h1>
&lt;p>Before we can start creating machine learning models and train them, we have to
get an idea about how our data is structured and what we want to achieve in
general.&lt;/p>
&lt;p>To get a better feeling about the amount of data, let’s look at how many issues
and pull requests have been created over time inside the Kubernetes repository:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
&lt;/code>&lt;/pre>&lt;p>The Python &lt;a href="https://matplotlib.org">matplotlib&lt;/a> module should pop up a graph which looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-all.svg" alt="created all">&lt;/p>
&lt;p>Okay, this looks not that spectacular but gives us an impression on how the
project has grown over the past 6 years. To get a better idea about the speed of
development of the project, we can look at the &lt;em>created-vs-closed&lt;/em> metric. This
means on our timeline, we add one to the y-axis if an issue or pull request got
created and subtracts one if closed. Now the chart looks like this:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-all.svg" alt="created vs closed all">&lt;/p>
&lt;p>At the beginning of 2018, the Kubernetes projects introduced some more enhanced
life-cycle management via the glorious &lt;a href="https://github.com/fejta-bot">fejta-bot&lt;/a>. This automatically
closes issues and pull requests after they got stale over a longer period of
time. This resulted in a massive closing of issues, which does not apply to pull
requests in the same amount. For example, if we look at the &lt;em>created-vs-closed&lt;/em>
metric only for pull requests.&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-pull-requests.svg" alt="created vs closed pull requests">&lt;/p>
&lt;p>The overall impact is not that obvious. What we can see is that the increasing
number of peaks in the PR chart indicates that the project is moving faster over
time. Usually, a candlestick chart would be a better choice for showing this kind
of volatility-related information. I’d also like to highlight that it looks like
the development of the project slowed down a bit in the beginning of 2020.&lt;/p>
&lt;p>Parsing raw JSON in every analysis iteration is not the fastest approach to do
in Python. This means that I decided to parse the more important information,
for example the content, title and creation time into dedicated &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/issue.py">issue&lt;/a> and
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/pull_request.py">PR classes&lt;/a>. This data will be &lt;a href="https://docs.python.org/3/library/pickle.html">pickle&lt;/a> serialized into the repository
as well, which allows an overall faster startup independently of the JSON blob.&lt;/p>
&lt;p>A pull request is more or less the same as an issue in my analysis, except that
it contains a release note.&lt;/p>
&lt;p>Release notes in Kubernetes are written in the PRs description into a separate
&lt;code>release-note&lt;/code> block like this:&lt;/p>
&lt;pre>&lt;code>```release-note
I changed something extremely important and you should note that.
```
&lt;/code>&lt;/pre>&lt;p>Those release notes are parsed by &lt;a href="https://github.com/kubernetes/release#tools">dedicated Release Engineering Tools like
&lt;code>krel&lt;/code>&lt;/a> during the release creation process and will be part of the various
&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">CHANGELOG.md&lt;/a> files and the &lt;a href="https://relnotes.k8s.io">Release Notes Website&lt;/a>. That seems like a
lot of magic, but in the end, the quality of the overall release notes is much
higher because they’re easy to edit, and the PR reviewers can ensure that we
only document real user-facing changes and nothing else.&lt;/p>
&lt;p>The quality of the input data is a key aspect when doing data science. I decided
to focus on the release notes because they seem to have the highest amount of
overall quality when comparing them to the plain descriptions in issues and PRs.
Besides that, they’re easy to parse, and we would not need to strip away
the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/.github/ISSUE_TEMPLATE">various issue&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/.github/PULL_REQUEST_TEMPLATE.md">PR template&lt;/a> text noise.&lt;/p>
&lt;h2 id="labels-labels-labels">Labels, Labels, Labels&lt;/h2>
&lt;p>Issues and pull requests in Kubernetes get different labels applied during its
life-cycle. They are usually grouped via a single slash (&lt;code>/&lt;/code>). For example, we
have &lt;code>kind/bug&lt;/code> and &lt;code>kind/api-change&lt;/code> or &lt;code>sig/node&lt;/code> and &lt;code>sig/network&lt;/code>. An easy
way to understand which label groups exist and how they’re distributed across
the repository is to plot them into a bar chart:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-group
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-group-all-top-25.svg" alt="labels by group all top 25">&lt;/p>
&lt;p>It looks like that &lt;code>sig/&lt;/code>, &lt;code>kind/&lt;/code> and &lt;code>area/&lt;/code> labels are pretty common.
Something like &lt;code>size/&lt;/code> can be ignored for now because these labels are
automatically applied based on the amount of the code changes for a pull
request. We said that we want to focus on release notes as input data, which
means that we have to check out the distribution of the labels for the PRs. This
means that the top 25 labels on pull requests are:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-name --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-name-pull-requests-top-25.svg" alt="labels by name pull requests top 25">&lt;/p>
&lt;p>Again, we can ignore labels like &lt;code>lgtm&lt;/code> (looks good to me), because every PR
which now should get merged has to look good. Pull requests containing release
notes automatically get the &lt;code>release-note&lt;/code> label applied, which enables further
filtering more easily. This does not mean that every PR containing that label
also contains the release notes block. The label could have been applied
manually and the parsing of the release notes block did not exist since the
beginning of the project. This means we will probably loose a decent amount of
input data on one hand. On the other hand we can focus on the highest possible
data quality, because applying labels the right way needs some enhanced maturity
of the project and its contributors.&lt;/p>
&lt;p>From a label group perspective I have chosen to focus on the &lt;code>kind/&lt;/code> labels.
Those labels are something which has to be applied manually by the author of the
PR, they are available on a good amount of pull requests and they’re related to
user-facing changes as well. Besides that, the &lt;code>kind/&lt;/code> choice has to be done for
every pull request because it is part of the PR template.&lt;/p>
&lt;p>Alright, how does the distribution of those labels look like when focusing only
on pull requests which have release notes?&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --release-notes-stats
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/release-notes-stats.svg" alt="release notes stats">&lt;/p>
&lt;p>Interestingly, we have approximately 7,000 overall pull requests containing
release notes, but only ~5,000 have a &lt;code>kind/&lt;/code> label applied. The distribution of
the labels is not equal, and one-third of them are labeled as &lt;code>kind/bug&lt;/code>. This
brings me to the next decision in my data science journey: I will build a binary
classifier which, for the sake of simplicity, is only able to distinguish between
bugs (via &lt;code>kind/bug&lt;/code>) and non-bugs (where the label is not applied).&lt;/p>
&lt;p>The main target is now to be able to classify newly incoming release notes if
they are related to a bug or not, based on the historical data we already have
from the community.&lt;/p>
&lt;p>Before doing that, I recommend that you play around with the &lt;code>./main analyze -h&lt;/code>
subcommand as well to explore the latest set of data. You can also check out all
the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/assets">continuously updated assets&lt;/a> I provide within the analysis repository.
For example, those are the top 25 PR creators inside the Kubernetes repository:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/users-by-created-pull-requests-top-25.svg" alt="users by created pull request">&lt;/p>
&lt;h1 id="building-the-machine-learning-model">Building the Machine Learning Model&lt;/h1>
&lt;p>Now we have an idea what the data set is about, and we can start building a first
machine learning model. Before actually building the model, we have to
pre-process all the extracted release notes from the PRs. Otherwise, the model
would not be able to understand our input.&lt;/p>
&lt;h2 id="doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/h2>
&lt;p>In the beginning, we have to define a vocabulary for which we want to train. I
decided to choose the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer&lt;/a> from the Python scikit-learn machine
learning library. This vectorizer is able to take our input texts and create a
single huge vocabulary out of it. This is our so-called &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words&lt;/a>,
which has a chosen n-gram range of &lt;code>(1, 2)&lt;/code> (unigrams and bigrams). Practically
this means that we always use the first word and the next one as a single
vocabulary entry (bigrams). We also use the single word as vocabulary entry
(unigram). The TfidfVectorizer is able to skip words that occur multiple times
(&lt;code>max_df&lt;/code>), and requires a minimum amount (&lt;code>min_df&lt;/code>) to add a word to the
vocabulary. I decided not to change those values in the first place, just
because I had the intuition that release notes are something unique to a
project.&lt;/p>
&lt;p>Parameters like &lt;code>min_df&lt;/code>, &lt;code>max_df&lt;/code> and the n-gram range can be seen as some of
our hyperparameters. Those parameters have to be optimized in a dedicated step
after the machine learning model has been built. This step is called
hyperparameter tuning and basically means that we train multiple times with
different parameters and compare the accuracy of the model. Afterwards, we choose
the parameters with the best accuracy.&lt;/p>
&lt;p>During the training, the vectorizer will produce a &lt;code>data/features.json&lt;/code> which
contains the whole vocabulary. This gives us a good understanding of how such a
vocabulary may look like:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">[
&lt;span style="">…&lt;/span>
&lt;span style="color:#b44">&amp;#34;hostname&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname address&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname and&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname as&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname being&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname bug&amp;#34;&lt;/span>,
&lt;span style="">…&lt;/span>
]
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This produces round about 50,000 entries in the overall bag-of-words, which is
pretty much. Previous analyses between different data sets showed that it is
simply not necessary to take so many features into account. Some general data
sets state that an overall vocabulary of 20,000 is enough and higher amounts do
not influence the accuracy any more. To do so we can use the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">SelectKBest&lt;/a>
feature selector to strip down the vocabulary to only choose the top features.
Anyway, I still decided to stick to the top 50,000 to not negatively influence
the model accuracy. We have a relatively low amount of data (appr. 7,000
samples) and a low number of words per sample (~15) which already made me wonder
if we have enough data at all.&lt;/p>
&lt;p>The vectorizer is not only able to create our bag-of-words, but it is also able to
encode the features in &lt;a href="https://en.wikipedia.org/wiki/Tf%e2%80%93idf">term frequency–inverse document frequency (tf-idf)&lt;/a>
format. That is where the vectorizer got its name, whereas the output of that
encoding is something the machine learning model can directly consume. All the
details of the vectorization process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L193-L235">source code&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/h2>
&lt;p>I decided to choose a simple MLP based model which is built with the help of the
popular &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras">TensorFlow&lt;/a> framework. Because we do not have that much input data,
we just use two hidden layers, so that the model basically looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/model.png" alt="model">&lt;/p>
&lt;p>There have to be &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L95-L100">multiple other&lt;/a> hyperparameters to be taken into account
when creating the model. I will not discuss them in detail here, but they’re
important to be optimized also in relation to the number of classes we want to
have in the model (only two in our case).&lt;/p>
&lt;h2 id="training-the-model">Training the Model&lt;/h2>
&lt;p>Before starting the actual training, we have to split up our input data into
training and validation data sets. I’ve chosen to use ~80% of the data for
training and 20% for validation purposes. We have to shuffle our input data as
well to ensure that the model is not affected by ordering issues. The technical
details of the training process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L91-L170">GitHub sources&lt;/a>. So now
we’re ready to finally start the training:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main train
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Training for label 'kind/bug'
INFO | 6980 items selected
INFO | Using 5584 training and 1395 testing texts
INFO | Number of classes: 2
INFO | Vocabulary len: 51772
INFO | Wrote features to file data/features.json
INFO | Using units: 1
INFO | Using activation function: sigmoid
INFO | Created model with 2 layers and 64 units
INFO | Compiling model
INFO | Starting training
Train on 5584 samples, validate on 1395 samples
Epoch 1/1000
5584/5584 - 3s - loss: 0.6895 - acc: 0.6789 - val_loss: 0.6856 - val_acc: 0.6860
Epoch 2/1000
5584/5584 - 2s - loss: 0.6822 - acc: 0.6827 - val_loss: 0.6782 - val_acc: 0.6860
Epoch 3/1000
…
Epoch 68/1000
5584/5584 - 2s - loss: 0.2587 - acc: 0.9257 - val_loss: 0.4847 - val_acc: 0.7728
INFO | Confusion matrix:
[[920 32]
[291 152]]
INFO | Confusion matrix normalized:
[[0.966 0.034]
[0.657 0.343]]
INFO | Saving model to file data/model.h5
INFO | Validation accuracy: 0.7727598547935486, loss: 0.48470408514836355
&lt;/code>&lt;/pre>&lt;p>The output of the &lt;a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix&lt;/a> shows us that we’re pretty good on
training accuracy, but the validation accuracy could be a bit higher. We now
could start a hyperparameter tuning to see if we can optimize the output of the
model even further. I will leave that experiment up to you with the hint to the
&lt;code>./main train --tune&lt;/code> flag.&lt;/p>
&lt;p>We saved the model (&lt;code>data/model.h5&lt;/code>), the vectorizer (&lt;code>data/vectorizer.pickle&lt;/code>)
and the feature selector (&lt;code>data/selector.pickle&lt;/code>) to disk to be able to use them
later on for prediction purposes without having a need for additional training
steps.&lt;/p>
&lt;h2 id="a-first-prediction">A first Prediction&lt;/h2>
&lt;p>We are now able to test the model by loading it from disk and predicting some
input text:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main predict --test
INFO | Testing positive text:
Fix concurrent map access panic
Don't watch .mount cgroups to reduce number of inotify watches
Fix NVML initialization race condition
Fix brtfs disk metrics when using a subdirectory of a subvolume
INFO | Got prediction result: 0.9940581321716309
INFO | Matched expected positive prediction result
INFO | Testing negative text:
action required
1. Currently, if users were to explicitly specify CacheSize of 0 for
KMS provider, they would end-up with a provider that caches up to
1000 keys. This PR changes this behavior.
Post this PR, when users supply 0 for CacheSize this will result in
a validation error.
2. CacheSize type was changed from int32 to *int32. This allows
defaulting logic to differentiate between cases where users
explicitly supplied 0 vs. not supplied any value.
3. KMS Provider's endpoint (path to Unix socket) is now validated when
the EncryptionConfiguration files is loaded. This used to be handled
by the GRPCService.
INFO | Got prediction result: 0.1251964420080185
INFO | Matched expected negative prediction result
&lt;/code>&lt;/pre>&lt;p>Both tests are real-world examples which already exist. We could also try
something completely different, like this random tweet I found a couple of
minutes ago:&lt;/p>
&lt;pre>&lt;code>./main predict &amp;quot;My dudes, if you can understand SYN-ACK, you can understand consent&amp;quot;
INFO | Got prediction result: 0.1251964420080185
ERROR | Result is lower than selected threshold 0.6
&lt;/code>&lt;/pre>&lt;p>Looks like it is not classified as bug for a release note, which seems to work.
Selecting a good threshold is also not that easy, but sticking to something &amp;gt;
50% should be the bare minimum.&lt;/p>
&lt;h1 id="automate-everything">Automate Everything&lt;/h1>
&lt;p>The next step is to find some way of automation to continuously update the model
with new data. If I change any source code within my repository, then I’d like
to get feedback about the test results of the model without having a need to run
the training on my own machine. I would like to utilize the GPUs in my
Kubernetes cluster to train faster and automatically update the data set if a PR
got merged.&lt;/p>
&lt;p>With the help of &lt;a href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview">Kubeflow pipelines&lt;/a> we can fulfill most of these
requirements. The pipeline I built looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/kubeflow-pipeline.png" alt="pipeline">&lt;/p>
&lt;p>First, we check out the source code of the PR, which will be passed on as output
artifact to all other steps. Then we incrementally update the API and internal
data before we run the training on an always up-to-date data set. The prediction
test verifies after the training that we did not badly influence the model with
our changes.&lt;/p>
&lt;p>We also built a container image within our pipeline. &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/Dockerfile-deploy">This container image&lt;/a>
copies the previously built model, vectorizer, and selector into a container and
runs &lt;code>./main serve&lt;/code>. When doing this, we spin up a &lt;a href="https://www.kubeflow.org/docs/components/serving/kfserving">kfserving&lt;/a> web server,
which can be used for prediction purposes. Do you want to try it out by yourself? Simply
do a JSON POST request like this and run the prediction against the endpoint:&lt;/p>
&lt;pre>&lt;code>&amp;gt; curl https://kfserving.k8s.saschagrunert.de/v1/models/kubernetes-analysis:predict \
-d '{&amp;quot;text&amp;quot;: &amp;quot;my test text&amp;quot;}'
{&amp;quot;result&amp;quot;: 0.1251964420080185}
&lt;/code>&lt;/pre>&lt;p>The &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/kfserver.py">custom kfserving&lt;/a> implementation is pretty straightforward, whereas the
deployment utilizes &lt;a href="https://knative.dev/docs/serving">Knative Serving&lt;/a> and an &lt;a href="https://istio.io">Istio&lt;/a> ingress gateway
under the hood to correctly route the traffic into the cluster and provide the
right set of services.&lt;/p>
&lt;p>The &lt;code>commit-changes&lt;/code> and &lt;code>rollout&lt;/code> step will only run if the pipeline runs on
the &lt;code>master&lt;/code> branch. Those steps make sure that we always have the latest data
set available on the master branch as well as in the kfserving deployment. The
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/rollout.py#L30-L51">rollout step&lt;/a> creates a new canary deployment, which only accepts 50% of the
incoming traffic in the first place. After the canary got deployed successfully,
it will be promoted as the new main instance of the service. This is a great way
to ensure that the deployment works as intended and allows additional testing
after rolling out the canary.&lt;/p>
&lt;p>But how to trigger Kubeflow pipelines when creating a pull request? Kubeflow has
no feature for that right now. That’s why I decided to use &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>,
Kubernetes test-infrastructure project for CI/CD purposes.&lt;/p>
&lt;p>First of all, a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/ci/config.yaml#L45-L61">24h periodic job&lt;/a> ensures that we have at least daily
up-to-date data available within the repository. Then, if we create a pull
request, Prow will run the whole Kubeflow pipeline without committing or rolling
out any changes. If we merge the pull request via Prow, another job runs on the
master and updates the data as well as the deployment. That’s pretty neat, isn’t
it?&lt;/p>
&lt;h1 id="automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/h1>
&lt;p>The prediction API is nice for testing, but now we need a real-world use case.
Prow supports external plugins which can be used to take action on any GitHub
event. I wrote &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/pkg">a plugin&lt;/a> which uses the kfserving API to make predictions
based on new pull requests. This means if we now create a new pull request in
the kubernetes-analysis repository, we will see the following:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-1.png" alt="pr 1">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-2.png" alt="pr 2">&lt;/p>
&lt;p>Okay cool, so now let’s change the release note based on a real bug from the
already existing dataset:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-3.png" alt="pr 3">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-4.png" alt="pr 4">&lt;/p>
&lt;p>The bot edits its own comment, predicts it with round about 90% as &lt;code>kind/bug&lt;/code>
and automatically adds the correct label! Now, if we change it back to some
different - obviously wrong - release note:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-5.png" alt="pr 5">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-6.png" alt="pr 6">&lt;/p>
&lt;p>The bot does the work for us, removes the label and informs us what it did!
Finally, if we change the release note to &lt;code>None&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-7.png" alt="pr 7">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-8.png" alt="pr 8">&lt;/p>
&lt;p>The bot removed the comment, which is nice and reduces the text noise on the PR.
Everything I demonstrated is running inside a single Kubernetes cluster, which
would make it unnecessary at all to expose the kfserving API to the public. This
introduces an indirect API rate limiting because the only usage would be
possible via the Prow bot user.&lt;/p>
&lt;p>If you want to try it out for yourself, feel free to open a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/issues/new?&amp;amp;template=release-notes-test.md">new test
issue&lt;/a> in &lt;code>kubernetes-analysis&lt;/code>. This works because I enabled the plugin
also for issues rather than only for pull requests.&lt;/p>
&lt;p>So then, we have a running CI bot which is able to classify new release notes
based on a machine learning model. If the bot would run in the official
Kubernetes repository, then we could correct wrong label predictions manually.
This way, the next training iteration would pick up the correction and result in
a continuously improved model over time. All totally automated!&lt;/p>
&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>Thank you for reading down to here! This was my little data science journey
through the Kubernetes GitHub repository. There are a lot of other things to
optimize, for example introducing more classes (than just &lt;code>kind/bug&lt;/code> or nothing)
or automatic hyperparameter tuning with Kubeflows &lt;a href="https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter">Katib&lt;/a>. If you have any
questions or suggestions, then feel free to get in touch with me anytime. See you
soon!&lt;/p></description></item><item><title>Blog: An Introduction to the K8s-Infrastructure Working Group</title><link>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/kiran_oliver">Kiran &amp;quot;Rin&amp;quot; Oliver&lt;/a> Storyteller, Kubernetes Upstream Marketing Team&lt;/p>
&lt;h1 id="an-introduction-to-the-k8s-infrastructure-working-group">An Introduction to the K8s-Infrastructure Working Group&lt;/h1>
&lt;p>&lt;em>Welcome to part one of a new series introducing the K8s-Infrastructure working group!&lt;/em>&lt;/p>
&lt;p>When Kubernetes was formed in 2014, Google undertook the task of building and maintaining the infrastructure necessary for keeping the project running smoothly. The tools itself were open source, but the Google Cloud Platform project used to run the infrastructure was internal-only, preventing contributors from being able to help out. In August 2018, Google granted the Cloud Native Computing Foundation &lt;a href="https://cloud.google.com/blog/products/gcp/google-cloud-grants-9m-in-credits-for-the-operation-of-the-kubernetes-project">$9M in credits for the operation of Kubernetes&lt;/a>. The sentiment behind this was that a project such as Kubernetes should be both maintained and operated by the community itself rather than by a single vendor.&lt;/p>
&lt;p>A group of community members enthusiastically undertook the task of collaborating on the path forward, realizing that there was a &lt;a href="https://github.com/kubernetes/community/issues/2715">more formal infrastructure necessary&lt;/a>. They joined together as a cross-team working group with ownership spanning across multiple Kubernetes SIGs (Architecture, Contributor Experience, Release, and Testing). &lt;a href="https://twitter.com/spiffxp">Aaron Crickenberger&lt;/a> worked with the Kubernetes Steering Committee to enable the formation of the working group, co-drafting a charter alongside long-time collaborator &lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a>, and by 2019 the working group was official.&lt;/p>
&lt;h2 id="what-issues-does-the-k8s-infrastructure-working-group-tackle">What Issues Does the K8s-Infrastructure Working Group Tackle?&lt;/h2>
&lt;p>The team took on the complex task of managing the many moving parts of the infrastructure that sustains Kubernetes as a project.&lt;/p>
&lt;p>The need started with necessity: the first problem they took on was a complete migration of all of the project's infrastructure from Google-owned infrastructure to the Cloud Native Computing Foundation (CNCF). This is being done so that the project is self-sustainable without the need of any direct assistance from individual vendors. This breaks down in the following ways:&lt;/p>
&lt;ul>
&lt;li>Identifying what infrastructure the Kubernetes project depends on.
&lt;ul>
&lt;li>What applications are running?&lt;/li>
&lt;li>Where does it run?&lt;/li>
&lt;li>Where is its source code?&lt;/li>
&lt;li>What is custom built?&lt;/li>
&lt;li>What is off-the-shelf?&lt;/li>
&lt;li>What services depend on each other?&lt;/li>
&lt;li>How is it administered?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting guidelines and policies for how to run the infrastructure as a community.
&lt;ul>
&lt;li>What are our access policies?&lt;/li>
&lt;li>How do we keep track of billing?&lt;/li>
&lt;li>How do we ensure privacy and security?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Migrating infrastructure over to the CNCF as-is.
&lt;ul>
&lt;li>What is the path of least resistance to migration?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Improving the state of the infrastructure for sustainability.
&lt;ul>
&lt;li>Moving from humans running scripts to a more automated GitOps model (YAML all the things!)&lt;/li>
&lt;li>Supporting community members who wish to develop new infrastructure&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting the state of our efforts, better defining goals, and completeness indicators.
&lt;ul>
&lt;li>The project and program management necessary to communicate this work to our &lt;a href="https://kubernetes.io/blog/2020/04/21/contributor-communication/">massive community of contributors&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="the-challenge-of-k8s-infrastructure-is-documentation">The challenge of K8s-Infrastructure is documentation&lt;/h2>
&lt;p>The most crucial problem the working group is trying to tackle is that the project is all volunteer-led. This leads to contributors, chairs, and others involved in the project quickly becoming overscheduled. As a result of this, certain areas such as documentation and organization often lack information, and efforts to progress are taking longer than the group would like to complete.&lt;/p>
&lt;p>Some of the infrastructure that is being migrated over hasn't been updated in a while, and its original authors or directly responsible individuals have moved on from working on Kubernetes. While this is great from the perspective of the fact that the code was able to run untouched for a long period of time, from the perspective of trying to migrate, this makes it difficult to identify how to operate these components, and how to move these infrastructure pieces where they need to be effectively.&lt;/p>
&lt;p>The lack of documentation is being addressed head-on by group member &lt;a href="https://twitter.com/bartsmykla">Bart Smykla&lt;/a>, but there is a definite need for others to support. If you're looking for a way to &lt;a href="https://github.com/kubernetes/community/labels/wg%2Fk8s-infra">get involved&lt;/a> and learn the infrastructure, you can become a new contributor to the working group!&lt;/p>
&lt;h2 id="celebrating-some-working-group-wins">Celebrating some Working Group wins&lt;/h2>
&lt;p>The team has made progress in the last few months that is well worth celebrating.&lt;/p>
&lt;ul>
&lt;li>The K8s-Infrastructure Working Group released an automated billing report that they start every meeting off by reviewing as a group.&lt;/li>
&lt;li>DNS for k8s.io and kubernetes.io are also fully &lt;a href="https://groups.google.com/g/kubernetes-dev/c/LZTYJorGh7c/m/u-ydk-yNEgAJ">community-owned&lt;/a>, with community members able to &lt;a href="https://github.com/kubernetes/k8s.io/issues/new?assignees=&amp;amp;labels=wg%2Fk8s-infra&amp;amp;template=dns-request.md&amp;amp;title=DNS+REQUEST%3A+%3Cyour-dns-record%3E">file issues&lt;/a> to manage records.&lt;/li>
&lt;li>The container registry &lt;a href="https://github.com/kubernetes/k8s.io/tree/master/k8s.gcr.io">k8s.gcr.io&lt;/a> is also fully community-owned and available for all Kubernetes subprojects to use.&lt;/li>
&lt;li>The Kubernetes &lt;a href="https://github.com/kubernetes/publishing-bot">publishing-bot&lt;/a> responsible for keeping k8s.io/kubernetes/staging repositories published to their own top-level repos (For example: &lt;a href="https://github.com/kubernetes/api">kubernetes/api&lt;/a>) runs on a community-owned cluster.&lt;/li>
&lt;li>The gcsweb.k8s.io service used to provide anonymous access to GCS buckets for kubernetes artifacts runs on a community-owned cluster.&lt;/li>
&lt;li>There is also an automated process of promoting all our container images. This includes a fully documented infrastructure, managed by the Kubernetes community, with automated processes for provisioning permissions.&lt;/li>
&lt;/ul>
&lt;p>These are just a few of the things currently happening in the K8s Infrastructure working group.&lt;/p>
&lt;p>If you're interested in getting involved, be sure to join the &lt;a href="https://app.slack.com/client/T09NY5SBT/CCK68P2Q2">#wg-K8s-infra Slack Channel&lt;/a>. Meetings are 60 minutes long, and are held every other Wednesday at 8:30 AM PT/16:30 UTC.&lt;/p>
&lt;p>Join to help with the documentation, stay to learn about the amazing infrastructure supporting the Kubernetes community.&lt;/p></description></item><item><title>Blog: WSL+Docker: Kubernetes on the Windows Desktop</title><link>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://twitter.com/nunixtech">Nuno do Carmo&lt;/a> Docker Captain and WSL Corsair; &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>, Developer Advocate, Cloud Native Computing Foundation&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>New to Windows 10 and WSL2, or new to Docker and Kubernetes? Welcome to this blog post where we will install from scratch Kubernetes in Docker &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> and &lt;a href="https://minikube.sigs.k8s.io/docs/">Minikube&lt;/a>.&lt;/p>
&lt;h1 id="why-kubernetes-on-windows">Why Kubernetes on Windows?&lt;/h1>
&lt;p>For the last few years, Kubernetes became a de-facto standard platform for running containerized services and applications in distributed environments. While a wide variety of distributions and installers exist to deploy Kubernetes in the cloud environments (public, private or hybrid), or within the bare metal environments, there is still a need to deploy and run Kubernetes locally, for example, on the developer's workstation.&lt;/p>
&lt;p>Kubernetes has been originally designed to be deployed and used in the Linux environments. However, a good number of users (and not only application developers) use Windows OS as their daily driver. When Microsoft revealed WSL - &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/">the Windows Subsystem for Linux&lt;/a>, the line between Windows and Linux environments became even less visible.&lt;/p>
&lt;p>Also, WSL brought an ability to run Kubernetes on Windows almost seamlessly!&lt;/p>
&lt;p>Below, we will cover in brief how to install and use various solutions to run Kubernetes locally.&lt;/p>
&lt;h1 id="prerequisites">Prerequisites&lt;/h1>
&lt;p>Since we will explain how to install KinD, we won't go into too much detail around the installation of KinD's dependencies.&lt;/p>
&lt;p>However, here is the list of the prerequisites needed and their version/lane:&lt;/p>
&lt;ul>
&lt;li>OS: Windows 10 version 2004, Build 19041&lt;/li>
&lt;li>&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-install">WSL2 enabled&lt;/a>
&lt;ul>
&lt;li>In order to install the distros as WSL2 by default, once WSL2 installed, run the command &lt;code>wsl.exe --set-default-version 2&lt;/code> in Powershell&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>WSL2 distro installed from the Windows Store - the distro used is Ubuntu-18.04&lt;/li>
&lt;li>&lt;a href="https://hub.docker.com/editions/community/docker-ce-desktop-windows">Docker Desktop for Windows&lt;/a>, stable channel - the version used is 2.2.0.4&lt;/li>
&lt;li>[Optional] Microsoft Terminal installed from the Windows Store
&lt;ul>
&lt;li>Open the Windows store and type &amp;quot;Terminal&amp;quot; in the search, it will be (normally) the first option&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-windows-store-terminal.png" alt="Windows Store Terminal">&lt;/p>
&lt;p>And that's actually it. For Docker Desktop for Windows, no need to configure anything yet as we will explain it in the next section.&lt;/p>
&lt;h1 id="wsl2-first-contact">WSL2: First contact&lt;/h1>
&lt;p>Once everything is installed, we can launch the WSL2 terminal from the Start menu, and type &amp;quot;Ubuntu&amp;quot; for searching the applications and documents:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-start-menu-search.png" alt="Start Menu Search">&lt;/p>
&lt;p>Once found, click on the name and it will launch the default Windows console with the Ubuntu bash shell running.&lt;/p>
&lt;p>Like for any normal Linux distro, you need to create a user and set a password:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-user-password.png" alt="User-Password">&lt;/p>
&lt;h2 id="optional-update-the-sudoers">[Optional] Update the &lt;code>sudoers&lt;/code>&lt;/h2>
&lt;p>As we are working, normally, on our local computer, it might be nice to update the &lt;code>sudoers&lt;/code> and set the group &lt;code>%sudo&lt;/code> to be password-less:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the sudoers with the visudo command&lt;/span>
sudo visudo
&lt;span style="color:#080;font-style:italic"># Change the %sudo group to be password-less&lt;/span>
%sudo &lt;span style="color:#b8860b">ALL&lt;/span>&lt;span style="color:#666">=(&lt;/span>ALL:ALL&lt;span style="color:#666">)&lt;/span> NOPASSWD: ALL
&lt;span style="color:#080;font-style:italic"># Press CTRL+X to exit&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Y to save&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Enter to confirm&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-visudo.png" alt="visudo">&lt;/p>
&lt;h2 id="update-ubuntu">Update Ubuntu&lt;/h2>
&lt;p>Before we move to the Docker Desktop settings, let's update our system and ensure we start in the best conditions:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Update the repositories and list of the packages available&lt;/span>
sudo apt update
&lt;span style="color:#080;font-style:italic"># Update the system based on the packages installed &amp;gt; the &amp;#34;-y&amp;#34; will approve the change automatically&lt;/span>
sudo apt upgrade -y
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-apt-update-upgrade.png" alt="apt-update-upgrade">&lt;/p>
&lt;h1 id="docker-desktop-faster-with-wsl2">Docker Desktop: faster with WSL2&lt;/h1>
&lt;p>Before we move into the settings, let's do a small test, it will display really how cool the new integration with Docker Desktop is:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-error.png" alt="kubectl-error">&lt;/p>
&lt;p>You got an error? Perfect! It's actually good news, so let's now move on to the settings.&lt;/p>
&lt;h2 id="docker-desktop-settings-enable-wsl2-integration">Docker Desktop settings: enable WSL2 integration&lt;/h2>
&lt;p>First let's start Docker Desktop for Windows if it's not still the case. Open the Windows start menu and type &amp;quot;docker&amp;quot;, click on the name to start the application:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-start.png" alt="docker-start">&lt;/p>
&lt;p>You should now see the Docker icon with the other taskbar icons near the clock:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-taskbar.png" alt="docker-taskbar">&lt;/p>
&lt;p>Now click on the Docker icon and choose settings. A new window will appear:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-general.png" alt="docker-settings-general">&lt;/p>
&lt;p>By default, the WSL2 integration is not active, so click the &amp;quot;Enable the experimental WSL 2 based engine&amp;quot; and click &amp;quot;Apply &amp;amp; Restart&amp;quot;:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-wsl2-activated.png" alt="docker-settings-wsl2">&lt;/p>
&lt;p>What this feature did behind the scenes was to create two new distros in WSL2, containing and running all the needed backend sockets, daemons and also the CLI tools (read: docker and kubectl command).&lt;/p>
&lt;p>Still, this first setting is still not enough to run the commands inside our distro. If we try, we will have the same error as before.&lt;/p>
&lt;p>In order to fix it, and finally be able to use the commands, we need to tell the Docker Desktop to &amp;quot;attach&amp;quot; itself to our distro also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-resources-wsl-integration.png" alt="docker-resources-wsl">&lt;/p>
&lt;p>Let's now switch back to our WSL2 terminal and see if we can (finally) launch the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-success.png" alt="docker-kubectl-success">&lt;/p>
&lt;blockquote>
&lt;p>Tip: if nothing happens, restart Docker Desktop and restart the WSL process in Powershell: &lt;code>Restart-Service LxssManager&lt;/code> and launch a new Ubuntu session&lt;/p>
&lt;/blockquote>
&lt;p>And success! The basic settings are now done and we move to the installation of KinD.&lt;/p>
&lt;h1 id="kind-kubernetes-made-easy-in-a-container">KinD: Kubernetes made easy in a container&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install KinD and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to on the &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">official KinD website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of KinD&lt;/span>
curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-linux-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./kind
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./kind /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install.png" alt="kind-install">&lt;/p>
&lt;h2 id="kind-the-first-cluster">KinD: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Create the cluster and give it a name (optional)&lt;/span>
kind create cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Check if the .kube has been created and populated with files&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create.png" alt="kind-cluster-create">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>The cluster has been successfully created, and because we are using Docker Desktop, the network is all set for us to use &amp;quot;as is&amp;quot;.&lt;/p>
&lt;p>So we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-k8s-master.png" alt="kind-browser-k8s-master">&lt;/p>
&lt;p>And this is the real strength from Docker Desktop for Windows with the WSL2 backend. Docker really did an amazing integration.&lt;/p>
&lt;h2 id="kind-counting-1-2-3">KinD: counting 1 - 2 - 3&lt;/h2>
&lt;p>Our first cluster was created and it's the &amp;quot;normal&amp;quot; one node cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-nodes-services.png" alt="kind-list-nodes-services">&lt;/p>
&lt;p>While this will be enough for most people, let's leverage one of the coolest feature, multi-node clustering:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Delete the existing cluster&lt;/span>
kind delete cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Create a config file for a 3 nodes cluster&lt;/span>
cat &lt;span style="color:#b44">&amp;lt;&amp;lt; EOF &amp;gt; kind-3nodes.yaml
&lt;/span>&lt;span style="color:#b44">kind: Cluster
&lt;/span>&lt;span style="color:#b44">apiVersion: kind.x-k8s.io/v1alpha4
&lt;/span>&lt;span style="color:#b44">nodes:
&lt;/span>&lt;span style="color:#b44"> - role: control-plane
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a new cluster with the config file&lt;/span>
kind create cluster --name wslkindmultinodes --config ./kind-3nodes.yaml
&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create-multinodes.png" alt="kind-cluster-create-multinodes">&lt;/p>
&lt;blockquote>
&lt;p>Tip: depending on how fast we run the &amp;quot;get nodes&amp;quot; command, it can be that not all the nodes are ready, wait few seconds and run it again, everything should be ready&lt;/p>
&lt;/blockquote>
&lt;p>And that's it, we have created a three-node cluster, and if we look at the services one more time, we will see several that have now three replicas:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-services-multinodes.png" alt="wsl2-kind-list-services-multinodes">&lt;/p>
&lt;h2 id="kind-can-i-see-a-nice-dashboard">KinD: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a> project has been created. The installation and first connection test is quite fast, so let's do it:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the Dashboard application into our cluster&lt;/span>
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
&lt;span style="color:#080;font-style:italic"># Check the resources it created based on the new namespace created&lt;/span>
kubectl get all -n kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install-dashboard.png" alt="kind-install-dashboard">&lt;/p>
&lt;p>As it created a service with a ClusterIP (read: internal network address), we cannot reach it if we type the URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-error.png" alt="kind-browse-dashboard-error">&lt;/p>
&lt;p>That's because we need to create a temporary proxy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Start a kubectl proxy&lt;/span>
kubectl proxy
&lt;span style="color:#080;font-style:italic"># Enter the URL on your browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-success.png" alt="kind-browse-dashboard-success">&lt;/p>
&lt;p>Finally to login, we can either enter a Token, which we didn't create, or enter the &lt;code>kubeconfig&lt;/code> file from our Cluster.&lt;/p>
&lt;p>If we try to login with the &lt;code>kubeconfig&lt;/code>, we will get the error &amp;quot;Internal error (500): Not enough data to create auth info structure&amp;quot;. This is due to the lack of credentials in the &lt;code>kubeconfig&lt;/code> file.&lt;/p>
&lt;p>So to avoid you ending with the same error, let's follow the &lt;a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">recommended RBAC approach&lt;/a>.&lt;/p>
&lt;p>Let's open a new WSL2 session:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a new ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a ClusterRoleBinding for the ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;span style="color:#b44">kind: ClusterRoleBinding
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44">roleRef:
&lt;/span>&lt;span style="color:#b44"> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;span style="color:#b44"> kind: ClusterRole
&lt;/span>&lt;span style="color:#b44"> name: cluster-admin
&lt;/span>&lt;span style="color:#b44">subjects:
&lt;/span>&lt;span style="color:#b44">- kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-rbac-serviceaccount.png" alt="kind-browse-dashboard-rbac-serviceaccount">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get the Token for the ServiceAccount&lt;/span>
kubectl -n kubernetes-dashboard describe secret &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &lt;span style="color:#b44">&amp;#39;{print $1}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#080;font-style:italic"># Copy the token and copy it into the Dashboard login and press &amp;#34;Sign in&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-login-success.png" alt="kind-browse-dashboard-login-success">&lt;/p>
&lt;p>Success! And let's see our nodes listed also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-browse-nodes.png" alt="kind-browse-dashboard-browse-nodes">&lt;/p>
&lt;p>A nice and shiny three nodes appear.&lt;/p>
&lt;h1 id="minikube-kubernetes-from-everywhere">Minikube: Kubernetes from everywhere&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install Minikube and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to from the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Kubernetes.io website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of Minikube&lt;/span>
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./minikube
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./minikube /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install.png" alt="minikube-install">&lt;/p>
&lt;h2 id="minikube-updating-the-host">Minikube: updating the host&lt;/h2>
&lt;p>If we follow the how-to, it states that we should use the &lt;code>--driver=none&lt;/code> flag in order to run Minikube directly on the host and Docker.&lt;/p>
&lt;p>Unfortunately, we will get an error about &amp;quot;conntrack&amp;quot; being required to run Kubernetes v 1.18:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error.png" alt="minikube-start-error">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>So let's fix the issue by installing the missing package:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the conntrack package&lt;/span>
sudo apt install -y conntrack
&lt;/code>&lt;/pre>&lt;/div>&lt;p>![minikube-install-conntrack](/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install conntrack.png)&lt;/p>
&lt;p>Let's try to launch it again:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;span style="color:#080;font-style:italic"># We got a permissions error &amp;gt; try again with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error-systemd.png" alt="minikube-start-error-systemd">&lt;/p>
&lt;p>Ok, this error cloud be problematic ... in the past. Luckily for us, there's a solution&lt;/p>
&lt;h2 id="minikube-enabling-systemd">Minikube: enabling SystemD&lt;/h2>
&lt;p>In order to enable SystemD on WSL2, we will apply the &lt;a href="https://forum.snapcraft.io/t/running-snaps-on-wsl2-insiders-only-for-now/13033">scripts&lt;/a> from &lt;a href="https://twitter.com/diddledan">Daniel Llewellyn&lt;/a>.&lt;/p>
&lt;p>I invite you to read the full blog post and how he came to the solution, and the various iterations he did to fix several issues.&lt;/p>
&lt;p>So in a nutshell, here are the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the needed packages&lt;/span>
sudo apt install -yqq daemonize dbus-user-session fontconfig
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-packages.png" alt="minikube-systemd-packages">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the start-systemd-namespace script&lt;/span>
sudo vi /usr/sbin/start-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">||&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f">set&lt;/span> -o posix; &lt;span style="color:#a2f">set&lt;/span>&lt;span style="color:#666">)&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^BASH&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^DIRSTACK=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^EUID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^GROUPS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^IFS=&amp;#39;.*&amp;#34;&lt;/span>&lt;span style="color:#b44">$&amp;#39;\n&amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#39;&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LANG=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LOGNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^MACHTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^NAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTERR=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTIND=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OSTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PIPESTATUS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^POSIXLY_CORRECT=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PPID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS1=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS4=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELLOPTS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHLVL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SYSTEMD_PID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^UID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^USER=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^_=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> cat - &amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;PATH=&amp;#39;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#39;&amp;#34;&lt;/span> &amp;gt;&amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> sudo /usr/sbin/enter-systemd-namespace &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$BASH_EXECUTION_STRING&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the enter-systemd-namespace&lt;/span>
sudo vi /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$UID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">0&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;You need to run &lt;/span>&lt;span style="color:#b8860b">$0&lt;/span>&lt;span style="color:#b44"> through sudo&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exit&lt;/span> &lt;span style="color:#666">1&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
/usr/sbin/daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit&lt;span style="color:#666">=&lt;/span>basic.target
&lt;span style="color:#a2f;font-weight:bold">while&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">do&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">done&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;/bin/bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /usr/bin/sudo -H -u &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/bash -c &lt;span style="color:#b44">&amp;#39;set -a; source &amp;#34;$HOME/.systemd-env&amp;#34;; set +a; exec bash -c &amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>&lt;span style="color:#a2f">printf&lt;/span> &lt;span style="color:#b44">&amp;#34;%q&amp;#34;&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$@&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">else&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/login -p -f &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>/bin/cat &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span> | grep -v &lt;span style="color:#b44">&amp;#34;^PATH=&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;Existential crisis&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the permissions of the enter-systemd-namespace script&lt;/span>
sudo chmod +x /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic"># Edit the bash.bashrc file&lt;/span>
sudo sed -i 2a&lt;span style="color:#b44">&amp;#34;# Start or enter a PID namespace in WSL2\nsource /usr/sbin/start-systemd-namespace\n&amp;#34;&lt;/span> /etc/bash.bashrc
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-files.png" alt="minikube-systemd-files">&lt;/p>
&lt;p>Finally, exit and launch a new session. You &lt;strong>do not&lt;/strong> need to stop WSL2, a new session is enough:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-enabled.png" alt="minikube-systemd-enabled">&lt;/p>
&lt;h2 id="minikube-the-first-cluster">Minikube: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Check if the .minikube directory is created &amp;gt; if yes, delete it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Create the cluster with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to be able to use &lt;code>kubectl&lt;/code> with our user, and not &lt;code>sudo&lt;/code>, Minikube recommends running the &lt;code>chown&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Change the owner of the .kube and .minikube directories&lt;/span>
sudo chown -R &lt;span style="color:#b8860b">$USER&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Check the access and if the cluster is running&lt;/span>
kubectl cluster-info
&lt;span style="color:#080;font-style:italic"># Check the resources created&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-fixed.png" alt="minikube-start-fixed">&lt;/p>
&lt;p>The cluster has been successfully created, and Minikube used the WSL2 IP, which is great for several reasons, and one of them is that we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master.png" alt="minikube-browse-k8s-master">&lt;/p>
&lt;p>And the real strength of WSL2 integration, the port &lt;code>8443&lt;/code> once open on WSL2 distro, it actually forwards it to Windows, so instead of the need to remind the IP address, we can also reach the &lt;code>Kubernetes master&lt;/code> URL via &lt;code>localhost&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master-localhost.png" alt="minikube-browse-k8s-master-localhost">&lt;/p>
&lt;h2 id="minikube-can-i-see-a-nice-dashboard">Minikube: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, Minikube embeded the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a>. Thanks to it, running and accessing the Dashboard is very simple:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Enable the Dashboard service&lt;/span>
sudo minikube dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard.png" alt="minikube-browse-dashboard">&lt;/p>
&lt;p>The command creates also a proxy, which means that once we end the command, by pressing &lt;code>CTRL+C&lt;/code>, the Dashboard will no more be accessible.&lt;/p>
&lt;p>Still, if we look at the namespace &lt;code>kubernetes-dashboard&lt;/code>, we will see that the service is still created:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-get-all.png" alt="minikube-dashboard-get-all">&lt;/p>
&lt;p>Let's edit the service and change it's type to &lt;code>LoadBalancer&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the Dashoard service&lt;/span>
kubectl edit service/kubernetes-dashboard --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Go to the very end and remove the last 2 lines&lt;/span>
status:
loadBalancer: &lt;span style="color:#666">{}&lt;/span>
&lt;span style="color:#080;font-style:italic"># Change the type from ClusterIO to LoadBalancer&lt;/span>
type: LoadBalancer
&lt;span style="color:#080;font-style:italic"># Save the file&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-type-loadbalancer.png" alt="minikube-dashboard-type-loadbalancer">&lt;/p>
&lt;p>Check again the Dashboard service and let's access the Dashboard via the LoadBalancer:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side with the URL: localhost:&amp;lt;port exposed&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard-loadbalancer.png" alt="minikube-browse-dashboard-loadbalancer">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>It's clear that we are far from done as we could have some LoadBalancing implemented and/or other services (storage, ingress, registry, etc...).&lt;/p>
&lt;p>Concerning Minikube on WSL2, as it needed to enable SystemD, we can consider it as an intermediate level to be implemented.&lt;/p>
&lt;p>So with two solutions, what could be the &amp;quot;best for you&amp;quot;? Both bring their own advantages and inconveniences, so here an overview from our point of view solely:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Criteria&lt;/th>
&lt;th>KinD&lt;/th>
&lt;th>Minikube&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Installation on WSL2&lt;/td>
&lt;td>Very Easy&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-node&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Plugins&lt;/td>
&lt;td>Manual install&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Persistence&lt;/td>
&lt;td>Yes, however not designed for&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Alternatives&lt;/td>
&lt;td>K3d&lt;/td>
&lt;td>Microk8s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We hope you could have a real taste of the integration between the different components: WSL2 - Docker Desktop - KinD/Minikube. And that gave you some ideas or, even better, some answers to your Kubernetes workflows with KinD and/or Minikube on Windows and WSL2.&lt;/p>
&lt;p>See you soon for other adventures in the Kubernetes ocean.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/nunixtech">Nuno&lt;/a> &amp;amp; &lt;a href="https://twitter.com/idvoretskyi">Ihor&lt;/a>&lt;/p></description></item><item><title>Blog: How Docs Handle Third Party and Dual Sourced Content</title><link>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>Late last summer, SIG Docs started a community conversation about third party content in Kubernetes docs. This conversation became a &lt;a href="https://github.com/kubernetes/enhancements/pull/1327">Kubernetes Enhancement Proposal&lt;/a> (KEP) and, after five months for review and comment, SIG Architecture approved the KEP as a &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/">content guide&lt;/a> for Kubernetes docs.&lt;/p>
&lt;p>Here's how Kubernetes docs handle third party content now:&lt;/p>
&lt;blockquote>
&lt;p>Links to active content in the Kubernetes project (projects in the kubernetes and kubernetes-sigs GitHub orgs) are always allowed.&lt;/p>
&lt;p>Kubernetes requires some third party content to function. Examples include container runtimes (containerd, CRI-O, Docker), networking policy (CNI plugins), Ingress controllers, and logging.&lt;/p>
&lt;p>Docs can link to third party open source software (OSS) outside the Kubernetes project if it’s necessary for Kubernetes to function.&lt;/p>
&lt;/blockquote>
&lt;p>These common sense guidelines make sure that Kubernetes docs document Kubernetes.&lt;/p>
&lt;h2 id="keeping-the-docs-focused">Keeping the docs focused&lt;/h2>
&lt;p>Our goal is for Kubernetes docs to be a trustworthy guide to Kubernetes features. To achieve this goal, SIG Docs is &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a> and removing any third party content that isn't both in the Kubernetes project &lt;em>and&lt;/em> required for Kubernetes to function.&lt;/p>
&lt;h3 id="re-homing-content">Re-homing content&lt;/h3>
&lt;p>Some content will be removed that readers may find helpful. To make sure readers have continous access to information, we're giving stakeholders until the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.19">1.19 release deadline for docs&lt;/a>, &lt;strong>July 9th, 2020&lt;/strong> to re-home any content slated for removal.&lt;/p>
&lt;p>Over the next few months you'll see less third party content in the docs as contributors open PRs to remove content.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Over time, SIG Docs observed increasing vendor content in the docs. Some content took the form of vendor-specific implementations that aren't required for Kubernetes to function in-project. Other content was thinly-disguised advertising with minimal to no feature content. Some vendor content was new; other content had been in the docs for years. It became clear that the docs needed clear, well-bounded guidelines for what kind of third party content is and isn't allowed. The &lt;a href="https://kubernetes.io/docs/contribute/content-guide/">content guide&lt;/a> emerged from an extensive period for review and comment from the community.&lt;/p>
&lt;p>Docs work best when they're accurate, helpful, trustworthy, and remain focused on features. In our experience, vendor content dilutes trust and accuracy.&lt;/p>
&lt;p>Put simply: feature docs aren't a place for vendors to advertise their products. Our content policy keeps the docs focused on helping developers and cluster admins, not on marketing.&lt;/p>
&lt;h2 id="dual-sourced-content">Dual sourced content&lt;/h2>
&lt;p>Less impactful but also important is how Kubernetes docs handle &lt;em>dual-sourced content&lt;/em>. Dual-sourced content is content published in more than one location, or from a non-canonical source.&lt;/p>
&lt;p>From the &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/#dual-sourced-content">Kubernetes content guide&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Wherever possible, Kubernetes docs link to canonical sources instead of hosting dual-sourced content.&lt;/p>
&lt;/blockquote>
&lt;p>Minimizing dual-sourced content streamlines the docs and makes content across the Web more searchable. We're working to consolidate and redirect dual-sourced content in the Kubernetes docs as well.&lt;/p>
&lt;h2 id="ways-to-contribute">Ways to contribute&lt;/h2>
&lt;p>We're tracking third-party content in an &lt;a href="https://github.com/kubernetes/website/issues/20232">issue in the Kubernetes website repository&lt;/a>. If you see third party content that's out of project and isn't required for Kubernetes to function, please comment on the tracking issue.&lt;/p>
&lt;p>Feel free to open a PR that removes non-conforming content once you've identified it!&lt;/p>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>For more information, read the issue description for &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing PodTopologySpread</title><link>https://kubernetes.io/blog/2020/05/Introducing-PodTopologySpread/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/Introducing-PodTopologySpread/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Wei Huang (IBM), Aldo Culquicondor (Google)&lt;/p>
&lt;p>Managing Pods distribution across a cluster is hard. The well-known Kubernetes
features for Pod affinity and anti-affinity, allow some control of Pod placement
in different topologies. However, these features only resolve part of Pods
distribution use cases: either place unlimited Pods to a single topology, or
disallow two Pods to co-locate in the same topology. In between these two
extreme cases, there is a common need to distribute the Pods evenly across the
topologies, so as to achieve better cluster utilization and high availability of
applications.&lt;/p>
&lt;p>The PodTopologySpread scheduling plugin (originally proposed as EvenPodsSpread)
was designed to fill that gap. We promoted it to beta in 1.18.&lt;/p>
&lt;h2 id="api-changes">API changes&lt;/h2>
&lt;p>A new field &lt;code>topologySpreadConstraints&lt;/code> is introduced in the Pod's spec API:&lt;/p>
&lt;pre>&lt;code>spec:
topologySpreadConstraints:
- maxSkew: &amp;lt;integer&amp;gt;
topologyKey: &amp;lt;string&amp;gt;
whenUnsatisfiable: &amp;lt;string&amp;gt;
labelSelector: &amp;lt;object&amp;gt;
&lt;/code>&lt;/pre>&lt;p>As this API is embedded in Pod's spec, you can use this feature in all the
high-level workload APIs, such as Deployment, DaemonSet, StatefulSet, etc.&lt;/p>
&lt;p>Let's see an example of a cluster to understand this API.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/api.png" alt="API">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>labelSelector&lt;/strong> is used to find matching Pods. For each topology, we count
the number of Pods that match this label selector. In the above example, given
the labelSelector as &amp;quot;app: foo&amp;quot;, the matching number in &amp;quot;zone1&amp;quot; is 2; while
the number in &amp;quot;zone2&amp;quot; is 0.&lt;/li>
&lt;li>&lt;strong>topologyKey&lt;/strong> is the key that defines a topology in the Nodes' labels. In
the above example, some Nodes are grouped into &amp;quot;zone1&amp;quot; if they have the label
&amp;quot;zone=zone1&amp;quot; label; while other ones are grouped into &amp;quot;zone2&amp;quot;.&lt;/li>
&lt;li>&lt;strong>maxSkew&lt;/strong> describes the maximum degree to which Pods can be unevenly
distributed. In the above example:
&lt;ul>
&lt;li>if we put the incoming Pod to &amp;quot;zone1&amp;quot;, the skew on &amp;quot;zone1&amp;quot; will become 3 (3
Pods matched in &amp;quot;zone1&amp;quot;; global minimum of 0 Pods matched on &amp;quot;zone2&amp;quot;), which
violates the &amp;quot;maxSkew: 1&amp;quot; constraint.&lt;/li>
&lt;li>if the incoming Pod is placed to &amp;quot;zone2&amp;quot;, the skew on &amp;quot;zone2&amp;quot; is 0 (1 Pod
matched in &amp;quot;zone2&amp;quot;; global minimum of 1 Pod matched on &amp;quot;zone2&amp;quot; itself),
which satisfies the &amp;quot;maxSkew: 1&amp;quot; constraint. Note that the skew is
calculated per each qualified Node, instead of a global skew.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>whenUnsatisfiable&lt;/strong> specifies, when &amp;quot;maxSkew&amp;quot; can't be satisfied, what
action should be taken:
&lt;ul>
&lt;li>&lt;code>DoNotSchedule&lt;/code> (default) tells the scheduler not to schedule it. It's a
hard constraint.&lt;/li>
&lt;li>&lt;code>ScheduleAnyway&lt;/code> tells the scheduler to still schedule it while prioritizing
Nodes that reduce the skew. It's a soft constraint.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-usage">Advanced usage&lt;/h2>
&lt;p>As the feature name &amp;quot;PodTopologySpread&amp;quot; implies, the basic usage of this feature
is to run your workload with an absolute even manner (maxSkew=1), or relatively
even manner (maxSkew&amp;gt;=2). See the &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">official
document&lt;/a>
for more details.&lt;/p>
&lt;p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.&lt;/p>
&lt;h3 id="usage-along-with-nodeselector-nodeaffinity">Usage along with NodeSelector / NodeAffinity&lt;/h3>
&lt;p>You may have found that we didn't have a &amp;quot;topologyValues&amp;quot; field to limit which
topologies the Pods are going to be scheduled to. By default, it is going to
search all Nodes and group them by &amp;quot;topologyKey&amp;quot;. Sometimes this may not be the
ideal case. For instance, suppose there is a cluster with Nodes tagged with
&amp;quot;env=prod&amp;quot;, &amp;quot;env=staging&amp;quot; and &amp;quot;env=qa&amp;quot;, and now you want to evenly place Pods to
the &amp;quot;qa&amp;quot; environment across zones, is it possible?&lt;/p>
&lt;p>The answer is yes. You can leverage the NodeSelector or NodeAffinity API spec.
Under the hood, the PodTopologySpread feature will &lt;strong>honor&lt;/strong> that and calculate
the spread constraints among the nodes that satisfy the selectors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-1.png" alt="Advanced-Usage-1">&lt;/p>
&lt;p>As illustrated above, you can specify &lt;code>spec.affinity.nodeAffinity&lt;/code> to limit the
&amp;quot;searching scope&amp;quot; to be &amp;quot;qa&amp;quot; environment, and within that scope, the Pod will be
scheduled to one zone which satisfies the topologySpreadConstraints. In this
case, it's &amp;quot;zone2&amp;quot;.&lt;/p>
&lt;h3 id="multiple-topologyspreadconstraints">Multiple TopologySpreadConstraints&lt;/h3>
&lt;p>It's intuitive to understand how one single TopologySpreadConstraint works.
What's the case for multiple TopologySpreadConstraints? Internally, each
TopologySpreadConstraint is calculated independently, and the result sets will
be merged to generate the eventual result set - i.e., suitable Nodes.&lt;/p>
&lt;p>In the following example, we want to schedule a Pod to a cluster with 2
requirements at the same time:&lt;/p>
&lt;ul>
&lt;li>place the Pod evenly with Pods across zones&lt;/li>
&lt;li>place the Pod evenly with Pods across nodes&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-2.png" alt="Advanced-Usage-2">&lt;/p>
&lt;p>For the first constraint, there are 3 Pods in zone1 and 2 Pods in zone2, so the
incoming Pod can be only put to zone2 to satisfy the &amp;quot;maxSkew=1&amp;quot; constraint. In
other words, the result set is nodeX and nodeY.&lt;/p>
&lt;p>For the second constraint, there are too many Pods in nodeB and nodeX, so the
incoming Pod can be only put to nodeA and nodeY.&lt;/p>
&lt;p>Now we can conclude the only qualified Node is nodeY - from the intersection of
the sets {nodeX, nodeY} (from the first constraint) and {nodeA, nodeY} (from the
second constraint).&lt;/p>
&lt;p>Multiple TopologySpreadConstraints is powerful, but be sure to understand the
difference with the preceding &amp;quot;NodeSelector/NodeAffinity&amp;quot; example: one is to
calculate result set independently and then interjoined; while the other is to
calculate topologySpreadConstraints based on the filtering results of node
constraints.&lt;/p>
&lt;p>Instead of using &amp;quot;hard&amp;quot; constraints in all topologySpreadConstraints, you can
also combine using &amp;quot;hard&amp;quot; constraints and &amp;quot;soft&amp;quot; constraints to adhere to more
diverse cluster situations.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> If two TopologySpreadConstraints are being applied for the same {topologyKey,
whenUnsatisfiable} tuple, the Pod creation will be blocked returning a
validation error.&lt;/div>
&lt;/blockquote>
&lt;h2 id="podtopologyspread-defaults">PodTopologySpread defaults&lt;/h2>
&lt;p>PodTopologySpread is a Pod level API. As such, to use the feature, workload
authors need to be aware of the underlying topology of the cluster, and then
specify proper &lt;code>topologySpreadConstraints&lt;/code> in the Pod spec for every workload.
While the Pod-level API gives the most flexibility it is also possible to
specify cluster-level defaults.&lt;/p>
&lt;p>The default PodTopologySpread constraints allow you to specify spreading for all
the workloads in the cluster, tailored for its topology. The constraints can be
specified by an operator/admin as PodTopologySpread plugin arguments in the
&lt;a href="https://kubernetes.io/docs/reference/scheduling/profiles/">scheduling profile configuration
API&lt;/a> when starting
kube-scheduler.&lt;/p>
&lt;p>A sample configuration could look like this:&lt;/p>
&lt;pre>&lt;code>apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration
profiles:
pluginConfig:
- name: PodTopologySpread
args:
defaultConstraints:
- maxSkew: 1
topologyKey: example.com/rack
whenUnsatisfiable: ScheduleAnyway
&lt;/code>&lt;/pre>&lt;p>When configuring default constraints, label selectors must be left empty.
kube-scheduler will deduce the label selectors from the membership of the Pod to
Services, ReplicationControllers, ReplicaSets or StatefulSets. Pods can
always override the default constraints by providing their own through the
PodSpec.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> When using default PodTopologySpread constraints, it is recommended to disable
the old DefaultTopologySpread plugin.&lt;/div>
&lt;/blockquote>
&lt;h2 id="wrap-up">Wrap-up&lt;/h2>
&lt;p>PodTopologySpread allows you to define spreading constraints for your workloads
with a flexible and expressive Pod-level API. In the past, workload authors used
Pod AntiAffinity rules to force or hint the scheduler to run a single Pod per
topology domain. In contrast, the new PodTopologySpread constraints allow Pods
to specify skew levels that can be required (hard) or desired (soft). The
feature can be paired with Node selectors and Node affinity to limit the
spreading to specific domains. Pod spreading constraints can be defined for
different topologies such as hostnames, zones, regions, racks, etc.&lt;/p>
&lt;p>Lastly, cluster operators can define default constraints to be applied to all
Pods. This way, Pods don't need to be aware of the underlying topology of the
cluster.&lt;/p></description></item><item><title>Blog: Two-phased Canary Rollout with Open Source Gloo</title><link>https://kubernetes.io/blog/2020/04/Two-phased-Canary-Rollout-With-Gloo/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/Two-phased-Canary-Rollout-With-Gloo/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rick Ducott | &lt;a href="https://github.com/rickducott/">GitHub&lt;/a> | &lt;a href="https://twitter.com/ducott">Twitter&lt;/a>&lt;/p>
&lt;p>Every day, my colleagues and I are talking to platform owners, architects, and engineers who are using &lt;a href="https://github.com/solo-io/gloo">Gloo&lt;/a> as an API gateway
to expose their applications to end users. These applications may span legacy monoliths, microservices, managed cloud services, and Kubernetes
clusters. Fortunately, Gloo makes it easy to set up routes to manage, secure, and observe application traffic while
supporting a flexible deployment architecture to meet the varying production needs of our users.&lt;/p>
&lt;p>Beyond the initial set up, platform owners frequently ask us to help design the operational workflows within their organization:
How do we bring a new application online? How do we upgrade an application? How do we divide responsibilities across our
platform, ops, and development teams?&lt;/p>
&lt;p>In this post, we're going to use Gloo to design a two-phased canary rollout workflow for application upgrades:&lt;/p>
&lt;ul>
&lt;li>In the first phase, we'll do canary testing by shifting a small subset of traffic to the new version. This allows you to safely perform smoke and correctness tests.&lt;/li>
&lt;li>In the second phase, we'll progressively shift traffic to the new version, allowing us to monitor the new version under load, and eventually, decommission the old version.&lt;/li>
&lt;/ul>
&lt;p>To keep it simple, we're going to focus on designing the workflow using &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a>, and we're going to deploy the gateway and
application to Kubernetes. At the end, we'll talk about a few extensions and advanced topics that could be interesting to explore in a follow up.&lt;/p>
&lt;h2 id="initial-setup">Initial setup&lt;/h2>
&lt;p>To start, we need a Kubernetes cluster. This example doesn't take advantage of any cloud specific
features, and can be run against a local test cluster such as &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube&lt;/a>.
This post assumes a basic understanding of Kubernetes and how to interact with it using &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>We'll install the latest &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a> to the &lt;code>gloo-system&lt;/code> namespace and deploy
version &lt;code>v1&lt;/code> of an example application to the &lt;code>echo&lt;/code> namespace. We'll expose this application outside the cluster
by creating a route in Gloo, to end up with a picture like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h3 id="deploying-gloo">Deploying Gloo&lt;/h3>
&lt;p>We'll install gloo with the &lt;code>glooctl&lt;/code> command line tool, which we can download and add to the &lt;code>PATH&lt;/code> with the following
commands:&lt;/p>
&lt;pre>&lt;code>curl -sL https://run.solo.io/gloo/install | sh
export PATH=$HOME/.gloo/bin:$PATH
&lt;/code>&lt;/pre>&lt;p>Now, you should be able to run &lt;code>glooctl version&lt;/code> to see that it is installed correctly:&lt;/p>
&lt;pre>&lt;code>➜ glooctl version
Client: {&amp;quot;version&amp;quot;:&amp;quot;1.3.15&amp;quot;}
Server: version undefined, could not find any version of gloo running
&lt;/code>&lt;/pre>&lt;p>Now we can install the gateway to our cluster with a simple command:&lt;/p>
&lt;pre>&lt;code>glooctl install gateway
&lt;/code>&lt;/pre>&lt;p>The console should indicate the install finishes successfully:&lt;/p>
&lt;pre>&lt;code>Creating namespace gloo-system... Done.
Starting Gloo installation...
Gloo was successfully installed!
&lt;/code>&lt;/pre>&lt;p>Before long, we can see all the Gloo pods running in the &lt;code>gloo-system&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get pod -n gloo-system
NAME READY STATUS RESTARTS AGE
discovery-58f8856bd7-4fftg 1/1 Running 0 13s
gateway-66f86bc8b4-n5crc 1/1 Running 0 13s
gateway-proxy-5ff99b8679-tbp65 1/1 Running 0 13s
gloo-66b8dc8868-z5c6r 1/1 Running 0 13s
&lt;/code>&lt;/pre>&lt;h3 id="deploying-the-application">Deploying the application&lt;/h3>
&lt;p>Our &lt;code>echo&lt;/code> application is a simple container (thanks to our friends at HashiCorp) that will
respond with the application version, to help demonstrate our canary workflows as we start testing and
shifting traffic to a &lt;code>v2&lt;/code> version of the application.&lt;/p>
&lt;p>Kubernetes gives us a lot of flexibility in terms of modeling this application. We'll adopt the following
conventions:&lt;/p>
&lt;ul>
&lt;li>We'll include the version in the deployment name so we can run two versions of the application
side-by-side and manage their lifecycle differently.&lt;/li>
&lt;li>We'll label pods with an app label (&lt;code>app: echo&lt;/code>) and a version label (&lt;code>version: v1&lt;/code>) to help with our canary rollout.&lt;/li>
&lt;li>We'll deploy a single Kubernetes &lt;code>Service&lt;/code> for the application to set up networking. Instead of updating
this or using multiple services to manage routing to different versions, we'll manage the rollout with Gloo configuration.&lt;/li>
&lt;/ul>
&lt;p>The following is our &lt;code>v1&lt;/code> echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Shout out to our friends at Hashi for this useful test server&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:8080&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And here is the &lt;code>echo&lt;/code> Kubernetes &lt;code>Service&lt;/code> object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For convenience, we've published this yaml in a repo so we can deploy it with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/echo.yaml
&lt;/code>&lt;/pre>&lt;p>We should see the following output:&lt;/p>
&lt;pre>&lt;code>namespace/echo created
deployment.apps/echo-v1 created
service/echo created
&lt;/code>&lt;/pre>&lt;p>And we should be able to see all the resources healthy in the &lt;code>echo&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get all -n echo
NAME READY STATUS RESTARTS AGE
pod/echo-v1-66dbfffb79-287s5 1/1 Running 0 6s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/echo ClusterIP 10.55.252.216 &amp;lt;none&amp;gt; 80/TCP 6s
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/echo-v1 1/1 1 1 7s
NAME DESIRED CURRENT READY AGE
replicaset.apps/echo-v1-66dbfffb79 1 1 1 7s
&lt;/code>&lt;/pre>&lt;h3 id="exposing-outside-the-cluster-with-gloo">Exposing outside the cluster with Gloo&lt;/h3>
&lt;p>We can now expose this service outside the cluster with Gloo. First, we'll model the application as a Gloo
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#upstreams">Upstream&lt;/a>, which is Gloo's abstraction
for a traffic destination:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Upstream&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kube&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">serviceNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subsetSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selectors&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">keys&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- version&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, we're setting up subsets based on the &lt;code>version&lt;/code> label. We don't have to use this in our routes, but later
we'll start to use it to support our canary workflow.&lt;/p>
&lt;p>We can now create a route to this upstream in Gloo by defining a
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#virtual-services">Virtual Service&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply these resources with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/upstream.yaml
kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/vs.yaml
&lt;/code>&lt;/pre>&lt;p>Once we apply these two resources, we can start to send traffic to the application through Gloo:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>Our setup is complete, and our cluster now looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h2 id="two-phased-rollout-strategy">Two-Phased Rollout Strategy&lt;/h2>
&lt;p>Now we have a new version &lt;code>v2&lt;/code> of the echo application that we wish to roll out. We know that when the
rollout is complete, we are going to end up with this picture:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>However, to get there, we may want to perform a few rounds of testing to ensure the new version of the application
meets certain correctness and/or performance acceptance criteria. In this post, we'll introduce a two-phased approach to
canary rollout with Gloo, that could be used to satisfy the vast majority of acceptance tests.&lt;/p>
&lt;p>In the first phase, we'll perform smoke and correctness tests by routing a small segment of the traffic to the new version
of the application. In this demo, we'll use a header &lt;code>stage: canary&lt;/code> to trigger routing to the new service, though in
practice it may be desirable to make this decision based on another part of the request, such as a claim in a verified JWT.&lt;/p>
&lt;p>In the second phase, we've already established correctness, so we are ready to shift all of the traffic over to the new
version of the application. We'll configure weighted destinations, and shift the traffic while monitoring certain business
metrics to ensure the service quality remains at acceptable levels. Once 100% of the traffic is shifted to the new version,
the old version can be decommissioned.&lt;/p>
&lt;p>In practice, it may be desirable to only use one of the phases for testing, in which case the other phase can be
skipped.&lt;/p>
&lt;h2 id="phase-1-initial-canary-rollout-of-v2">Phase 1: Initial canary rollout of v2&lt;/h2>
&lt;p>In this phase, we'll deploy &lt;code>v2&lt;/code>, and then use a header &lt;code>stage: canary&lt;/code> to start routing a small amount of specific
traffic to the new version. We'll use this header to perform some basic smoke testing and make sure &lt;code>v2&lt;/code> is working the
way we'd expect:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/subset-routing.png" alt="Subset Routing">&lt;/p>
&lt;h3 id="setting-up-subset-routing">Setting up subset routing&lt;/h3>
&lt;p>Before deploying our &lt;code>v2&lt;/code> service, we'll update our virtual service to only route to pods that have the subset label
&lt;code>version: v1&lt;/code>, using a Gloo feature called &lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/subsets/">subset routing&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply them to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="deploying-echo-v2">Deploying echo v2&lt;/h3>
&lt;p>Now we can safely deploy &lt;code>v2&lt;/code> of the echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:8080&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/echo-v2.yaml
&lt;/code>&lt;/pre>&lt;p>Since our gateway is configured to route specifically to the &lt;code>v1&lt;/code> subset, this should have no effect. However, it does enable
&lt;code>v2&lt;/code> to be routable from the gateway if the &lt;code>v2&lt;/code> subset is configured for a route.&lt;/p>
&lt;p>Make sure &lt;code>v2&lt;/code> is running before moving on:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubectl get pod -n &lt;span style="color:#a2f">echo&lt;/span>
NAME READY STATUS RESTARTS AGE
echo-v1-66dbfffb79-2qw86 1/1 Running &lt;span style="color:#666">0&lt;/span> 5m25s
echo-v2-86584fbbdb-slp44 1/1 Running &lt;span style="color:#666">0&lt;/span> 93s
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="adding-a-route-to-v2-for-canary-testing">Adding a route to v2 for canary testing&lt;/h3>
&lt;p>We'll route to the &lt;code>v2&lt;/code> subset when the &lt;code>stage: canary&lt;/code> header is supplied on the request. If the header isn't
provided, we'll continue to route to the &lt;code>v1&lt;/code> subset as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;h3 id="canary-testing">Canary testing&lt;/h3>
&lt;p>Now that we have this route, we can do some testing. First let's ensure that the existing route is working as expected:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>And now we can start to canary test our new application version:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/ -H &amp;quot;stage: canary&amp;quot;
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="advanced-use-cases-for-subset-routing">Advanced use cases for subset routing&lt;/h3>
&lt;p>We may decide that this approach, using user-provided request headers, is too open. Instead, we may
want to restrict canary testing to a known, authorized user.&lt;/p>
&lt;p>A common implementation of this that we've seen is for the canary route to require a valid JWT that contains
a specific claim to indicate the subject is authorized for canary testing. Enterprise Gloo has out of the box
support for verifying JWTs, updating the request headers based on the JWT claims, and recomputing the
routing destination based on the updated headers. We'll save that for a future post covering more advanced use
cases in canary testing.&lt;/p>
&lt;h2 id="phase-2-shifting-all-traffic-to-v2-and-decommissioning-v1">Phase 2: Shifting all traffic to v2 and decommissioning v1&lt;/h2>
&lt;p>At this point, we've deployed &lt;code>v2&lt;/code>, and created a route for canary testing. If we are satisfied with the
results of the testing, we can move on to phase 2 and start shifting the load from &lt;code>v1&lt;/code> to &lt;code>v2&lt;/code>. We'll use
&lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/multi_destination/">weighted destinations&lt;/a>
in Gloo to manage the load during the migration.&lt;/p>
&lt;h3 id="setting-up-the-weighted-destinations">Setting up the weighted destinations&lt;/h3>
&lt;p>We can change the Gloo route to route to both of these destinations, with weights to decide how much of the traffic should
go to the &lt;code>v1&lt;/code> versus the &lt;code>v2&lt;/code> subset. To start, we're going to set it up so 100% of the traffic continues to get routed to the
&lt;code>v1&lt;/code> subset, unless the &lt;code>stage: canary&lt;/code> header was provided as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># We&amp;#39;ll keep our route from before if we want to continue testing with this header&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now we&amp;#39;ll route the rest of the traffic to the upstream, load balanced across the two subsets.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this virtual service update to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>Now the cluster looks like this, for any request that doesn't have the &lt;code>stage: canary&lt;/code> header:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/init-traffic-shift.png" alt="Initialize Traffic Shift">&lt;/p>
&lt;p>With the initial weights, we should see the gateway continue to serve &lt;code>v1&lt;/code> for all traffic.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ curl &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>glooctl proxy url&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>/
version:v1
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="commence-rollout">Commence rollout&lt;/h3>
&lt;p>To simulate a load test, let's shift half the traffic to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/load-test.png" alt="Load Test">&lt;/p>
&lt;p>This can be expressed on our virtual service by adjusting the weights:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Update the weight so 50% of the traffic hits v1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># And 50% is routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see half of the requests return &lt;code>version:v1&lt;/code> and the
other half return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>In practice, during this process it's likely you'll be monitoring some performance and business metrics
to ensure the traffic shift isn't resulting in a decline in the overall quality of service. We can even
leverage operators like &lt;a href="https://github.com/weaveworks/flagger">Flagger&lt;/a> to help automate this Gloo
workflow. Gloo Enterprise integrates with your metrics backend and provides out of the box and dynamic,
upstream-based dashboards that can be used to monitor the health of the rollout.
We will save these topics for a future post on advanced canary testing use cases with Gloo.&lt;/p>
&lt;h3 id="finishing-the-rollout">Finishing the rollout&lt;/h3>
&lt;p>We will continue adjusting weights until eventually, all of the traffic is now being routed to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/final-shift.png" alt="Final Shift">&lt;/p>
&lt;p>Our virtual service will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># No traffic will be sent to v1 anymore&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now all the traffic will be routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply that to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-3.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see all of the requests return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="decommissioning-v1">Decommissioning v1&lt;/h3>
&lt;p>At this point, we have deployed the new version of our application, conducted correctness tests using subset routing,
conducted load and performance tests by progressively shifting traffic to the new version, and finished
the rollout. The only remaining task is to clean up our &lt;code>v1&lt;/code> resources.&lt;/p>
&lt;p>First, we'll clean up our routes. We'll leave the subset specified on the route so we are all setup for future upgrades.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this update with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/4-decommissioning-v1/vs.yaml
&lt;/code>&lt;/pre>&lt;p>And we can delete the &lt;code>v1&lt;/code> deployment, which is no longer serving any traffic.&lt;/p>
&lt;pre>&lt;code>kubectl delete deploy -n echo echo-v1
&lt;/code>&lt;/pre>&lt;p>Now our cluster looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>And requests to the gateway return this:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;p>We have now completed our two-phased canary rollout of an application update using Gloo!&lt;/p>
&lt;h2 id="other-advanced-topics">Other Advanced Topics&lt;/h2>
&lt;p>Over the course of this post, we collected a few topics that could be a good starting point for advanced exploration:&lt;/p>
&lt;ul>
&lt;li>Using the &lt;strong>JWT&lt;/strong> filter to verify JWTs, extract claims onto headers, and route to canary versions depending on a claim value.&lt;/li>
&lt;li>Looking at &lt;strong>Prometheus metrics&lt;/strong> and &lt;strong>Grafana dashboards&lt;/strong> created by Gloo to monitor the health of the rollout.&lt;/li>
&lt;li>Automating the rollout by integrating &lt;strong>Flagger&lt;/strong> with &lt;strong>Gloo&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>A few other topics that warrant further exploration:&lt;/p>
&lt;ul>
&lt;li>Supporting &lt;strong>self-service&lt;/strong> upgrades by giving teams ownership over their upstream and route configuration&lt;/li>
&lt;li>Utilizing Gloo's &lt;strong>delegation&lt;/strong> feature and Kubernetes &lt;strong>RBAC&lt;/strong> to decentralize the configuration management safely&lt;/li>
&lt;li>Fully automating the continuous delivery process by applying &lt;strong>GitOps&lt;/strong> principles and using tools like &lt;strong>Flux&lt;/strong> to push config to the cluster&lt;/li>
&lt;li>Supporting &lt;strong>hybrid&lt;/strong> or &lt;strong>non-Kubernetes&lt;/strong> application use-cases by setting up Gloo with a different deployment pattern&lt;/li>
&lt;li>Utilizing &lt;strong>traffic shadowing&lt;/strong> to begin testing the new version with realistic data before shifting production traffic to it&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-in-the-gloo-community">Get Involved in the Gloo Community&lt;/h2>
&lt;p>Gloo has a large and growing community of open source users, in addition to an enterprise customer base. To learn more about
Gloo:&lt;/p>
&lt;ul>
&lt;li>Check out the &lt;a href="https://github.com/solo-io/gloo">repo&lt;/a>, where you can see the code and file issues&lt;/li>
&lt;li>Check out the &lt;a href="https://docs.solo.io/gloo/latest">docs&lt;/a>, which have an extensive collection of guides and examples&lt;/li>
&lt;li>Join the &lt;a href="http://slack.solo.io/">slack channel&lt;/a> and start chatting with the Solo engineering team and user community&lt;/li>
&lt;/ul>
&lt;p>If you'd like to get in touch with me (feedback is always appreciated!), you can find me on the
&lt;a href="http://slack.solo.io/">Solo slack&lt;/a> or email me at &lt;strong>&lt;a href="mailto:rick.ducott@solo.io">rick.ducott@solo.io&lt;/a>&lt;/strong>.&lt;/p></description></item><item><title>Blog: Cluster API v1alpha3 Delivers New Features and an Improved User Experience</title><link>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Daniel Lipovetsky (D2IQ)&lt;/p>
&lt;img src="https://kubernetes.io/images/blog/2020-04-21-Cluster-API-v1alpha3-Delivers-New-Features-and-an-Improved-User-Experience/kubernetes-cluster-logos_final-02.svg" align="right" width="25%" alt="Cluster API Logo: Turtles All The Way Down">
&lt;p>The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. It provides optional, additive functionality on top of core Kubernetes to manage the lifecycle of a Kubernetes cluster.&lt;/p>
&lt;p>Following the v1alpha2 release in October 2019, many members of the Cluster API community met in San Francisco, California, to plan the next release. The project had just gone through a major transformation, delivering a new architecture that promised to make the project easier for users to adopt, and faster for the community to build. Over the course of those two days, we found our common goals: To implement the features critical to managing production clusters, to make its user experience more intuitive, and to make it a joy to develop.&lt;/p>
&lt;p>The v1alpha3 release of Cluster API brings significant features for anyone running Kubernetes in production and at scale. Among the highlights:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#declarative-control-plane-management">Declarative Control Plane Management&lt;/a>&lt;/li>
&lt;li>&lt;a href="#distributing-control-plane-nodes-to-reduce-risk">Support for Distributing Control Plane Nodes Across Failure Domains To Reduce Risk&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#infrastructure-managed-node-groups">Support for Infrastructure-Managed Node Groups&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For anyone who wants to understand the API, or prizes a simple, but powerful, command-line interface, the new release brings:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#clusterctl">Redesigned clusterctl, a command-line tool (and go library) for installing and managing the lifecycle of Cluster API&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-cluster-api-book">Extensive and up-to-date documentation in The Cluster API Book&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Finally, for anyone extending the Cluster API for their custom infrastructure or software needs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#end-to-end-test-framework">New End-to-End (e2e) Test Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="#provider-implementer-s-guide">Documentation for integrating Cluster API into your cluster lifecycle stack&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All this was possible thanks to the hard work of many contributors.&lt;/p>
&lt;h2 id="declarative-control-plane-management">Declarative Control Plane Management&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/detiber/">Jason DeTiberus&lt;/a>, &lt;a href="https://github.com/randomvariable">Naadir Jeewa&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Kubeadm-based Control Plane (KCP) provides a declarative API to deploy and scale the Kubernetes control plane, including etcd. This is the feature many Cluster API users have been waiting for! Until now, to deploy and scale up the control plane, users had to create specially-crafted Machine resources. To scale down the control plane, they had to manually remove members from the etcd cluster. KCP automates deployment, scaling, and upgrades.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is the Kubernetes Control Plane?&lt;/strong>
The Kubernetes control plane is, at its core, kube-apiserver and etcd. If either of these are unavailable, no API requests can be handled. This impacts not only core Kubernetes APIs, but APIs implemented with CRDs. Other components, like kube-scheduler and kube-controller-manager, are also important, but do not have the same impact on availability.&lt;/p>
&lt;p>The control plane was important in the beginning because it scheduled workloads. However, some workloads could continue to run during a control plane outage. Today, workloads depend on operators, service meshes, and API gateways, which all use the control plane as a platform. Therefore, the control plane's availability is more important than ever.&lt;/p>
&lt;p>Managing the control plane is one of the most complex parts of cluster operation. Because the typical control plane includes etcd, it is stateful, and operations must be done in the correct sequence. Control plane replicas can and do fail, and maintaining control plane availability means being able to replace failed nodes.&lt;/p>
&lt;p>The control plane can suffer a complete outage (e.g. permanent loss of quorum in etcd), and recovery (along with regular backups) is sometimes the only feasible option.&lt;/p>
&lt;p>For more details, read about &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Components&lt;/a> in the Kubernetes documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Here's an example of a 3-replica control plane for the Cluster API Docker Infrastructure, which the project maintains for testing and development. For brevity, other required resources, like Cluster, and Infrastructure Template, referenced by its name and namespace, are not shown.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>controlplane.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmControlPlane&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructureTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubeadmConfigSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterConfiguration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1.16.3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy this control plane with kubectl:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f example-docker-control-plane.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Scale the control plane the same way you scale other Kubernetes resources:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl scale kubeadmcontrolplane example --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>kubeadmcontrolplane.controlplane.cluster.x-k8s.io/example scaled
&lt;/code>&lt;/pre>&lt;p>Upgrade the control plane to a newer patch of the Kubernetes release:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch kubeadmcontrolplane example --type&lt;span style="color:#666">=&lt;/span>json -p &lt;span style="color:#b44">&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/version&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;1.16.4&amp;#34;}]&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Number of Control Plane Replicas&lt;/strong>
By default, KCP is configured to manage etcd, and requires an odd number of replicas. If KCP is configured to not manage etcd, an odd number is recommended, but not required. An odd number of replicas ensures optimal etcd configuration. To learn why your etcd cluster should have an odd number of members, see the &lt;a href="https://etcd.io/docs/v3.4.0/faq/#why-an-odd-number-of-cluster-members">etcd FAQ&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Because it is a core Cluster API component, KCP can be used with any v1alpha3-compatible Infrastructure Provider that provides a fixed control plane endpoint, i.e., a load balancer or virtual IP. This endpoint enables requests to reach multiple control plane replicas.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is an Infrastructure Provider?&lt;/strong>
A source of computational resources (e.g. machines, networking, etc.). The community maintains providers for AWS, Azure, Google Cloud, and VMWare. For details, see the &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">list of providers&lt;/a> in the Cluster API Book.&lt;/p>
&lt;/blockquote>
&lt;h2 id="distributing-control-plane-nodes-to-reduce-risk">Distributing Control Plane Nodes To Reduce Risk&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/vincepri/">Vince Prignano&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>Cluster API users can now deploy nodes in different failure domains, reducing the risk of a cluster failing due to a domain outage. This is especially important for the control plane: If nodes in one domain fail, the cluster can continue to operate as long as the control plane is available to nodes in other domains.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is a Failure Domain?&lt;/strong>
A failure domain is a way to group the resources that would be made unavailable by some failure. For example, in many public clouds, an &amp;quot;availability zone&amp;quot; is the default failure domain. A zone corresponds to a data center. So, if a specific data center is brought down by a power outage or natural disaster, all resources in that zone become unavailable. If you run Kubernetes on your own hardware, your failure domain might be a rack, a network switch, or power distribution unit.&lt;/p>
&lt;/blockquote>
&lt;p>The Kubeadm-based ControlPlane distributes nodes across failure domains. To minimize the chance of losing multiple nodes in the event of a domain outage, it tries to distribute them evenly: it deploys a new node in the failure domain with the fewest existing nodes, and it removes an existing node in the failure domain with the most existing nodes.&lt;/p>
&lt;p>MachineDeployments and MachineSets do not distribute nodes across failure domains. To deploy your worker nodes across multiple failure domains, create a MachineDeployment or MachineSet for each failure domain.&lt;/p>
&lt;p>The Failure Domain API works on any infrastructure. That's because every Infrastructure Provider maps failure domains in its own way. The API is optional, so if your infrastructure is not complex enough to need failure domains, you do not need to support it. This example is for the Cluster API Docker Infrastructure Provider. Note that two of the domains are marked as suitable for control plane nodes, while a third is not. The Kubeadm-based ControlPlane will only deploy nodes to domains marked suitable.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerCluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlaneEndpoint&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">172.17.0.4&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">6443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">failureDomains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domain-one&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domain-two&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">domain-three&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws">AWS Infrastructure Provider&lt;/a> (CAPA), maintained by the Cluster API project, maps failure domains to AWS Availability Zones. Using CAPA, you can deploy a cluster across multiple Availability Zones. First, define subnets for multiple Availability Zones. The CAPA controller will define a failure domain for each Availability Zone. Deploy the control plane with the KubeadmControlPlane: it will distribute replicas across the failure domains. Finally, create a separate MachineDeployment for each failure domain.&lt;/p>
&lt;h2 id="automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/enxebre">Alberto García Lamela&lt;/a>, and &lt;a href="http://github.com/joelspeed">Joel Speed&lt;/a>&lt;/em>&lt;/p>
&lt;p>There are many reasons why a node might be unhealthy. The kubelet process may stop. The container runtime might have a bug. The kernel might have a memory leak. The disk may run out of space. CPU, disk, or memory hardware may fail. A power outage may happen. Failures like these are especially common in larger clusters.&lt;/p>
&lt;p>Kubernetes is designed to tolerate them, and to help your applications tolerate them as well. Nevertheless, only a finite number of nodes can be unhealthy before the cluster runs out of resources, and Pods are evicted or not scheduled in the first place. Unhealthy nodes should be repaired or replaced at the earliest opportunity.&lt;/p>
&lt;p>The Cluster API now includes a MachineHealthCheck resource, and a controller that monitors node health. When it detects an unhealthy node, it removes it. (Another Cluster API controller detects the node has been removed and replaces it.) You can configure the controller to suit your needs. You can configure how long to wait before removing the node. You can also set a threshold for the number of unhealthy nodes. When the threshold is reached, no more nodes are removed. The wait can be used to tolerate short-lived outages, and the threshold to prevent too many nodes from being replaced at the same time.&lt;/p>
&lt;p>The controller will remove only nodes managed by a Cluster API MachineSet. The controller does not remove control plane nodes, whether managed by the Kubeadm-based Control Plane, or by the user, as in v1alpha2. For more, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html#limitations-and-caveats-of-a-machinehealthcheck">Limits and Caveats of a MachineHealthCheck&lt;/a>.&lt;/p>
&lt;p>Here is an example of a MachineHealthCheck. For more details, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html">Configure a MachineHealthCheck&lt;/a> in the Cluster API book.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>MachineHealthCheck&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-node-unhealthy-5m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxUnhealthy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">33&lt;/span>%&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeStartupTimeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodepool&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nodepool-0&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">unhealthyConditions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Unknown&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;False&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="infrastructure-managed-node-groups">Infrastructure-Managed Node Groups&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/juan-lee">Juan-Lee Pang&lt;/a> and &lt;a href="https://github.com/CecileRobertMichon">Cecile Robert-Michon&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you run large clusters, you need to create and destroy hundreds of nodes, sometimes in minutes. Although public clouds make it possible to work with large numbers of nodes, having to make a separate API request to create or delete every node may scale poorly. For example, API requests may have to be delayed to stay within rate limits.&lt;/p>
&lt;p>Some public clouds offer APIs to manage groups of nodes as one single entity. For example, AWS has AutoScaling Groups, Azure has Virtual Machine Scale Sets, and GCP has Managed Instance Groups. With this release of Cluster API, Infrastructure Providers can add support for these APIs, and users can deploy groups of Cluster API Machines by using the MachinePool Resource. For more information, see the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/blob/bf51a2502f9007b531f6a9a2c1a4eae1586fb8ca/docs/proposals/20190919-machinepool-api.md">proposal&lt;/a> in the Cluster API repository.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Experimental Feature&lt;/strong>
The MachinePool API is an experimental feature that is not enabled by default. Users are encouraged to try it and report on how well it meets their needs.&lt;/p>
&lt;/blockquote>
&lt;h2 id="the-cluster-api-user-experience-reimagined">The Cluster API User Experience, Reimagined&lt;/h2>
&lt;h3 id="clusterctl">clusterctl&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/fabriziopandini">Fabrizio Pandini&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you are new to Cluster API, your first experience will probably be with the project's command-line tool, clusterctl. And with the new Cluster API release, it has been re-designed to be more pleasing to use than before. The tool is all you need to deploy your first &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html?highlight=pool#workload-cluster">workload cluster&lt;/a> in just a few steps.&lt;/p>
&lt;p>First, use &lt;code>clusterctl init&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/init.html">fetch the configuration&lt;/a> for your Infrastructure and Bootstrap Providers and deploy all of the components that make up the Cluster API. Second, use &lt;code>clusterctl config cluster&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/config-cluster.html">create the workload cluster manifest&lt;/a>. This manifest is just a collection of Kubernetes objects. To create the workload cluster, just &lt;code>kubectl apply&lt;/code> the manifest. Don't be surprised if this workflow looks familiar: Deploying a workload cluster with Cluster API is just like deploying an application workload with Kubernetes!&lt;/p>
&lt;p>Clusterctl also helps with the &amp;quot;day 2&amp;quot; operations. Use &lt;code>clusterctl move&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">migrate Cluster API custom resources&lt;/a>, such as Clusters, and Machines, from one &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#management-cluster">Management Cluster&lt;/a> to another. This step--also known as a &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#pivot">pivot&lt;/a>--is necessary to create a workload cluster that manages itself with Cluster API. Finally, use &lt;code>clusterctl upgrade&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/upgrade.html">upgrade all of the installed components&lt;/a> when a new Cluster API release becomes available.&lt;/p>
&lt;p>One more thing! Clusterctl is not only a command-line tool. It is also a Go library! Think of the library as an integration point for projects that build on top of Cluster API. All of clusterctl's command-line functionality is available in the library, making it easy to integrate into your stack. To get started with the library, please read its &lt;a href="https://pkg.go.dev/sigs.k8s.io/cluster-api@v0.3.1/cmd/clusterctl/client?tab=doc">documentation&lt;/a>.&lt;/p>
&lt;h3 id="the-cluster-api-book">The Cluster API Book&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The &lt;a href="https://cluster-api.sigs.k8s.io/">project's documentation&lt;/a> is extensive. New users should get some background on the &lt;a href="https://cluster-api.sigs.k8s.io/user/concepts.html">architecture&lt;/a>, and then create a cluster of their own with the &lt;a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">Quick Start&lt;/a>. The clusterctl tool has its own &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html">reference&lt;/a>. The &lt;a href="https://cluster-api.sigs.k8s.io/developer/guide.html">Developer Guide&lt;/a> has plenty of information for anyone interested in contributing to the project.&lt;/p>
&lt;p>Above and beyond the content itself, the project's documentation site is a pleasure to use. It is searchable, has an outline, and even supports different color themes. If you think the site a lot like the documentation for a different community project, &lt;a href="https://book.kubebuilder.io/">Kubebuilder&lt;/a>, that is no coincidence! Many thanks to Kubebuilder authors for creating a great example of documentation. And many thanks to the &lt;a href="https://github.com/rust-lang/mdBook">mdBook&lt;/a> authors for creating a great tool for building documentation.&lt;/p>
&lt;h2 id="integrate-customize">Integrate &amp;amp; Customize&lt;/h2>
&lt;h3 id="end-to-end-test-framework">End-to-End Test Framework&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Cluster API project is designed to be extensible. For example, anyone can develop their own Infrastructure and Bootstrap Providers. However, it's important that Providers work in a uniform way. And, because the project is still evolving, it takes work to make sure that Providers are up-to-date with new releases of the core.&lt;/p>
&lt;p>The End-to-End Test Framework provides a set of standard tests for developers to verify that their Providers integrate correctly with the current release of Cluster API, and help identify any regressions that happen after a new release of the Cluster API, or the Provider.&lt;/p>
&lt;p>For more details on the Framework, see &lt;a href="https://cluster-api.sigs.k8s.io/developer/testing.html?highlight=e2e#running-the-end-to-end-tests">Testing&lt;/a> in the Cluster API Book, and the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/tree/master/test/framework">README&lt;/a> in the repository.&lt;/p>
&lt;h3 id="provider-implementer-s-guide">Provider Implementer's Guide&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The community maintains &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">Infrastructure Providers&lt;/a> for a many popular infrastructures. However, if you want to build your own Infrastructure or Bootstrap Provider, the &lt;a href="https://cluster-api.sigs.k8s.io/developer/providers/implementers-guide/overview.html">Provider Implementer's&lt;/a> guide explains the entire process, from creating a git repository, to creating CustomResourceDefinitions for your Providers, to designing, implementing, and testing the controllers.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Under Active Development&lt;/strong>
The Provider Implementer's Guide is actively under development, and may not yet reflect all of the changes in the v1alpha3 release.&lt;/p>
&lt;/blockquote>
&lt;h2 id="join-us">Join Us!&lt;/h2>
&lt;p>The Cluster API project is a very active project, and covers many areas of interest. If you are an infrastructure expert, you can contribute to one of the Infrastructure Providers. If you like building controllers, you will find opportunities to innovate. If you're curious about testing distributed systems, you can help develop the project's end-to-end test framework. Whatever your interests and background, you can make a real impact on the project.&lt;/p>
&lt;p>Come introduce yourself to the community at our weekly meeting, where we dedicate a block of time for a Q&amp;amp;A session. You can also find maintainers and users on the Kubernetes Slack, and in the Kubernetes forum. Please check out the links below. We look forward to seeing you!&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="http://slack.k8s.io/">Slack&lt;/a>:&lt;a href="https://kubernetes.slack.com/archives/C8TSNPY4T"> #cluster-api&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/forum/">sig-cluster-lifecycle&lt;/a> Google Group to receive calendar invites and gain access to documents&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/861487554">Zoom meeting&lt;/a>, every Wednesday at 10:00 Pacific Time&lt;/li>
&lt;li>Post to the &lt;a href="https://discuss.kubernetes.io/c/contributors/cluster-api">Cluster API community forum&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: How Kubernetes contributors are building a better communication process</title><link>https://kubernetes.io/blog/2020/04/21/contributor-communication/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/contributor-communication/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Paris Pittman&lt;/p>
&lt;blockquote>
&lt;p>&amp;quot;Perhaps we just need to use a different word. We may need to use community development or project advocacy as a word in the open source realm as opposed to marketing, and perhaps then people will realize that they need to do it.&amp;quot;
~ &lt;a href="https://todogroup.org/www.linkedin.com/in/nithyaruff/">&lt;em>Nithya Ruff&lt;/em>&lt;/a> (from &lt;a href="https://todogroup.org/guides/marketing-open-source-projects/">&lt;em>TODO Group&lt;/em>&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;p>A common way to participate in the Kubernetes contributor community is
to be everywhere.&lt;/p>
&lt;p>We have an active &lt;a href="https://slack.k8s.io">Slack&lt;/a>, many mailing lists, Twitter account(s), and
dozens of community-driven podcasts and newsletters that highlight all
end-user, contributor, and ecosystem topics. And to add on to that, we also have &lt;a href="http://github.com/kubernetes/community">repositories of amazing documentation&lt;/a>, tons of &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">meetings&lt;/a> that drive the project forward, and &lt;a href="https://www.youtube.com/watch?v=yqB_le-N6EE">recorded code deep dives&lt;/a>. All of this information is incredibly valuable,
but it can be too much.&lt;/p>
&lt;p>Keeping up with thousands of contributors can be a challenge for anyone,
but this task of consuming information straight from the firehose is
particularly challenging for new community members. It's no secret that
the project is vast for contributors and users alike.&lt;/p>
&lt;p>To paint a picture with numbers:&lt;/p>
&lt;ul>
&lt;li>43,000 contributors&lt;/li>
&lt;li>6,579 members in #kubernetes-dev slack channel&lt;/li>
&lt;li>52 mailing lists (kubernetes-dev@ has thousands of members; sig-networking@ has 1000 alone)&lt;/li>
&lt;li>40 &lt;a href="https://github.com/kubernetes/community/blob/master/governance.md#community-groups">community groups&lt;/a>&lt;/li>
&lt;li>30 &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">upstream meetings&lt;/a> &lt;em>this&lt;/em> week alone&lt;/li>
&lt;/ul>
&lt;p>All of these numbers are only growing in scale, and with that comes the need to simplify how contributors get the information right information front-and-center.&lt;/p>
&lt;h2 id="how-we-got-here">How we got here&lt;/h2>
&lt;p>Kubernetes (K8s for short) communication grew out of a need for people
to connect in our growing community. With the best of intentions, the
community spun up channels for people to connect. This energy was part
of what helped Kubernetes grow so fast, and it also had us in sprawling out far and wide. As adoption grew, &lt;a href="https://github.com/kubernetes/community/issues/2466">contributors knew there was a need for standardization&lt;/a>.&lt;/p>
&lt;p>This new attention to how the community communicates led to the discovery
of a complex web of options. There were so many options, and it was a
challenge for anyone to be sure they were in the right place to receive
the right information. We started taking immediate action combining communication streams and thinking about how to reach out best to serve our community. We also asked for feedback from all our
contributors directly via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience/surveys">&lt;strong>annual surveys&lt;/strong>&lt;/a>
to see where folks were actually reading the news that influences their
experiences here in our community.&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/1744971/79478603-3a3a1980-7fd1-11ea-8b7a-d36aac7a097b.png" alt="Kubernetes channel access">&lt;/p>
&lt;p>With over 43,000 contributors, our contributor base is larger than many enterprise companies. You can imagine what it's like getting important messages across to make sure they are landing and folks are taking action.&lt;/p>
&lt;h2 id="contributing-to-better-communication">Contributing to better communication&lt;/h2>
&lt;p>Think about how your company/employer solves for this kind of communication challenge. Many have done so
by building internal marketing and communication focus areas in
marketing departments. So that's what we are doing. This has also been
applied &lt;a href="https://fedoraproject.org/wiki/Marketing">at Fedora&lt;/a> and at a smaller
scale in our very &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">own release&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/d0fd6c16f7ee754b08082cc15658eb8db7afeaf8/events/events-team/marketing/README.md">contributor summit&lt;/a> planning
teams as roles.&lt;/p>
&lt;p>We have hit the accelerator on an &lt;strong>upstream marketing group&lt;/strong> under SIG
Contributor Experience and we want to tackle this challenge straight on.
We've learned in other contributor areas that creating roles for
contributors is super helpful - onboarding, breaking down work, and
ownership. &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team">Here's our team charting the course&lt;/a>.&lt;/p>
&lt;p>Journey your way through our other documents like our &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md">charter&lt;/a> if you are
interested in our mission and scope.&lt;/p>
&lt;p>Many of you close to the ecosystem might be scratching your head - isn't
this what CNCF does?&lt;/p>
&lt;p>Yes and no. The CNCF has 40+ other projects that need to be marketed to
a countless number of different types of community members in distinct
ways and they aren't responsible for the day to day operations of their
projects. They absolutely do partner with us to highlight what we need
and when we need it, and they do a fantastic job of it (one example is
the &lt;a href="https://twitter.com/kubernetesio">&lt;em>@kubernetesio Twitter account&lt;/em>&lt;/a> and its 200,000
followers).&lt;/p>
&lt;p>Where this group differs is in its scope: we are entirely
focused on elevating the hard work being done throughout the Kubernetes
community by its contributors.&lt;/p>
&lt;h2 id="what-to-expect-from-us">What to expect from us&lt;/h2>
&lt;p>You can expect to see us on the Kubernetes &lt;a href="https://github.com/kubernetes/community/tree/master/communication">communication channels&lt;/a> supporting you by:&lt;/p>
&lt;ul>
&lt;li>Finding ways of adding our human touch to potentially overwhelming
quantities of info by storytelling and other methods - we want to
highlight the work you are doing and provide useful information!&lt;/li>
&lt;li>Keeping you in the know of the comings and goings of contributor
community events, activities, mentoring initiatives, KEPs, and more.&lt;/li>
&lt;li>Creating a presence on Twitter specifically for contributors via
@k8scontributors that is all about being a contributor in all its
forms.&lt;/li>
&lt;/ul>
&lt;p>What does this look like in the wild? Our &lt;a href="https://kubernetes.io/blog/2020/03/19/join-sig-scalability/">first post&lt;/a> in a series about our 36 community groups landed recently. Did you see it?
More articles like this and additional themes of stories to flow through
&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#purpose">our storytellers&lt;/a>.&lt;/p>
&lt;p>We will deliver this with an &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md#ethosvision">ethos&lt;/a> behind us aligned to the Kubernetes project as a whole, and we're
committed to using the same tools as all the other SIGs to do so. Check out our &lt;a href="https://github.com/orgs/kubernetes/projects/39">project board&lt;/a> to view our roadmap of upcoming work.&lt;/p>
&lt;h2 id="join-us-and-be-part-of-the-story">Join us and be part of the story&lt;/h2>
&lt;p>This initiative is in an early phase and we still have important roles to fill to make it successful.&lt;/p>
&lt;p>If you are interested in open sourcing marketing functions – it's a fun
ride – join us! Specific immediate roles include storytelling through
blogs and as a designer. We also have plenty of work in progress on our project board.
Add a comment to any open issue to let us know you're interested in getting involved.&lt;/p>
&lt;p>Also, if you're reading this, you're exactly the type of person we are
here to support. We would love to hear about how to improve, feedback,
or how we can work together.&lt;/p>
&lt;p>Reach out at one of the contact methods listed on &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contact-us">our README&lt;/a>. We would love to hear from you.&lt;/p></description></item><item><title>Blog: API Priority and Fairness Alpha</title><link>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Min Kim (Ant Financial), Mike Spreitzer (IBM), Daniel Smith (Google)&lt;/p>
&lt;p>This blog describes “API Priority And Fairness”, a new alpha feature in Kubernetes 1.18. API Priority And Fairness permits cluster administrators to divide the concurrency of the control plane into different weighted priority levels. Every request arriving at a kube-apiserver will be categorized into one of the priority levels and get its fair share of the control plane’s throughput.&lt;/p>
&lt;h2 id="what-problem-does-this-solve">What problem does this solve?&lt;/h2>
&lt;p>Today the apiserver has a simple mechanism for protecting itself against CPU and memory overloads: max-in-flight limits for mutating and for readonly requests. Apart from the distinction between mutating and readonly, no other distinctions are made among requests; consequently, there can be undesirable scenarios where one subset of the requests crowds out other requests.&lt;/p>
&lt;p>In short, it is far too easy for Kubernetes workloads to accidentally DoS the apiservers, causing other important traffic--like system controllers or leader elections---to fail intermittently. In the worst cases, a few broken nodes or controllers can push a busy cluster over the edge, turning a local problem into a control plane outage.&lt;/p>
&lt;h2 id="how-do-we-solve-the-problem">How do we solve the problem?&lt;/h2>
&lt;p>The new feature “API Priority and Fairness” is about generalizing the existing max-in-flight request handler in each apiserver, to make the behavior more intelligent and configurable. The overall approach is as follows.&lt;/p>
&lt;ol>
&lt;li>Each request is matched by a &lt;em>Flow Schema&lt;/em>. The Flow Schema states the Priority Level for requests that match it, and assigns a “flow identifier” to these requests. Flow identifiers are how the system determines whether requests are from the same source or not.&lt;/li>
&lt;li>Priority Levels may be configured to behave in several ways. Each Priority Level gets its own isolated concurrency pool. Priority levels also introduce the concept of queuing requests that cannot be serviced immediately.&lt;/li>
&lt;li>To prevent any one user or namespace from monopolizing a Priority Level, they may be configured to have multiple queues. &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/#What_is_shuffle_sharding.3F">“Shuffle Sharding”&lt;/a> is used to assign each flow of requests to a subset of the queues.&lt;/li>
&lt;li>Finally, when there is capacity to service a request, a &lt;a href="https://en.wikipedia.org/wiki/Fair_queuing">“Fair Queuing”&lt;/a> algorithm is used to select the next request. Within each priority level the queues compete with even fairness.&lt;/li>
&lt;/ol>
&lt;p>Early results have been very promising! Take a look at this &lt;a href="https://github.com/kubernetes/kubernetes/pull/88177#issuecomment-588945806">analysis&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-try-this-out">How do I try this out?&lt;/h2>
&lt;p>You are required to prepare the following things in order to try out the feature:&lt;/p>
&lt;ul>
&lt;li>Download and install a kubectl greater than v1.18.0 version&lt;/li>
&lt;li>Enabling the new API groups with the command line flag &lt;code>--runtime-config=&amp;quot;flowcontrol.apiserver.k8s.io/v1alpha1=true&amp;quot;&lt;/code> on the kube-apiservers&lt;/li>
&lt;li>Switch on the feature gate with the command line flag &lt;code>--feature-gates=APIPriorityAndFairness=true&lt;/code> on the kube-apiservers&lt;/li>
&lt;/ul>
&lt;p>After successfully starting your kube-apiservers, you will see a few default FlowSchema and PriorityLevelConfiguration resources in the cluster. These default configurations are designed for a general protection and traffic management for your cluster.
You can examine and customize the default configuration by running the usual tools, e.g.:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubectl get flowschemas&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get prioritylevelconfigurations&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-this-work-under-the-hood">How does this work under the hood?&lt;/h2>
&lt;p>Upon arrival at the handler, a request is assigned to exactly one priority level and exactly one flow within that priority level. Hence understanding how FlowSchema and PriorityLevelConfiguration works will be helping you manage the request traffic going through your kube-apiservers.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>FlowSchema: FlowSchema will identify a PriorityLevelConfiguration object and the way to compute the request’s “flow identifier”. Currently we support matching requests according to: the identity making the request, the verb, and the target object. The identity can match in terms of: a username, a user group name, or a ServiceAccount. And as for the target objects, we can match by apiGroup, resource[/subresource], and namespace.&lt;/p>
&lt;ul>
&lt;li>The flow identifier is used for shuffle sharding, so it’s important that requests have the same flow identifier if they are from the same source! We like to consider scenarios with “elephants” (which send many/heavy requests) vs “mice” (which send few/light requests): it is important to make sure the elephant’s requests all get the same flow identifier, otherwise they will look like many different mice to the system!&lt;/li>
&lt;li>See the API Documentation &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io">here&lt;/a>!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>PriorityLevelConfiguration: Defines a priority level.&lt;/p>
&lt;ul>
&lt;li>For apiserver self requests, and any reentrant traffic (e.g., admission webhooks which themselves make API requests), a Priority Level can be marked “exempt”, which means that no queueing or limiting of any sort is done. This is to prevent priority inversions.&lt;/li>
&lt;li>Each non-exempt Priority Level is configured with a number of &amp;quot;concurrency shares&amp;quot; and gets an isolated pool of concurrency to use. Requests of that Priority Level run in that pool when it is not full, never anywhere else. Each apiserver is configured with a total concurrency limit (taken to be the sum of the old limits on mutating and readonly requests), and this is then divided among the Priority Levels in proportion to their concurrency shares.&lt;/li>
&lt;li>A non-exempt Priority Level may select a number of queues and a &amp;quot;hand size&amp;quot; to use for the shuffle sharding. Shuffle sharding maps flows to queues in a way that is better than consistent hashing. A given flow has access to a small collection of queues, and for each incoming request the shortest queue is chosen. When a Priority Level has queues, it also sets a limit on queue length. There is also a limit placed on how long a request can wait in its queue; this is a fixed fraction of the apiserver's request timeout. A request that cannot be executed and cannot be queued (any longer) is rejected.&lt;/li>
&lt;li>Alternatively, a non-exempt Priority Level may select immediate rejection instead of waiting in a queue.&lt;/li>
&lt;li>See the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io">API documentation&lt;/a> for this feature.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-missing-when-will-there-be-a-beta">What’s missing? When will there be a beta?&lt;/h2>
&lt;p>We’re already planning a few enhancements based on alpha and there will be more as users send feedback to our community. Here’s a list of them:&lt;/p>
&lt;ul>
&lt;li>Traffic management for WATCH and EXEC requests&lt;/li>
&lt;li>Adjusting and improving the default set of FlowSchema/PriorityLevelConfiguration&lt;/li>
&lt;li>Enhancing observability on how this feature works&lt;/li>
&lt;li>Join the discussion &lt;a href="https://github.com/kubernetes/enhancements/pull/1632">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Possibly treat LIST requests differently depending on an estimate of how big their result will be.&lt;/p>
&lt;h2 id="how-can-i-get-involved">How can I get involved?&lt;/h2>
&lt;p>As always! Reach us on slack &lt;a href="https://kubernetes.slack.com/messages/sig-api-machinery">#sig-api-machinery&lt;/a>, or through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery">mailing list&lt;/a>. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>Many thanks to the contributors that have gotten this feature this far: Aaron Prindle, Daniel Smith, Jonathan Tomer, Mike Spreitzer, Min Kim, Bruce Ma, Yu Liao, Mengyi Zhou!&lt;/p></description></item><item><title>Blog: Introducing Windows CSI support alpha for Kubernetes</title><link>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</link><pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Authors: Deep Debroy [Docker], Jing Xu [Google], Krishnakumar R (KK) [Microsoft]&lt;/p>
&lt;p>&lt;em>The alpha version of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSI Proxy&lt;/a> for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/em>&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. All new storage features will utilize CSI, therefore it is important to get CSI drivers to work on Windows.&lt;/p>
&lt;p>A CSI driver in Kubernetes has two main components: a controller plugin and a node plugin. The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services (e.g. cloud storage service). The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. This was previously not possible for containers on Windows. With the release of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a>, CSI drivers can now perform storage operations on the node. This inturn enables containerized CSI Drivers to run on Windows.&lt;/p>
&lt;h2 id="csi-support-for-windows-clusters">CSI support for Windows clusters&lt;/h2>
&lt;p>CSI drivers (e.g. AzureDisk, GCE PD, etc.) are recommended to be deployed as containers. CSI driver’s node plugin typically runs on every worker node in the cluster (as a DaemonSet). Node plugin containers need to run with elevated privileges to perform storage related operations. However, Windows currently does not support privileged containers. To solve this problem, &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> makes it so that node plugins can now be deployed as unprivileged pods and then use the proxy to perform privileged storage operations on the node.&lt;/p>
&lt;h2 id="node-plugin-interactions-with-csiproxy">Node plugin interactions with CSIProxy&lt;/h2>
&lt;p>The design of the CSI proxy is captured in this &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190714-windows-csi-support.md">KEP&lt;/a>. The following diagram depicts the interactions with the CSI node plugin and CSI proxy.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyOverview.png">
&lt;/p>
&lt;p>The CSI proxy runs as a process directly on the host on every windows node - very similar to kubelet. The CSI code in kubelet interacts with the &lt;a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html">node driver registrar&lt;/a> component and the CSI node plugin. The node driver registrar is a community maintained CSI project which handles the registration of vendor specific node plugins. The kubelet initiates CSI gRPC calls like NodeStageVolume/NodePublishVolume on the node plugin as described in the figure. Node plugins interface with the CSIProxy process to perform local host OS storage related operations such as creation/enumeration of volumes, mounting/unmounting, etc.&lt;/p>
&lt;h2 id="csi-proxy-architecture-and-implementation">CSI proxy architecture and implementation&lt;/h2>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyArchitecture.png">
&lt;/p>
&lt;p>In the alpha release, CSIProxy supports the following API groups:&lt;/p>
&lt;ol>
&lt;li>Filesystem&lt;/li>
&lt;li>Disk&lt;/li>
&lt;li>Volume&lt;/li>
&lt;li>SMB&lt;/li>
&lt;/ol>
&lt;p>CSI proxy exposes each API group via a Windows named pipe. The communication is performed using gRPC over these pipes. The client library from the CSI proxy project uses these pipes to interact with the CSI proxy APIs. For example, the filesystem APIs are exposed via a pipe like &lt;code>\.\pipe\csi-proxy-filesystem-v1alpha1&lt;/code> and volume APIs under the &lt;code>\.\pipe\csi-proxy-volume-v1alpha1&lt;/code>, and so on.&lt;/p>
&lt;p>From each API group service, the calls are routed to the host API layer. The host API calls into the host Windows OS by either Powershell or Go standard library calls. For example, when the filesystem API &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.proto">Rmdir&lt;/a> is called the API group service would decode the grpc structure &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.pb.go">RmdirRequest&lt;/a> and find the directory to be removed and call into the Host APIs layer. This would result in a call to &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/internal/os/filesystem/api.go">os.Remove&lt;/a>, a Go standard library call, to perform the remove operation.&lt;/p>
&lt;h2 id="control-flow-details">Control flow details&lt;/h2>
&lt;p>The following figure uses CSI call NodeStageVolume as an example to explain the interaction between kubelet, CSI plugin, and CSI proxy for provisioning a fresh volume. After the node plugin receives a CSI RPC call, it makes a few calls to CSIproxy accordingly. As a result of the NodeStageVolume call, first the required disk is identified using either of the Disk API calls: ListDiskLocations (in AzureDisk driver) or GetDiskNumberByName (in GCE PD driver). If the disk is not partitioned, then the PartitionDisk (Disk API group) is called. Subsequently, Volume API calls such as ListVolumesOnDisk, FormatVolume and MountVolume are called to perform the rest of the required operations. Similar operations are performed in case of NodeUnstageVolume, NodePublishVolume, NodeUnpublishedVolume, etc.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyControlFlow.png">
&lt;/p>
&lt;h2 id="current-support">Current support&lt;/h2>
&lt;p>CSI proxy is now available as alpha. You can find more details on the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository. There are currently two cloud providers that provide alpha support for CSI drivers on Windows: Azure and GCE.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>One key area of focus in beta is going to be Windows based build and CI/CD setup to improve the stability and quality of the code base. Another area is using Go based calls directly instead of Powershell commandlets to improve performance. Enhancing debuggability and adding more tests are other areas which the team will be looking into.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>For those interested in more details, the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository is a good place to start. In addition, the &lt;a href="https://kubernetes.slack.com/messages/csi-windows">#csi-windows&lt;/a> channel on kubernetes slack is available for discussions specific to the CSI on Windows.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We would like to thank Michelle Au for guiding us throughout this journey to alpha. We would like to thank Jean Rougé for contributions during the initial CSI proxy effort. We would like to thank Saad Ali for all the guidance with respect to the project and review/feedback on a draft of this blog. We would like to thank Patrick Lang and Mark Rossetti for helping us with Windows specific questions and details. Special thanks to Andy Zhang for reviews and guidance with respect to Azuredisk and Azurefile work. A big thank you to Paul Burt and Karen Chu for the review and suggestions on improving this blog post.&lt;/p>
&lt;p>Last but not the least, we would like to thank the broader Kubernetes community who contributed at every step of the project.&lt;/p></description></item><item><title>Blog: Improvements to the Ingress API in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</link><pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Rob Scott (Google), Christopher M Luciano (IBM)&lt;/p>
&lt;p>The Ingress API in Kubernetes has enabled a large number of controllers to provide simple and powerful ways to manage inbound network traffic to Kubernetes workloads. In Kubernetes 1.18, we've made 3 significant additions to this API:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field that can specify how Ingress paths should be matched.&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource that can specify how Ingresses should be implemented by controllers.&lt;/li>
&lt;li>Support for wildcards in hostnames.&lt;/li>
&lt;/ul>
&lt;h2 id="better-path-matching-with-path-types">Better Path Matching With Path Types&lt;/h2>
&lt;p>The new concept of a path type allows you to specify how a path should be matched. There are three supported types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ImplementationSpecific (default):&lt;/strong> With this path type, matching is up to the controller implementing the &lt;code>IngressClass&lt;/code>. Implementations can treat this as a separate &lt;code>pathType&lt;/code> or treat it identically to the &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code> path types.&lt;/li>
&lt;li>&lt;strong>Exact:&lt;/strong> Matches the URL path exactly and with case sensitivity.&lt;/li>
&lt;li>&lt;strong>Prefix:&lt;/strong> Matches based on a URL path prefix split by &lt;code>/&lt;/code>. Matching is case sensitive and done on a path element by element basis.&lt;/li>
&lt;/ul>
&lt;h2 id="extended-configuration-with-ingress-classes">Extended Configuration With Ingress Classes&lt;/h2>
&lt;p>The Ingress resource was designed with simplicity in mind, providing a simple set of fields that would be applicable in all use cases. Over time, as use cases evolved, implementations began to rely on a long list of custom annotations for further configuration. The new &lt;code>IngressClass&lt;/code> resource provides a way to replace some of those annotations.&lt;/p>
&lt;p>Each &lt;code>IngressClass&lt;/code> specifies which controller should implement Ingresses of the class and can reference a custom resource with additional parameters.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressClass&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/ingress-controller&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;k8s.example.com/v1alpha&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressParameters&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="specifying-the-class-of-an-ingress">Specifying the Class of an Ingress&lt;/h3>
&lt;p>A new &lt;code>ingressClassName&lt;/code> field has been added to the Ingress spec that is used to reference the &lt;code>IngressClass&lt;/code> that should be used to implement this Ingress.&lt;/p>
&lt;h3 id="deprecating-the-ingress-class-annotation">Deprecating the Ingress Class Annotation&lt;/h3>
&lt;p>Before the &lt;code>IngressClass&lt;/code> resource was added in Kubernetes 1.18, a similar concept of Ingress class was often specified with a &lt;code>kubernetes.io/ingress.class&lt;/code> annotation on the Ingress. Although this annotation was never formally defined, it was widely supported by Ingress controllers, and should now be considered formally deprecated.&lt;/p>
&lt;h3 id="setting-a-default-ingressclass">Setting a Default IngressClass&lt;/h3>
&lt;p>It’s possible to mark a specific &lt;code>IngressClass&lt;/code> as default in a cluster. Setting the
&lt;code>ingressclass.kubernetes.io/is-default-class&lt;/code> annotation to true on an
IngressClass resource will ensure that new Ingresses without an &lt;code>ingressClassName&lt;/code> specified will be assigned this default &lt;code>IngressClass&lt;/code>.&lt;/p>
&lt;h2 id="support-for-hostname-wildcards">Support for Hostname Wildcards&lt;/h2>
&lt;p>Many Ingress providers have supported wildcard hostname matching like &lt;code>*.foo.com&lt;/code> matching &lt;code>app1.foo.com&lt;/code>, but until now the spec assumed an exact FQDN match of the host. Hosts can now be precise matches (for example “&lt;code>foo.bar.com&lt;/code>”) or a wildcard (for example “&lt;code>*.foo.com&lt;/code>”). Precise matches require that the http host header matches the Host setting. Wildcard matches require the http host header is equal to the suffix of the wildcard rule.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Host&lt;/th>
&lt;th>Host header&lt;/th>
&lt;th>Match?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>bar.foo.com&lt;/code>&lt;/td>
&lt;td>Matches based on shared suffix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>baz.bar.foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="putting-it-all-together">Putting it All Together&lt;/h3>
&lt;p>These new Ingress features allow for much more configurability. Here’s an example of an Ingress that makes use of pathType, &lt;code>ingressClassName&lt;/code>, and a hostname wildcard:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingressClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;*.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">http&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">paths&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/example&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pathType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Prefix&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">backend&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-service&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ingress-controller-support">Ingress Controller Support&lt;/h3>
&lt;p>Since these features are new in Kubernetes 1.18, each Ingress controller implementation will need some time to develop support for these new features. Check the documentation for your preferred Ingress controllers to see when they will support this new functionality.&lt;/p>
&lt;h2 id="the-future-of-ingress">The Future of Ingress&lt;/h2>
&lt;p>The Ingress API is on pace to graduate from beta to a stable API in Kubernetes 1.19. It will continue to provide a simple way to manage inbound network traffic for Kubernetes workloads. This API has intentionally been kept simple and lightweight, but there has been a desire for greater configurability for more advanced use cases.&lt;/p>
&lt;p>Work is currently underway on a new highly configurable set of APIs that will provide an alternative to Ingress in the future. These APIs are being referred to as the new “Service APIs”. They are not intended to replace any existing APIs, but instead provide a more configurable alternative for complex use cases. For more information, check out the &lt;a href="http://github.com/kubernetes-sigs/service-apis">Service APIs repo on GitHub&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.18 Feature Server-side Apply Beta 2</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Antoine Pelisse (Google)&lt;/p>
&lt;h2 id="what-is-server-side-apply">What is Server-side Apply?&lt;/h2>
&lt;p>Server-side Apply is an important effort to migrate “kubectl apply” to the apiserver. It was started in 2018 by the Apply working group.&lt;/p>
&lt;p>The use of kubectl to declaratively apply resources has exposed the following challenges:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>One needs to use the kubectl go code, or they have to shell out to kubectl.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Strategic merge-patch, the patch format used by kubectl, grew organically and was challenging to fix while maintaining compatibility with various api-server versions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Some features are hard to implement directly on the client, for example, unions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field.&lt;/p>
&lt;h2 id="how-does-it-work-what-s-managedfields">How does it work, what’s managedFields?&lt;/h2>
&lt;p>Server-side Apply works by keeping track of which actor of the system has changed each field of an object. It does so by diffing all updates to objects, and recording all the fields that have changed as well the time of the operation. All this information is stored in the managedFields in the metadata of objects. Since objects can have many fields, this field can be quite large.&lt;/p>
&lt;p>When someone applies, we can then use the information stored within managedFields to report relevant conflicts and help the merge algorithm to do the right thing.&lt;/p>
&lt;h2 id="wasn-t-it-already-beta-before-1-18">Wasn’t it already Beta before 1.18?&lt;/h2>
&lt;p>Yes, Server-side Apply has been Beta since 1.16, but it didn’t track the owner for fields associated with objects that had not been applied. This means that most objects didn’t have the managedFields metadata stored, and conflicts for these objects cannot be resolved. With Kubernetes 1.18, all new objects will have the managedFields attached to them and provide accurate information on conflicts.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>The most common way to use this is through kubectl: &lt;code>kubectl apply --server-side&lt;/code>. This is likely to show conflicts with other actors, including client-side apply. When that happens, conflicts can be forced by using the &lt;code>--force-conflicts&lt;/code> flag, which will grab the ownership for the fields that have changed.&lt;/p>
&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>We have two important limitations right now, especially with sub-resources. The first is that if you apply with a status, the status is going to be ignored. We are still going to try and acquire the fields, which may lead to invalid conflicts. The other is that we do not update the managedFields on some sub-resources, including scale, so you may not see information about a horizontal pod autoscaler changing the number of replicas.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>We are working hard to improve the experience of using server-side apply with kubectl, and we are trying to make it the default. As part of that, we want to improve the migration from client-side to server-side.&lt;/p>
&lt;h2 id="can-i-help">Can I help?&lt;/h2>
&lt;p>Of course! The working-group apply is available on slack #wg-apply, through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-wg-apply">mailing list&lt;/a> and we also meet every other Tuesday at 9.30 PT on Zoom. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this new beta possible:&lt;/p>
&lt;ul>
&lt;li>Daniel Smith&lt;/li>
&lt;li>Jenny Buckley&lt;/li>
&lt;li>Joe Betz&lt;/li>
&lt;li>Julian Modesto&lt;/li>
&lt;li>Kevin Wiesmüller&lt;/li>
&lt;li>Maria Ntalla&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes Topology Manager Moves to Beta - Align Up!</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Kevin Klues (NVIDIA), Victor Pickard (Red Hat), Conor Nolan (Intel)&lt;/p>
&lt;p>This blog post describes the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, a beta feature of Kubernetes in release 1.18. The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> feature enables NUMA alignment of CPUs and peripheral devices (such as SR-IOV VFs and GPUs), allowing your workload to run in an environment optimized for low-latency.&lt;/p>
&lt;p>Prior to the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications. With the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, we now have a way to avoid this.&lt;/p>
&lt;p>This blog post covers:&lt;/p>
&lt;ol>
&lt;li>A brief introduction to NUMA and why it is important&lt;/li>
&lt;li>The policies available to end-users to ensure NUMA alignment of CPUs and devices&lt;/li>
&lt;li>The internal details of how the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> works&lt;/li>
&lt;li>Current limitations of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;li>Future directions of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h2 id="so-what-is-numa-and-why-do-i-care">So, what is NUMA and why do I care?&lt;/h2>
&lt;p>The term NUMA stands for Non-Uniform Memory Access. It is a technology available on multi-cpu systems that allows different CPUs to access different parts of memory at different speeds. Any memory directly connected to a CPU is considered &amp;quot;local&amp;quot; to that CPU and can be accessed very fast. Any memory not directly connected to a CPU is considered &amp;quot;non-local&amp;quot; and will have variable access times depending on how many interconnects must be passed through in order to reach it. On modern systems, the idea of having &amp;quot;local&amp;quot; vs. &amp;quot;non-local&amp;quot; memory can also be extended to peripheral devices such as NICs or GPUs. For high performance, CPUs and devices should be allocated such that they have access to the same local memory.&lt;/p>
&lt;p>All memory on a NUMA system is divided into a set of &amp;quot;NUMA nodes&amp;quot;, with each node representing the local memory for a set of CPUs or devices. We talk about an individual CPU as being part of a NUMA node if its local memory is associated with that NUMA node.&lt;/p>
&lt;p>We talk about a peripheral device as being part of a NUMA node based on the shortest number of interconnects that must be passed through in order to reach it.&lt;/p>
&lt;p>For example, in Figure 1, CPUs 0-3 are said to be part of NUMA node 0, whereas CPUs 4-7 are part of NUMA node 1. Likewise GPU 0 and NIC 0 are said to be part of NUMA node 0 because they are attached to Socket 0, whose CPUs are all part of NUMA node 0. The same is true for GPU 1 and NIC 1 on NUMA node 1.&lt;/p>
&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/example-numa-system.png">
&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1.&lt;/p>
&lt;p>Although the example above shows a 1-1 mapping of NUMA Node to Socket, this is not necessarily true in the general case. There may be multiple sockets on a single NUMA node, or individual CPUs of a single socket may be connected to different NUMA nodes. Moreover, emerging technologies such as Sub-NUMA Clustering (&lt;a href="https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview">available on recent intel CPUs&lt;/a>) allow single CPUs to be associated with multiple NUMA nodes so long as their memory access times to both nodes are the same (or have a negligible difference).&lt;/p>
&lt;p>The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> has been built to handle all of these scenarios.&lt;/p>
&lt;h2 id="align-up-it-s-a-team-effort">Align Up! It's a TeaM Effort!&lt;/h2>
&lt;p>As previously stated, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> allows users to align their CPU and peripheral device allocations by NUMA node. There are several policies available for this:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>none:&lt;/code>&lt;/strong> this policy will not attempt to do any alignment of resources. It will act the same as if the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> were not present at all. This is the default policy.&lt;/li>
&lt;li>&lt;strong>&lt;code>best-effort:&lt;/code>&lt;/strong> with this policy, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node.&lt;/li>
&lt;li>&lt;strong>&lt;code>restricted:&lt;/code>&lt;/strong> this policy is the same as the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policy, some allocations may come from multiple NUMA nodes if it is impossible to &lt;em>ever&lt;/em> satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes).&lt;/li>
&lt;li>&lt;strong>&lt;code>single-numa-node:&lt;/code>&lt;/strong> this policy is the most restrictive and will only allow a pod to be admitted if &lt;em>all&lt;/em> requested CPUs and devices can be allocated from exactly one NUMA node.&lt;/li>
&lt;/ul>
&lt;p>It is important to note that the selected policy is applied to each container in a pod spec individually, rather than aligning resources across all containers together.&lt;/p>
&lt;p>Moreover, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis). We hope to relax this restriction in the future.&lt;/p>
&lt;p>The &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag to set one of these policies can be seen below:&lt;/p>
&lt;pre>&lt;code>--topology-manager-policy=
[none | best-effort | restricted | single-numa-node]
&lt;/code>&lt;/pre>&lt;p>Additionally, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> is protected by a feature gate. This feature gate has been available since Kubernetes 1.16, but has only been enabled by default since 1.18.&lt;/p>
&lt;p>The feature gate can be enabled or disabled as follows (as described in more detail &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">here&lt;/a>):&lt;/p>
&lt;pre>&lt;code>--feature-gates=&amp;quot;...,TopologyManager=&amp;lt;true|false&amp;gt;&amp;quot;
&lt;/code>&lt;/pre>&lt;p>In order to trigger alignment according to the selected policy, a user must request CPUs and peripheral devices in their pod spec, according to a certain set of requirements.&lt;/p>
&lt;p>For peripheral devices, this means requesting devices from the available resources provided by a device plugin (e.g. &lt;strong>&lt;code>intel.com/sriov&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nvidia.com/gpu&lt;/code>&lt;/strong>, etc.). This will only work if the device plugin has been extended to integrate properly with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. Currently, the only plugins known to have this extension are the &lt;a href="https://github.com/NVIDIA/k8s-device-plugin/blob/5cb45d52afdf5798a40f8d0de049bce77f689865/nvidia.go#L74">Nvidia GPU device plugin&lt;/a>, and the &lt;a href="https://github.com/intel/sriov-network-device-plugin/blob/30e33f1ce2fc7b45721b6de8c8207e65dbf2d508/pkg/resources/pciNetDevice.go#L80">Intel SRIOV network device plugin&lt;/a>. Details on how to extend a device plugin to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be found &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">here&lt;/a>.&lt;/p>
&lt;p>For CPUs, this requires that the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> has been configured with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and that the pod is running in the Guaranteed QoS class (i.e. all CPU and memory &lt;strong>&lt;code>limits&lt;/code>&lt;/strong> are equal to their respective CPU and memory &lt;strong>&lt;code>requests&lt;/code>&lt;/strong>). CPUs must also be requested in whole number values (e.g. &lt;strong>&lt;code>1&lt;/code>&lt;/strong>, &lt;strong>&lt;code>2&lt;/code>&lt;/strong>, &lt;strong>&lt;code>1000m&lt;/code>&lt;/strong>, etc). Details on how to set the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> policy can be found &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies">here&lt;/a>.&lt;/p>
&lt;p>For example, assuming the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> is running with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and the device plugins for &lt;strong>&lt;code>gpu-vendor.com&lt;/code>&lt;/strong>, and &lt;strong>&lt;code>nic-vendor.com&lt;/code>&lt;/strong> have been extended to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> properly, the pod spec below is sufficient to trigger the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to run its selected policy:&lt;/p>
&lt;pre>&lt;code>spec:
containers:
- name: numa-aligned-container
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/code>&lt;/pre>&lt;p>Following Figure 1 from the previous section, this would result in one of the following aligned allocations:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
{cpu: {0, 2}, gpu: 0, nic: 0}
{cpu: {0, 3}, gpu: 0, nic: 0}
{cpu: {1, 2}, gpu: 0, nic: 0}
{cpu: {1, 3}, gpu: 0, nic: 0}
{cpu: {2, 3}, gpu: 0, nic: 0}
{cpu: {4, 5}, gpu: 1, nic: 1}
{cpu: {4, 6}, gpu: 1, nic: 1}
{cpu: {4, 7}, gpu: 1, nic: 1}
{cpu: {5, 6}, gpu: 1, nic: 1}
{cpu: {5, 7}, gpu: 1, nic: 1}
{cpu: {6, 7}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p>And that’s it! Just follow this pattern to have the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> ensure NUMA alignment across containers that request topology-aware devices and exclusive CPUs.&lt;/p>
&lt;p>**NOTE:** if a pod is rejected by one of the **&lt;code>TopologyManager&lt;/code>** policies, it will be placed in a **&lt;code>Terminated&lt;/code>** state with a pod admission error and a reason of &amp;quot;&lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;strong>&amp;quot;. Once a pod is in this state, the Kubernetes scheduler will not attempt to reschedule it. It is therefore recommended to use a &lt;/strong>&lt;code>Deployment&lt;/code>&lt;/strong>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">&lt;/a> with replicas to trigger a redeploy of the pod on such a failure. An &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">external control loop&lt;/a> can also be implemented to trigger a redeployment of pods that have a &lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="this-is-great-so-how-does-it-work-under-the-hood">This is great, so how does it work under the hood?&lt;/h2>
&lt;p>Pseudocode for the primary logic carried out by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be seen below:&lt;/p>
&lt;pre>&lt;code>for container := range append(InitContainers, Containers...) {
for provider := range HintProviders {
hints += provider.GetTopologyHints(container)
}
bestHint := policy.Merge(hints)
for provider := range HintProviders {
provider.Allocate(container, bestHint)
}
}
&lt;/code>&lt;/pre>&lt;p>The following diagram summarizes the steps taken during this loop:&lt;/p>
&lt;p align="center">
&lt;img weight="200" height="200" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-steps-during-loop.png">
&lt;/p>
&lt;p>The steps themselves are:&lt;/p>
&lt;ol>
&lt;li>Loop over all containers in a pod.&lt;/li>
&lt;li>For each container, gather &amp;quot;&lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong>&amp;quot; from a set of &amp;quot;&lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>&amp;quot; for each topology-aware resource type requested by the container (e.g. &lt;strong>&lt;code>gpu-vendor.com/gpu&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nic-vendor.com/nic&lt;/code>&lt;/strong>, &lt;strong>&lt;code>cpu&lt;/code>&lt;/strong>, etc.).&lt;/li>
&lt;li>Using the selected policy, merge the gathered &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> to find the &amp;quot;best&amp;quot; hint that aligns resource allocations across all resource types.&lt;/li>
&lt;li>Loop back over the set of hint providers, instructing them to allocate the resources they control using the merged hint as a guide.&lt;/li>
&lt;li>This loop runs at pod admission time and will fail to admit the pod if any of these steps fail or alignment cannot be satisfied according to the selected policy. Any resources allocated before the failure are cleaned up accordingly.&lt;/li>
&lt;/ol>
&lt;p>The following sections go into more detail on the exact structure of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> and &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>, as well as some details on the merge strategies used by each policy.&lt;/p>
&lt;h3 id="topologyhints">TopologyHints&lt;/h3>
&lt;p>A &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> encodes a set of constraints from which a given resource request can be satisfied. At present, the only constraint we consider is NUMA alignment. It is defined as follows:&lt;/p>
&lt;pre>&lt;code>type TopologyHint struct {
NUMANodeAffinity bitmask.BitMask
Preferred bool
}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> field contains a bitmask of NUMA nodes where a resource request can be satisfied. For example, the possible masks on a system with 2 NUMA nodes include:&lt;/p>
&lt;pre>&lt;code>{00}, {01}, {10}, {11}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field contains a boolean that encodes whether the given hint is &amp;quot;preferred&amp;quot; or not. With the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, preferred hints will be given preference over non-preferred hints when generating a &amp;quot;best&amp;quot; hint. With the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, non-preferred hints will be rejected.&lt;/p>
&lt;p>In general, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> by looking at the set of currently available resources that can satisfy a resource request. More specifically, they generate one &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> for every possible mask of NUMA nodes where that resource request can be satisfied. If a mask cannot satisfy the request, it is omitted. For example, a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> might provide the following hints on a system with 2 NUMA nodes when being asked to allocate 2 resources. These hints encode that both resources could either come from a single NUMA node (either 0 or 1), or they could each come from different NUMA nodes (but we prefer for them to come from just one).&lt;/p>
&lt;pre>&lt;code>{01: True}, {10: True}, {11: False}
&lt;/code>&lt;/pre>&lt;p>At present, all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> set the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if and only if the &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> encodes a &lt;em>minimal&lt;/em> set of NUMA nodes that can satisfy the resource request. Normally, this will only be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> for &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> with a single NUMA node set in their bitmask. However, it may also be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if the only way to &lt;em>ever&lt;/em> satisfy the resource request is to span multiple NUMA nodes (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes):&lt;/p>
&lt;pre>&lt;code>{0011: True}, {0111: False}, {1011: False}, {1111: False}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>NOTE:&lt;/strong> Setting of the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field in this way is &lt;em>not&lt;/em> based on the set of currently available resources. It is based on the ability to physically allocate the number of requested resources on some minimal set of NUMA nodes.&lt;/p>
&lt;p>In this way, it is possible for a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to return a list of hints with &lt;em>all&lt;/em> &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> fields set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> if an actual preferred allocation cannot be satisfied until other containers release their resources. For example, consider the following scenario from the system in Figure 1:&lt;/p>
&lt;ol>
&lt;li>All but 2 CPUs are currently allocated to containers&lt;/li>
&lt;li>The 2 remaining CPUs are on different NUMA nodes&lt;/li>
&lt;li>A new container comes along asking for 2 CPUs&lt;/li>
&lt;/ol>
&lt;p>In this case, the only generated hint would be &lt;strong>&lt;code>{11: False}&lt;/code>&lt;/strong> and not &lt;strong>&lt;code>{11: True}&lt;/code>&lt;/strong>. This happens because it &lt;em>is&lt;/em> possible to allocate 2 CPUs from the same NUMA node on this system (just not right now, given the current allocation state). The idea being that it is better to fail pod admission and retry the deployment when the minimal alignment can be satisfied than to allow a pod to be scheduled with sub-optimal alignment.&lt;/p>
&lt;h3 id="hintproviders">HintProviders&lt;/h3>
&lt;p>A &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> is a component internal to the &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> that coordinates aligned resource allocations with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. At present, the only &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> in Kubernetes are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. We plan to add support for &lt;strong>&lt;code>HugePages&lt;/code>&lt;/strong> soon.&lt;/p>
&lt;p>As discussed previously, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> both gathers &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> as well as triggers aligned resource allocations on them using a merged &amp;quot;best&amp;quot; hint. As such, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> implement the following interface:&lt;/p>
&lt;pre>&lt;code>type HintProvider interface {
GetTopologyHints(*v1.Pod, *v1.Container) map[string][]TopologyHint
Allocate(*v1.Pod, *v1.Container) error
}
&lt;/code>&lt;/pre>&lt;p>Notice that the call to &lt;strong>&lt;code>GetTopologyHints()&lt;/code>&lt;/strong> returns a &lt;strong>&lt;code>map[string][]TopologyHint&lt;/code>&lt;/strong>. This allows a single &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to provide hints for multiple resource types instead of just one. For example, the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong> requires this in order to pass hints back for every resource type registered by its plugins.&lt;/p>
&lt;p>As &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate their hints, they only consider how alignment could be satisfied for &lt;em>currently&lt;/em> available resources on the system. Any resources already allocated to other containers are not considered.&lt;/p>
&lt;p>For example, consider the system in Figure 1, with the following two containers requesting resources from it:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">&lt;strong>&lt;code>Container0&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>Container1&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container0
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container1
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>If &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec.&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider1.png">
&lt;/p>
&lt;p>When considering &lt;strong>&lt;code>Container1&lt;/code>&lt;/strong> these resources are then presumed to be unavailable, and thus only the following set of hints will be generated:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{10: True}}
nic-vendor.com/nic: {{10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {4, 5}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider2.png">
&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> Unlike the pseudocode provided at the beginning of this section, the call to &lt;strong>&lt;code>Allocate()&lt;/code>&lt;/strong> does not actually take a parameter for the merged &amp;quot;best&amp;quot; hint directly. Instead, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> implements the following &lt;strong>&lt;code>Store&lt;/code>&lt;/strong> interface that &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> can query to retrieve the hint generated for a particular container once it has been generated:&lt;/p>
&lt;pre>&lt;code>type Store interface {
GetAffinity(podUID string, containerName string) TopologyHint
}
&lt;/code>&lt;/pre>&lt;p>Separating this out into its own API call allows one to access this hint outside of the pod admission loop. This is useful for debugging as well as for reporting generated hints in tools such as &lt;strong>&lt;code>kubectl&lt;/code>&lt;/strong>(not yet available).&lt;/p>
&lt;h3 id="policy-merge">Policy.Merge&lt;/h3>
&lt;p>The merge strategy defined by a given policy dictates how it combines the set of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated by all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> into a single &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> that can be used to inform aligned resource allocations.&lt;/p>
&lt;p>The general merge strategy for all supported policies begins the same:&lt;/p>
&lt;ol>
&lt;li>Take the cross-product of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated for each resource type&lt;/li>
&lt;li>For each entry in the cross-product, &lt;strong>&lt;code>bitwise-and&lt;/code>&lt;/strong> the NUMA affinities of each &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> together. Set this as the NUMA affinity in a resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If all of the hints in an entry have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If even one of the hints in an entry has &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint. Also set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the &amp;quot;merged&amp;quot; hint if its NUMA affinity contains all 0s.&lt;/li>
&lt;/ol>
&lt;p>Following the example from the previous section with hints for &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> generated as:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>The above algorithm results in the following set of cross-product entries and &amp;quot;merged&amp;quot; hints:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">cross-product entry
&lt;p>
&lt;strong>&lt;code>{cpu, gpu-vendor.com/gpu, nic-vendor.com/nic}&lt;/code>&lt;/strong>
&lt;/p>
&lt;/td>
&lt;td align="center">"merged" hint
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td align="center">
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{10: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>Once this list of &amp;quot;merged&amp;quot; hints has been generated, it is the job of the specific &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> policy in use to decide which one to consider as the &amp;quot;best&amp;quot; hint.&lt;/p>
&lt;p>In general, this involves:&lt;/p>
&lt;ol>
&lt;li>Sorting merged hints by their &amp;quot;narrowness&amp;quot;. Narrowness is defined as the number of bits set in a hint’s NUMA affinity mask. The fewer bits set, the narrower the hint. For hints that have the same number of bits set in their NUMA affinity mask, the hint with the most low order bits set is considered narrower.&lt;/li>
&lt;li>Sorting merged hints by their &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field. Hints that have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> are considered more likely candidates than hints with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong>.&lt;/li>
&lt;li>Selecting the narrowest hint with the best possible setting for &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>In the case of the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy this algorithm will always result in &lt;em>some&lt;/em> hint being selected as the &amp;quot;best&amp;quot; hint and the pod being admitted. This &amp;quot;best&amp;quot; hint is then made available to &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> so they can make their resource allocations based on it.&lt;/p>
&lt;p>However, in the case of the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, any selected hint with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> will be rejected immediately, causing pod admission to fail and no resources to be allocated. Moreover, the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> will also reject a selected hint that has more than one NUMA node set in its affinity mask.&lt;/p>
&lt;p>In the example above, the pod would be admitted by all policies with a hint of &lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="upcoming-enhancements">Upcoming enhancements&lt;/h2>
&lt;p>While the 1.18 release and promotion to Beta brings along some great enhancements and fixes, there are still a number of limitations, described &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#known-limitations">here&lt;/a>. We are already underway working to address these limitations and more.&lt;/p>
&lt;p>This section walks through the set of enhancements we plan to implement for the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> in the near future. This list is not exhaustive, but it gives a good idea of the direction we are moving in. It is ordered by the timeframe in which we expect to see each enhancement completed.&lt;/p>
&lt;p>If you would like to get involved in helping with any of these enhancements, please &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">join the weekly Kubernetes SIG-node meetings&lt;/a> to learn more and become part of the community effort!&lt;/p>
&lt;h3 id="supporting-device-specific-constraints">Supporting device-specific constraints&lt;/h3>
&lt;p>Currently, NUMA affinity is the only constraint considered by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> for resource alignment. Moreover, the only scalable extensions that can be made to a &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> involve &lt;em>node-level&lt;/em> constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any &lt;em>device-specific&lt;/em> constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices).&lt;/p>
&lt;p>As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions.&lt;/p>
&lt;p>Details of this proposal can be found &lt;a href="https://github.com/kubernetes/enhancements/pull/1121">here&lt;/a>, and should be available as soon as Kubernetes 1.19.&lt;/p>
&lt;h3 id="numa-alignment-for-hugepages">NUMA alignment for hugepages&lt;/h3>
&lt;p>As stated previously, the only two &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> currently available to the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. However, work is currently underway to add support for hugepages as well. With the completion of this work, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will finally be able to allocate memory, hugepages, CPUs and PCI devices all on the same NUMA node.&lt;/p>
&lt;p>A &lt;a href="https://github.com/kubernetes/enhancements/blob/253f1e5bdd121872d2d0f7020a5ac0365b229e30/keps/sig-node/20200203-memory-manager.md">KEP&lt;/a> for this work is currently under review, and a prototype is underway to get this feature implemented very soon.&lt;/p>
&lt;h3 id="scheduler-awareness">Scheduler awareness&lt;/h3>
&lt;p>Currently, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> acts as a Pod Admission controller. It is not directly involved in the scheduling decision of where a pod will be placed. Rather, when the kubernetes scheduler (or whatever scheduler is running in the deployment), places a pod on a node to run, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will decide if the pod should be &amp;quot;admitted&amp;quot; or &amp;quot;rejected&amp;quot;. If the pod is rejected due to lack of available NUMA aligned resources, things can get a little interesting. This kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/issues/84869">issue&lt;/a> highlights and discusses this situation well.&lt;/p>
&lt;p>So how do we go about addressing this limitation? We have the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md">Kubernetes Scheduling Framework&lt;/a> to the rescue! This framework provides a new set of plugin APIs that integrate with the existing Kubernetes Scheduler and allow scheduling features, such as NUMA alignment, to be implemented without having to resort to other, perhaps less appealing alternatives, including writing your own scheduler, or even worse, creating a fork to add your own scheduler secret sauce.&lt;/p>
&lt;p>The details of how to implement these extensions for integration with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> have not yet been worked out. We still need to answer questions like:&lt;/p>
&lt;ul>
&lt;li>Will we require duplicated logic to determine device affinity in the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and the scheduler?&lt;/li>
&lt;li>Do we need a new API to get &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to the scheduler plugin?&lt;/li>
&lt;/ul>
&lt;p>Work on this feature should begin in the next couple of months, so stay tuned!&lt;/p>
&lt;h3 id="per-pod-alignment-policy">Per-pod alignment policy&lt;/h3>
&lt;p>As stated previously, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis).&lt;/p>
&lt;p>While we agree that this would be a great feature to have, there are quite a few hurdles that need to be overcome before it is achievable. The biggest hurdle being that this enhancement will require an API change to be able to express the desired alignment policy in either the Pod spec or its associated &lt;strong>&lt;code>&lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass&lt;/a>&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We are only now starting to have serious discussions around this feature, and it is still a few releases away, at the best, from being available.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>With the promotion of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to Beta in 1.18, we encourage everyone to give it a try and look forward to any feedback you may have. Many fixes and enhancements have been worked on in the past several releases, greatly improving the functionality and reliability of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and its &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>. While there are still a number of limitations, we have a set of enhancements planned to address them, and look forward to providing you with a number of new features in upcoming releases.&lt;/p>
&lt;p>If you have ideas for additional enhancements or a desire for certain features, don’t hesitate to let us know. The team is always open to suggestions to enhance and improve the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We hope you have found this blog informative and useful! Let us know if you have any questions or comments. And, happy deploying…..Align Up!&lt;/p>
&lt;!-- Docs to Markdown version 1.0β20 --></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 Release Team&lt;/a>&lt;/p>
&lt;p>We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.&lt;/p>
&lt;p>Kubernetes 1.18 is a &amp;quot;fit and finish&amp;quot; release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="kubernetes-topology-manager-moves-to-beta-align-up">Kubernetes Topology Manager Moves to Beta - Align Up!&lt;/h3>
&lt;p>A beta feature of Kubernetes in release 1.18, the &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">Topology Manager feature&lt;/a> enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.&lt;/p>
&lt;h3 id="serverside-apply-introduces-beta-2">Serverside Apply Introduces Beta 2&lt;/h3>
&lt;p>Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.&lt;/p>
&lt;h3 id="extending-ingress-with-and-replacing-a-deprecated-annotation-with-ingressclass">Extending Ingress with and replacing a deprecated annotation with IngressClass&lt;/h3>
&lt;p>In Kubernetes 1.18, there are two significant additions to Ingress: A new &lt;code>pathType&lt;/code> field and a new &lt;code>IngressClass&lt;/code> resource. The &lt;code>pathType&lt;/code> field allows specifying how paths should be matched. In addition to the default &lt;code>ImplementationSpecific&lt;/code> type, there are new &lt;code>Exact&lt;/code> and &lt;code>Prefix&lt;/code> path types.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new &lt;code>ingressClassName&lt;/code> field on Ingresses. This new resource and field replace the deprecated &lt;code>kubernetes.io/ingress.class&lt;/code> annotation.&lt;/p>
&lt;h3 id="sig-cli-introduces-kubectl-alpha-debug">SIG-CLI introduces kubectl alpha debug&lt;/h3>
&lt;p>SIG-CLI was debating the need for a debug utility for quite some time already. With the development of &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers&lt;/a>, it became more obvious how we can support developers with tooling built on top of &lt;code>kubectl exec&lt;/code>. The addition of the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> command&lt;/a> (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.&lt;/p>
&lt;h3 id="introducing-windows-csi-support-alpha-for-kubernetes">Introducing Windows CSI support alpha for Kubernetes&lt;/h3>
&lt;p>The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable 💯&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">Taint Based Eviction&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI Block storage support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API Server dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">Pass Pod information in CSI calls&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">Support Out-of-Tree vSphere Cloud Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">Support GMSA for Windows workloads&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">Skip attach for non-attachable CSI volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC cloning&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">RunAsUserName for Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">AppProtocol for Services and Endpoints&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>Check out the full details of the Kubernetes 1.18 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.18 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>. You can also easily install 1.18 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">release team&lt;/a> led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h3 id="release-logo">Release Logo&lt;/h3>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 Release Logo">&lt;/p>
&lt;h4 id="why-the-lhc">Why the LHC?&lt;/h4>
&lt;p>The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! &amp;quot;A Bit Quarky&amp;quot; as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.&lt;/p>
&lt;h4 id="about-the-designer">About the designer&lt;/h4>
&lt;p>Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Ericsson is using Kubernetes and other cloud native technology to deliver a &lt;a href="https://www.cncf.io/case-study/ericsson/">highly demanding 5G network&lt;/a> that resulted in up to 90 percent CI/CD savings.&lt;/li>
&lt;li>Zendesk is using Kubernetes to &lt;a href="https://www.cncf.io/case-study/zendesk/">run around 70% of its existing applications&lt;/a>. It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.&lt;/li>
&lt;li>LifeMiles has &lt;a href="https://www.cncf.io/case-study/lifemiles/">reduced infrastructure spending by 50%&lt;/a> because of its move to Kubernetes. It has also allowed them to double its available resource capacity.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>The CNCF published the results of its &lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">annual survey&lt;/a> showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.&lt;/li>
&lt;li>The “Introduction to Kubernetes” course hosted by the CNCF &lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">surpassed 100,000 registrations&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.&lt;/p>
&lt;p>This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h3 id="event-update">Event Update&lt;/h3>
&lt;p>Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">Novel Coronavirus Update page&lt;/a>.&lt;/p>
&lt;h3 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h3>
&lt;p>Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-18/">https://www.cncf.io/webinars/kubernetes-1-18/&lt;/a>.&lt;/p>
&lt;h3 id="get-involved">Get Involved&lt;/h3>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Join SIG Scalability and Learn Kubernetes the Hard Way</title><link>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</link><pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Handy&lt;/p>
&lt;p>Contributing to SIG Scalability is a great way to learn Kubernetes in all its depth and breadth, and the team would love to have you &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">join as a contributor&lt;/a>. I took a look at the value of learning the hard way and interviewed the current SIG chairs to give you an idea of what contribution feels like.&lt;/p>
&lt;h2 id="the-value-of-learning-the-hard-way">The value of Learning The Hard Way&lt;/h2>
&lt;p>There is a belief in the software development community that pushes for the most challenging and rigorous possible method of learning a new language or system. These tend to go by the moniker of &amp;quot;Learn __ the Hard Way.&amp;quot; Examples abound: Learn Code the Hard Way, Learn Python the Hard Way, and many others originating with Zed Shaw's courses in the topic.&lt;/p>
&lt;p>While there are folks out there who offer you a &amp;quot;Learn Kubernetes the Hard Way&amp;quot; type experience (most notably &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kelsey Hightower's&lt;/a>), any &amp;quot;Hard Way&amp;quot; project should attempt to cover every aspect of the core topic's principles.&lt;/p>
&lt;p>Therefore, the real way to &amp;quot;Learn Kubernetes the Hard Way,&amp;quot; is to join the CNCF and get involved in the project itself. And there is only one SIG that could genuinely offer a full-stack learning experience for Kubernetes: SIG Scalability.&lt;/p>
&lt;p>The team behind SIG Scalability is responsible for detecting and dealing with issues that arise when Kubernetes clusters are working with upwards of a thousand nodes. Said &lt;a href="https://github.com/wojtek-t">Wojiciech Tyczynski&lt;/a>, a staff software engineer at Google and a member of SIG Scalability, the standard size for a test cluster for this SIG is over 5,000 nodes.&lt;/p>
&lt;p>And yet, this SIG is not composed of Ph.D.'s in highly scalable systems designs. Many of the folks working with Tyczynski, for example, joined the SIG knowing very little about these types of issues, and often, very little about Kubernetes.&lt;/p>
&lt;p>Working on SIG Scalability is like jumping into the deep end of the pool to learn to swim, and the SIG is inherently concerned with the entire Kubernetes project. SIG Scalability focuses on how Kubernetes functions as a whole and at scale. The SIG Scalability team members have an impetus to learn about every system and to understand how all systems interact with one another.&lt;/p>
&lt;h2 id="a-complex-and-rewarding-contributor-experience">A complex and rewarding contributor experience&lt;/h2>
&lt;p>While that may sound complicated (and it is!), that doesn't mean it's outside the reach of an average developer, tester, or administrator. Google software developer Matt Matejczyk has only been on the team since the beginning of 2019, and he's been a valued member of the team since then, ferreting out bugs.&lt;/p>
&lt;p>&amp;quot;I am new here,&amp;quot; said Matejczyk. &amp;quot;I joined the team in January [2019]. Before that, I worked on AdWords at Google in New York. Why did I join? I knew some people there, so that was one of the decisions for me to move. I thought at that time that Kubernetes is a unique, cutting edge technology. I thought it'd be cool to work on that.&amp;quot;&lt;/p>
&lt;p>Matejczyk was correct about the coolness. &amp;quot;It's cool,&amp;quot; he said. &amp;quot;So actually, ramping up on scalability is not easy. There are many things you need to understand. You need to understand Kubernetes very well. It can use every part of Kubernetes. I am still ramping up after these 8 months. I think it took me maybe 3 months to get up to decent speed.&amp;quot;&lt;/p>
&lt;p>When Matejczyk spoke to what he had worked on during those 8 months, he answered, &amp;quot;An interesting example is a regression I have been working on recently. We noticed the overall slowness of Kubernetes control plane in specific scenarios, and we couldn't attribute it to any particular component. In the end, we realized that everything boiled down to the memory allocation on the golang level. It was very counterintuitive to have two completely separate pieces of code (running as a part of the same binary) affecting the performance of each other only because one of them was allocating memory too fast. But connecting all the dots and getting to the bottom of regression like this gives great satisfaction.&amp;quot;&lt;/p>
&lt;p>Tyczynski said that &amp;quot;It's not only debugging regressions, but it's also debugging and finding bottlenecks. In general, those can be regressions, but those can be things we can improve. The other significant area is extending what we want to guarantee to users. Extending SLA and SLO coverage of the system so users can rely on what they can expect from the system in terms of performance and scalability. Matt is doing much work in extending our tests to be more representative and cover more Kubernetes concepts.&amp;quot;&lt;/p>
&lt;h2 id="give-sig-scalability-a-try">Give SIG Scalability a try&lt;/h2>
&lt;p>The SIG Scalability team is always in need of new members, and if you're the sort of developer or tester who loves taking on new complex challenges, and perhaps loves learning things the hard way, consider joining this SIG. As the team points out, adding Kubernetes expertise to your resume is never a bad idea, and this is the one SIG where you can learn it all from top to bottom.&lt;/p>
&lt;p>See &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">the SIG's documentation&lt;/a> to learn about upcoming meetings, its charter, and more. You can also join the &lt;a href="https://kubernetes.slack.com/archives/C09QZTRH7">#sig-scalability Slack channel&lt;/a> to see what it's like. We hope to see you join in to take advantage of this great opportunity to learn Kubernetes and contribute back at the same time.&lt;/p></description></item><item><title>Blog: Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</title><link>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kevin Chen, Kong&lt;/p>
&lt;p>Kubernetes has become the de facto way to orchestrate containers and the services within services. But how do we give services outside our cluster access to what is within? Kubernetes comes with the Ingress API object that manages external access to services within a cluster.&lt;/p>
&lt;p>Ingress is a group of rules that will proxy inbound connections to endpoints defined by a backend. However, Kubernetes does not know what to do with Ingress resources without an Ingress controller, which is where an open source controller can come into play. In this post, we are going to use one option for this: the Kong Ingress Controller. The Kong Ingress Controller was open-sourced a year ago and recently reached one million downloads. In the recent 0.7 release, service mesh support was also added. Other features of this release include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Built-In Kubernetes Admission Controller&lt;/strong>, which validates Custom Resource Definitions (CRD) as they are created or updated and rejects any invalid configurations.&lt;/li>
&lt;li>&lt;strong>In-memory Mode&lt;/strong> - Each pod’s controller actively configures the Kong container in its pod, which limits the blast radius of failure of a single container of Kong or controller container to that pod only.&lt;/li>
&lt;li>&lt;strong>Native gRPC Routing&lt;/strong> - gRPC traffic can now be routed via Kong Ingress Controller natively with support for method-based routing.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/KIC-gRPC.png" alt="K4K-gRPC">&lt;/p>
&lt;p>If you would like a deeper dive into Kong Ingress Controller 0.7, please check out the &lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub repository&lt;/a>.&lt;/p>
&lt;p>But let’s get back to the service mesh support since that will be the main focal point of this blog post. Service mesh allows organizations to address microservices challenges related to security, reliability, and observability by abstracting inter-service communication into a mesh layer. But what if our mesh layer sits within Kubernetes and we still need to expose certain services beyond our cluster? Then you need an Ingress controller such as the Kong Ingress Controller. In this blog post, we’ll cover how to deploy Kong Ingress Controller as your Ingress layer to an Istio mesh. Let’s dive right in:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/k4k8s.png" alt="Kong Kubernetes Ingress Controller">&lt;/p>
&lt;h3 id="part-0-set-up-istio-on-kubernetes">Part 0: Set up Istio on Kubernetes&lt;/h3>
&lt;p>This blog will assume you have Istio set up on Kubernetes. If you need to catch up to this point, please check out the &lt;a href="https://istio.io/docs/setup/">Istio documentation&lt;/a>. It will walk you through setting up Istio on Kubernetes.&lt;/p>
&lt;h3 id="1-install-the-bookinfo-application">1. Install the Bookinfo Application&lt;/h3>
&lt;p>First, we need to label the namespaces that will host our application and Kong proxy. To label our default namespace where the bookinfo app sits, run this command:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace default istio-injection=enabled
namespace/default labeled
&lt;/code>&lt;/pre>&lt;p>Then create a new namespace that will be hosting our Kong gateway and the Ingress controller:&lt;/p>
&lt;pre>&lt;code>$ kubectl create namespace kong
namespace/kong created
&lt;/code>&lt;/pre>&lt;p>Because Kong will be sitting outside the default namespace, be sure you also label the Kong namespace with istio-injection enabled as well:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace kong istio-injection=enabled
namespace/kong labeled
&lt;/code>&lt;/pre>&lt;p>Having both namespaces labeled &lt;code>istio-injection=enabled&lt;/code> is necessary. Or else the default configuration will not inject a sidecar container into the pods of your namespaces.&lt;/p>
&lt;p>Now deploy your BookInfo application with the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f http://bit.ly/bookinfoapp
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
&lt;/code>&lt;/pre>&lt;p>Let’s double-check our Services and Pods to make sure that we have it all set up correctly:&lt;/p>
&lt;pre>&lt;code>$ kubectl get services
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
details ClusterIP 10.97.125.254 &amp;lt;none&amp;gt; 9080/TCP 29s
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 29h
productpage ClusterIP 10.97.62.68 &amp;lt;none&amp;gt; 9080/TCP 28s
ratings ClusterIP 10.96.15.180 &amp;lt;none&amp;gt; 9080/TCP 28s
reviews ClusterIP 10.104.207.136 &amp;lt;none&amp;gt; 9080/TCP 28s
&lt;/code>&lt;/pre>&lt;p>You should see four new services: details, productpage, ratings, and reviews. None of them have an external IP so we will use the &lt;a href="https://github.com/Kong/kong">Kong gateway&lt;/a> to expose the necessary services. And to check pods, run the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods
NAME READY STATUS RESTARTS AGE
details-v1-c5b5f496d-9wm29 2/2 Running 0 101s
productpage-v1-7d6cfb7dfd-5mc96 2/2 Running 0 100s
ratings-v1-f745cf57b-hmkwf 2/2 Running 0 101s
reviews-v1-85c474d9b8-kqcpt 2/2 Running 0 101s
reviews-v2-ccffdd984-9jnsj 2/2 Running 0 101s
reviews-v3-98dc67b68-nzw97 2/2 Running 0 101s
&lt;/code>&lt;/pre>&lt;p>This command outputs useful data, so let’s take a second to understand it. If you examine the READY column, each pod has two containers running: the service and an Envoy sidecar injected alongside it. Another thing to highlight is that there are three review pods but only 1 review service. The Envoy sidecar will load balance the traffic to three different review pods that contain different versions, giving us the ability to A/B test our changes. We have one step before we can access the deployed application. We need to add an additional annotation to the &lt;code>productpage&lt;/code> service. To do so, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate service productpage ingress.kubernetes.io/service-upstream=true
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Both the API gateway (Kong) and the service mesh (Istio) can handle the load-balancing. Without the additional &lt;code>ingress.kubernetes.io/service-upstream: &amp;quot;true&amp;quot;&lt;/code> annotation, Kong will try to load-balance by selecting its own endpoint/target from the productpage service. This causes Envoy to receive that pod’s IP as the upstream local address, instead of the service’s cluster IP. But we want the service's cluster IP so that Envoy can properly load balance.&lt;/p>
&lt;p>With that added, you should now be able to access your product page!&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="2-kong-kubernetes-ingress-controller-without-database">2. Kong Kubernetes Ingress Controller Without Database&lt;/h3>
&lt;p>To expose your services to the world, we will deploy Kong as the north-south traffic gateway. &lt;a href="https://github.com/Kong/kong/releases/tag/1.1.2">Kong 1.1&lt;/a> released with declarative configuration and DB-less mode. Declarative configuration allows you to specify the desired system state through a YAML or JSON file instead of a sequence of API calls. Using declarative config provides several key benefits to reduce complexity, increase automation and enhance system performance. And with the Kong Ingress Controller, any Ingress rules you apply to the cluster will automatically be configured on the Kong proxy. Let’s set up the Kong Ingress Controller and the actual Kong proxy first like this:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f https://bit.ly/k4k8s
namespace/kong configured
customresourcedefinition.apiextensions.k8s.io/kongconsumers.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongcredentials.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongingresses.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongplugins.configuration.konghq.com created
serviceaccount/kong-serviceaccount created
clusterrole.rbac.authorization.k8s.io/kong-ingress-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/kong-ingress-clusterrole-nisa-binding created
configmap/kong-server-blocks created
service/kong-proxy created
service/kong-validation-webhook created
deployment.apps/ingress-kong created
&lt;/code>&lt;/pre>&lt;p>To check if the Kong pod is up and running, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kong
NAME READY STATUS RESTARTS AGE
pod/ingress-kong-8b44c9856-9s42v 3/3 Running 0 2m26s
&lt;/code>&lt;/pre>&lt;p>There will be three containers within this pod. The first container is the Kong Gateway that will be the Ingress point to your cluster. The second container is the Ingress controller. It uses Ingress resources and updates the proxy to follow rules defined in the resource. And lastly, the third container is the Envoy proxy injected by Istio. Kong will route traffic through the Envoy sidecar proxy to the appropriate service. To send requests into the cluster via our newly deployed Kong Gateway, setup an environment variable with the a URL based on the IP address at which Kong is accessible.&lt;/p>
&lt;pre>&lt;code>$ export PROXY_URL=&amp;quot;$(minikube service -n kong kong-proxy --url | head -1)&amp;quot;
$ echo $PROXY_URL
http://192.168.99.100:32728
&lt;/code>&lt;/pre>&lt;p>Next, we need to change some configuration so that the side-car Envoy process can route the request correctly based on the host/authority header of the request. Run the following to stop the route from preserving host:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
name: do-not-preserve-host
route:
preserve_host: false
upstream:
host_header: productpage.default.svc
&amp;quot; | kubectl apply -f -
kongingress.configuration.konghq.com/do-not-preserve-host created
&lt;/code>&lt;/pre>&lt;p>And annotate the existing productpage service to set service-upstream as true:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate svc productpage Ingress.kubernetes.io/service-upstream=&amp;quot;true&amp;quot;
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Now that we have everything set up, we can look at how to use the Ingress resource to help route external traffic to the services within your Istio mesh. We’ll create an Ingress rule that routes all traffic with the path of &lt;code>/&lt;/code> to our productpage service:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: productpage
annotations:
configuration.konghq.com: do-not-preserve-host
spec:
rules:
- http:
paths:
- path: /
backend:
serviceName: productpage
servicePort: 9080
&amp;quot; | kubectl apply -f -
ingress.extensions/productpage created
&lt;/code>&lt;/pre>&lt;p>And just like that, the Kong Ingress Controller is able to understand the rules you defined in the Ingress resource and routes it to the productpage service! To view the product page service’s GUI, go to &lt;code>$PROXY_URL/productpage&lt;/code> in your browser. Or to test it in your command line, try:&lt;/p>
&lt;pre>&lt;code>$ curl $PROXY_URL/productpage
&lt;/code>&lt;/pre>&lt;p>That is all I have for this walk-through. If you enjoyed the technologies used in this post, please check out their repositories since they are all open source and would love to have more contributors! Here are their links for your convenience:&lt;/p>
&lt;ul>
&lt;li>Kong: [&lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub&lt;/a>] [&lt;a href="https://twitter.com/thekonginc">Twitter&lt;/a>]&lt;/li>
&lt;li>Kubernetes: [&lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a>] [&lt;a href="https://twitter.com/kubernetesio">Twitter&lt;/a>]&lt;/li>
&lt;li>Istio: [&lt;a href="https://github.com/istio/istio">GitHub&lt;/a>] [&lt;a href="https://twitter.com/IstioMesh">Twitter&lt;/a>]&lt;/li>
&lt;li>Envoy: [&lt;a href="https://github.com/envoyproxy/envoy">GitHub&lt;/a>] [&lt;a href="https://twitter.com/EnvoyProxy">Twitter&lt;/a>]&lt;/li>
&lt;/ul>
&lt;p>Thank you for following along!&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Postponed</title><link>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Dawn Foster (VMware), Jorge Castro (VMware)&lt;/p>
&lt;p>The CNCF has announced that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">KubeCon + CloudNativeCon EU has been delayed&lt;/a> until July/August of 2020. As a result the Contributor Summit planning team is weighing options for how to proceed. Here’s the current plan:&lt;/p>
&lt;ul>
&lt;li>There will be an in-person Contributor Summit as planned when KubeCon + CloudNativeCon is rescheduled.&lt;/li>
&lt;li>We are looking at options for having additional virtual contributor activities in the meantime.&lt;/li>
&lt;/ul>
&lt;p>We will communicate via this blog and the usual communications channels on the final plan. Please bear with us as we adapt when we get more information. Thank you for being patient as the team pivots to bring you a great Contributor Summit!&lt;/p></description></item><item><title>Blog: Bring your ideas to the world with kubectl plugins</title><link>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Cornelius Weig (TNG Technology Consulting GmbH)&lt;/p>
&lt;p>&lt;code>kubectl&lt;/code> is the most critical tool to interact with Kubernetes and has to address multiple user personas, each with their own needs and opinions.
One way to make &lt;code>kubectl&lt;/code> do what you need is to build new functionality into &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;h2 id="challenges-with-building-commands-into-kubectl">Challenges with building commands into &lt;code>kubectl&lt;/code>&lt;/h2>
&lt;p>However, that's easier said than done. Being such an important cornerstone of
Kubernetes, any meaningful change to &lt;code>kubectl&lt;/code> needs to undergo a Kubernetes
Enhancement Proposal (KEP) where the intended change is discussed beforehand.&lt;/p>
&lt;p>When it comes to implementation, you'll find that &lt;code>kubectl&lt;/code> is an ingenious and
complex piece of engineering. It might take a long time to get used to
the processes and style of the codebase to get done what you want to achieve. Next
comes the review process which may go through several rounds until it meets all
the requirements of the Kubernetes maintainers -- after all, they need to take
over ownership of this feature and maintain it from the day it's merged.&lt;/p>
&lt;p>When everything goes well, you can finally rejoice. Your code will be shipped
with the next Kubernetes release. Well, that could mean you need to wait
another 3 months to ship your idea in &lt;code>kubectl&lt;/code> if you are unlucky.&lt;/p>
&lt;p>So this was the happy path where everything goes well. But there are good
reasons why your new functionality may never make it into &lt;code>kubectl&lt;/code>. For one,
&lt;code>kubectl&lt;/code> has a particular look and feel and violating that style will not be
acceptable by the maintainers. For example, an interactive command that
produces output with colors would be inconsistent with the rest of &lt;code>kubectl&lt;/code>.
Also, when it comes to tools or commands useful only to a minuscule proportion
of users, the maintainers may simply reject your proposal as &lt;code>kubectl&lt;/code> needs to
address common needs.&lt;/p>
&lt;p>But this doesn’t mean you can’t ship your ideas to &lt;code>kubectl&lt;/code> users.&lt;/p>
&lt;h2 id="what-if-you-didn-t-have-to-change-kubectl-to-add-functionality">What if you didn’t have to change &lt;code>kubectl&lt;/code> to add functionality?&lt;/h2>
&lt;p>This is where &lt;code>kubectl&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">plugins&lt;/a> shine.
Since &lt;code>kubectl&lt;/code> v1.12, you can simply
drop executables into your &lt;code>PATH&lt;/code>, which follows the naming pattern
&lt;code>kubectl-myplugin&lt;/code>. Then you can execute this plugin as &lt;code>kubectl myplugin&lt;/code>, and
it will just feel like a normal sub-command of &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>Plugins give you the opportunity to try out new experiences like terminal UIs,
colorful output, specialized functionality, or other innovative ideas. You can
go creative, as you’re the owner of your own plugin.&lt;/p>
&lt;p>Further, plugins offer safe experimentation space for commands you’d like to
propose to &lt;code>kubectl&lt;/code>. By pre-releasing as a plugin, you can push your
functionality faster to the end-users and quickly gather feedback. For example,
the &lt;a href="https://github.com/verb/kubectl-debug">kubectl-debug&lt;/a> plugin is proposed
to become a built-in command in &lt;code>kubectl&lt;/code> in a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">KEP&lt;/a>).
In the meanwhile, the plugin author can ship the functionality and collect
feedback using the plugin mechanism.&lt;/p>
&lt;h2 id="how-to-get-started-with-developing-plugins">How to get started with developing plugins&lt;/h2>
&lt;p>If you already have an idea for a plugin, how do you best make it happen?
First you have to ask yourself if you can implement it as a wrapper around
existing &lt;code>kubectl&lt;/code> functionality. If so, writing the plugin as a shell script
is often the best way forward, because the resulting plugin will be small,
works cross-platform, and has a high level of trust because it is not
compiled.&lt;/p>
&lt;p>On the other hand, if the plugin logic is complex, a general-purpose language
is usually better. The canonical choice here is Go, because you can use the
excellent &lt;code>client-go&lt;/code> library to interact with the Kubernetes API. The Kubernetes
maintained &lt;a href="https://github.com/kubernetes/sample-cli-plugin">sample-cli-plugin&lt;/a>
demonstrates some best practices and can be used as a template for new plugin
projects.&lt;/p>
&lt;p>When the development is done, you just need to ship your plugin to the
Kubernetes users. For the best plugin installation experience and discoverability,
you should consider doing so via the
&lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> plugin manager. For an in-depth
discussion about the technical details around &lt;code>kubectl&lt;/code> plugins, refer to the
documentation on &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">kubernetes.io&lt;/a>.&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Schedule Announced</title><link>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</link><pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jeffrey Sica (Red Hat), Amanda Katona (VMware)&lt;/p>
&lt;p>tl;dr &lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a> and the &lt;a href="https://kcseu2020.sched.com/">schedule is live&lt;/a> so register now and we’ll see you in Amsterdam!&lt;/p>
&lt;h2 id="kubernetes-contributor-summit">Kubernetes Contributor Summit&lt;/h2>
&lt;p>&lt;strong>Sunday, March 29, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Evening Contributor Celebration:
&lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=KubeCon+Amsterdam+2020&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 22, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 18:00 - 21:00&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Monday, March 30, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>All Day Contributor Summit:&lt;/li>
&lt;li>&lt;a href="https://www.rai.nl/en/">Amsterdam RAI&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=kubecon+amsterdam+2020&amp;amp;oq=kubecon+amste&amp;amp;aqs=chrome.0.35i39j69i57j0l4j69i61l2.3957j1j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 24, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 09:00 - 17:00 (Breakfast at 08:00)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-02-18-Contributor-Summit-Amsterdam-Schedule-Announced/contribsummit.jpg" alt="Contributor Summit">&lt;/p>
&lt;p>Hello everyone and Happy 2020! It’s hard to believe that KubeCon EU 2020 is less than six weeks away, and with that another contributor summit! This year we have the pleasure of being in Amsterdam in early spring, so be sure to pack some warmer clothing. This summit looks to be exciting with a lot of fantastic community-driven content. We received &lt;strong>26&lt;/strong> submissions from the CFP. From that, the events team selected &lt;strong>12&lt;/strong> sessions. Each of the sessions falls into one of four categories:&lt;/p>
&lt;ul>
&lt;li>Community&lt;/li>
&lt;li>Contributor Improvement&lt;/li>
&lt;li>Sustainability&lt;/li>
&lt;li>In-depth Technical&lt;/li>
&lt;/ul>
&lt;p>On top of the presentations, there will be a dedicated Docs Sprint as well as the New Contributor Workshop 101 and 201 Sessions. All told, we will have five separate rooms of content throughout the day on Monday. Please &lt;strong>&lt;a href="https://kcseu2020.sched.com/">see the full schedule&lt;/a>&lt;/strong> to see what sessions you’d be interested in. We hope between the content provided and the inevitable hallway track, everyone has a fun and enriching experience.&lt;/p>
&lt;p>Speaking of fun, the social Sunday night should be a blast! We’re hosting this summit’s social close to the conference center, at &lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>. There will be games, bingo, and unconference sign-up throughout the evening. It should be a relaxed way to kick off the week.&lt;/p>
&lt;p>&lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a>! Space is limited so it’s always a good idea to register early.&lt;/p>
&lt;p>If you have any questions, reach out to the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2020/03-contributor-summit#team">Amsterdam Team&lt;/a> on Slack in the &lt;a href="https://kubernetes.slack.com/archives/C7J893413">#contributor-summit&lt;/a> channel.&lt;/p>
&lt;p>Hope to see you there!&lt;/p></description></item></channel></rss>