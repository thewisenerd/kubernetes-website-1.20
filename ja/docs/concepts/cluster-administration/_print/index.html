<!doctype html><html lang=ja class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/cluster-administration/><link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/concepts/cluster-administration/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/><link rel=alternate hreflang=pt href=https://kubernetes.io/pt/docs/concepts/cluster-administration/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/cluster-administration/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/ja/docs/concepts/cluster-administration/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>クラスターの管理 | Kubernetes</title><meta property="og:title" content="クラスターの管理"><meta property="og:description" content="プロダクショングレードのコンテナ管理基盤"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/ja/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="クラスターの管理"><meta itemprop=description content="プロダクショングレードのコンテナ管理基盤"><meta name=twitter:card content="summary"><meta name=twitter:title content="クラスターの管理"><meta name=twitter:description content="プロダクショングレードのコンテナ管理基盤"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/ja/docs/concepts/cluster-administration/"><meta property="og:title" content="クラスターの管理"><meta name=twitter:title content="クラスターの管理"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/ja/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ja/docs/>ドキュメント</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/training/>トレーニング</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/partners/>パートナー</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/community/>コミュニティ</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/case-studies/>ケーススタディ</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>バージョン</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/ja/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/ja/docs/concepts/cluster-administration/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>日本語 Japanese</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/cluster-administration/>English</a>
<a class=dropdown-item href=/zh/docs/concepts/cluster-administration/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 Korean</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español</a>
<a class=dropdown-item href=/pt/docs/concepts/cluster-administration/>Português</a>
<a class=dropdown-item href=/id/docs/concepts/cluster-administration/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/ja/docs/concepts/cluster-administration/>Return to the regular view of this page</a>.</p></div><h1 class=title>クラスターの管理</h1><ul><li>1: <a href=#pg-fb494ea3b1874bd753dcd11c3f35c2dc>クラスター管理の概要</a></li><li>2: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>証明書</a></li><li>3: <a href=#pg-d0e81230313a2684e7b7e40b21834e30>クラウドプロバイダー</a></li><li>4: <a href=#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>リソースの管理</a></li><li>5: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>クラスターのネットワーク</a></li><li>6: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Kubernetesのプロキシー</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-fb494ea3b1874bd753dcd11c3f35c2dc>1 - クラスター管理の概要</h1><p>このページはKubernetesクラスターの作成や管理者向けの内容です。Kubernetesのコア<a href=/ja/docs/concepts/>コンセプト</a>についてある程度精通していることを前提とします。</p><h2 id=クラスターのプランニング>クラスターのプランニング</h2><p>Kubernetesクラスターの計画、セットアップ、設定の例を知るには<a href=/ja/docs/setup/>設定</a>のガイドを参照してください。この記事で列挙されているソリューションは<em>ディストリビューション</em> と呼ばれます。</p><p>ガイドを選択する前に、いくつかの考慮事項を挙げます。</p><ul><li>ユーザーのコンピューター上でKubernetesを試したいでしょうか、それとも高可用性のあるマルチノードクラスターを構築したいでしょうか？あなたのニーズにあったディストリビューションを選択してください。</li><li><strong>もしあなたが高可用性を求める場合</strong>、 <a href=/docs/concepts/cluster-administration/federation/>複数ゾーンにまたがるクラスター</a>の設定について学んでください。</li><li><a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>のような<strong>ホストされているKubernetesクラスター</strong>を使用するのか、それとも<strong>自分自身でクラスターをホストするのでしょうか</strong>？</li><li>使用するクラスターは<strong>オンプレミス</strong>なのか、それとも<strong>クラウド(IaaS)</strong> でしょうか？Kubernetesはハイブリッドクラスターを直接サポートしていません。その代わりユーザーは複数のクラスターをセットアップできます。</li><li>Kubernetesを <strong>「ベアメタル」なハードウェア</strong>上で稼働させますか？それとも<strong>仮想マシン(VMs)</strong> 上で稼働させますか？</li><li><strong>もしオンプレミスでKubernetesを構築する場合</strong>、どの<a href=/ja/docs/concepts/cluster-administration/networking/>ネットワークモデル</a>が最適か検討してください。</li><li><strong>ただクラスターを稼働させたいだけ</strong>でしょうか、それとも<strong>Kubernetesプロジェクトのコードの開発</strong>を行いたいでしょうか？もし後者の場合、開発が進行中のディストリビューションを選択してください。いくつかのディストリビューションはバイナリリリースのみ使用していますが、多くの選択肢があります。</li><li>クラスターを稼働させるのに必要な<a href=/ja/docs/concepts/overview/components/>コンポーネント</a>についてよく理解してください。</li></ul><p>注意: 全てのディストリビューションがアクティブにメンテナンスされている訳ではありません。最新バージョンのKubernetesでテストされたディストリビューションを選択してください。</p><h2 id=クラスターの管理>クラスターの管理</h2><ul><li><p><a href=/docs/tasks/administer-cluster/cluster-management/>クラスターの管理</a>では、クラスターのライフサイクルに関するいくつかのトピックを紹介しています。例えば、新規クラスターの作成、クラスターのマスターやワーカーノードのアップグレード、ノードのメンテナンスの実施(例: カーネルのアップグレード)、稼働中のクラスターのKubernetes APIバージョンのアップグレードについてです。</p></li><li><p><a href=/ja/docs/concepts/architecture/nodes/>ノードの管理</a>方法について学んでください。</p></li><li><p>共有クラスターにおける<a href=/docs/concepts/policy/resource-quotas/>リソースクォータ</a>のセットアップと管理方法について学んでください。</p></li></ul><h2 id=クラスターをセキュアにする>クラスターをセキュアにする</h2><ul><li><p><a href=/docs/concepts/cluster-administration/certificates/>Certificates</a>では、異なるツールチェインを使用して証明書を作成する方法を説明します。</p></li><li><p><a href=/ja/docs/concepts/containers/container-environment/>Kubernetes コンテナの環境</a>では、Kubernetesノード上でのKubeletが管理するコンテナの環境について説明します。</p></li><li><p><a href=/docs/reference/access-authn-authz/controlling-access/>Kubernetes APIへのアクセス制御</a>では、ユーザーとサービスアカウントの権限の設定方法について説明します。</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>認証</a>では、様々な認証オプションを含むKubernetesでの認証について説明します。</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>認可</a>では、認証とは別に、HTTPリクエストの処理方法を制御します。</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>アドミッションコントローラーの使用</a>では、認証と認可の後にKubernetes APIに対するリクエストをインターセプトするプラグインについて説明します。</p></li><li><p><a href=/docs/concepts/cluster-administration/sysctl-cluster/>Kubernetesクラスターでのsysctlの使用</a>では、管理者向けにカーネルパラメーターを設定するため<code>sysctl</code>コマンドラインツールの使用方法について解説します。</p></li><li><p><a href=/docs/tasks/debug-application-cluster/audit/>クラスターの監査</a>では、Kubernetesの監査ログの扱い方について解説します。</p></li></ul><h3 id=kubeletをセキュアにする>kubeletをセキュアにする</h3><ul><li><a href=/ja/docs/concepts/architecture/master-node-communication/>マスターとノードのコミュニケーション</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLSのブートストラップ</a></li><li><a href=/docs/admin/kubelet-authentication-authorization/>Kubeletの認証/認可</a></li></ul><h2 id=オプションのクラスターサービス>オプションのクラスターサービス</h2><ul><li><p><a href=/ja/docs/concepts/services-networking/dns-pod-service/>DNSのインテグレーション</a>では、DNS名をKubernetes Serviceに直接名前解決する方法を解説します。</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>クラスターアクティビィのロギングと監視</a>では、Kubernetesにおけるロギングがどのように行われ、どう実装されているかについて解説します。</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>2 - 証明書</h1><p>クライアント証明書認証を使用する場合、<code>easyrsa</code>や<code>openssl</code>、<code>cfssl</code>を用いて、手動で証明書を生成できます。</p><h3 id=easyrsa>easyrsa</h3><p><strong>easyrsa</strong>を用いると、クラスターの証明書を手動で生成できます。</p><ol><li><p>パッチを当てたバージョンのeasyrsa3をダウンロードして解凍し、初期化します。</p><pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre></li><li><p>新しい認証局(CA)を生成します。<code>--batch</code>は自動モードを設定し、<code>--req-cn</code>はCAの新しいルート証明書の共通名(CN)を指定します。</p><pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre></li><li><p>サーバー証明書と鍵を生成します。
引数<code>--subject-alt-name</code>は、APIサーバーへのアクセスに使用できるIPおよびDNS名を設定します。
<code>MASTER_CLUSTER_IP</code>は通常、APIサーバーとコントローラーマネージャーコンポーネントの両方で引数<code>--service-cluster-ip-range</code>として指定されるサービスCIDRの最初のIPです。
引数<code>--days</code>は、証明書の有効期限が切れるまでの日数を設定するために使われます。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p><pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre></li><li><p><code>pki/ca.crt</code>、<code>pki/issued/server.crt</code>、<code>pki/private/server.key</code>をディレクトリーにコピーします。</p></li><li><p>以下のパラメーターを、APIサーバーの開始パラメーターとして追加します。</p><pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre></li></ol><h3 id=openssl>openssl</h3><p><strong>openssl</strong>はクラスターの証明書を手動で生成できます。</p><ol><li><p>2048ビットのca.keyを生成します。</p><pre><code>openssl genrsa -out ca.key 2048
</code></pre></li><li><p>ca.keyに応じて、ca.crtを生成します。証明書の有効期間を設定するには、-daysを使用します。</p><pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre></li><li><p>2048ビットのserver.keyを生成します。</p><pre><code>openssl genrsa -out server.key 2048
</code></pre></li><li><p>証明書署名要求(CSR)を生成するための設定ファイルを生成します。
ファイル(例: <code>csr.conf</code>)に保存する前に、かぎ括弧で囲まれた値(例: <code>&lt;MASTER_IP></code>)を必ず実際の値に置き換えてください。
<code>MASTER_CLUSTER_IP</code>の値は、前節で説明したAPIサーバーのサービスクラスターIPであることに注意してください。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p><pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre></li><li><p>設定ファイルに基づいて、証明書署名要求を生成します。</p><pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre></li><li><p>ca.key、ca.crt、server.csrを使用してサーバー証明書を生成します。</p><pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre></li><li><p>証明書を表示します。</p><pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre></li></ol><p>最後にAPIサーバーの起動パラメーターに、同様のパラメーターを追加します。</p><h3 id=cfssl>cfssl</h3><p><strong>cfssl</strong>も証明書を生成するためのツールです。</p><ol><li><p>以下のように、ダウンロードして解凍し、コマンドラインツールを用意します。
使用しているハードウェアアーキテクチャやcfsslのバージョンに応じて、サンプルコマンドの調整が必要な場合があります。</p><pre><code>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
chmod +x cfssl
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
chmod +x cfssljson
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre></li><li><p>アーティファクトを保持するディレクトリーを生成し、cfsslを初期化します。</p><pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre></li><li><p>CAファイルを生成するためのJSON設定ファイル(例: <code>ca-config.json</code>)を生成します。</p><pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre></li><li><p>CA証明書署名要求(CSR)用のJSON設定ファイル(例: <code>ca-csr.json</code>)を生成します。
かぎ括弧で囲まれた値は、必ず使用したい実際の値に置き換えてください。</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>CA鍵(<code>ca-key.pem</code>)と証明書(<code>ca.pem</code>)を生成します。</p><pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre></li><li><p>APIサーバーの鍵と証明書を生成するためのJSON設定ファイル(例: <code>server-csr.json</code>)を生成します。
かぎ括弧で囲まれた値は、必ず使用したい実際の値に置き換えてください。
<code>MASTER_CLUSTER_IP</code>の値は、前節で説明したAPIサーバーのサービスクラスターIPです。
以下の例は、デフォルトのDNSドメイン名として<code>cluster.local</code>を使用していることを前提とします。</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>APIサーバーの鍵と証明書を生成します。デフォルトでは、それぞれ<code>server-key.pem</code>と<code>server.pem</code>というファイルに保存されます。</p><pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre></li></ol><h2 id=自己署名ca証明書の配布>自己署名CA証明書の配布</h2><p>クライアントノードは、自己署名CA証明書を有効だと認識しないことがあります。
プロダクション用でない場合や、会社のファイアウォールの背後で実行する場合は、自己署名CA証明書をすべてのクライアントに配布し、有効な証明書のローカルリストを更新できます。</p><p>各クライアントで、以下の操作を実行します。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=証明書api>証明書API</h2><p><code>certificates.k8s.io</code>APIを用いることで、<a href=/ja/docs/tasks/tls/managing-tls-in-a-cluster>こちら</a>のドキュメントにあるように、認証に使用するx509証明書をプロビジョニングすることができます。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d0e81230313a2684e7b7e40b21834e30>3 - クラウドプロバイダー</h1><p>このページでは、特定のクラウドプロバイダーで実行されているKubernetesを管理する方法について説明します。</p><h3 id=kubeadm>kubeadm</h3><p><a href=/ja/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadm</a>は、Kubernetesクラスターを作成する選択肢として人気があります。
kubeadmには、クラウドプロバイダーの設定情報を指定する設定オプションがあります。
例えば、典型的なインツリークラウドプロバイダーは、以下のようにkubeadmを使用して設定することができます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.13.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>典型的なインツリークラウドプロバイダーは、通常、<a href=/ja/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver</a>および<a href=ja//docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager</a>、<a href=/ja/docs/reference/command-line-tools-reference/kubelet/>kubelet</a>のコマンドラインで指定される<code>--cloud-provider</code>と<code>--cloud-config</code>の両方が必要です。
プロバイダーごとに<code>--cloud-config</code>で指定されるファイルの内容についても、以下に記載します。</p><p>すべての外部クラウドプロバイダーについては、以下の見出しに列挙されている個々のリポジトリーの案内に従ってください。または<a href="https://github.com/kubernetes?q=cloud-provider-&type=&language=">すべてのリポジトリーのリスト</a>もご覧ください。</p><h2 id=aws>AWS</h2><p>ここでは、Amazon Web ServicesでKubernetesを実行する際に使用できるすべての設定について説明します。</p><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes/cloud-provider-aws#readme>kubernetes/cloud-provider-aws</a>リポジトリーを参照してください。</p><h3 id=ノード名>ノード名</h3><p>AWSクラウドプロバイダーは、AWSインスタンスのプライベートDNS名をKubernetesのNodeオブジェクトの名前として使用します。</p><h3 id=ロードバランサー>ロードバランサー</h3><p>以下のようにアノテーションを設定することで、<a href=/ja/docs/tasks/access-application-cluster/create-external-load-balancer/>外部ロードバランサー</a>をAWS上で特定の機能を利用するように構成できます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>run</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span>:<span style=color:#bbb> </span>arn:aws:acm:xx-xxxx-x:xxxxxxxxx:xxxxxxx/xxxxx-xxxx-xxxx-xxxx-xxxxxxxxx<span style=color:#bbb> </span><span style=color:#080;font-style:italic>#replace this value</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>443</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>5556</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span></code></pre></div><p>AWSのロードバランサーサービスには、<em>アノテーション</em> を使ってさまざまな設定を適用することができます。以下では、AWS ELBでサポートされているアノテーションについて説明します。</p><ul><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</code>: アクセスログの送信間隔を指定するために使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</code>: アクセスログを有効または無効にするためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</code>: アクセスログ用のs3バケット名を指定するために使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</code>: アクセスログ用のs3バケットのプレフィックスを指定するために使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</code>: ELBに追加タグとして記録されるキーとバリューのペアのコンマ区切りリストとして指定するためにサービスで使用します。例えば、<code>"Key1=Val1,Key2=Val2,KeyNoVal1=,KeyNoVal2"</code>のように指定できます。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</code>: リスナーの背後にあるバックエンド(Pod)が使用するプロトコルを指定するためにサービスで使用します。<code>http</code>(デフォルト)または<code>https</code>を指定すると、接続を終端してヘッダーを解析するHTTPSリスナーが生成されます。<code>ssl</code>または<code>tcp</code>を指定すると、「生の」SSLリスナーが使われます。<code>http</code>を指定して<code>aws-load-balancer-ssl-cert</code>を使わない場合は、HTTPリスナーが使われます。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</code>: セキュアなリスナーを要求するためにサービスで使用します。値は有効な証明書のARNです。詳細は、<a href=https://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-listener-config.html>ELBリスナーの設定</a>を参照してください。CertARNは、IAMまたはCM証明書のARNで、例えば<code>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012</code>のようになります。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</code>: 接続ドレインを有効または無効にするためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</code>: 接続ドレインのタイムアウトを指定するためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</code>: アイドル接続タイムアウトを指定するためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</code>: クロスゾーン負荷分散を有効または無効にするためにサービスで使用されます。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code>: 作成されたELBに追加するセキュリティーグループを指定するために使用します。これは、以前にELBに割り当てられた他のすべてのセキュリティーグループを置き換えます。ここで定義されたセキュリティーグループは、サービス間で共有してはいけません。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</code>: 作成されたELBに加える追加のセキュリティーグループを指定するためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-internal</code>: 内部ELBが必要であることを示すためにサービスで使用します。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</code>: ELB上でプロキシープロトコルを有効にするためにサービスで使用します。現在は、すべてのELBバックエンドでプロキシープロトコルを有効にすることを意味する<code>*</code>という値しか受け付けません。将来的には、特定のバックエンドでのみプロキシープロトコルを設定できるように調整できます。</li><li><code>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</code>: SSL/HTTPSリスナーを使用するポートのコンマ区切りリストを指定するためにサービスで使用します。デフォルトは<code>*</code>(すべて)です。</li></ul><p>AWSのアノテーションの情報は、<a href=https://github.com/kubernetes/legacy-cloud-providers/blob/master/aws/aws.go>aws.go</a>のコメントから引用しています。</p><h2 id=azure>Azure</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes/cloud-provider-azure#readme>kubernetes/cloud-provider-azure</a>リポジトリーを参照してください。</p><h3 id=ノード名-1>ノード名</h3><p>Azureクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名は、Azure VM名と一致しなければならないことに注意してください。</p><h2 id=cloudstack>CloudStack</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/apache/cloudstack-kubernetes-provider>apache/cloudstack-kubernetes-provider</a>リポジトリーを参照してください。</p><h3 id=ノード名-2>ノード名</h3><p>CloudStackクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名は、CloudStack VM名と一致しなければならないことに注意してください。</p><h2 id=gce>GCE</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes/cloud-provider-gcp#readme>kubernetes/cloud-provider-gcp</a>リポジトリーを参照してください。</p><h3 id=ノード名-3>ノード名</h3><p>GCEクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名の最初のセグメントは、GCEインスタンス名と一致しなければならないことに注意してください。例えば、<code>kubernetes-node-2.c.my-proj.internal</code>という名前のノードは、<code>kubernetes-node-2</code>という名前のインスタンスに対応していなければなりません。</p><h2 id=huawei-cloud>HUAWEI CLOUD</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes-sigs/cloud-provider-huaweicloud>kubernetes-sigs/cloud-provider-huaweicloud</a>リポジトリーを参照してください。</p><h3 id=ノード名-4>ノード名</h3><p>HUAWEI CLOUDプロバイダーは、ノードのプライベートIPアドレスをKubernetesノード名として使用します。
ノードでkubeletを開始するときは、必ず<code>--hostname-override=&lt;node private IP></code>を指定してください。</p><h2 id=openstack>OpenStack</h2><p>ここでは、OpenStackでKubernetesを実行する際に使用できるすべての設定について説明します。</p><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes/cloud-provider-openstack#readme>kubernetes/cloud-provider-openstack</a>リポジトリーを参照してください。</p><h3 id=ノード名-5>ノード名</h3><p>OpenStackクラウドプロバイダーは、インスタンス名(OpenStackのメタデータで決定されたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
インスタンス名は必ず、Kubernetesノード名は、CloudStack VM名と一致しなければならないことに注意してください。
kubeletがNodeオブジェクトを正常に登録できるように、インスタンス名は有効なKubernetesノード名である必要があります。</p><h3 id=サービス>サービス</h3><p>KubernetesのOpenStackクラウドプロバイダーの実装では、利用可能な場合、基盤となるクラウドからこれらのOpenStackのサービスの使用をサポートします。</p><table><thead><tr><th>サービス名</th><th>APIバージョン</th><th>必須か</th></tr></thead><tbody><tr><td>Block Storage (Cinder)</td><td>V1†, V2, V3</td><td>No</td></tr><tr><td>Compute (Nova)</td><td>V2</td><td>No</td></tr><tr><td>Identity (Keystone)</td><td>V2‡, V3</td><td>Yes</td></tr><tr><td>Load Balancing (Neutron)</td><td>V1§, V2</td><td>No</td></tr><tr><td>Load Balancing (Octavia)</td><td>V2</td><td>No</td></tr></tbody></table><p>† Block Storage V1 APIのサポートは非推奨ですが、Kubernetes 1.9ではBlock Storage V3 APIのサポートが追加されました。</p><p>‡ Identity V2 APIのサポートは非推奨となり、将来のリリースでプロバイダーから削除される予定です。「Queens」のリリース時点で、OpenStackはIdentity V2 APIを公開しません。</p><p>§ Load Balancing V1 APIのサポートは、Kubernetes 1.9で削除されました。</p><p>サービスディスカバリーは、プロバイダー設定で提供される<code>auth-url</code>を使用して、OpenStack Identity(Keystone)が管理するサービスカタログを一覧表示することで実現されます。
プロバイダーは、Keystone以外のOpenStackサービスが利用できなくなった場合には、機能を緩やかに低下させ、影響を受ける機能のサポートを放棄します。
特定の機能は、基盤となるクラウドでNeutronが公開している拡張機能のリストに基づいて有効または無効にされます。</p><h3 id=cloud-conf>cloud.conf</h3><p>Kubernetesはcloud.confというファイルを介して、OpenStackとのやりとり方法を知っています。
これは、KubernetesにOpenStack認証エンドポイントの認証情報と場所を提供するファイルです。
ファイル内に以下の詳細を指定することで、cloud.confファイルを作成できます。</p><h4 id=典型的な設定>典型的な設定</h4><p>以下の設定例は、最も頻繁に設定が必要な値に関するものです。
プロバイダーをOpenStackクラウドのKeystoneエンドポイントに指定し、そのエンドポイントでの認証方法の詳細を提供し、さらにロードバランサーを設定します。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>[Global]<span style=color:#bbb>
</span><span style=color:#bbb></span>username=user<span style=color:#bbb>
</span><span style=color:#bbb></span>password=pass<span style=color:#bbb>
</span><span style=color:#bbb></span>auth-url=https://&lt;keystone_ip&gt;/identity/v3<span style=color:#bbb>
</span><span style=color:#bbb></span>tenant-id=c869168a828847f39f7f06edd7305637<span style=color:#bbb>
</span><span style=color:#bbb></span>domain-id=2a73b8f597c04551a0fdc8e95544be8a<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span>[LoadBalancer]<span style=color:#bbb>
</span><span style=color:#bbb></span>subnet-id=6937f8fa-858d-4bc9-a3a5-18d2c957166a<span style=color:#bbb>
</span></code></pre></div><h5 id=グローバル>グローバル</h5><p>これらのOpenStackプロバイダーの設定オプションは、グローバル設定に関連しており、<code>cloud.conf</code>ファイルの<code>[Global]</code>セクションに記述する必要があります。</p><ul><li><code>auth-url</code>(必死): 認証に使用するKeystone APIのURLです。OpenStackのコントロールパネルでは、Access and Security > API Access > Credentialsで確認できます。</li><li><code>username</code>(必須): Keystoneに設定されている有効なユーザーのユーザー名を参照します。</li><li><code>password</code>(必須): Keystoneで設定された有効なユーザーのパスワードを参照します。</li><li><code>tenant-id</code>(必須): リソースを作成するプロジェクトのIDを指定するために使用します。</li><li><code>tenant-name</code>(任意): リソースを作成するプロジェクトの名前を指定します。</li><li><code>trust-id</code>(任意): 認証に使用するtrustの識別子を指定するために使用します。trustは、ユーザー(委託者)が他のユーザー(受託者)に役割を委譲したり、受託者が委託者になりすますことを許可したりする権限を表します。利用可能なtrustは、Keystone APIの<code>/v3/OS-TRUST/trusts</code>エンドポイントの下にあります。</li><li><code>domain-id</code>(任意): ユーザーが所属するドメインのIDを指定するために使用します。</li><li><code>domain-name</code>(任意): ユーザーが所属するドメイン名を指定するために使用します。</li><li><code>region</code>(任意): マルチリージョンのOpenStackクラウド上で実行する際に使うリージョンの識別子を指定するために使用します。リージョンはOpenStackデプロイメントの一般的な区分です。リージョンには厳密な地理的な意味合いはありませんが、デプロイメントでは<code>us-east</code>のような地理的な名前をリージョンの識別子に使うことができます。利用可能なリージョンはKeystone APIの<code>/v3/regions</code>エンドポイントの下にあります。</li><li><code>ca-file</code>(任意): カスタムCAファイルのパスを指定するために使用します。</li></ul><p>テナントをプロジェクトに変更するKeystone V3を使用している場合、<code>tenant-id</code>の値は自動的にAPIのプロジェクト構造体にマッピングされます。</p><h5 id=ロードバランサー-1>ロードバランサー</h5><p>これらのOpenStackプロバイダーの設定オプションは、ロードバランサー設定に関連しており、<code>cloud.conf</code>ファイルの<code>[LoadBalancer]</code>セクションに記述する必要があります。</p><ul><li><code>lb-version</code>(任意): 自動バージョン検出を上書きするために使用します。有効な値は<code>v1</code>または<code>v2</code>です。値が指定されていない場合、自動検出は基盤となるOpenStackクラウドが公開するサポートバージョンのうち、最も高いものを選択します。</li><li><code>use-octavia</code>(任意): Octavia LBaaS V2サービスカタログエンドポイントを探して、利用するかどうかを決定するために使用します。有効な値は<code>true</code>または<code>false</code>です。<code>true</code>が指定され、Octaiva LBaaS V2エントリーが見つからなかった場合、プロバイダーはフォールバックして代わりにNeutron LBaaS V2エンドポイントを見つけようとします。デフォルト値は<code>false</code> です。</li><li><code>subnet-id</code>(任意): ロードバランサーを作成したいサブネットのIDを指定します。Network > Networksにあります。サブネットを取得するには、それぞれのネットワークをクリックします。</li><li><code>floating-network-id</code>(任意): 指定した場合、ロードバランサーのフローティングIPを作成します。</li><li><code>lb-method</code>(任意): ロードバランサープールのメンバー間で負荷分散させるアルゴリズムを指定するために使用します。値には<code>ROUND_ROBIN</code>、<code>LEAST_CONNECTIONS</code>、<code>SOURCE_IP</code>を指定できます。何も指定しない場合のデフォルトの動作は<code>ROUND_ROBIN</code> です。</li><li><code>lb-provider</code>(任意): ロードバランサーのプロバイダーを指定するために使用します。指定しない場合は、Neutronで設定されたデフォルトのプロバイダサービスが使用されます。</li><li><code>create-monitor</code>(任意): Neutronロードバランサーのヘルスモニターを作成するかどうかを表します。有効な値は<code>true</code>と<code>false</code>で、デフォルト値は<code>false</code>です。<code>true</code>を指定した場合は、<code>monitor-delay</code>、<code>monitor-timeout</code>、<code>monitor-max-retries</code>も設定しなければなりません。</li><li><code>monitor-delay</code>(任意): ロードバランサーのメンバーにプローブを送信するまでの時間です。有効な時間単位を指定してください。有効な時間単位は"ns"、"us"(または"μs")、"ms"、"s"、"m"、"h"です。</li><li><code>monitor-timeout</code>(任意): モニタリングがタイムアウトする前にpingの応答を待つための最大の時間です。この値はdelay値よりも小さくする必要があります。有効な時間単位を指定してください。有効な時間単位は"ns"、"us"(または"μs")、"ms"、"s"、"m"、"h"です。</li><li><code>monitor-max-retries</code>(任意): ロードバランサーメンバーのステータスをINACTIVEに変更する前に許容されるpingの失敗の数です。1から10の間の数値でなければなりません。</li><li><code>manage-security-groups</code>(任意): ロードバランサーがセキュリティーグループのルールを自動的に管理するかどうかを決定します。有効な値は<code>true</code>と<code>false</code>で、デフォルト値は<code>false</code>です。<code>true</code>を指定した場合は、<code>node-security-group</code>も指定しなければなりません。</li><li><code>node-security-group</code>(任意): 管理するセキュリティーグループのIDです。</li></ul><h5 id=ブロックストレージ>ブロックストレージ</h5><p>これらのOpenStackプロバイダーの設定オプションは、ブロックストレージ設定に関連しており、<code>cloud.conf</code>ファイルの<code>[BlockStorage]</code>セクションに記述する必要があります。</p><ul><li><code>bs-version</code>(任意): 自動バージョン検出を上書きするために使用します。有効な値は<code>v1</code>、<code>v2</code>、<code>v3</code>、<code>auto</code>です。<code>auto</code>が指定された場合、自動検出は基盤となるOpenStackクラウドが公開するサポートバージョンのうち、最も高いものを選択します。何も指定しない場合のデフォルト値は<code>auto</code>です。</li><li><code>trust-device-path</code>(任意): ほとんどのシナリオでは、Cinderが提供するブロックデバイス名(例: <code>/dev/vda</code>)は信頼できません。このブール値はこの動作をトグルします。<code>true</code>に設定すると、Cinderが提供するブロックデバイス名を信頼することになります。デフォルト値の<code>false</code>は、シリアル番号と<code>/dev/disk/by-id</code>のマッピングに基づいてデバイスのパスを検出します。</li><li><code>ignore-volume-az</code>(任意): Cinderボリュームをアタッチする際のアベイラビリティーゾーンの使用に影響を与えます。NovaとCinderのアベイラビリティーゾーンが異なる場合は、<code>true</code>に設定する必要があります。これは、Novaのアベイラビリティーゾーンが多くあるにも関わらず、Cinderのアベイラビリティーゾーンが1つしかない場合によく見られます。デフォルト値は以前のリリースで使用されていた動作を維持するために<code>false</code>になっていますが、将来的には変更される可能性があります。</li><li><code>node-volume-attach-limit</code>(任意): ノードにアタッチできるボリュームの最大数で、デフォルトはCinderの256です。</li></ul><p>エンドポイントを区別するためにポートではなくパスを使用しているOpenStackデプロイメントにおいて、Kubernetesのバージョン1.8以下をデプロイする際、明示的に<code>bs-version</code>パラメーターの設定が必要な場合があります。パスベースのエンドポイントは<code>http://foo.bar/volume</code>の形式であり、ポートベースのエンドポイントは<code>http://foo.bar:xxx</code>の形式です。</p><p>パスベースのエンドポイントを使う環境で、Kubernetesが古い自動検出ロジックを使用している場合、ボリュームの切り離しを試みると<code>BS API version autodetection failed.</code>というエラーが返されます。この問題を回避するには、クラウドプロバイダー設定に以下のように追記することで、Cinder APIバージョン2を強制的に使用することができます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>[BlockStorage]<span style=color:#bbb>
</span><span style=color:#bbb></span>bs-version=v2<span style=color:#bbb>
</span></code></pre></div><h5 id=メタデータ>メタデータ</h5><p>これらのOpenStackプロバイダーの設定オプションは、メタデータ設定に関連しており、<code>cloud.conf</code>ファイルの<code>[Metadata]</code>セクションに記述する必要があります。</p><ul><li><p><code>search-order</code>(任意): この設定のキーは、プロバイダーが実行するインスタンスに関連するメタデータの取得方法に影響します。デフォルト値の<code>configDrive,metadataService</code>について、プロバイダーは、コンフィグドライブが利用可能な場合は最初にインスタンスに関連するメタデータをそこから取得し、次にメタデータサービスから取得します。代替値は以下の通りです。</p><ul><li><code>configDrive</code> - コンフィグドライブからのみ、インスタンスのメタデータを取得します。</li><li><code>metadataService</code> - メタデータサービスからのみ、インスタンスのメタデータを取得します。</li><li><code>metadataService,configDrive</code> - 最初にメタデータサービスからインスタンスのメタデータを取得し、次にコンフィグドライブから取得します。</li></ul><p>コンフィグドライブ上のメタデータは時間の経過とともに陳腐化する可能性がありますが、メタデータサービスは常に最新の情報を提供するため、この動作を調整するのが望ましいです。しかし、すべてのOpenStackクラウドがコンフィグドライブとメタデータサービスの両方を提供しているわけではなく、どちらか一方しか利用できない場合もあるため、デフォルトでは両方をチェックするようになっています。</p></li></ul><h5 id=ルート>ルート</h5><p>これらのOpenStackプロバイダーの設定オプションは、<a href=/ja/docs/concepts/cluster-administration/network-plugins/#kubenet>kubenet</a>のKubernetesネットワークプラグインに関連しており、<code>cloud.conf</code>ファイルの<code>[Route]</code>セクションに記述する必要があります。</p><ul><li><code>router-id</code>(任意): 基盤となるクラウドのNeutronデプロイメントが<code>extraroutes</code>拡張機能をサポートしている場合は、<code>router-id</code>を使用してルートを追加するルーターを指定します。選択したルーターは、クラスターノードを含むプライベートネットワークにまたがっていなければなりません(通常、ノードネットワークは1つしかないので、この値はノードネットワークのデフォルトルーターになります)。この値は、OpenStackで<a href=/docs/concepts/cluster-administration/network-plugins/#kubenet>kubenet</a>を使用するために必要です。</li></ul><h2 id=ovirt>OVirt</h2><h3 id=ノード名-6>ノード名</h3><p>OVirtクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名は、VMのFQDN(<code>&lt;vm>&lt;guest_info>&lt;fqdn>...&lt;/fqdn>&lt;/guest_info>&lt;/vm></code>の下でOVirtによって報告されたもの)と一致しなければならないことに注意してください。</p><h2 id=photon>Photon</h2><h3 id=ノード名-7>ノード名</h3><p>Photonクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名はPhoton VM名と一致しなければならないことに注意してください(もしくは、<code>--cloud-config</code>で<code>overrideIP</code>がtrueに設定されている場合は、Kubernetesノード名はPhoton VMのIPアドレスと一致しなければなりません)。</p><h2 id=vsphere>vSphere</h2><ul class="nav nav-tabs" id=vsphere-cloud-provider role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#vsphere-cloud-provider-0 role=tab aria-controls=vsphere-cloud-provider-0 aria-selected=true>vSphere 6.7U3以上</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#vsphere-cloud-provider-1 role=tab aria-controls=vsphere-cloud-provider-1>vSphere 6.7U3未満</a></li></ul><div class=tab-content id=vsphere-cloud-provider><div id=vsphere-cloud-provider-0 class="tab-pane show active" role=tabpanel aria-labelledby=vsphere-cloud-provider-0><p><p>vSphere 6.7U3以上のすべてのvSphereデプロイメントでは、<a href=https://github.com/kubernetes/cloud-provider-vsphere>external vSphere cloud provider</a>と<a href=https://github.com/kubernetes-sigs/vsphere-csi-driver>vSphere CSI driver</a>の使用を推奨します。クイックスタートガイドについては、<a href=https://cloud-provider-vsphere.sigs.k8s.io/tutorials/kubernetes-on-vsphere-with-kubeadm.html>Deploying a Kubernetes Cluster on vSphere with CSI and CPI</a>を参照してください。</p></div><div id=vsphere-cloud-provider-1 class=tab-pane role=tabpanel aria-labelledby=vsphere-cloud-provider-1><p><p>vSphere 6.7U3未満を実行している場合は、インツリーのvSphereクラウドプロバイダーを推奨します。クイックスタートガイドについては、<a href=https://cloud-provider-vsphere.sigs.k8s.io/tutorials/k8s-vcp-on-vsphere-with-kubeadm.html>Running a Kubernetes Cluster on vSphere with kubeadm</a>を参照してください。</p></div></div><p>vSphereクラウドプロバイダーの詳細なドキュメントについては、<a href=https://cloud-provider-vsphere.sigs.k8s.io>vSphereクラウドプロバイダーのドキュメントサイト</a>を参照してください。</p><h2 id=ibm-cloud-kubernetes-service>IBM Cloud Kubernetes Service</h2><h3 id=コンピュートノード>コンピュートノード</h3><p>IBM Cloud Kubernetes Serviceプロバイダーを使用することで、仮想ノードと物理ノード(ベアメタル)を混在させたクラスターを単一のゾーン、またはリージョン内の複数のゾーンにまたがって作成することができます。詳細については、<a href="https://cloud.ibm.com/docs/containers?topic=containers-planning_worker_nodes">Planning your cluster and worker node setup</a>を参照してください。</p><p>Kubernetes Nodeオブジェクトの名前は、IBM Cloud Kubernetes ServiceワーカーノードインスタンスのプライベートIPアドレスです。</p><h3 id=ネットワーク>ネットワーク</h3><p>IBM Cloud Kubernetes Serviceプロバイダーは、高品質なネットワークパフォーマンスとノードのネットワーク分離のためにVLANを提供します。カスタムファイアウォールやCalicoネットワークポリシーを設定して、クラスターにセキュリティーの追加レイヤーを加えたり、VPNを使用してクラスターをオンプレミスデータセンターに接続したりすることができます。詳細については、<a href="https://cloud.ibm.com/docs/containers?topic=containers-plan_clusters">Planning your cluster network setup</a>を参照してください。</p><p>アプリケーションをパブリックまたはクラスター内で公開するには、NodePort、LoadBalancer、Ingressサービスを利用できます。また、Ingressアプリケーションのロードバランサーをアノテーションでカスタマイズすることもできます。詳細については、<a href="https://cloud.ibm.com/docs/containers?topic=containers-cs_network_planning#cs_network_planning">Choosing an app exposure service</a>を参照してください。</p><h3 id=ストレージ>ストレージ</h3><p>IBM Cloud Kubernetes Serviceプロバイダーは、Kubernetesネイティブの永続ボリュームを活用して、ユーザーがファイル、ブロック、およびクラウドオブジェクトストレージをアプリケーションにマウントできるようにします。また、データの永続ストレージにDatabase as a Serviceやサードパーティーのアドオンを使用することもできます。詳しくは、<a href="https://cloud.ibm.com/docs/containers?topic=containers-storage_planning#storage_planning">Planning highly available persistent storage</a>を参照してください。</p><h2 id=baidu-cloud-container-engine>Baidu Cloud Container Engine</h2><h3 id=ノード名-8>ノード名</h3><p>Baiduクラウドプロバイダーは、ノードのプライベートIPアドレス(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名はBaidu VMのプライベートIPと一致しなければならないことに注意してください。</p><h2 id=tencent-kubernetes-engine>Tencent Kubernetes Engine</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/TencentCloud/tencentcloud-cloud-controller-manager>TencentCloud/tencentcloud-cloud-controller-manager</a>リポジトリーを参照してください。</p><h3 id=ノード名-9>ノード名</h3><p>Baiduクラウドプロバイダーは、ノードのホスト名(kubeletで決定されたもの、または<code>--hostname-override</code>で上書きされたもの)を、Kubernetes Nodeオブジェクトの名前として使用します。
Kubernetesノード名はTencent VMのプライベートIPと一致しなければならないことに注意してください。</p><h2 id=alibaba-cloud-kubernetes>Alibaba Cloud Kubernetes</h2><p>この外部クラウドプロバイダーを利用したい場合、<a href=https://github.com/kubernetes/cloud-provider-alibaba-cloud>kubernetes/cloud-provider-alibaba-cloud</a>リポジトリーを参照してください。</p><h3 id=ノード名-10>ノード名</h3><p>Alibaba Cloudではノード名の書式は必要ありませんが、kubeletでは<code>--provider-id=${REGION_ID}.${INSTANCE_ID}</code>を追加する必要があります。パラメーター<code>${REGION_ID}</code>はKubernetesのリージョンのIDを、<code>${INSTANCE_ID}</code>はAlibaba ECS(Elastic Compute Service)のIDを表します。</p><h3 id=ロードバランサー-2>ロードバランサー</h3><p><a href=https://www.alibabacloud.com/help/en/doc-detail/86531.htm>アノテーション</a>を設定することで、Alibaba Cloudの特定の機能を使用するように外部のロードバランサーを設定できます。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>4 - リソースの管理</h1><p>アプリケーションをデプロイし、Serviceを介して外部に公開できました。さて、どうしますか？Kubernetesは、スケーリングや更新など、アプリケーションのデプロイを管理するための多くのツールを提供します。
我々が取り上げる機能についての詳細は<a href=/ja/docs/concepts/configuration/overview/>設定ファイル</a>と<a href=/ja/docs/concepts/overview/working-with-objects/labels/>ラベル</a>について詳細に説明します。</p><h2 id=リソースの設定を管理する>リソースの設定を管理する</h2><p>多くのアプリケーションではDeploymentやServiceなど複数のリソースの作成を要求します。複数のリソースの管理は、同一のファイルにひとまとめにしてグループ化すると簡単になります(YAMLファイル内で<code>---</code>で区切る)。
例えば:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/master/content/ja/examples/application/nginx-app.yaml download=application/nginx-app.yaml><code>application/nginx-app.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('application-nginx-app-yaml')" title="Copy application/nginx-app.yaml to clipboard"></img></div><div class=includecode id=application-nginx-app-yaml><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx-svc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div></div></div><p>複数のリソースは単一のリソースと同様の方法で作成できます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>service/my-nginx-svc created
deployment.apps/my-nginx created
</code></pre></div><p>リソースは、ファイル内に記述されている順番通りに作成されます。そのため、Serviceを最初に指定するのが理想です。スケジューラーがServiceに関連するPodを、Deploymentなどのコントローラーによって作成されるときに確実に拡散できるようにするためです。</p><p><code>kubectl apply</code>もまた、複数の<code>-f</code>による引数指定を許可しています。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><p>個別のファイルに加えて、-fの引数としてディレクトリ名も指定できます:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/
</code></pre></div><p><code>kubectl</code>は<code>.yaml</code>、<code>.yml</code>、<code>.json</code>といったサフィックスの付くファイルを読み込みます。</p><p>同じマイクロサービス、アプリケーションティアーのリソースは同一のファイルにまとめ、アプリケーションに関するファイルをグループ化するために、それらのファイルを同一のディレクトリに配備するのを推奨します。アプリケーションのティアーがDNSを通じて互いにバインドされると、アプリケーションスタックの全てのコンポーネントをひとまとめにして簡単にデプロイできます。</p><p>リソースの設定ソースとして、URLも指定できます。githubから取得した設定ファイルから直接手軽にデプロイができます:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx created
</code></pre></div><h2 id=kubectlによる一括操作>kubectlによる一括操作</h2><p><code>kubectl</code>が一括して実行できる操作はリソースの作成のみではありません。作成済みのリソースの削除などの他の操作を実行するために、設定ファイルからリソース名を取得することができます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</code></pre></div><p>2つのリソースだけを削除する場合には、コマンドラインでリソース/名前というシンタックスを使うことで簡単に指定できます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployments/my-nginx services/my-nginx-svc
</code></pre></div><p>さらに多くのリソースに対する操作では、リソースをラベルでフィルターするために<code>-l</code>や<code>--selector</code>を使ってセレクター(ラベルクエリ)を指定するのが簡単です:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl delete deployment,services -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</code></pre></div><p><code>kubectl</code>は同様のシンタックスでリソース名を出力するので、<code>$()</code>や<code>xargs</code>を使ってパイプで操作するのが容易です:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get <span style=color:#a2f;font-weight:700>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>      AGE
my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</code></pre></div><p>上記のコマンドで、最初に<code>examples/application/nginx/</code>配下でリソースを作成し、<code>-o name</code>という出力フォーマットにより、作成されたリソースの名前を表示します(各リソースをresource/nameという形式で表示)。そして"service"のみ<code>grep</code>し、<code>kubectl get</code>を使って表示させます。</p><p>あるディレクトリ内の複数のサブディレクトリをまたいでリソースを管理するような場合、<code>--filename,-f</code>フラグと合わせて<code>--recursive</code>や<code>-R</code>を指定することでサブディレクトリに対しても再帰的に操作が可能です。</p><p>例えば、開発環境用に必要な全ての<a class=glossary-tooltip title="A serialized specification of one or more Kubernetes API objects." data-toggle=tooltip data-placement=top href="/ja/docs/reference/glossary/?all=true#term-manifest" target=_blank aria-label=マニフェスト>マニフェスト</a>をリソースタイプによって整理している<code>project/k8s/development</code>というディレクトリがあると仮定します。</p><pre><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><p>デフォルトでは、<code>project/k8s/development</code>における一括操作は、どのサブディレクトリも処理せず、ディレクトリの第1階層で処理が止まります。下記のコマンドによってこのディレクトリ配下でリソースを作成しようとすると、エラーが発生します。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/development
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>error: you must provide one or more resources by argument or filename <span style=color:#666>(</span>.json|.yaml|.yml|stdin<span style=color:#666>)</span>
</code></pre></div><p>代わりに、下記のように<code>--filename,-f</code>フラグと合わせて<code>--recursive</code>や<code>-R</code>を指定してください:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/development --recursive
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre></div><p><code>--recursive</code>フラグは<code>kubectl {create,get,delete,describe,rollout}</code>などのような<code>--filename,-f</code>フラグを扱うどの操作でも有効です。</p><p>また、<code>--recursive</code>フラグは複数の<code>-f</code>フラグの引数を指定しても有効です。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>namespace/development created
namespace/staging created
configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre></div><p><code>kubectl</code>についてさらに知りたい場合は、<a href=/docs/reference/kubectl/overview/>kubectlの概要</a>を参照してください。</p><h2 id=ラベルを有効に使う>ラベルを有効に使う</h2><p>これまで取り上げた例では、リソースに対して最大1つのラベルを適用してきました。リソースのセットを他のセットと区別するために、複数のラベルが必要な状況があります。</p><p>例えば、異なるアプリケーション間では、異なる<code>app</code>ラベルを使用したり、<a href=https://github.com/kubernetes/examples/tree/v1.20.15/guestbook/>ゲストブックの例</a>のようなマルチティアーのアプリケーションでは、各ティアーを区別する必要があります。frontendというティアーでは下記のラベルを持ちます。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></code></pre></div><p>Redisマスターやスレーブでは異なる<code>tier</code>ラベルを持ち、加えて<code>role</code>ラベルも持つことでしょう。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></code></pre></div><p>そして</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>slave<span style=color:#bbb>
</span></code></pre></div><p>ラベルを使用すると、ラベルで指定された任意の次元に沿ってリソースを分割できます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
kubectl get pods -Lapp -Ltier -Lrole
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
guestbook-fe-4nlpb             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-ght6d             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-jpy62             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
guestbook-redis-master-5pg3b   1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    master
guestbook-redis-slave-2q2yf    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
guestbook-redis-slave-qgazl    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
my-nginx-divi2                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
my-nginx-o0ef1                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -lapp<span style=color:#666>=</span>guestbook,role<span style=color:#666>=</span>slave
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                          READY     STATUS    RESTARTS   AGE
guestbook-redis-slave-2q2yf   1/1       Running   <span style=color:#666>0</span>          3m
guestbook-redis-slave-qgazl   1/1       Running   <span style=color:#666>0</span>          3m
</code></pre></div><h2 id=canary-deployments-カナリアデプロイ>Canary deployments カナリアデプロイ</h2><p>複数のラベルが必要な他の状況として、異なるリリース間でのDeploymentや、同一コンポーネントの設定を区別することが挙げられます。よく知られたプラクティスとして、本番環境の実際のトラフィックを受け付けるようにするために、新しいリリースを完全にロールアウトする前に、新しい<em>カナリア版</em>のアプリケーションを過去のリリースと合わせてデプロイする方法があります。</p><p>例えば、異なるリリースバージョンを分けるために<code>track</code>ラベルを使用できます。</p><p>主要な安定板のリリースでは<code>track</code>ラベルに<code>stable</code>という値をつけることがあるでしょう。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v3<span style=color:#bbb>
</span></code></pre></div><p>そして2つの異なるPodのセットを上書きしないようにするため、<code>track</code>ラベルに異なる値を持つ(例: <code>canary</code>)ようなguestbookフロントエンドの新しいリリースを作成できます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-canary<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v4<span style=color:#bbb>
</span></code></pre></div><p>frontend Serviceは、トラフィックを両方のアプリケーションにリダイレクトさせるために、両方のアプリケーションに共通したラベルのサブセットを選択して両方のレプリカを扱えるようにします。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></code></pre></div><p>安定版とカナリア版リリースで本番環境の実際のトラフィックを転送する割合を決めるため、双方のレプリカ数を変更できます(このケースでは3対1)。
最新版のリリースをしても大丈夫な場合、安定版のトラックを新しいアプリケーションにして、カナリア版を削除します。</p><p>さらに具体的な例については、<a href=https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary>tutorial of deploying Ghost</a>を参照してください。</p><h2 id=ラベルの更新>ラベルの更新</h2><p>新しいリソースを作成する前に、既存のPodと他のリソースのラベルの変更が必要な状況があります。これは<code>kubectl label</code>で実行できます。
例えば、全てのnginx Podを frontendティアーとしてラベル付けするには、下記のコマンドを実行するのみです。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl label pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx <span style=color:#b8860b>tier</span><span style=color:#666>=</span>fe
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>pod/my-nginx-2035384211-j5fhi labeled
pod/my-nginx-2035384211-u2c7e labeled
pod/my-nginx-2035384211-u3t6x labeled
</code></pre></div><p>これは最初に"app=nginx"というラベルのついたPodをフィルターし、そのPodに対して"tier=fe"というラベルを追加します。
ラベル付けしたPodを確認するには、下記のコマンドを実行してください。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx -L tier
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   <span style=color:#666>0</span>          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   <span style=color:#666>0</span>          23m       fe
</code></pre></div><p>このコマンドでは"app=nginx"というラベルのついた全てのPodを出力し、Podのtierという項目も表示します(<code>-L</code>または<code>--label-columns</code>で指定)。</p><p>さらなる情報は、<a href=/docs/concepts/overview/working-with-objects/labels/>ラベル</a>や<a href=/docs/reference/generated/kubectl/kubectl-commands/#label>kubectl label</a>を参照してください。</p><h2 id=アノテーションの更新>アノテーションの更新</h2><p>リソースに対してアノテーションを割り当てたい状況があります。アノテーションは、ツール、ライブラリなどのAPIクライアントによって取得するための任意の非識別メタデータです。アノテーションの割り当ては<code>kubectl annotate</code>で可能です。例:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl annotate pods my-nginx-v4-9gw19 <span style=color:#b8860b>description</span><span style=color:#666>=</span><span style=color:#b44>&#39;my frontend running nginx&#39;</span>
kubectl get pods my-nginx-v4-9gw19 -o yaml
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>apiVersion: v1
kind: pod
metadata:
  annotations:
    description: my frontend running nginx
...
</code></pre></div><p>さらなる情報は、<a href=/docs/concepts/overview/working-with-objects/annotations/>アノテーション</a> や、<a href=/docs/reference/generated/kubectl/kubectl-commands/#annotate>kubectl annotate</a>を参照してください。</p><h2 id=アプリケーションのスケール>アプリケーションのスケール</h2><p>アプリケーションの負荷が増減するとき、<code>kubectl</code>を使って簡単にスケールできます。例えば、nginxのレプリカを3から1に減らす場合、下記を実行します:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale deployment/my-nginx --replicas<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx scaled
</code></pre></div><p>実行すると、Deploymentによって管理されるPod数が1となります。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>NAME                        READY     STATUS    RESTARTS   AGE
my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          30m
</code></pre></div><p>システムに対してnginxのレプリカ数を自動で選択させるには、下記のように1から3の範囲で指定します。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl autoscale deployment/my-nginx --min<span style=color:#666>=</span><span style=color:#666>1</span> --max<span style=color:#666>=</span><span style=color:#666>3</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</code></pre></div><p>実行すると、nginxのレプリカは必要に応じて自動でスケールアップ、スケールダウンします。</p><p>さらなる情報は、<a href=/docs/reference/generated/kubectl/kubectl-commands/#scale>kubectl scale</a>、<a href=/docs/reference/generated/kubectl/kubectl-commands/#autoscale>kubectl autoscale</a> and <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>horizontal pod autoscaler</a>を参照してください。</p><h2 id=リソースの直接的アップデート>リソースの直接的アップデート</h2><p>場合によっては、作成したリソースに対して処理を中断させずに更新を行う必要があります。</p><h3 id=kubectl-apply>kubectl apply</h3><p>開発者が設定するリソースをコードとして管理しバージョニングも行えるように、設定ファイルのセットをソースによって管理する方法が推奨されています。
この場合、クラスターに対して設定の変更をプッシュするために<a href=/docs/reference/generated/kubectl/kubectl-commands/#apply><code>kubectl apply</code></a>を使用できます。</p><p>このコマンドは、リソース設定の過去のバージョンと、今適用した変更を比較し、差分に現れないプロパティーに対して上書き変更することなくクラスターに適用させます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
deployment.apps/my-nginx configured
</code></pre></div><p>注意として、前回の変更適用時からの設定の変更内容を決めるため、<code>kubectl apply</code>はリソースに対してアノテーションを割り当てます。変更が実施されると<code>kubectl apply</code>は、1つ前の設定内容と、今回変更しようとする入力内容と、現在のリソースの設定との3つの間で変更内容の差分をとります。</p><p>現在、リソースはこのアノテーションなしで作成されました。そのため、最初の<code>kubectl paply</code>の実行においては、与えられたにゅうチョクト、現在のリソースの設定の2つの間の差分が取られ、フォールバックします。この最初の実行の間、リソースが作成された時にプロパティーセットの削除を検知できません。この理由により、プロパティーの削除はされません。</p><p><code>kubectl apply</code>の実行後の全ての呼び出しや、<code>kubectl replace</code>や<code>kubectl edit</code>などの設定を変更する他のコマンドではアノテーションを更新します。<code>kubectl apply</code>した後の全ての呼び出しにおいて3-wayの差分取得によってプロパティの検知と削除を実施します。</p><h3 id=kubectl-edit>kubectl edit</h3><p>その他に、<code>kubectl edit</code>によってリソースの更新もできます。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit deployment/my-nginx
</code></pre></div><p>このコマンドは、最初にリソースを<code>get</code>し、テキストエディタでリソースを編集し、更新されたバージョンでリソースを<code>apply</code>します。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
vi /tmp/nginx.yaml
<span style=color:#080;font-style:italic># yamlファイルを編集し、ファイルを保存します。</span>

kubectl apply -f /tmp/nginx.yaml
deployment.apps/my-nginx configured

rm /tmp/nginx.yaml
</code></pre></div><p>このコマンドによってより重大な変更を簡単に行えます。注意として、あなたの<code>EDITOR</code>や<code>KUBE_EDITOR</code>といった環境変数も指定できます。</p><p>さらなる情報は、<a href=/docs/reference/generated/kubectl/kubectl-commands/#edit>kubectl edit</a>を参照してください。</p><h3 id=kubectl-patch>kubectl patch</h3><p>APIオブジェクトの更新には<code>kubectl patch</code>を使うことができます。このコマンドはJSON patch、JSON merge patch、戦略的merge patchをサポートしています。
<a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>kubectl patchを使ったAPIオブジェクトの更新</a>や<a href=/docs/reference/generated/kubectl/kubectl-commands/#patch>kubectl patch</a>を参照してください。</p><h2 id=破壊的なアップデート>破壊的なアップデート</h2><p>一度初期化された後、更新できないようなリソースフィールドの更新が必要な場合や、Deploymentによって作成され、壊れている状態のPodを修正するなど、再帰的な変更を即座に行いたい場合があります。このようなフィールドを変更するため、リソースの削除と再作成を行う<code>replace --force</code>を使用してください。このケースでは、シンプルに元の設定ファイルを修正するのみです。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx deleted
deployment.apps/my-nginx replaced
</code></pre></div><h2 id=サービス停止なしでアプリケーションを更新する>サービス停止なしでアプリケーションを更新する</h2><p>ある時点で、前述したカナリアデプロイのシナリオにおいて、新しいイメージやイメージタグを指定することによって、デプロイされたアプリケーションを更新が必要な場合があります。<code>kubectl</code>ではいくつかの更新操作をサポートしており、それぞれの操作が異なるシナリオに対して適用可能です。</p><p>ここでは、Deploymentを使ってアプリケーションの作成と更新についてガイドします。</p><p>まずnginxのバージョン1.14.2を稼働させていると仮定します。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment my-nginx --image<span style=color:#666>=</span>nginx:1.14.2
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>deployment.apps/my-nginx created
</code></pre></div><p>レプリカ数を3にします(新旧のリビジョンは混在します)。:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale deployment my-nginx --current-replicas<span style=color:#666>=</span><span style=color:#666>1</span> --replicas<span style=color:#666>=</span><span style=color:#666>3</span>
</code></pre></div><pre><code>deployment.apps/my-nginx scaled
</code></pre><p>バージョン1.16.1に更新するには、上述したkubectlコマンドを使って<code>.spec.template.spec.containers[0].image</code>の値を<code>nginx:1.14.2</code>から<code>nginx:1.16.1</code>に変更するだけでできます。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl edit deployment/my-nginx
</code></pre></div><p>できました!Deploymentはデプロイされたnginxのアプリケーションを宣言的にプログレッシブに更新します。更新途中では、決まった数の古いレプリカのみダウンし、一定数の新しいレプリカが希望するPod数以上作成されても良いことを保証します。詳細について学ぶには<a href=/docs/concepts/workloads/controllers/deployment/>Deployment page</a>を参照してください。</p><h2 id=次の項目>次の項目</h2><ul><li><a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>アプリケーションの調査とデバッグのための<code>kubectl</code>の使用方法</a>について学んでください。</li><li><a href=/ja/docs/concepts/configuration/overview/>設定のベストプラクティスとTIPS</a>を参照してください。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d649067a69d8d5c7e71564b42b96909e>5 - クラスターのネットワーク</h1><p>ネットワークはKubernetesにおける中心的な部分ですが、どのように動作するかを正確に理解することは難解な場合もあります。
Kubernetesには、4つの異なる対応すべきネットワークの問題があります:</p><ol><li>高度に結合されたコンテナ間の通信: これは、<a class=glossary-tooltip title="一番小さく一番シンプルな Kubernetes のオブジェクト。Pod とはクラスターで動作しているいくつかのコンテナのまとまりです。" data-toggle=tooltip data-placement=top href=/ja/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>および<code>localhost</code>通信によって解決されます。</li><li>Pod間の通信: 本ドキュメントの主な焦点です。</li><li>Podからサービスへの通信：これは<a href=/ja/docs/concepts/services-networking/service/>Service</a>でカバーされています。</li><li>外部からサービスへの通信：これは<a href=/ja/docs/concepts/services-networking/service/>Service</a>でカバーされています。</li></ol><p>Kubernetesは、言ってしまえばアプリケーション間でマシンを共有するためのものです。通常、マシンを共有するには、2つのアプリケーションが同じポートを使用しないようにする必要があります。
複数の開発者間でポートを調整することは、大規模に行うことは非常に難しく、ユーザーが制御できないクラスターレベルの問題に見合うことがあります。</p><p>動的ポート割り当てはシステムに多くの複雑さをもたらします。すべてのアプリケーションはパラメータとしてポートを管理する必要があり、APIサーバーにて動的なポート番号を設定値として注入する方法が必要となり、各サービスはお互いにお互いを見つける方法が必要です。Kubernetesはこれに対処するのではなく、別のアプローチを取ります。</p><h2 id=the-kubernetes-network-model>Kubernetesのネットワークモデル</h2><p>すべての<code>Pod</code>は独自のIPアドレスを持ちます。これは、<code>Pod</code>間のリンクを明示的に作成する必要がなく、コンテナポートをホストポートにマッピングする必要がほとんどないことを意味します。こうすることで、ポート割り当て、名前解決、サービスディスカバリー、負荷分散、アプリケーション設定、および移行の観点から、<code>Pod</code>をVMまたは物理ホストと同様に扱うことができる、クリーンで後方互換性のあるモデルを生み出しています。</p><p>Kubernetesは、ネットワークの実装に次の基本的な要件を課しています(意図的なネットワークセグメンテーションポリシーを除きます):</p><ul><li>ノード上のPodが、NATなしですべてのノード上のすべてのPodと通信できること</li><li>systemdやkubeletなどノード上にあるエージェントが、そのノード上のすべてのPodと通信できること</li></ul><p>注: ホストネットワークで実行される<code>Pod</code>をサポートするプラットフォームの場合(Linuxなど):</p><ul><li>ノードのホストネットワーク内のPodは、NATなしですべてのノード上のすべてのPodと通信できます</li></ul><p>このモデルは全体としてそれほど複雑ではないことに加え、KubernetesがVMからコンテナへのアプリへの移植を簡単にするという要望と基本的に互換性があります。ジョブがVMで実行されていた頃も、VMにはIPがあってプロジェクト内の他のVMと通信できました。これは同じ基本モデルです。</p><p>KubernetesのIPアドレスは<code>Pod</code>スコープに存在します。<code>Pod</code>内のコンテナは、IPアドレスを含むネットワーク名前空間を共有します。これは、<code>Pod</code>内のコンテナがすべて<code>localhost</code>上の互いのポートに到達できることを意味します。また、<code>Pod</code>内のコンテナがポートの使用を調整する必要があることも意味しますが、これもVM内のプロセスと同じです。これのことを「IP-per-pod(Pod毎のIP)」モデルと呼びます。</p><p>この実装方法は実際に使われているコンテナランタイムの詳細部分です。</p><p><code>Pod</code>に転送する<code>ノード</code>自体のポート(ホストポートと呼ばれる)を要求することは可能ですが、これは非常にニッチな操作です。このポート転送の実装方法も、コンテナランタイムの詳細部分です。<code>Pod</code>自体は、ホストポートの有無を認識しません。</p><h2 id=how-to-implement-the-kubernetes-networking-model>Kubernetesネットワークモデルの実装方法</h2><p>このネットワークモデルを実装する方法はいくつかあります。このドキュメントは、こうした方法を網羅的にはカバーしませんが、いくつかの技術の紹介として、また出発点として役立つことを願っています。</p><p>この一覧はアルファベット順にソートされており、順序は優先ステータスを意味するものではありません。</p><h3 id=aci>ACI</h3><p><a href=https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html>Cisco Application Centric Infrastructure</a> offers an integrated overlay and underlay SDN solution that supports containers, virtual machines, and bare metal servers.
<a href=https://www.github.com/noironetworks/aci-containers>ACI</a> provides container networking integration for ACI.
An overview of the integration is provided <a href=https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf>here</a>.</p><h3 id=antrea>Antrea</h3><p>Project <a href=https://github.com/vmware-tanzu/antrea>Antrea</a> is an opensource Kubernetes networking solution intended to be Kubernetes native.
It leverages Open vSwitch as the networking data plane.
Open vSwitch is a high-performance programmable virtual switch that supports both Linux and Windows.
Open vSwitch enables Antrea to implement Kubernetes Network Policies in a high-performance and efficient manner.
Thanks to the "programmable" characteristic of Open vSwitch, Antrea is able to implement an extensive set of networking and security features and services on top of Open vSwitch.</p><h3 id=aos-from-apstra>AOS from Apstra</h3><p><a href=https://www.apstra.com/products/aos/>AOS</a> is an Intent-Based Networking system that creates and manages complex datacenter environments from a simple integrated platform. AOS leverages a highly scalable distributed design to eliminate network outages while minimizing costs.</p><p>The AOS Reference Design currently supports Layer-3 connected hosts that eliminate legacy Layer-2 switching problems. These Layer-3 hosts can be Linux servers (Debian, Ubuntu, CentOS) that create BGP neighbor relationships directly with the top of rack switches (TORs). AOS automates the routing adjacencies and then provides fine grained control over the route health injections (RHI) that are common in a Kubernetes deployment.</p><p>AOS has a rich set of REST API endpoints that enable Kubernetes to quickly change the network policy based on application requirements. Further enhancements will integrate the AOS Graph model used for the network design with the workload provisioning, enabling an end to end management system for both private and public clouds.</p><p>AOS supports the use of common vendor equipment from manufacturers including Cisco, Arista, Dell, Mellanox, HPE, and a large number of white-box systems and open network operating systems like Microsoft SONiC, Dell OPX, and Cumulus Linux.</p><p>Details on how the AOS system works can be accessed here: <a href=https://www.apstra.com/products/how-it-works/>https://www.apstra.com/products/how-it-works/</a></p><h3 id=aws-vpc-cni-for-kubernetes>AWS VPC CNI for Kubernetes</h3><p><a href=https://github.com/aws/amazon-vpc-cni-k8s>AWS VPC CNI</a>は、Kubernetesクラスター向けの統合されたAWS Virtual Private Cloud(VPC)ネットワーキングを提供します。このCNIプラグインは、高いスループットと可用性、低遅延、および最小のネットワークジッタを提供します。さらに、ユーザーは、Kubernetesクラスターを構築するための既存のAWS VPCネットワーキングとセキュリティのベストプラクティスを適用できます。これには、ネットワークトラフィックの分離にVPCフローログ、VPCルーティングポリシー、およびセキュリティグループを使用する機能が含まれます。</p><p>このCNIプラグインを使用すると、Kubernetes PodはVPCネットワーク上と同じIPアドレスをPod内に持つことができます。CNIはAWS Elastic Networking Interface(ENI)を各Kubernetesノードに割り当て、ノード上のPodに各ENIのセカンダリIP範囲を使用します。このCNIには、Podの起動時間を短縮するためのENIとIPアドレスの事前割り当ての制御が含まれており、最大2,000ノードの大規模クラスターが可能です。</p><p>さらに、このCNIは<a href=https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/calico.html>ネットワークポリシーの適用のためにCalico</a>と一緒に実行できます。AWS VPC CNIプロジェクトは、<a href=https://github.com/aws/amazon-vpc-cni-k8s>GitHubのドキュメント</a>とともにオープンソースで公開されています。</p><h3 id=azure-cni-for-kubernetes>Azure CNI for Kubernetes</h3><p><a href=https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview>Azure CNI</a> is an <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>open source</a> plugin that integrates Kubernetes Pods with an Azure Virtual Network (also known as VNet) providing network performance at par with VMs. Pods can connect to peered VNet and to on-premises over Express Route or site-to-site VPN and are also directly reachable from these networks. Pods can access Azure services, such as storage and SQL, that are protected by Service Endpoints or Private Link. You can use VNet security policies and routing to filter Pod traffic. The plugin assigns VNet IPs to Pods by utilizing a pool of secondary IPs pre-configured on the Network Interface of a Kubernetes node.</p><p>Azure CNI is available natively in the [Azure Kubernetes Service (AKS)] (<a href=https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni)>https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni)</a>.</p><h3 id=big-cloud-fabric-from-big-switch-networks>Big Cloud Fabric from Big Switch Networks</h3><p><a href=https://www.bigswitch.com/container-network-automation>Big Cloud Fabric</a> is a cloud native networking architecture, designed to run Kubernetes in private cloud/on-premises environments. Using unified physical & virtual SDN, Big Cloud Fabric tackles inherent container networking problems such as load balancing, visibility, troubleshooting, security policies & container traffic monitoring.</p><p>With the help of the Big Cloud Fabric's virtual pod multi-tenant architecture, container orchestration systems such as Kubernetes, RedHat OpenShift, Mesosphere DC/OS & Docker Swarm will be natively integrated alongside with VM orchestration systems such as VMware, OpenStack & Nutanix. Customers will be able to securely inter-connect any number of these clusters and enable inter-tenant communication between them if needed.</p><p>BCF was recognized by Gartner as a visionary in the latest <a href=https://go.bigswitch.com/17GatedDocuments-MagicQuadrantforDataCenterNetworking_Reg.html>Magic Quadrant</a>. One of the BCF Kubernetes on-premises deployments (which includes Kubernetes, DC/OS & VMware running on multiple DCs across different geographic regions) is also referenced <a href=https://portworx.com/architects-corner-kubernetes-satya-komala-nio/>here</a>.</p><h3 id=cilium>Cilium</h3><p><a href=https://github.com/cilium/cilium>Cilium</a> is open source software for
providing and transparently securing network connectivity between application
containers. Cilium is L7/HTTP aware and can enforce network policies on L3-L7
using an identity based security model that is decoupled from network
addressing, and it can be used in combination with other CNI plugins.</p><h3 id=cni-genie-from-huawei>CNI-Genie from Huawei</h3><p><a href=https://github.com/Huawei-PaaS/CNI-Genie>CNI-Genie</a> is a CNI plugin that enables Kubernetes to <a href=https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables>simultaneously have access to different implementations</a> of the <a href=/ja/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model>Kubernetes network model</a> in runtime. This includes any implementation that runs as a <a href=https://github.com/containernetworking/cni#3rd-party-plugins>CNI plugin</a>, such as <a href=https://github.com/coreos/flannel#flannel>Flannel</a>, <a href=http://docs.projectcalico.org/>Calico</a>, <a href=http://romana.io>Romana</a>, <a href=https://www.weave.works/products/weave-net/>Weave-net</a>.</p><p>CNI-Genie also supports <a href=https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-addresses-per-pod>assigning multiple IP addresses to a pod</a>, each from a different CNI plugin.</p><h3 id=cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</h3><p><a href=https://github.com/lyft/cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</a> contains a set
of CNI and IPAM plugins to provide a simple, host-local, low latency, high
throughput, and compliant networking stack for Kubernetes within Amazon Virtual
Private Cloud (VPC) environments by making use of Amazon Elastic Network
Interfaces (ENI) and binding AWS-managed IPs into Pods using the Linux kernel's
IPvlan driver in L2 mode.</p><p>The plugins are designed to be straightforward to configure and deploy within a
VPC. Kubelets boot and then self-configure and scale their IP usage as needed
without requiring the often recommended complexities of administering overlay
networks, BGP, disabling source/destination checks, or adjusting VPC route
tables to provide per-instance subnets to each host (which is limited to 50-100
entries per VPC). In short, cni-ipvlan-vpc-k8s significantly reduces the
network complexity required to deploy Kubernetes at scale within AWS.</p><h3 id=contiv>Contiv</h3><p><a href=https://github.com/contiv/netplugin>Contiv</a> provides configurable networking (native l3 using BGP, overlay using vxlan, classic l2, or Cisco-SDN/ACI) for various use cases. <a href=https://contiv.io>Contiv</a> is all open sourced.</p><h3 id=contrail-tungsten-fabric>Contrail / Tungsten Fabric</h3><p><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, based on <a href=https://tungsten.io>Tungsten Fabric</a>, is a truly open, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with various orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide different isolation modes for virtual machines, containers/pods and bare metal workloads.</p><h3 id=danm>DANM</h3><p><a href=https://github.com/nokia/danm>DANM</a> is a networking solution for telco workloads running in a Kubernetes cluster. It's built up from the following components:</p><ul><li>A CNI plugin capable of provisioning IPVLAN interfaces with advanced features</li><li>An in-built IPAM module with the capability of managing multiple, cluster-wide, discontinuous L3 networks and provide a dynamic, static, or no IP allocation scheme on-demand</li><li>A CNI metaplugin capable of attaching multiple network interfaces to a container, either through its own CNI, or through delegating the job to any of the popular CNI solution like SRI-OV, or Flannel in parallel</li><li>A Kubernetes controller capable of centrally managing both VxLAN and VLAN interfaces of all Kubernetes hosts</li><li>Another Kubernetes controller extending Kubernetes' Service-based service discovery concept to work over all network interfaces of a Pod</li></ul><p>With this toolset DANM is able to provide multiple separated network interfaces, the possibility to use different networking back ends and advanced IPAM features for the pods.</p><h3 id=flannel>Flannel</h3><p><a href=https://github.com/coreos/flannel#flannel>Flannel</a> is a very simple overlay
network that satisfies the Kubernetes requirements. Many
people have reported success with Flannel and Kubernetes.</p><h3 id=google-compute-engine-gce>Google Compute Engine (GCE)</h3><p>For the Google Compute Engine cluster configuration scripts, <a href=https://cloud.google.com/vpc/docs/routes>advanced
routing</a> is used to
assign each VM a subnet (default is <code>/24</code> - 254 IPs). Any traffic bound for that
subnet will be routed directly to the VM by the GCE network fabric. This is in
addition to the "main" IP address assigned to the VM, which is NAT'ed for
outbound internet access. A linux bridge (called <code>cbr0</code>) is configured to exist
on that subnet, and is passed to docker's <code>--bridge</code> flag.</p><p>Docker is started with:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--bridge=cbr0 --iptables=false --ip-masq=false&#34;</span>
</code></pre></div><p>This bridge is created by Kubelet (controlled by the <code>--network-plugin=kubenet</code>
flag) according to the <code>Node</code>'s <code>.spec.podCIDR</code>.</p><p>Docker will now allocate IPs from the <code>cbr-cidr</code> block. Containers can reach
each other and <code>Nodes</code> over the <code>cbr0</code> bridge. Those IPs are all routable
within the GCE project network.</p><p>GCE itself does not know anything about these IPs, though, so it will not NAT
them for outbound internet traffic. To achieve that an iptables rule is used
to masquerade (aka SNAT - to make it seem as if packets came from the <code>Node</code>
itself) traffic that is bound for IPs outside the GCE project network
(10.0.0.0/8).</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE
</code></pre></div><p>Lastly IP forwarding is enabled in the kernel (so the kernel will process
packets for bridged containers):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sysctl net.ipv4.ip_forward<span style=color:#666>=</span><span style=color:#666>1</span>
</code></pre></div><p>The result of all this is that all <code>Pods</code> can reach each other and can egress
traffic to the internet.</p><h3 id=jaguar>Jaguar</h3><p><a href=https://gitlab.com/sdnlab/jaguar>Jaguar</a> is an open source solution for Kubernetes's network based on OpenDaylight. Jaguar provides overlay network using vxlan and Jaguar CNIPlugin provides one IP address per pod.</p><h3 id=k-vswitch>k-vswitch</h3><p><a href=https://github.com/k-vswitch/k-vswitch>k-vswitch</a> is a simple Kubernetes networking plugin based on <a href=https://www.openvswitch.org/>Open vSwitch</a>. It leverages existing functionality in Open vSwitch to provide a robust networking plugin that is easy-to-operate, performant and secure.</p><h3 id=knitter>Knitter</h3><p><a href=https://github.com/ZTE/Knitter/>Knitter</a> is a network solution which supports multiple networking in Kubernetes. It provides the ability of tenant management and network management. Knitter includes a set of end-to-end NFV container networking solutions besides multiple network planes, such as keeping IP address for applications, IP address migration, etc.</p><h3 id=kube-ovn>Kube-OVN</h3><p><a href=https://github.com/alauda/kube-ovn>Kube-OVN</a> is an OVN-based kubernetes network fabric for enterprises. With the help of OVN/OVS, it provides some advanced overlay network features like subnet, QoS, static IP allocation, traffic mirroring, gateway, openflow-based network policy and service proxy.</p><h3 id=kube-router>Kube-router</h3><p><a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> is a purpose-built networking solution for Kubernetes that aims to provide high performance and operational simplicity. Kube-router provides a Linux <a href=https://www.linuxvirtualserver.org/software/ipvs.html>LVS/IPVS</a>-based service proxy, a Linux kernel forwarding-based pod-to-pod networking solution with no overlays, and iptables/ipset-based network policy enforcer.</p><h3 id=l2-networks-and-linux-bridging>L2 networks and linux bridging</h3><p>If you have a "dumb" L2 network, such as a simple switch in a "bare-metal"
environment, you should be able to do something similar to the above GCE setup.
Note that these instructions have only been tried very casually - it seems to
work, but has not been thoroughly tested. If you use this technique and
perfect the process, please let us know.</p><p>Follow the "With Linux Bridge devices" section of
<a href=http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/>this very nice tutorial</a> from
Lars Kellogg-Stedman.</p><h3 id=multus-a-multi-network-plugin>Multus (a Multi Network plugin)</h3><p><a href=https://github.com/Intel-Corp/multus-cni>Multus</a> is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using CRD based network objects in Kubernetes.</p><p>Multus supports all <a href=https://github.com/containernetworking/plugins>reference plugins</a> (eg. <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>Flannel</a>, <a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp>DHCP</a>, <a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan>Macvlan</a>) that implement the CNI specification and 3rd party plugins (eg. <a href=https://github.com/projectcalico/cni-plugin>Calico</a>, <a href=https://github.com/weaveworks/weave>Weave</a>, <a href=https://github.com/cilium/cilium>Cilium</a>, <a href=https://github.com/contiv/netplugin>Contiv</a>). In addition to it, Multus supports <a href=https://github.com/hustcat/sriov-cni>SRIOV</a>, <a href=https://github.com/Intel-Corp/sriov-cni>DPDK</a>, <a href=https://github.com/intel/vhost-user-net-plugin>OVS-DPDK & VPP</a> workloads in Kubernetes with both cloud native and NFV based applications in Kubernetes.</p><h3 id=ovn4nfv-k8s-plugin-ovn-based-cni-controller-plugin>OVN4NFV-K8s-Plugin (OVN based CNI controller & plugin)</h3><p><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a> is OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC), Multiple OVN overlay networking, dynamic subnet creation, dynamic creation of virtual networks, VLAN Provider network, Direct provider network and pluggable with other Multi-network plugins, ideal for edge based cloud native workloads in Multi-cluster networking</p><h3 id=nsx-t>NSX-T</h3><p><a href=https://docs.vmware.com/en/VMware-NSX-T/index.html>VMware NSX-T</a> is a network virtualization and security platform. NSX-T can provide network virtualization for a multi-cloud and multi-hypervisor environment and is focused on emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks. In addition to vSphere hypervisors, these environments include other hypervisors such as KVM, containers, and bare metal.</p><p><a href=https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf>NSX-T Container Plug-in (NCP)</a> provides integration between NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.</p><h3 id=nuage-networks-vcs-virtualized-cloud-services>Nuage Networks VCS (Virtualized Cloud Services)</h3><p><a href=https://www.nuagenetworks.net>Nuage</a> provides a highly scalable policy-based Software-Defined Networking (SDN) platform. Nuage uses the open source Open vSwitch for the data plane along with a feature rich SDN Controller built on open standards.</p><p>The Nuage platform uses overlays to provide seamless policy-based networking between Kubernetes Pods and non-Kubernetes environments (VMs and bare metal servers). Nuage's policy abstraction model is designed with applications in mind and makes it easy to declare fine-grained policies for applications.The platform's real-time analytics engine enables visibility and security monitoring for Kubernetes applications.</p><h3 id=openvswitch>OpenVSwitch</h3><p><a href=https://www.openvswitch.org/>OpenVSwitch</a> is a somewhat more mature but also
complicated way to build an overlay network. This is endorsed by several of the
"Big Shops" for networking.</p><h3 id=ovn-open-virtual-networking>OVN (Open Virtual Networking)</h3><p>OVN is an opensource network virtualization solution developed by the
Open vSwitch community. It lets one create logical switches, logical routers,
stateful ACLs, load-balancers etc to build different virtual networking
topologies. The project has a specific Kubernetes plugin and documentation
at <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>.</p><h3 id=project-calico>Project Calico</h3><p><a href=https://docs.projectcalico.org/>Project Calico</a> is an open source container networking provider and network policy engine.</p><p>Calico provides a highly scalable networking and network policy solution for connecting Kubernetes pods based on the same IP networking principles as the internet, for both Linux (open source) and Windows (proprietary - available from <a href=https://www.tigera.io/essentials/>Tigera</a>). Calico can be deployed without encapsulation or overlays to provide high-performance, high-scale data center networking. Calico also provides fine-grained, intent based network security policy for Kubernetes pods via its distributed firewall.</p><p>Calico can also be run in policy enforcement mode in conjunction with other networking solutions such as Flannel, aka <a href=https://github.com/tigera/canal>canal</a>, or native GCE, AWS or Azure networking.</p><h3 id=romana>Romana</h3><p><a href=https://romana.io>Romana</a> is an open source network and security automation solution that lets you deploy Kubernetes without an overlay network. Romana supports Kubernetes <a href=/docs/concepts/services-networking/network-policies/>Network Policy</a> to provide isolation across network namespaces.</p><h3 id=weave-net-from-weaveworks>Weave Net from Weaveworks</h3><p><a href=https://www.weave.works/products/weave-net/>Weave Net</a> is a
resilient and simple to use network for Kubernetes and its hosted applications.
Weave Net runs as a <a href=https://www.weave.works/docs/net/latest/cni-plugin/>CNI plug-in</a>
or stand-alone. In either version, it doesn't require any configuration or extra code
to run, and in both cases, the network provides one IP address per pod - as is standard for Kubernetes.</p><h2 id=次の項目>次の項目</h2><p>ネットワークモデルの初期設計とその根拠、および将来の計画については、<a href=https://git.k8s.io/community/contributors/design-proposals/network/networking.md>ネットワーク設計ドキュメント</a>で詳細に説明されています。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>6 - Kubernetesのプロキシー</h1><p>このページではKubernetesと併用されるプロキシーについて説明します。</p><h2 id=プロキシー>プロキシー</h2><p>Kubernetesを使用する際に、いくつかのプロキシーを使用する場面があります。</p><ol><li><p><a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectlのプロキシー</a>:</p><ul><li>ユーザーのデスクトップ上かPod内で稼働します</li><li>ローカルホストのアドレスからKubernetes apiserverへのプロキシーを行います</li><li>クライアントからプロキシー間ではHTTPを使用します</li><li>プロキシーからapiserverへはHTTPSを使用します</li><li>apiserverの場所を示します</li><li>認証用のヘッダーを追加します</li></ul></li><li><p><a href=/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>apiserverのプロキシー</a>:</p><ul><li>apiserver内で動作する踏み台となります</li><li>これがなければ到達不可能であるクラスターIPへ、クラスターの外部からのユーザーを接続します</li><li>apiserverのプロセス内で稼働します</li><li>クライアントからプロキシー間ではHTTPSを使用します(apiserverの設定により、HTTPを使用します)</li><li>プロキシーからターゲット間では利用可能な情報を使用して、プロキシーによって選択されたHTTPかHTTPSのいずれかを使用します</li><li>Node、Pod、Serviceへ到達するのに使えます</li><li>Serviceへ到達するときは負荷分散を行います</li></ul></li><li><p><a href=/ja/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>各ノード上で稼働します</li><li>UDP、TCP、SCTPをプロキシーします</li><li>HTTPを解釈しません</li><li>負荷分散機能を提供します</li><li>Serviceへ到達させるためのみに使用されます</li></ul></li><li><p>apiserverの前段にあるプロキシー/ロードバランサー:</p><ul><li>実際に存在するかどうかと実装はクラスターごとに異なります(例: nginx)</li><li>全てのクライアントと、1つ以上のapiserverの間に位置します</li><li>複数のapiserverがあるときロードバランサーとして稼働します</li></ul></li><li><p>外部サービス上で稼働するクラウドロードバランサー:</p><ul><li>いくつかのクラウドプロバイダーによって提供されます(例: AWS ELB、Google Cloud Load Balancer)</li><li><code>LoadBalancer</code>というtypeのKubernetes Serviceが作成されたときに自動で作成されます</li><li>たいていのクラウドロードバランサーはUDP/TCPのみサポートしています</li><li>SCTPのサポートはクラウドプロバイダーのロードバランサーの実装によって異なります</li><li>ロードバランサーの実装はクラウドプロバイダーによって異なります</li></ul></li></ol><p>Kubernetesユーザーのほとんどは、最初の2つのタイプ以外に心配する必要はありません。クラスター管理者はそれ以外のタイプのロードバランサーを正しくセットアップすることを保証します。</p><h2 id=リダイレクトの要求>リダイレクトの要求</h2><p>プロキシーはリダイレクトの機能を置き換えました。リダイレクトの使用は非推奨となります。</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ja/docs/home/>ホーム</a>
<a class=text-white href=/ja/training/>トレーニング</a>
<a class=text-white href=/ja/partners/>パートナー</a>
<a class=text-white href=/ja/community/>コミュニティ</a>
<a class=text-white href=/ja/case-studies/>ケーススタディ</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>