<!doctype html><html lang=en class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/setup/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/setup/><link rel=alternate hreflang=pt href=https://kubernetes.io/pt/docs/setup/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/setup/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/setup/><link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/setup/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/setup/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Getting started | Kubernetes</title><meta property="og:title" content="Getting started"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/setup/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Getting started"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Getting started"><meta name=twitter:description content="Production-Grade Container Orchestration"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="This section lists the different ways to set up and run Kubernetes. When you install Kubernetes, choose an installation type based on: ease of maintenance, security, control, available resources, and expertise required to operate and manage a cluster.
You can deploy a Kubernetes cluster on a local machine, cloud, on-prem datacenter, or choose a managed Kubernetes cluster. There are also custom solutions across a wide range of cloud providers, or bare metal environments."><meta property="og:description" content="This section lists the different ways to set up and run Kubernetes. When you install Kubernetes, choose an installation type based on: ease of maintenance, security, control, available resources, and expertise required to operate and manage a cluster.
You can deploy a Kubernetes cluster on a local machine, cloud, on-prem datacenter, or choose a managed Kubernetes cluster. There are also custom solutions across a wide range of cloud providers, or bare metal environments."><meta name=twitter:description content="This section lists the different ways to set up and run Kubernetes. When you install Kubernetes, choose an installation type based on: ease of maintenance, security, control, available resources, and expertise required to operate and manage a cluster.
You can deploy a Kubernetes cluster on a local machine, cloud, on-prem datacenter, or choose a managed Kubernetes cluster. There are also custom solutions across a wide range of cloud providers, or bare metal environments."><meta property="og:url" content="https://kubernetes.io/docs/setup/"><meta property="og:title" content="Getting started"><meta name=twitter:title content="Getting started"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/docs/setup/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/setup/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/setup/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/setup/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/docs/setup/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh/docs/setup/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/setup/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/setup/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/setup/>Français</a>
<a class=dropdown-item href=/de/docs/setup/>Deutsch</a>
<a class=dropdown-item href=/pt/docs/setup/>Português</a>
<a class=dropdown-item href=/es/docs/setup/>Español</a>
<a class=dropdown-item href=/id/docs/setup/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/setup/>Русский</a>
<a class=dropdown-item href=/pl/docs/setup/>Polski</a>
<a class=dropdown-item href=/uk/docs/setup/>Українська</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/setup/>Return to the regular view of this page</a>.</p></div><h1 class=title>Getting started</h1><ul><li>1: <a href=#pg-d33663ac044e1981b406949f9124cc04>Release notes and version skew</a></li><ul><li>1.1: <a href=#pg-a49e4163901749c7ce817c73e249f1bc>v1.20 Release Notes</a></li><li>1.2: <a href=#pg-85b7e96ac42e5e28ec570ad43f0ef5cd>Kubernetes version and version skew support policy</a></li></ul><li>2: <a href=#pg-0b597086a9d1382f86abadcfeab657d6>Learning environment</a></li><ul></ul><li>3: <a href=#pg-4e14853fdaa3bd273f31a60112b9b5ac>Production environment</a></li><ul><li>3.1: <a href=#pg-a77d3feb6e6d9978f32fa14622642e9a>Container runtimes</a></li><li>3.2: <a href=#pg-00e1646f68aeb89f9722cf6f6cfcad94>Installing Kubernetes with deployment tools</a></li><ul><li>3.2.1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>Bootstrapping clusters with kubeadm</a></li><ul><li>3.2.1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>Installing kubeadm</a></li><li>3.2.1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>Troubleshooting kubeadm</a></li><li>3.2.1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>Creating a cluster with kubeadm</a></li><li>3.2.1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>Customizing control plane configuration with kubeadm</a></li><li>3.2.1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>Options for Highly Available topology</a></li><li>3.2.1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>Creating Highly Available clusters with kubeadm</a></li><li>3.2.1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>Set up a High Availability etcd cluster with kubeadm</a></li><li>3.2.1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>Configuring each kubelet in your cluster using kubeadm</a></li><li>3.2.1.9: <a href=#pg-ed857e09999827b013ee9062dc9c59bb>Configuring your kubernetes cluster to self-host the control plane</a></li></ul><li>3.2.2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>Installing Kubernetes with kops</a></li><li>3.2.3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>Installing Kubernetes with Kubespray</a></li></ul><li>3.3: <a href=#pg-d2f55eefe7222b7c637875af9c3ec199>Turnkey Cloud Solutions</a></li><li>3.4: <a href=#pg-acce7e24090fea04715a7a516ba3e69b>Windows in Kubernetes</a></li><ul><li>3.4.1: <a href=#pg-a307d413f1f7430fced233023087e2a1>Intro to Windows support in Kubernetes</a></li><li>3.4.2: <a href=#pg-3a51e66c5de55f9093a8dc55742006d3>Guide for scheduling Windows containers in Kubernetes</a></li></ul></ul><li>4: <a href=#pg-84b6491601d6a2b3da4cd5a105c866ba>Best practices</a></li><ul><li>4.1: <a href=#pg-c797ee17120176c685455db89ae091a9>Considerations for large clusters</a></li><li>4.2: <a href=#pg-970615c97499e3651fd3a98e0387cefc>Running in multiple zones</a></li><li>4.3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>Validate node setup</a></li><li>4.4: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI certificates and requirements</a></li></ul></ul><div class=content><p>This section lists the different ways to set up and run Kubernetes.
When you install Kubernetes, choose an installation type based on: ease of maintenance, security,
control, available resources, and expertise required to operate and manage a cluster.</p><p>You can deploy a Kubernetes cluster on a local machine, cloud, on-prem datacenter, or choose a managed Kubernetes cluster. There are also custom solutions across a wide range of cloud providers, or bare metal environments.</p><h2 id=learning-environment>Learning environment</h2><p>If you're learning Kubernetes, use the tools supported by the Kubernetes community, or tools in the ecosystem to set up a Kubernetes cluster on a local machine.</p><h2 id=production-environment>Production environment</h2><p>When evaluating a solution for a production environment, consider which aspects of operating a Kubernetes cluster (or <em>abstractions</em>) you want to manage yourself or offload to a provider.</p><p><a href=https://kubernetes.io/partners/#conformance>Kubernetes Partners</a> includes a list of <a href=https://github.com/cncf/k8s-conformance/#certified-kubernetes>Certified Kubernetes</a> providers.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d33663ac044e1981b406949f9124cc04>1 - Release notes and version skew</h1></div><div class=td-content><h1 id=pg-a49e4163901749c7ce817c73e249f1bc>1.1 - v1.20 Release Notes</h1><h1 id=v1-20-0>v1.20.0</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-20-0>Downloads for v1.20.0</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>ebfe49552bbda02807034488967b3b62bf9e3e507d56245e298c4c19090387136572c1fca789e772a5e8a19535531d01dcedb61980e42ca7b0461d3864df2c14</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>bcbd67ed0bb77840828c08c6118ad0c9bf2bcda16763afaafd8731fd6ce735be654feef61e554bcc34c77c65b02a25dae565adc5e1dc49a2daaa0d115bf1efe6</code></td></tr></tbody></table><h3 id=client-binaries>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>3609f6483f4244676162232b3294d7a2dc40ae5bdd86a842a05aa768f5223b8f50e1d6420fd8afb2d0ce19de06e1d38e5e5b10154ba0cb71a74233e6dc94d5a0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>e06c08016a08137d39804383fdc33a40bb2567aa77d88a5c3fd5b9d93f5b581c635b2c4faaa718ed3bb2d120cb14fe91649ed4469ba72c3a3dda1e343db545ed</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>081472833601aa4fa78e79239f67833aa4efcb4efe714426cd01d4ddf6f36fbf304ef7e1f5373bff0fdff44a845f7560165c093c108bd359b5ab4189f36b1f2f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>037f84a2f29fe62d266cab38ac5600d058cce12cbc4851bcf062fafba796c1fbe23a0c2939cd15784854ca7cd92383e5b96a11474fc71fb614b47dbf98a477d9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>275727e1796791ca3cbe52aaa713a2660404eab6209466fdc1cfa8559c9b361fe55c64c6bcecbdeba536b6d56213ddf726e58adc60f959b6f77e4017834c5622</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>7a9965293029e9fcdb2b7387467f022d2026953b8461e6c84182abf35c28b7822d2389a6d8e4d8e532d2ea5d5d67c6fee5fb6c351363cb44c599dc8800649b04</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>85fc449ce1980f5f030cc32e8c8e2198c1cc91a448e04b15d27debc3ca56aa85d283f44b4f4e5fed26ac96904cc12808fa3e9af3d8bf823fc928befb9950d6f5</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>4c0a27dba1077aaee943e0eb7a787239dd697e1d968e78d1933c1e60b02d5d233d58541d5beec59807a4ffe3351d5152359e11da120bf64cacb3ee29fbc242e6</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>29336faf7c596539b8329afbbdceeddc843162501de4afee44a40616278fa1f284d8fc48c241fc7d52c65dab70f76280cc33cec419c8c5dbc2625d9175534af8</code></td></tr></tbody></table><h3 id=server-binaries>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>fb56486a55dbf7dbacb53b1aaa690bae18d33d244c72a1e2dc95fb0fcce45108c44ba79f8fa04f12383801c46813dc33d2d0eb2203035cdce1078871595e446e</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>735ed9993071fe35b292bf06930ee3c0f889e3c7edb983195b1c8e4d7113047c12c0f8281fe71879fc2fcd871e1ee587f03b695a03c8512c873abad444997a19</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>ffab155531d5a9b82487ee1abf4f6ef49626ea58b2de340656a762e46cf3e0f470bdbe7821210901fe1114224957c44c1d9cc1e32efb5ee24e51fe63990785b2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>9d5730d35c4ddfb4c5483173629fe55df35d1e535d96f02459468220ac2c97dc01b995f577432a6e4d1548b6edbfdc90828dc9c1f7cf7464481af6ae10aaf118</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>6e4c165306940e8b99dd6e590f8542e31aed23d2c7a6808af0357fa425cec1a57016dd66169cf2a95f8eb8ef70e1f29e2d500533aae889e2e3d9290d04ab8721</code></td></tr></tbody></table><h3 id=node-binaries>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>3e6c90561dd1c27fa1dff6953c503251c36001f7e0f8eff3ec918c74ae2d9aa25917d8ac87d5b4224b8229f620b1830442e6dce3b2a497043f8497eee3705696</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>26db385d9ae9a97a1051a638e7e3de22c4bbff389d5a419fe40d5893f9e4fa85c8b60a2bd1d370fd381b60c3ca33c5d72d4767c90898caa9dbd4df6bd116a247</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>5b8b63f617e248432b7eb913285a8ef8ba028255216332c05db949666c3f9e9cb9f4c393bbd68d00369bda77abf9bfa2da254a5c9fe0d79ffdad855a77a9d8ed</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>60da7715996b4865e390640525d6e98593ba3cd45c6caeea763aa5355a7f989926da54f58cc5f657f614c8134f97cd3894b899f8b467d100dca48bc22dd4ff63</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>9407dc55412bd04633f84fcefe3a1074f3eaa772a7cb9302242b8768d6189b75d37677a959f91130e8ad9dc590f9ba8408ba6700a0ceff6827315226dd5ee1e6</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>9d4261af343cc330e6359582f80dbd6efb57d41f882747a94bbf47b4f93292d43dd19a86214d4944d268941622dfbc96847585e6fec15fddc4dbd93d17015fa8</code></td></tr></tbody></table><h2 id=changelog-since-v1-19-0>Changelog since v1.19.0</h2><h2 id=what-s-new-major-themes>What's New (Major Themes)</h2><h3 id=dockershim-deprecation>Dockershim deprecation</h3><p>Docker as an underlying runtime is being deprecated. Docker-produced images will continue to work in your cluster with all runtimes, as they always have.
The Kubernetes community <a href=https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/>has written a blog post about this in detail</a> with <a href=https://blog.k8s.io/2020/12/02/dockershim-faq/>a dedicated FAQ page for it</a>.</p><h3 id=external-credential-provider-for-client-go>External credential provider for client-go</h3><p>The client-go credential plugins can now be passed in the current cluster information via the <code>KUBERNETES_EXEC_INFO</code> environment variable. Learn more about this on <a href=https://docs.k8s.io/reference/access-authn-authz/authentication/#client-go-credential-plugins/>client-go credential plugins documentation</a>.</p><h3 id=cronjob-controller-v2-is-available-through-feature-gate>CronJob controller v2 is available through feature gate</h3><p>An alternative implementation of <code>CronJob</code> controller is now available as an alpha feature in this release, which has experimental performance improvement by using informers instead of polling. While this will be the default behavior in the future, you can <a href=https://docs.k8s.io/concepts/workloads/controllers/cron-jobs/>try them in this release through a feature gate</a>.</p><h3 id=pid-limits-graduates-to-general-availability>PID Limits graduates to General Availability</h3><p>PID Limits features are now generally available on both <code>SupportNodePidsLimit</code> (node-to-pod PID isolation) and <code>SupportPodPidsLimit</code> (ability to limit PIDs per pod), after being enabled-by-default in beta stage for a year.</p><h3 id=api-priority-and-fairness-graduates-to-beta>API Priority and Fairness graduates to Beta</h3><p>Initially introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows <code>kube-apiserver</code> to <a href=https://docs.k8s.io/concepts/cluster-administration/flow-control/>categorize incoming requests by priority levels</a>.</p><h3 id=ipv4-ipv6-run>IPv4/IPv6 run</h3><p>IPv4/IPv6 dual-stack has been reimplemented for 1.20 to support dual-stack Services, based on user and community feedback. If your cluster has dual-stack enabled, you can create Services which can use IPv4, IPv6, or both, and you can change this setting for existing Services. Details are available in updated <a href=https://docs.k8s.io/concepts/services-networking/dual-stack/>IPv4/IPv6 dual-stack docs</a>, which cover the nuanced array of options.</p><p>We expect this implementation to progress from alpha to beta and GA in coming releases, so we’re eager to have you comment about your dual-stack experiences in <a href=https://kubernetes.slack.com/messages/k8s-dual-stack>#k8s-dual-stack</a> or in <a href=https://features.k8s.io/563>enhancements #563</a>.</p><h3 id=go1-15-5>go1.15.5</h3><p>go1.15.5 has been integrated to Kubernetes project as of this release, <a href=https://github.com/kubernetes/kubernetes/pull/95776>including other infrastructure related updates on this effort</a>.</p><h3 id=csi-volume-snapshot-graduates-to-general-availability>CSI Volume Snapshot graduates to General Availability</h3><p>CSI Volume Snapshot moves to GA in the 1.20 release. This feature provides a standard way to trigger volume snapshot operations in Kubernetes and allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of supporting underlying storage providers.
Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
Note that snapshot support will require Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. In addition, a CSI driver supporting the snapshot functionality must also be deployed on the cluster.</p><h3 id=non-recursive-volume-ownership-fsgroup-graduates-to-beta>Non-recursive Volume Ownership (FSGroup) graduates to Beta</h3><p>By default, the <code>fsgroup</code> setting, if specified, recursively updates permissions for every file in a volume on every mount. This can make mount, and pod startup, very slow if the volume has many files.
This setting enables a pod to specify a <code>PodFSGroupChangePolicy</code> that indicates that volume ownership and permissions will be changed only when permission and ownership of the root directory does not match with expected permissions on the volume.</p><h3 id=csidriver-policy-for-fsgroup-graduates-to-beta>CSIDriver policy for FSGroup graduates to Beta</h3><p>The FSGroup's CSIDriver Policy is now beta in 1.20. This allows CSIDrivers to explicitly indicate if they want Kubernetes to manage permissions and ownership for their volumes via <code>fsgroup</code>.</p><h3 id=security-improvements-for-csi-drivers-alpha>Security Improvements for CSI Drivers (Alpha)</h3><p>In 1.20, we introduce a new alpha feature <code>CSIServiceAccountToken</code>. This feature allows CSI drivers to impersonate the pods that they mount the volumes for. This improves the security posture in the mounting process where the volumes are ACL’ed on the pods’ service account without handing out unnecessary permissions to the CSI drivers’ service account. This feature is especially important for secret-handling CSI drivers, such as the secrets-store-csi-driver. Since these tokens can be rotated and short-lived, this feature also provides a knob for CSI drivers to receive <code>NodePublishVolume</code> RPC calls periodically with the new token. This knob is also useful when volumes are short-lived, e.g. certificates.</p><h3 id=introducing-graceful-node-shutdown-alpha>Introducing Graceful Node Shutdown (Alpha)</h3><p>The <code>GracefulNodeShutdown</code> feature is now in Alpha. This allows kubelet to be aware of node system shutdowns, enabling graceful termination of pods during a system shutdown. This feature can be <a href=https://docs.k8s.io/concepts/architecture/nodes/#graceful-node-shutdown>enabled through feature gate</a>.</p><h3 id=runtime-log-sanitation>Runtime log sanitation</h3><p>Logs can now be configured to use runtime protection from leaking sensitive data. <a href=https://docs.k8s.io/concepts/cluster-administration/system-logs/#log-sanitization>Details for this experimental feature is available in documentation</a>.</p><h3 id=pod-resource-metrics>Pod resource metrics</h3><p>On-demand metrics calculation is now available through <code>/metrics/resources</code>. <a href=https://docs.k8s.io/concepts/cluster-administration/system-metrics#kube-scheduler-metrics>When enabled</a>, the endpoint will report the requested resources and the desired limits of all running pods.</p><h3 id=introducing-rootcaconfigmap>Introducing <code>RootCAConfigMap</code></h3><p><code>RootCAConfigMap</code> graduates to Beta, seperating from <code>BoundServiceAccountTokenVolume</code>. The <code>kube-root-ca.crt</code> ConfigMap is now available to every namespace, by default. It contains the Certificate Authority bundle for verify kube-apiserver connections.</p><h3 id=kubectl-debug-graduates-to-beta><code>kubectl debug</code> graduates to Beta</h3><p><code>kubectl alpha debug</code> graduates from alpha to beta in 1.20, becoming <code>kubectl debug</code>.
<code>kubectl debug</code> provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of <code>kubectl</code> include:
Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.
Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)
Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.
Note that as a new builtin command, <code>kubectl debug</code> takes priority over any <code>kubectl</code> plugin named “debug”. You will need to rename the affected plugin.
Invocations using <code>kubectl alpha debug</code> are now deprecated and will be removed in a subsequent release. Update your scripts to use <code>kubectl debug</code> instead of <code>kubectl alpha debug</code>!
For more information about kubectl debug, see Debugging Running Pods on the Kubernetes website, kubectl help debug, or reach out to SIG CLI by visiting #sig-cli or commenting on <a href=https://features.k8s.io/1441>enhancement #1441</a>.</p><h3 id=removing-deprecated-flags-in-kubeadm>Removing deprecated flags in kubeadm</h3><p><code>kubeadm</code> applies a number of deprecations and removals of deprecated features in this release. More details are available in the Urgent Upgrade Notes and Kind / Deprecation sections.</p><h3 id=pod-hostname-as-fqdn-graduates-to-beta>Pod Hostname as FQDN graduates to Beta</h3><p>Previously introduced in 1.19 behind a feature gate, <code>SetHostnameAsFQDN</code> is now enabled by default. More details on this behavior is available in <a href=https://docs.k8s.io/concepts/services-networking/dns-pod-service/#pod-sethostnameasfqdn-field>documentation for DNS for Services and Pods</a></p><h3 id=tokenrequest-tokenrequestprojection-graduates-to-general-availability><code>TokenRequest</code> / <code>TokenRequestProjection</code> graduates to General Availability</h3><p>Service account tokens bound to pod is now a stable feature. The feature gates will be removed in 1.21 release. For more information, refer to notes below on the changelogs.</p><h3 id=runtimeclass-feature-graduates-to-general-availability>RuntimeClass feature graduates to General Availability.</h3><p>The <code>node.k8s.io</code> API groups are promoted from <code>v1beta1</code> to <code>v1</code>. <code>v1beta1</code> is now deprecated and will be removed in a future release, please start using <code>v1</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/95718>#95718</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>) [SIG Apps, Auth, Node, Scheduling and Testing]</p><h3 id=cloud-controller-manager-now-exclusively-shipped-by-cloud-provider>Cloud Controller Manager now exclusively shipped by Cloud Provider</h3><p>Kubernetes will no longer ship an instance of the Cloud Controller Manager binary. Each Cloud Provider is expected to ship their own instance of this binary. Details for a Cloud Provider to create an instance of such a binary can be found under <a href=https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/cloud-provider/sample>here</a>. Anyone with questions on building a Cloud Controller Manager should reach out to SIG Cloud Provider. Questions about the Cloud Controller Manager on a Managed Kubernetes solution should go to the relevant Cloud Provider. Questions about the Cloud Controller Manager on a non managed solution can be brought up with SIG Cloud Provider.</p><h2 id=known-issues>Known Issues</h2><h3 id=summary-api-in-kubelet-doesn-t-have-accelerator-metrics>Summary API in kubelet doesn't have accelerator metrics</h3><p>Currently, cadvisor_stats_provider provides AcceleratorStats but cri_stats_provider does not. As a result, when using cri_stats_provider, kubelet's Summary API does not have accelerator metrics. <a href=https://github.com/kubernetes/kubernetes/pull/96873>There is an open work in progress to fix this</a>.</p><h2 id=urgent-upgrade-notes>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade>(No, really, you MUST read this before you upgrade)</h3><ul><li><p>A bug was fixed in kubelet where exec probe timeouts were not respected. This may result in unexpected behavior since the default timeout (if not specified) is <code>1s</code> which may be too small for some exec probes. Ensure that pods relying on this behavior are updated to correctly handle probe timeouts. See <a href=https://docs.k8s.io/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes>configure probe</a> section of the documentation for more details.</p><ul><li>This change in behavior may be unexpected for some clusters and can be disabled by turning off the <code>ExecProbeTimeout</code> feature gate. This gate will be locked and removed in future releases so that exec probe timeouts are always respected. (<a href=https://github.com/kubernetes/kubernetes/pull/94115>#94115</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Node and Testing]</li></ul></li><li><p>RuntimeClass feature graduates to General Availability. Promote <code>node.k8s.io</code> API groups from <code>v1beta1</code> to <code>v1</code>. <code>v1beta1</code> is now deprecated and will be removed in a future release, please start using <code>v1</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/95718>#95718</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>) [SIG Apps, Auth, Node, Scheduling and Testing]</p></li><li><p>API priority and fairness graduated to beta. 1.19 servers with APF turned on should not be run in a multi-server cluster with 1.20+ servers. (<a href=https://github.com/kubernetes/kubernetes/pull/96527>#96527</a>, <a href=https://github.com/adtac>@adtac</a>) [SIG API Machinery and Testing]</p></li><li><p>For CSI drivers, kubelet no longer creates the target_path for NodePublishVolume in accordance with the CSI spec. Kubelet also no longer checks if staging and target paths are mounts or corrupted. CSI drivers need to be idempotent and do any necessary mount verification. (<a href=https://github.com/kubernetes/kubernetes/pull/88759>#88759</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</p></li><li><p>Kubeadm: <a href=http://git.k8s.io/enhancements/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint/README.md>http://git.k8s.io/enhancements/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint/README.md</a> (<a href=https://github.com/kubernetes/kubernetes/pull/95382>#95382</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p><ul><li>The label applied to control-plane nodes "node-role.kubernetes.io/master" is now deprecated and will be removed in a future release after a GA deprecation period.</li><li>Introduce a new label "node-role.kubernetes.io/control-plane" that will be applied in parallel to "node-role.kubernetes.io/master" until the removal of the "node-role.kubernetes.io/master" label.</li><li>Make "kubeadm upgrade apply" add the "node-role.kubernetes.io/control-plane" label on existing nodes that only have the "node-role.kubernetes.io/master" label during upgrade.</li><li>Please adapt your tooling built on top of kubeadm to use the "node-role.kubernetes.io/control-plane" label.</li><li>The taint applied to control-plane nodes "node-role.kubernetes.io/master:NoSchedule" is now deprecated and will be removed in a future release after a GA deprecation period.</li><li>Apply toleration for a new, future taint "node-role.kubernetes.io/control-plane:NoSchedule" to the kubeadm CoreDNS / kube-dns managed manifests. Note that this taint is not yet applied to kubeadm control-plane nodes.</li><li>Please adapt your workloads to tolerate the same future taint preemptively.</li></ul></li><li><p>Kubeadm: improve the validation of serviceSubnet and podSubnet.
ServiceSubnet has to be limited in size, due to implementation details, and the mask can not allocate more than 20 bits.
PodSubnet validates against the corresponding cluster "--node-cidr-mask-size" of the kube-controller-manager, it fail if the values are not compatible.
kubeadm no longer sets the node-mask automatically on IPv6 deployments, you must check that your IPv6 service subnet mask is compatible with the default node mask /64 or set it accordenly.
Previously, for IPv6, if the podSubnet had a mask lower than /112, kubeadm calculated a node-mask to be multiple of eight and splitting the available bits to maximise the number used for nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/95723>#95723</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Cluster Lifecycle]</p></li><li><p>The deprecated flag --experimental-kustomize is now removed from kubeadm commands. Use --experimental-patches instead, which was introduced in 1.19. Migration information available in --help description for --experimental-patches. (<a href=https://github.com/kubernetes/kubernetes/pull/94871>#94871</a>, <a href=https://github.com/neolit123>@neolit123</a>)</p></li><li><p>Windows hyper-v container featuregate is deprecated in 1.20 and will be removed in 1.21 (<a href=https://github.com/kubernetes/kubernetes/pull/95505>#95505</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</p></li><li><p>The kube-apiserver ability to serve on an insecure port, deprecated since v1.10, has been removed. The insecure address flags <code>--address</code> and <code>--insecure-bind-address</code> have no effect in kube-apiserver and will be removed in v1.24. The insecure port flags <code>--port</code> and <code>--insecure-port</code> may only be set to 0 and will be removed in v1.24. (<a href=https://github.com/kubernetes/kubernetes/pull/95856>#95856</a>, <a href=https://github.com/knight42>@knight42</a>, [SIG API Machinery, Node, Testing])</p></li><li><p>Add dual-stack Services (alpha). This is a BREAKING CHANGE to an alpha API.
It changes the dual-stack API wrt Service from a single ipFamily field to 3
fields: ipFamilyPolicy (SingleStack, PreferDualStack, RequireDualStack),
ipFamilies (a list of families assigned), and clusterIPs (inclusive of
clusterIP). Most users do not need to set anything at all, defaulting will
handle it for them. Services are single-stack unless the user asks for
dual-stack. This is all gated by the "IPv6DualStack" feature gate. (<a href=https://github.com/kubernetes/kubernetes/pull/91824>#91824</a>, <a href=https://github.com/khenidak>@khenidak</a>) [SIG API Machinery, Apps, CLI, Network, Node, Scheduling and Testing]</p></li><li><p><code>TokenRequest</code> and <code>TokenRequestProjection</code> are now GA features. The following flags are required by the API server:</p><ul><li><code>--service-account-issuer</code>, should be set to a URL identifying the API server that will be stable over the cluster lifetime.</li><li><code>--service-account-key-file</code>, set to one or more files containing one or more public keys used to verify tokens.</li><li><code>--service-account-signing-key-file</code>, set to a file containing a private key to use to sign service account tokens. Can be the same file given to <code>kube-controller-manager</code> with <code>--service-account-private-key-file</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/95896>#95896</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Auth, Cluster Lifecycle]</li></ul></li><li><p>kubeadm: make the command "kubeadm alpha kubeconfig user" accept a "--config" flag and remove the following flags:</p><ul><li>apiserver-advertise-address / apiserver-bind-port: use either localAPIEndpoint from InitConfiguration or controlPlaneEndpoint from ClusterConfiguration.</li><li>cluster-name: use clusterName from ClusterConfiguration</li><li>cert-dir: use certificatesDir from ClusterConfiguration (<a href=https://github.com/kubernetes/kubernetes/pull/94879>#94879</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cluster Lifecycle]</li></ul></li><li><p>Resolves non-deterministic behavior of the garbage collection controller when ownerReferences with incorrect data are encountered. Events with a reason of <code>OwnerRefInvalidNamespace</code> are recorded when namespace mismatches between child and owner objects are detected. The <a href=https://github.com/kubernetes-sigs/kubectl-check-ownerreferences>kubectl-check-ownerreferences</a> tool can be run prior to upgrading to locate existing objects with invalid ownerReferences.</p><ul><li>A namespaced object with an ownerReference referencing a uid of a namespaced kind which does not exist in the same namespace is now consistently treated as though that owner does not exist, and the child object is deleted.</li><li>A cluster-scoped object with an ownerReference referencing a uid of a namespaced kind is now consistently treated as though that owner is not resolvable, and the child object is ignored by the garbage collector. (<a href=https://github.com/kubernetes/kubernetes/pull/92743>#92743</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps and Testing]</li></ul></li></ul><h2 id=changes-by-kind>Changes by Kind</h2><h3 id=deprecation>Deprecation</h3><ul><li>Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet uses a module called "dockershim" which implements CRI support for Docker and it has seen maintenance issues in the Kubernetes community. We encourage you to evaluate moving to a container runtime that is a full-fledged implementation of CRI (v1alpha1 or v1 compliant) as they become available. (<a href=https://github.com/kubernetes/kubernetes/pull/94624>#94624</a>, <a href=https://github.com/dims>@dims</a>) [SIG Node]</li><li>Kubeadm: deprecate self-hosting support. The experimental command "kubeadm alpha self-hosting" is now deprecated and will be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/95125>#95125</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: graduate the "kubeadm alpha certs" command to a parent command "kubeadm certs". The command "kubeadm alpha certs" is deprecated and will be removed in a future release. Please migrate. (<a href=https://github.com/kubernetes/kubernetes/pull/94938>#94938</a>, <a href=https://github.com/yagonobre>@yagonobre</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the deprecated "kubeadm alpha kubelet config enable-dynamic" command. To continue using the feature please defer to the guide for "Dynamic Kubelet Configuration" at k8s.io. This change also removes the parent command "kubeadm alpha kubelet" as there are no more sub-commands under it for the time being. (<a href=https://github.com/kubernetes/kubernetes/pull/94668>#94668</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the deprecated --kubelet-config flag for the command "kubeadm upgrade node" (<a href=https://github.com/kubernetes/kubernetes/pull/94869>#94869</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubectl: deprecate --delete-local-data (<a href=https://github.com/kubernetes/kubernetes/pull/95076>#95076</a>, <a href=https://github.com/dougsland>@dougsland</a>) [SIG CLI, Cloud Provider and Scalability]</li><li>Kubelet's deprecated endpoint <code>metrics/resource/v1alpha1</code> has been removed, please adopt <code>metrics/resource</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94272>#94272</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG Instrumentation and Node]</li><li>Removes deprecated scheduler metrics DeprecatedSchedulingDuration, DeprecatedSchedulingAlgorithmPredicateEvaluationSecondsDuration, DeprecatedSchedulingAlgorithmPriorityEvaluationSecondsDuration (<a href=https://github.com/kubernetes/kubernetes/pull/94884>#94884</a>, <a href=https://github.com/arghya88>@arghya88</a>) [SIG Instrumentation and Scheduling]</li><li>Scheduler alpha metrics binding_duration_seconds and scheduling_algorithm_preemption_evaluation_seconds are deprecated, Both of those metrics are now covered as part of framework_extension_point_duration_seconds, the former as a PostFilter the latter and a Bind plugin. The plan is to remove both in 1.21 (<a href=https://github.com/kubernetes/kubernetes/pull/95001>#95001</a>, <a href=https://github.com/arghya88>@arghya88</a>) [SIG Instrumentation and Scheduling]</li><li>Support 'controlplane' as a valid EgressSelection type in the EgressSelectorConfiguration API. 'Master' is deprecated and will be removed in v1.22. (<a href=https://github.com/kubernetes/kubernetes/pull/95235>#95235</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery]</li><li>The v1alpha1 PodPreset API and admission plugin has been removed with no built-in replacement. Admission webhooks can be used to modify pods on creation. (<a href=https://github.com/kubernetes/kubernetes/pull/94090>#94090</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Apps, CLI, Cloud Provider, Scalability and Testing]</li></ul><h3 id=api-change>API Change</h3><ul><li><code>TokenRequest</code> and <code>TokenRequestProjection</code> features have been promoted to GA. This feature allows generating service account tokens that are not visible in Secret objects and are tied to the lifetime of a Pod object. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection</a> for details on configuring and using this feature. The <code>TokenRequest</code> and <code>TokenRequestProjection</code> feature gates will be removed in v1.21.<ul><li>kubeadm's kube-apiserver Pod manifest now includes the following flags by default "--service-account-key-file", "--service-account-signing-key-file", "--service-account-issuer". (<a href=https://github.com/kubernetes/kubernetes/pull/93258>#93258</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Auth, Cluster Lifecycle, Storage and Testing]</li></ul></li><li>A new <code>nofuzz</code> go build tag now disables gofuzz support. Release binaries enable this. (<a href=https://github.com/kubernetes/kubernetes/pull/92491>#92491</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG API Machinery]</li><li>Add WindowsContainerResources and Annotations to CRI-API UpdateContainerResourcesRequest (<a href=https://github.com/kubernetes/kubernetes/pull/95741>#95741</a>, <a href=https://github.com/katiewasnothere>@katiewasnothere</a>) [SIG Node]</li><li>Add a <code>serving</code> and <code>terminating</code> condition to the EndpointSlice API.
<code>serving</code> tracks the readiness of endpoints regardless of their terminating state. This is distinct from <code>ready</code> since <code>ready</code> is only true when pods are not terminating.
<code>terminating</code> is true when an endpoint is terminating. For pods this is any endpoint with a deletion timestamp. (<a href=https://github.com/kubernetes/kubernetes/pull/92968>#92968</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps and Network]</li><li>Add dual-stack Services (alpha). This is a BREAKING CHANGE to an alpha API.
It changes the dual-stack API wrt Service from a single ipFamily field to 3
fields: ipFamilyPolicy (SingleStack, PreferDualStack, RequireDualStack),
ipFamilies (a list of families assigned), and clusterIPs (inclusive of
clusterIP). Most users do not need to set anything at all, defaulting will
handle it for them. Services are single-stack unless the user asks for
dual-stack. This is all gated by the "IPv6DualStack" feature gate. (<a href=https://github.com/kubernetes/kubernetes/pull/91824>#91824</a>, <a href=https://github.com/khenidak>@khenidak</a>) [SIG API Machinery, Apps, CLI, Network, Node, Scheduling and Testing]</li><li>Add support for hugepages to downward API (<a href=https://github.com/kubernetes/kubernetes/pull/86102>#86102</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>) [SIG API Machinery, Apps, CLI, Network, Node, Scheduling and Testing]</li><li>Adds kubelet alpha feature, <code>GracefulNodeShutdown</code> which makes kubelet aware of node system shutdowns and result in graceful termination of pods during a system shutdown. (<a href=https://github.com/kubernetes/kubernetes/pull/96129>#96129</a>, <a href=https://github.com/bobbypage>@bobbypage</a>) [SIG Node]</li><li>AppProtocol is now GA for Endpoints and Services. The ServiceAppProtocol feature gate will be deprecated in 1.21. (<a href=https://github.com/kubernetes/kubernetes/pull/96327>#96327</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</li><li>Automatic allocation of NodePorts for services with type LoadBalancer can now be disabled by setting the (new) parameter
Service.spec.allocateLoadBalancerNodePorts=false. The default is to allocate NodePorts for services with type LoadBalancer which is the existing behavior. (<a href=https://github.com/kubernetes/kubernetes/pull/92744>#92744</a>, <a href=https://github.com/uablrek>@uablrek</a>) [SIG Apps and Network]</li><li>Certain fields on Service objects will be automatically cleared when changing the service's <code>type</code> to a mode that does not need those fields. For example, changing from type=LoadBalancer to type=ClusterIP will clear the NodePort assignments, rather than forcing the user to clear them. (<a href=https://github.com/kubernetes/kubernetes/pull/95196>#95196</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG API Machinery, Apps, Network and Testing]</li><li>Document that ServiceTopology feature is required to use <code>service.spec.topologyKeys</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/96528>#96528</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps]</li><li>EndpointSlice has a new NodeName field guarded by the EndpointSliceNodeName feature gate.<ul><li>EndpointSlice topology field will be deprecated in an upcoming release.</li><li>EndpointSlice "IP" address type is formally removed after being deprecated in Kubernetes 1.17.</li><li>The discovery.k8s.io/v1alpha1 API is deprecated and will be removed in Kubernetes 1.21. (<a href=https://github.com/kubernetes/kubernetes/pull/96440>#96440</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps and Network]</li></ul></li><li>External facing API podresources is now available under k8s.io/kubelet/pkg/apis/ (<a href=https://github.com/kubernetes/kubernetes/pull/92632>#92632</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Node and Testing]</li><li>Fewer candidates are enumerated for preemption to improve performance in large clusters. (<a href=https://github.com/kubernetes/kubernetes/pull/94814>#94814</a>, <a href=https://github.com/adtac>@adtac</a>)</li><li>Fix conversions for custom metrics. (<a href=https://github.com/kubernetes/kubernetes/pull/94481>#94481</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Instrumentation]</li><li>GPU metrics provided by kubelet are now disabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/95184>#95184</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>)</li><li>If BoundServiceAccountTokenVolume is enabled, cluster admins can use metric <code>serviceaccount_stale_tokens_total</code> to monitor workloads that are depending on the extended tokens. If there are no such workloads, turn off extended tokens by starting <code>kube-apiserver</code> with flag <code>--service-account-extend-token-expiration=false</code> (<a href=https://github.com/kubernetes/kubernetes/pull/96273>#96273</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery and Auth]</li><li>Introduce alpha support for exec-based container registry credential provider plugins in the kubelet. (<a href=https://github.com/kubernetes/kubernetes/pull/94196>#94196</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Node and Release]</li><li>Introduces a metric source for HPAs which allows scaling based on container resource usage. (<a href=https://github.com/kubernetes/kubernetes/pull/90691>#90691</a>, <a href=https://github.com/arjunrn>@arjunrn</a>) [SIG API Machinery, Apps, Autoscaling and CLI]</li><li>Kube-apiserver now deletes expired kube-apiserver Lease objects:<ul><li>The feature is under feature gate <code>APIServerIdentity</code>.</li><li>A flag is added to kube-apiserver: <code>identity-lease-garbage-collection-check-period-seconds</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95895>#95895</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery, Apps, Auth and Testing]</li></ul></li><li>Kube-controller-manager: volume plugins can be restricted from contacting local and loopback addresses by setting <code>--volume-host-allow-local-loopback=false</code>, or from contacting specific CIDR ranges by setting <code>--volume-host-cidr-denylist</code> (for example, <code>--volume-host-cidr-denylist=127.0.0.1/28,feed::/16</code>) (<a href=https://github.com/kubernetes/kubernetes/pull/91785>#91785</a>, <a href=https://github.com/mattcary>@mattcary</a>) [SIG API Machinery, Apps, Auth, CLI, Network, Node, Storage and Testing]</li><li>Migrate scheduler, controller-manager and cloud-controller-manager to use LeaseLock (<a href=https://github.com/kubernetes/kubernetes/pull/94603>#94603</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery, Apps, Cloud Provider and Scheduling]</li><li>Modify DNS-1123 error messages to indicate that RFC 1123 is not followed exactly (<a href=https://github.com/kubernetes/kubernetes/pull/94182>#94182</a>, <a href=https://github.com/mattfenwick>@mattfenwick</a>) [SIG API Machinery, Apps, Auth, Network and Node]</li><li>Move configurable fsgroup change policy for pods to beta (<a href=https://github.com/kubernetes/kubernetes/pull/96376>#96376</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Apps and Storage]</li><li>New flag is introduced, i.e. --topology-manager-scope=container|pod.
The default value is the "container" scope. (<a href=https://github.com/kubernetes/kubernetes/pull/92967>#92967</a>, <a href=https://github.com/cezaryzukowski>@cezaryzukowski</a>) [SIG Instrumentation, Node and Testing]</li><li>New parameter <code>defaultingType</code> for <code>PodTopologySpread</code> plugin allows to use k8s defined or user provided default constraints (<a href=https://github.com/kubernetes/kubernetes/pull/95048>#95048</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>NodeAffinity plugin can be configured with AddedAffinity. (<a href=https://github.com/kubernetes/kubernetes/pull/96202>#96202</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Node, Scheduling and Testing]</li><li>Promote RuntimeClass feature to GA.
Promote node.k8s.io API groups from v1beta1 to v1. (<a href=https://github.com/kubernetes/kubernetes/pull/95718>#95718</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>) [SIG Apps, Auth, Node, Scheduling and Testing]</li><li>Reminder: The labels "failure-domain.beta.kubernetes.io/zone" and "failure-domain.beta.kubernetes.io/region" are deprecated in favor of "topology.kubernetes.io/zone" and "topology.kubernetes.io/region" respectively. All users of the "failure-domain.beta..." labels should switch to the "topology..." equivalents. (<a href=https://github.com/kubernetes/kubernetes/pull/96033>#96033</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG API Machinery, Apps, CLI, Cloud Provider, Network, Node, Scheduling, Storage and Testing]</li><li>Server Side Apply now treats LabelSelector fields as atomic (meaning the entire selector is managed by a single writer and updated together), since they contain interrelated and inseparable fields that do not merge in intuitive ways. (<a href=https://github.com/kubernetes/kubernetes/pull/93901>#93901</a>, <a href=https://github.com/jpbetz>@jpbetz</a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Storage and Testing]</li><li>Services will now have a <code>clusterIPs</code> field to go with <code>clusterIP</code>. <code>clusterIPs[0]</code> is a synonym for <code>clusterIP</code> and will be syncronized on create and update operations. (<a href=https://github.com/kubernetes/kubernetes/pull/95894>#95894</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG Network]</li><li>The ServiceAccountIssuerDiscovery feature gate is now Beta and enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/91921>#91921</a>, <a href=https://github.com/mtaufen>@mtaufen</a>) [SIG Auth]</li><li>The status of v1beta1 CRDs without "preserveUnknownFields:false" now shows a violation, "spec.preserveUnknownFields: Invalid value: true: must be false". (<a href=https://github.com/kubernetes/kubernetes/pull/93078>#93078</a>, <a href=https://github.com/vareti>@vareti</a>)</li><li>The usage of mixed protocol values in the same LoadBalancer Service is possible if the new feature gate MixedProtocolLBService is enabled. The feature gate is disabled by default. The user has to enable it for the API Server. (<a href=https://github.com/kubernetes/kubernetes/pull/94028>#94028</a>, <a href=https://github.com/janosi>@janosi</a>) [SIG API Machinery and Apps]</li><li>This PR will introduce a feature gate CSIServiceAccountToken with two additional fields in <code>CSIDriverSpec</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/93130>#93130</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Apps, Auth, CLI, Network, Node, Storage and Testing]</li><li>Users can try the cronjob controller v2 using the feature gate. This will be the default controller in future releases. (<a href=https://github.com/kubernetes/kubernetes/pull/93370>#93370</a>, <a href=https://github.com/alaypatel07>@alaypatel07</a>) [SIG API Machinery, Apps, Auth and Testing]</li><li>VolumeSnapshotDataSource moves to GA in 1.20 release (<a href=https://github.com/kubernetes/kubernetes/pull/95282>#95282</a>, <a href=https://github.com/xing-yang>@xing-yang</a>) [SIG Apps]</li><li>WinOverlay feature graduated to beta (<a href=https://github.com/kubernetes/kubernetes/pull/94807>#94807</a>, <a href=https://github.com/ksubrmnn>@ksubrmnn</a>) [SIG Windows]</li></ul><h3 id=feature>Feature</h3><ul><li><p><strong>Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.</strong>:</p></li><li><p>A new metric <code>apiserver_request_filter_duration_seconds</code> has been introduced that
measures request filter latency in seconds. (<a href=https://github.com/kubernetes/kubernetes/pull/95207>#95207</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery and Instrumentation]</p></li><li><p>A new set of alpha metrics are reported by the Kubernetes scheduler under the <code>/metrics/resources</code> endpoint that allow administrators to easily see the resource consumption (requests and limits for all resources on the pods) and compare it to actual pod usage or node capacity. (<a href=https://github.com/kubernetes/kubernetes/pull/94866>#94866</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG API Machinery, Instrumentation, Node and Scheduling]</p></li><li><p>Add --experimental-logging-sanitization flag enabling runtime protection from leaking sensitive data in logs (<a href=https://github.com/kubernetes/kubernetes/pull/96370>#96370</a>, <a href=https://github.com/serathius>@serathius</a>) [SIG API Machinery, Cluster Lifecycle and Instrumentation]</p></li><li><p>Add a StorageVersionAPI feature gate that makes API server update storageversions before serving certain write requests.
This feature allows the storage migrator to manage storage migration for built-in resources.
Enabling internal.apiserver.k8s.io/v1alpha1 API and APIServerIdentity feature gate are required to use this feature. (<a href=https://github.com/kubernetes/kubernetes/pull/93873>#93873</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery, Auth and Testing]</p></li><li><p>Add a metric for time taken to perform recursive permission change (<a href=https://github.com/kubernetes/kubernetes/pull/95866>#95866</a>, <a href=https://github.com/JornShen>@JornShen</a>) [SIG Instrumentation and Storage]</p></li><li><p>Add a new <code>vSphere</code> metric: <code>cloudprovider_vsphere_vcenter_versions</code>. It's content show <code>vCenter</code> hostnames with the associated server version. (<a href=https://github.com/kubernetes/kubernetes/pull/94526>#94526</a>, <a href=https://github.com/Danil-Grigorev>@Danil-Grigorev</a>) [SIG Cloud Provider and Instrumentation]</p></li><li><p>Add a new flag to set priority for the kubelet on Windows nodes so that workloads cannot overwhelm the node there by disrupting kubelet process. (<a href=https://github.com/kubernetes/kubernetes/pull/96051>#96051</a>, <a href=https://github.com/ravisantoshgudimetla>@ravisantoshgudimetla</a>) [SIG Node and Windows]</p></li><li><p>Add feature to size memory backed volumes (<a href=https://github.com/kubernetes/kubernetes/pull/94444>#94444</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>) [SIG Storage and Testing]</p></li><li><p>Add foreground cascading deletion to kubectl with the new <code>kubectl delete foreground|background|orphan</code> option. (<a href=https://github.com/kubernetes/kubernetes/pull/93384>#93384</a>, <a href=https://github.com/zhouya0>@zhouya0</a>)</p></li><li><p>Add metrics for azure service operations (route and loadbalancer). (<a href=https://github.com/kubernetes/kubernetes/pull/94124>#94124</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider and Instrumentation]</p></li><li><p>Add network rule support in Azure account creation. (<a href=https://github.com/kubernetes/kubernetes/pull/94239>#94239</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>)</p></li><li><p>Add node_authorizer_actions_duration_seconds metric that can be used to estimate load to node authorizer. (<a href=https://github.com/kubernetes/kubernetes/pull/92466>#92466</a>, <a href=https://github.com/mborsz>@mborsz</a>) [SIG API Machinery, Auth and Instrumentation]</p></li><li><p>Add pod_ based CPU and memory metrics to Kubelet's /metrics/resource endpoint (<a href=https://github.com/kubernetes/kubernetes/pull/95839>#95839</a>, <a href=https://github.com/egernst>@egernst</a>) [SIG Instrumentation, Node and Testing]</p></li><li><p>Added <code>get-users</code> and <code>delete-user</code> to the <code>kubectl config</code> subcommand (<a href=https://github.com/kubernetes/kubernetes/pull/89840>#89840</a>, <a href=https://github.com/eddiezane>@eddiezane</a>) [SIG CLI]</p></li><li><p>Added counter metric "apiserver_request_self" to count API server self-requests with labels for verb, resource, and subresource. (<a href=https://github.com/kubernetes/kubernetes/pull/94288>#94288</a>, <a href=https://github.com/LogicalShark>@LogicalShark</a>) [SIG API Machinery, Auth, Instrumentation and Scheduling]</p></li><li><p>Added new k8s.io/component-helpers repository providing shared helper code for (core) components. (<a href=https://github.com/kubernetes/kubernetes/pull/92507>#92507</a>, <a href=https://github.com/ingvagabund>@ingvagabund</a>) [SIG Apps, Node, Release and Scheduling]</p></li><li><p>Adds <code>create ingress</code> command to <code>kubectl</code> (<a href=https://github.com/kubernetes/kubernetes/pull/78153>#78153</a>, <a href=https://github.com/amimof>@amimof</a>) [SIG CLI and Network]</p></li><li><p>Adds a headless service on node-local-cache addon. (<a href=https://github.com/kubernetes/kubernetes/pull/88412>#88412</a>, <a href=https://github.com/stafot>@stafot</a>) [SIG Cloud Provider and Network]</p></li><li><p>Allow cross compilation of kubernetes on different platforms. (<a href=https://github.com/kubernetes/kubernetes/pull/94403>#94403</a>, <a href=https://github.com/bnrjee>@bnrjee</a>) [SIG Release]</p></li><li><p>Azure: Support multiple services sharing one IP address (<a href=https://github.com/kubernetes/kubernetes/pull/94991>#94991</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>CRDs: For structural schemas, non-nullable null map fields will now be dropped and defaulted if a default is available. null items in list will continue being preserved, and fail validation if not nullable. (<a href=https://github.com/kubernetes/kubernetes/pull/95423>#95423</a>, <a href=https://github.com/apelisse>@apelisse</a>) [SIG API Machinery]</p></li><li><p>Changed: default "Accept: <em>/</em>" header added to HTTP probes. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes>https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes</a> (<a href=https://github.com/kubernetes/website/pull/24756>https://github.com/kubernetes/website/pull/24756</a>) (<a href=https://github.com/kubernetes/kubernetes/pull/95641>#95641</a>, <a href=https://github.com/fonsecas72>@fonsecas72</a>) [SIG Network and Node]</p></li><li><p>Client-go credential plugins can now be passed in the current cluster information via the KUBERNETES_EXEC_INFO environment variable. (<a href=https://github.com/kubernetes/kubernetes/pull/95489>#95489</a>, <a href=https://github.com/ankeesler>@ankeesler</a>) [SIG API Machinery and Auth]</p></li><li><p>Command to start network proxy changes from 'KUBE_ENABLE_EGRESS_VIA_KONNECTIVITY_SERVICE ./cluster/kube-up.sh' to 'KUBE_ENABLE_KONNECTIVITY_SERVICE=true ./hack/kube-up.sh' (<a href=https://github.com/kubernetes/kubernetes/pull/92669>#92669</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG Cloud Provider]</p></li><li><p>Configure AWS LoadBalancer health check protocol via service annotations. (<a href=https://github.com/kubernetes/kubernetes/pull/94546>#94546</a>, <a href=https://github.com/kishorj>@kishorj</a>)</p></li><li><p>DefaultPodTopologySpread graduated to Beta. The feature gate is enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/95631>#95631</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling and Testing]</p></li><li><p>E2e test for PodFsGroupChangePolicy (<a href=https://github.com/kubernetes/kubernetes/pull/96247>#96247</a>, <a href=https://github.com/saikat-royc>@saikat-royc</a>) [SIG Storage and Testing]</p></li><li><p>Ephemeral containers now apply the same API defaults as initContainers and containers (<a href=https://github.com/kubernetes/kubernetes/pull/94896>#94896</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Apps and CLI]</p></li><li><p>Gradudate the Pod Resources API to G.A
Introduces the pod_resources_endpoint_requests_total metric which tracks the total number of requests to the pod resources API (<a href=https://github.com/kubernetes/kubernetes/pull/92165>#92165</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Instrumentation, Node and Testing]</p></li><li><p>In dual-stack bare-metal clusters, you can now pass dual-stack IPs to <code>kubelet --node-ip</code>.
eg: <code>kubelet --node-ip 10.1.0.5,fd01::0005</code>. This is not yet supported for non-bare-metal
clusters.</p><p>In dual-stack clusters where nodes have dual-stack addresses, hostNetwork pods
will now get dual-stack PodIPs. (<a href=https://github.com/kubernetes/kubernetes/pull/95239>#95239</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Network and Node]</p></li><li><p>Introduce api-extensions category which will return: mutating admission configs, validating admission configs, CRDs and APIServices when used in kubectl get, for example. (<a href=https://github.com/kubernetes/kubernetes/pull/95603>#95603</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG API Machinery]</p></li><li><p>Introduces a new GCE specific cluster creation variable KUBE_PROXY_DISABLE. When set to true, this will skip over the creation of kube-proxy (whether the daemonset or static pod). This can be used to control the lifecycle of kube-proxy separately from the lifecycle of the nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/91977>#91977</a>, <a href=https://github.com/varunmar>@varunmar</a>) [SIG Cloud Provider]</p></li><li><p>Kube-apiserver now maintains a Lease object to identify itself:</p><ul><li>The feature is under feature gate <code>APIServerIdentity</code>.</li><li>Two flags are added to kube-apiserver: <code>identity-lease-duration-seconds</code>, <code>identity-lease-renew-interval-seconds</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95533>#95533</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery]</li></ul></li><li><p>Kube-apiserver: The timeout used when making health check calls to etcd can now be configured with <code>--etcd-healthcheck-timeout</code>. The default timeout is 2 seconds, matching the previous behavior. (<a href=https://github.com/kubernetes/kubernetes/pull/93244>#93244</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG API Machinery]</p></li><li><p>Kube-apiserver: added support for compressing rotated audit log files with <code>--audit-log-compress</code> (<a href=https://github.com/kubernetes/kubernetes/pull/94066>#94066</a>, <a href=https://github.com/lojies>@lojies</a>) [SIG API Machinery and Auth]</p></li><li><p>Kubeadm now prints warnings instead of throwing errors if the current system time is outside of the NotBefore and NotAfter bounds of a loaded certificate. (<a href=https://github.com/kubernetes/kubernetes/pull/94504>#94504</a>, <a href=https://github.com/neolit123>@neolit123</a>)</p></li><li><p>Kubeadm: Add a preflight check that the control-plane node has at least 1700MB of RAM (<a href=https://github.com/kubernetes/kubernetes/pull/93275>#93275</a>, <a href=https://github.com/xlgao-zju>@xlgao-zju</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: add the "--cluster-name" flag to the "kubeadm alpha kubeconfig user" to allow configuring the cluster name in the generated kubeconfig file (<a href=https://github.com/kubernetes/kubernetes/pull/93992>#93992</a>, <a href=https://github.com/prabhu43>@prabhu43</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: add the "--kubeconfig" flag to the "kubeadm init phase upload-certs" command to allow users to pass a custom location for a kubeconfig file. (<a href=https://github.com/kubernetes/kubernetes/pull/94765>#94765</a>, <a href=https://github.com/zhanw15>@zhanw15</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make etcd pod request 100m CPU, 100Mi memory and 100Mi ephemeral_storage by default (<a href=https://github.com/kubernetes/kubernetes/pull/94479>#94479</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make the command "kubeadm alpha kubeconfig user" accept a "--config" flag and remove the following flags:</p><ul><li>apiserver-advertise-address / apiserver-bind-port: use either localAPIEndpoint from InitConfiguration or controlPlaneEndpoint from ClusterConfiguration.</li><li>cluster-name: use clusterName from ClusterConfiguration</li><li>cert-dir: use certificatesDir from ClusterConfiguration (<a href=https://github.com/kubernetes/kubernetes/pull/94879>#94879</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cluster Lifecycle]</li></ul></li><li><p>Kubectl create now supports creating ingress objects. (<a href=https://github.com/kubernetes/kubernetes/pull/94327>#94327</a>, <a href=https://github.com/rikatz>@rikatz</a>) [SIG CLI and Network]</p></li><li><p>Kubectl rollout history sts/sts-name --revision=some-revision will start showing the detailed view of the sts on that specified revision (<a href=https://github.com/kubernetes/kubernetes/pull/86506>#86506</a>, <a href=https://github.com/dineshba>@dineshba</a>) [SIG CLI]</p></li><li><p>Kubectl: Previously users cannot provide arguments to a external diff tool via KUBECTL_EXTERNAL_DIFF env. This release now allow users to specify args to KUBECTL_EXTERNAL_DIFF env. (<a href=https://github.com/kubernetes/kubernetes/pull/95292>#95292</a>, <a href=https://github.com/dougsland>@dougsland</a>) [SIG CLI]</p></li><li><p>Kubemark now supports both real and hollow nodes in a single cluster. (<a href=https://github.com/kubernetes/kubernetes/pull/93201>#93201</a>, <a href=https://github.com/ellistarn>@ellistarn</a>) [SIG Scalability]</p></li><li><p>Kubernetes E2E test image manifest lists now contain Windows images. (<a href=https://github.com/kubernetes/kubernetes/pull/77398>#77398</a>, <a href=https://github.com/claudiubelu>@claudiubelu</a>) [SIG Testing and Windows]</p></li><li><p>Kubernetes is now built using go1.15.2</p><ul><li><p>build: Update to <a href=mailto:k/repo-infra@v0.1.1>k/repo-infra@v0.1.1</a> (supports go1.15.2)</p></li><li><p>build: Use go-runner:buster-v2.0.1 (built using go1.15.1)</p></li><li><p>bazel: Replace --features with Starlark build settings flag</p></li><li><p>hack/lib/util.sh: some bash cleanups</p><ul><li>switched one spot to use kube::logging</li><li>make kube::util::find-binary return an error when it doesn't find
anything so that hack scripts fail fast instead of with '' binary not
found errors.</li><li>this required deleting some genfeddoc stuff. the binary no longer
exists in k/k repo since we removed federation/, and I don't see it
in <a href=https://github.com/kubernetes-sigs/kubefed/>https://github.com/kubernetes-sigs/kubefed/</a> either. I'm assuming
that it's gone for good now.</li></ul></li><li><p>bazel: output go_binary rule directly from go_binary_conditional_pure</p><p>From: <a href=https://github.com/mikedanese>@mikedanese</a>:
Instead of aliasing. Aliases are annoying in a number of ways. This is
specifically bugging me now because they make the action graph harder to
analyze programmatically. By using aliases here, we would need to handle
potentially aliased go_binary targets and dereference to the effective
target.</p><p>The comment references an issue with <code>pure = select(...)</code> which appears
to be resolved considering this now builds.</p></li><li><p>make kube::util::find-binary not dependent on bazel-out/ structure</p><p>Implement an aspect that outputs go_build_mode metadata for go binaries,
and use that during binary selection. (<a href=https://github.com/kubernetes/kubernetes/pull/94449>#94449</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Architecture, CLI, Cluster Lifecycle, Node, Release and Testing]</p></li></ul></li><li><p>Kubernetes is now built using go1.15.5</p><ul><li>build: Update to <a href=mailto:k/repo-infra@v0.1.2>k/repo-infra@v0.1.2</a> (supports go1.15.5) (<a href=https://github.com/kubernetes/kubernetes/pull/95776>#95776</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Cloud Provider, Instrumentation, Release and Testing]</li></ul></li><li><p>New default scheduling plugins order reduces scheduling and preemption latency when taints and node affinity are used (<a href=https://github.com/kubernetes/kubernetes/pull/95539>#95539</a>, <a href=https://github.com/soulxu>@soulxu</a>) [SIG Scheduling]</p></li><li><p>Only update Azure data disks when attach/detach (<a href=https://github.com/kubernetes/kubernetes/pull/94265>#94265</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Promote SupportNodePidsLimit to GA to provide node-to-pod PID isolation.
Promote SupportPodPidsLimit to GA to provide ability to limit PIDs per pod. (<a href=https://github.com/kubernetes/kubernetes/pull/94140>#94140</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>)</p></li><li><p>SCTP support in API objects (Pod, Service, NetworkPolicy) is now GA.
Note that this has no effect on whether SCTP is enabled on nodes at the kernel level,
and note that some cloud platforms and network plugins do not support SCTP traffic. (<a href=https://github.com/kubernetes/kubernetes/pull/95566>#95566</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Apps and Network]</p></li><li><p>Scheduler now ignores Pod update events if the resourceVersion of old and new Pods are identical. (<a href=https://github.com/kubernetes/kubernetes/pull/96071>#96071</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</p></li><li><p>Scheduling Framework: expose Run[Pre]ScorePlugins functions to PreemptionHandle which can be used in PostFilter extention point. (<a href=https://github.com/kubernetes/kubernetes/pull/93534>#93534</a>, <a href=https://github.com/everpeace>@everpeace</a>) [SIG Scheduling and Testing]</p></li><li><p>SelectorSpreadPriority maps to PodTopologySpread plugin when DefaultPodTopologySpread feature is enabled (<a href=https://github.com/kubernetes/kubernetes/pull/95448>#95448</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</p></li><li><p>Send GCE node startup scripts logs to console and journal. (<a href=https://github.com/kubernetes/kubernetes/pull/95311>#95311</a>, <a href=https://github.com/karan>@karan</a>)</p></li><li><p>SetHostnameAsFQDN has been graduated to Beta and therefore it is enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/95267>#95267</a>, <a href=https://github.com/javidiaz>@javidiaz</a>) [SIG Node]</p></li><li><p>Support [service.beta.kubernetes.io/azure-pip-ip-tags] annotations to allow customers to specify ip-tags to influence public-ip creation in Azure [Tag1=Value1, Tag2=Value2, etc.] (<a href=https://github.com/kubernetes/kubernetes/pull/94114>#94114</a>, <a href=https://github.com/MarcPow>@MarcPow</a>) [SIG Cloud Provider]</p></li><li><p>Support custom tags for cloud provider managed resources (<a href=https://github.com/kubernetes/kubernetes/pull/96450>#96450</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Support customize load balancer health probe protocol and request path (<a href=https://github.com/kubernetes/kubernetes/pull/96338>#96338</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Support for Windows container images (OS Versions: 1809, 1903, 1909, 2004) was added the pause:3.4 image. (<a href=https://github.com/kubernetes/kubernetes/pull/91452>#91452</a>, <a href=https://github.com/claudiubelu>@claudiubelu</a>) [SIG Node, Release and Windows]</p></li><li><p>Support multiple standard load balancers in one cluster (<a href=https://github.com/kubernetes/kubernetes/pull/96111>#96111</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>The beta <code>RootCAConfigMap</code> feature gate is enabled by default and causes kube-controller-manager to publish a "kube-root-ca.crt" ConfigMap to every namespace. This ConfigMap contains a CA bundle used for verifying connections to the kube-apiserver. (<a href=https://github.com/kubernetes/kubernetes/pull/96197>#96197</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Apps, Auth and Testing]</p></li><li><p>The kubelet_runtime_operations_duration_seconds metric buckets were set to 0.005 0.0125 0.03125 0.078125 0.1953125 0.48828125 1.220703125 3.0517578125 7.62939453125 19.073486328125 47.6837158203125 119.20928955078125 298.0232238769531 and 745.0580596923828 seconds (<a href=https://github.com/kubernetes/kubernetes/pull/96054>#96054</a>, <a href=https://github.com/alvaroaleman>@alvaroaleman</a>) [SIG Instrumentation and Node]</p></li><li><p>There is a new pv_collector_total_pv_count metric that counts persistent volumes by the volume plugin name and volume mode. (<a href=https://github.com/kubernetes/kubernetes/pull/95719>#95719</a>, <a href=https://github.com/tsmetana>@tsmetana</a>) [SIG Apps, Instrumentation, Storage and Testing]</p></li><li><p>Volume snapshot e2e test to validate PVC and VolumeSnapshotContent finalizer (<a href=https://github.com/kubernetes/kubernetes/pull/95863>#95863</a>, <a href=https://github.com/RaunakShah>@RaunakShah</a>) [SIG Cloud Provider, Storage and Testing]</p></li><li><p>Warns user when executing kubectl apply/diff to resource currently being deleted. (<a href=https://github.com/kubernetes/kubernetes/pull/95544>#95544</a>, <a href=https://github.com/SaiHarshaK>@SaiHarshaK</a>) [SIG CLI]</p></li><li><p><code>kubectl alpha debug</code> has graduated to beta and is now <code>kubectl debug</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/96138>#96138</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI and Testing]</p></li><li><p><code>kubectl debug</code> gains support for changing container images when copying a pod for debugging, similar to how <code>kubectl set image</code> works. See <code>kubectl help debug</code> for more information. (<a href=https://github.com/kubernetes/kubernetes/pull/96058>#96058</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI]</p></li></ul><h3 id=documentation>Documentation</h3><ul><li>Fake dynamic client: document that List does not preserve TypeMeta in UnstructuredList (<a href=https://github.com/kubernetes/kubernetes/pull/95117>#95117</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery]</li><li>Kubelet: remove alpha warnings for CNI flags. (<a href=https://github.com/kubernetes/kubernetes/pull/94508>#94508</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Network and Node]</li><li>Updates docs and guidance on cloud provider InstancesV2 and Zones interface for external cloud providers:<ul><li>removes experimental warning for InstancesV2</li><li>document that implementation of InstancesV2 will disable calls to Zones</li><li>deprecate Zones in favor of InstancesV2 (<a href=https://github.com/kubernetes/kubernetes/pull/96397>#96397</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Cloud Provider]</li></ul></li></ul><h3 id=failing-test>Failing Test</h3><ul><li>Resolves an issue running Ingress conformance tests on clusters which use finalizers on Ingress objects to manage releasing load balancer resources (<a href=https://github.com/kubernetes/kubernetes/pull/96742>#96742</a>, <a href=https://github.com/spencerhance>@spencerhance</a>) [SIG Network and Testing]</li><li>The Conformance test "validates that there is no conflict between pods with same hostPort but different hostIP and protocol" now validates the connectivity to each hostPort, in addition to the functionality. (<a href=https://github.com/kubernetes/kubernetes/pull/96627>#96627</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Scheduling and Testing]</li></ul><h3 id=bug-or-regression>Bug or Regression</h3><ul><li><p>Add kubectl wait --ignore-not-found flag (<a href=https://github.com/kubernetes/kubernetes/pull/90969>#90969</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</p></li><li><p>Added support to kube-proxy for externalTrafficPolicy=Local setting via Direct Server Return (DSR) load balancers on Windows. (<a href=https://github.com/kubernetes/kubernetes/pull/93166>#93166</a>, <a href=https://github.com/elweb9858>@elweb9858</a>) [SIG Network]</p></li><li><p>Alter wording to describe pods using a pvc (<a href=https://github.com/kubernetes/kubernetes/pull/95635>#95635</a>, <a href=https://github.com/RaunakShah>@RaunakShah</a>) [SIG CLI]</p></li><li><p>An issues preventing volume expand controller to annotate the PVC with <code>volume.kubernetes.io/storage-resizer</code> when the PVC StorageClass is already updated to the out-of-tree provisioner is now fixed. (<a href=https://github.com/kubernetes/kubernetes/pull/94489>#94489</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG API Machinery, Apps and Storage]</p></li><li><p>Azure ARM client: don't segfault on empty response and http error (<a href=https://github.com/kubernetes/kubernetes/pull/94078>#94078</a>, <a href=https://github.com/bpineau>@bpineau</a>) [SIG Cloud Provider]</p></li><li><p>Azure armclient backoff step defaults to 1 (no retry). (<a href=https://github.com/kubernetes/kubernetes/pull/94180>#94180</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</p></li><li><p>Azure: fix a bug that kube-controller-manager would panic if wrong Azure VMSS name is configured (<a href=https://github.com/kubernetes/kubernetes/pull/94306>#94306</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cloud Provider]</p></li><li><p>Both apiserver_request_duration_seconds metrics and RequestReceivedTimestamp fields of an audit event now take into account the time a request spends in the apiserver request filters. (<a href=https://github.com/kubernetes/kubernetes/pull/94903>#94903</a>, <a href=https://github.com/tkashem>@tkashem</a>)</p></li><li><p>Build/lib/release: Explicitly use '--platform' in building server images</p><p>When we switched to go-runner for building the apiserver,
controller-manager, and scheduler server components, we no longer
reference the individual architectures in the image names, specifically
in the 'FROM' directive of the server image Dockerfiles.</p><p>As a result, server images for non-amd64 images copy in the go-runner
amd64 binary instead of the go-runner that matches that architecture.</p><p>This commit explicitly sets the '--platform=linux/${arch}' to ensure
we're pulling the correct go-runner arch from the manifest list.</p><p>Before:
<code>FROM ${base_image}</code></p><p>After:
<code>FROM --platform=linux/${arch} ${base_image}</code> (<a href=https://github.com/kubernetes/kubernetes/pull/94552>#94552</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release]</p></li><li><p>Bump node-problem-detector version to v0.8.5 to fix OOM detection in with Linux kernels 5.1+ (<a href=https://github.com/kubernetes/kubernetes/pull/96716>#96716</a>, <a href=https://github.com/tosi3k>@tosi3k</a>) [SIG Cloud Provider, Scalability and Testing]</p></li><li><p>CSIDriver object can be deployed during volume attachment. (<a href=https://github.com/kubernetes/kubernetes/pull/93710>#93710</a>, <a href=https://github.com/Jiawei0227>@Jiawei0227</a>) [SIG Apps, Node, Storage and Testing]</p></li><li><p>Ceph RBD volume expansion now works even when ceph.conf was not provided. (<a href=https://github.com/kubernetes/kubernetes/pull/92027>#92027</a>, <a href=https://github.com/juliantaylor>@juliantaylor</a>)</p></li><li><p>Change plugin name in fsgroupapplymetrics of csi and flexvolume to distinguish different driver (<a href=https://github.com/kubernetes/kubernetes/pull/95892>#95892</a>, <a href=https://github.com/JornShen>@JornShen</a>) [SIG Instrumentation, Storage and Testing]</p></li><li><p>Change the calculation of pod UIDs so that static pods get a unique value - will cause all containers to be killed and recreated after in-place upgrade. (<a href=https://github.com/kubernetes/kubernetes/pull/87461>#87461</a>, <a href=https://github.com/bboreham>@bboreham</a>) [SIG Node]</p></li><li><p>Change the mount way from systemd to normal mount except ceph and glusterfs intree-volume. (<a href=https://github.com/kubernetes/kubernetes/pull/94916>#94916</a>, <a href=https://github.com/smileusd>@smileusd</a>) [SIG Apps, Cloud Provider, Network, Node, Storage and Testing]</p></li><li><p>Changes to timeout parameter handling in 1.20.0-beta.2 have been reverted to avoid breaking backwards compatibility with existing clients. (<a href=https://github.com/kubernetes/kubernetes/pull/96727>#96727</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Testing]</p></li><li><p>Clear UDP conntrack entry on endpoint changes when using nodeport (<a href=https://github.com/kubernetes/kubernetes/pull/71573>#71573</a>, <a href=https://github.com/JacobTanenbaum>@JacobTanenbaum</a>) [SIG Network]</p></li><li><p>Cloud node controller: handle empty providerID from getProviderID (<a href=https://github.com/kubernetes/kubernetes/pull/95342>#95342</a>, <a href=https://github.com/nicolehanjing>@nicolehanjing</a>) [SIG Cloud Provider]</p></li><li><p>Disable watchcache for events (<a href=https://github.com/kubernetes/kubernetes/pull/96052>#96052</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery]</p></li><li><p>Disabled <code>LocalStorageCapacityIsolation</code> feature gate is honored during scheduling. (<a href=https://github.com/kubernetes/kubernetes/pull/96092>#96092</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</p></li><li><p>Do not fail sorting empty elements. (<a href=https://github.com/kubernetes/kubernetes/pull/94666>#94666</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</p></li><li><p>Dual-stack: make nodeipam compatible with existing single-stack clusters when dual-stack feature gate become enabled by default (<a href=https://github.com/kubernetes/kubernetes/pull/90439>#90439</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG API Machinery]</p></li><li><p>Duplicate owner reference entries in create/update/patch requests now get deduplicated by the API server. The client sending the request now receives a warning header in the API response. Clients should stop sending requests with duplicate owner references. The API server may reject such requests as early as 1.24. (<a href=https://github.com/kubernetes/kubernetes/pull/96185>#96185</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery and Testing]</p></li><li><p>Endpoint slice controller now mirrors parent's service label to its corresponding endpoint slices. (<a href=https://github.com/kubernetes/kubernetes/pull/94443>#94443</a>, <a href=https://github.com/aojea>@aojea</a>)</p></li><li><p>Ensure getPrimaryInterfaceID not panic when network interfaces for Azure VMSS are null (<a href=https://github.com/kubernetes/kubernetes/pull/94355>#94355</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>Exposes and sets a default timeout for the SubjectAccessReview client for DelegatingAuthorizationOptions (<a href=https://github.com/kubernetes/kubernetes/pull/95725>#95725</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery and Cloud Provider]</p></li><li><p>Exposes and sets a default timeout for the TokenReview client for DelegatingAuthenticationOptions (<a href=https://github.com/kubernetes/kubernetes/pull/96217>#96217</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery and Cloud Provider]</p></li><li><p>Fix CVE-2020-8555 for Quobyte client connections. (<a href=https://github.com/kubernetes/kubernetes/pull/95206>#95206</a>, <a href=https://github.com/misterikkit>@misterikkit</a>) [SIG Storage]</p></li><li><p>Fix IP fragmentation of UDP and TCP packets not supported issues on LoadBalancer rules (<a href=https://github.com/kubernetes/kubernetes/pull/96464>#96464</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Fix a bug that DefaultPreemption plugin is disabled when using (legacy) scheduler policy. (<a href=https://github.com/kubernetes/kubernetes/pull/96439>#96439</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling and Testing]</p></li><li><p>Fix a bug where loadbalancer deletion gets stuck because of missing resource group. (<a href=https://github.com/kubernetes/kubernetes/pull/93962>#93962</a>, <a href=https://github.com/phiphi282>@phiphi282</a>)</p></li><li><p>Fix a concurrent map writes error in kubelet (<a href=https://github.com/kubernetes/kubernetes/pull/93773>#93773</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Node]</p></li><li><p>Fix a panic in <code>kubectl debug</code> when a pod has multiple init or ephemeral containers. (<a href=https://github.com/kubernetes/kubernetes/pull/94580>#94580</a>, <a href=https://github.com/kiyoshim55>@kiyoshim55</a>)</p></li><li><p>Fix a regression where kubeadm bails out with a fatal error when an optional version command line argument is supplied to the "kubeadm upgrade plan" command (<a href=https://github.com/kubernetes/kubernetes/pull/94421>#94421</a>, <a href=https://github.com/rosti>@rosti</a>) [SIG Cluster Lifecycle]</p></li><li><p>Fix azure disk attach failure for disk size bigger than 4TB (<a href=https://github.com/kubernetes/kubernetes/pull/95463>#95463</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix azure disk data loss issue on Windows when unmount disk (<a href=https://github.com/kubernetes/kubernetes/pull/95456>#95456</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fix azure file migration panic (<a href=https://github.com/kubernetes/kubernetes/pull/94853>#94853</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix bug in JSON path parser where an error occurs when a range is empty (<a href=https://github.com/kubernetes/kubernetes/pull/95933>#95933</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG API Machinery]</p></li><li><p>Fix client-go prometheus metrics to correctly present the API path accessed in some environments. (<a href=https://github.com/kubernetes/kubernetes/pull/74363>#74363</a>, <a href=https://github.com/aanm>@aanm</a>) [SIG API Machinery]</p></li><li><p>Fix detach azure disk issue when vm not exist (<a href=https://github.com/kubernetes/kubernetes/pull/95177>#95177</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix etcd_object_counts metric reported by kube-apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/94773>#94773</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery]</p></li><li><p>Fix incorrectly reported verbs for kube-apiserver metrics for CRD objects (<a href=https://github.com/kubernetes/kubernetes/pull/93523>#93523</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Instrumentation]</p></li><li><p>Fix k8s.io/apimachinery/pkg/api/meta.SetStatusCondition to update ObservedGeneration (<a href=https://github.com/kubernetes/kubernetes/pull/95961>#95961</a>, <a href=https://github.com/KnicKnic>@KnicKnic</a>) [SIG API Machinery]</p></li><li><p>Fix kubectl SchemaError on CRDs with schema using x-kubernetes-preserve-unknown-fields on array types. (<a href=https://github.com/kubernetes/kubernetes/pull/94888>#94888</a>, <a href=https://github.com/sttts>@sttts</a>) [SIG API Machinery]</p></li><li><p>Fix memory leak in kube-apiserver when underlying time goes forth and back. (<a href=https://github.com/kubernetes/kubernetes/pull/96266>#96266</a>, <a href=https://github.com/chenyw1990>@chenyw1990</a>) [SIG API Machinery]</p></li><li><p>Fix missing csi annotations on node during parallel csinode update. (<a href=https://github.com/kubernetes/kubernetes/pull/94389>#94389</a>, <a href=https://github.com/pacoxu>@pacoxu</a>) [SIG Storage]</p></li><li><p>Fix network_programming_latency metric reporting for Endpoints/EndpointSlice deletions, where we don't have correct timestamp (<a href=https://github.com/kubernetes/kubernetes/pull/95363>#95363</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG Network and Scalability]</p></li><li><p>Fix paging issues when Azure API returns empty values with non-empty nextLink (<a href=https://github.com/kubernetes/kubernetes/pull/96211>#96211</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>Fix pull image error from multiple ACRs using azure managed identity (<a href=https://github.com/kubernetes/kubernetes/pull/96355>#96355</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix race condition on timeCache locks. (<a href=https://github.com/kubernetes/kubernetes/pull/94751>#94751</a>, <a href=https://github.com/auxten>@auxten</a>)</p></li><li><p>Fix regression on <code>kubectl portforward</code> when TCP and UCP services were configured on the same port. (<a href=https://github.com/kubernetes/kubernetes/pull/94728>#94728</a>, <a href=https://github.com/amorenoz>@amorenoz</a>)</p></li><li><p>Fix scheduler cache snapshot when a Node is deleted before its Pods (<a href=https://github.com/kubernetes/kubernetes/pull/95130>#95130</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</p></li><li><p>Fix the <code>cloudprovider_azure_api_request_duration_seconds</code> metric buckets to correctly capture the latency metrics. Previously, the majority of the calls would fall in the "+Inf" bucket. (<a href=https://github.com/kubernetes/kubernetes/pull/94873>#94873</a>, <a href=https://github.com/marwanad>@marwanad</a>) [SIG Cloud Provider and Instrumentation]</p></li><li><p>Fix vSphere volumes that could be erroneously attached to wrong node (<a href=https://github.com/kubernetes/kubernetes/pull/96224>#96224</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fix verb & scope reporting for kube-apiserver metrics (LIST reported instead of GET) (<a href=https://github.com/kubernetes/kubernetes/pull/95562>#95562</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Testing]</p></li><li><p>Fix vsphere detach failure for static PVs (<a href=https://github.com/kubernetes/kubernetes/pull/95447>#95447</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fix: azure disk resize error if source does not exist (<a href=https://github.com/kubernetes/kubernetes/pull/93011>#93011</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix: detach azure disk broken on Azure Stack (<a href=https://github.com/kubernetes/kubernetes/pull/94885>#94885</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix: resize Azure disk issue when it's in attached state (<a href=https://github.com/kubernetes/kubernetes/pull/96705>#96705</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix: smb valid path error (<a href=https://github.com/kubernetes/kubernetes/pull/95583>#95583</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</p></li><li><p>Fix: use sensitiveOptions on Windows mount (<a href=https://github.com/kubernetes/kubernetes/pull/94126>#94126</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fixed a bug causing incorrect formatting of <code>kubectl describe ingress</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94985>#94985</a>, <a href=https://github.com/howardjohn>@howardjohn</a>) [SIG CLI and Network]</p></li><li><p>Fixed a bug in client-go where new clients with customized <code>Dial</code>, <code>Proxy</code>, <code>GetCert</code> config may get stale HTTP transports. (<a href=https://github.com/kubernetes/kubernetes/pull/95427>#95427</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery]</p></li><li><p>Fixed a bug that prevents kubectl to validate CRDs with schema using x-kubernetes-preserve-unknown-fields on object fields. (<a href=https://github.com/kubernetes/kubernetes/pull/96369>#96369</a>, <a href=https://github.com/gautierdelorme>@gautierdelorme</a>) [SIG API Machinery and Testing]</p></li><li><p>Fixed a bug that prevents the use of ephemeral containers in the presence of a validating admission webhook. (<a href=https://github.com/kubernetes/kubernetes/pull/94685>#94685</a>, <a href=https://github.com/verb>@verb</a>) [SIG Node and Testing]</p></li><li><p>Fixed a bug where aggregator_unavailable_apiservice metrics were reported for deleted apiservices. (<a href=https://github.com/kubernetes/kubernetes/pull/96421>#96421</a>, <a href=https://github.com/dgrisonnet>@dgrisonnet</a>) [SIG API Machinery and Instrumentation]</p></li><li><p>Fixed a bug where improper storage and comparison of endpoints led to excessive API traffic from the endpoints controller (<a href=https://github.com/kubernetes/kubernetes/pull/94112>#94112</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Network and Testing]</p></li><li><p>Fixed a regression which prevented pods with <code>docker/default</code> seccomp annotations from being created in 1.19 if a PodSecurityPolicy was in place which did not allow <code>runtime/default</code> seccomp profiles. (<a href=https://github.com/kubernetes/kubernetes/pull/95985>#95985</a>, <a href=https://github.com/saschagrunert>@saschagrunert</a>) [SIG Auth]</p></li><li><p>Fixed bug in reflector that couldn't recover from "Too large resource version" errors with API servers 1.17.0-1.18.5 (<a href=https://github.com/kubernetes/kubernetes/pull/94316>#94316</a>, <a href=https://github.com/janeczku>@janeczku</a>) [SIG API Machinery]</p></li><li><p>Fixed bug where kubectl top pod output is not sorted when --sort-by and --containers flags are used together (<a href=https://github.com/kubernetes/kubernetes/pull/93692>#93692</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</p></li><li><p>Fixed kubelet creating extra sandbox for pods with RestartPolicyOnFailure after all containers succeeded (<a href=https://github.com/kubernetes/kubernetes/pull/92614>#92614</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Node and Testing]</p></li><li><p>Fixes an issue proxying to ipv6 pods without specifying a port (<a href=https://github.com/kubernetes/kubernetes/pull/94834>#94834</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Network]</p></li><li><p>Fixes code generation for non-namespaced create subresources fake client test. (<a href=https://github.com/kubernetes/kubernetes/pull/96586>#96586</a>, <a href=https://github.com/Doude>@Doude</a>) [SIG API Machinery]</p></li><li><p>Fixes high CPU usage in kubectl drain (<a href=https://github.com/kubernetes/kubernetes/pull/95260>#95260</a>, <a href=https://github.com/amandahla>@amandahla</a>) [SIG CLI]</p></li><li><p>For vSphere Cloud Provider, If VM of worker node is deleted, the node will also be deleted by node controller (<a href=https://github.com/kubernetes/kubernetes/pull/92608>#92608</a>, <a href=https://github.com/lubronzhan>@lubronzhan</a>) [SIG Cloud Provider]</p></li><li><p>Gracefully delete nodes when their parent scale set went missing (<a href=https://github.com/kubernetes/kubernetes/pull/95289>#95289</a>, <a href=https://github.com/bpineau>@bpineau</a>) [SIG Cloud Provider]</p></li><li><p>HTTP/2 connection health check is enabled by default in all Kubernetes clients. The feature should work out-of-the-box. If needed, users can tune the feature via the HTTP2_READ_IDLE_TIMEOUT_SECONDS and HTTP2_PING_TIMEOUT_SECONDS environment variables. The feature is disabled if HTTP2_READ_IDLE_TIMEOUT_SECONDS is set to 0. (<a href=https://github.com/kubernetes/kubernetes/pull/95981>#95981</a>, <a href=https://github.com/caesarxuchao>@caesarxuchao</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Node]</p></li><li><p>If the user specifies an invalid timeout in the request URL, the request will be aborted with an HTTP 400.</p><ul><li>If the user specifies a timeout in the request URL that exceeds the maximum request deadline allowed by the apiserver, the request will be aborted with an HTTP 400. (<a href=https://github.com/kubernetes/kubernetes/pull/96061>#96061</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery, Network and Testing]</li></ul></li><li><p>If we set SelectPolicy MinPolicySelect on scaleUp behavior or scaleDown behavior,Horizontal Pod Autoscaler doesn`t automatically scale the number of pods correctly (<a href=https://github.com/kubernetes/kubernetes/pull/95647>#95647</a>, <a href=https://github.com/JoshuaAndrew>@JoshuaAndrew</a>) [SIG Apps and Autoscaling]</p></li><li><p>Ignore apparmor for non-linux operating systems (<a href=https://github.com/kubernetes/kubernetes/pull/93220>#93220</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</p></li><li><p>Ignore root user check when windows pod starts (<a href=https://github.com/kubernetes/kubernetes/pull/92355>#92355</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</p></li><li><p>Improve error messages related to nodePort endpoint changes conntrack entries cleanup. (<a href=https://github.com/kubernetes/kubernetes/pull/96251>#96251</a>, <a href=https://github.com/ravens>@ravens</a>) [SIG Network]</p></li><li><p>In dual-stack clusters, kubelet will now set up both IPv4 and IPv6 iptables rules, which may
fix some problems, eg with HostPorts. (<a href=https://github.com/kubernetes/kubernetes/pull/94474>#94474</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Network and Node]</p></li><li><p>Increase maximum IOPS of AWS EBS io1 volume to current maximum (64,000). (<a href=https://github.com/kubernetes/kubernetes/pull/90014>#90014</a>, <a href=https://github.com/jacobmarble>@jacobmarble</a>)</p></li><li><p>Ipvs: ensure selected scheduler kernel modules are loaded (<a href=https://github.com/kubernetes/kubernetes/pull/93040>#93040</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Network]</p></li><li><p>K8s.io/apimachinery: runtime.DefaultUnstructuredConverter.FromUnstructured now handles converting integer fields to typed float values (<a href=https://github.com/kubernetes/kubernetes/pull/93250>#93250</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</p></li><li><p>Kube-proxy now trims extra spaces found in loadBalancerSourceRanges to match Service validation. (<a href=https://github.com/kubernetes/kubernetes/pull/94107>#94107</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Network]</p></li><li><p>Kubeadm ensures "kubeadm reset" does not unmount the root "/var/lib/kubelet" directory if it is mounted by the user. (<a href=https://github.com/kubernetes/kubernetes/pull/93702>#93702</a>, <a href=https://github.com/thtanaka>@thtanaka</a>)</p></li><li><p>Kubeadm now makes sure the etcd manifest is regenerated upon upgrade even when no etcd version change takes place (<a href=https://github.com/kubernetes/kubernetes/pull/94395>#94395</a>, <a href=https://github.com/rosti>@rosti</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm now warns (instead of error out) on missing "ca.key" files for root CA, front-proxy CA and etcd CA, during "kubeadm join --control-plane" if the user has provided all certificates, keys and kubeconfig files which require signing with the given CA keys. (<a href=https://github.com/kubernetes/kubernetes/pull/94988>#94988</a>, <a href=https://github.com/neolit123>@neolit123</a>)</p></li><li><p>Kubeadm: add missing "--experimental-patches" flag to "kubeadm init phase control-plane" (<a href=https://github.com/kubernetes/kubernetes/pull/95786>#95786</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: avoid a panic when determining if the running version of CoreDNS is supported during upgrades (<a href=https://github.com/kubernetes/kubernetes/pull/94299>#94299</a>, <a href=https://github.com/zouyee>@zouyee</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: ensure the etcd data directory is created with 0700 permissions during control-plane init and join (<a href=https://github.com/kubernetes/kubernetes/pull/94102>#94102</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: fix coredns migration should be triggered when there are newdefault configs during kubeadm upgrade (<a href=https://github.com/kubernetes/kubernetes/pull/96907>#96907</a>, <a href=https://github.com/pacoxu>@pacoxu</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: fix the bug that kubeadm tries to call 'docker info' even if the CRI socket was for another CR (<a href=https://github.com/kubernetes/kubernetes/pull/94555>#94555</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: for Docker as the container runtime, make the "kubeadm reset" command stop containers before removing them (<a href=https://github.com/kubernetes/kubernetes/pull/94586>#94586</a>, <a href=https://github.com/BedivereZero>@BedivereZero</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make the kubeconfig files for the kube-controller-manager and kube-scheduler use the LocalAPIEndpoint instead of the ControlPlaneEndpoint. This makes kubeadm clusters more reseliant to version skew problems during immutable upgrades: <a href=https://kubernetes.io/docs/setup/release/version-skew-policy/#kube-controller-manager-kube-scheduler-and-cloud-controller-manager>https://kubernetes.io/docs/setup/release/version-skew-policy/#kube-controller-manager-kube-scheduler-and-cloud-controller-manager</a> (<a href=https://github.com/kubernetes/kubernetes/pull/94398>#94398</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: relax the validation of kubeconfig server URLs. Allow the user to define custom kubeconfig server URLs without erroring out during validation of existing kubeconfig files (e.g. when using external CA mode). (<a href=https://github.com/kubernetes/kubernetes/pull/94816>#94816</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubectl: print error if users place flags before plugin name (<a href=https://github.com/kubernetes/kubernetes/pull/92343>#92343</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG CLI]</p></li><li><p>Kubelet: assume that swap is disabled when <code>/proc/swaps</code> does not exist (<a href=https://github.com/kubernetes/kubernetes/pull/93931>#93931</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Node]</p></li><li><p>New Azure instance types do now have correct max data disk count information. (<a href=https://github.com/kubernetes/kubernetes/pull/94340>#94340</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Port mapping now allows the same <code>containerPort</code> of different containers to different <code>hostPort</code> without naming the mapping explicitly. (<a href=https://github.com/kubernetes/kubernetes/pull/94494>#94494</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>)</p></li><li><p>Print go stack traces at -v=4 and not -v=2 (<a href=https://github.com/kubernetes/kubernetes/pull/94663>#94663</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</p></li><li><p>Recreate EndpointSlices on rapid Service creation. (<a href=https://github.com/kubernetes/kubernetes/pull/94730>#94730</a>, <a href=https://github.com/robscott>@robscott</a>)</p></li><li><p>Reduce volume name length for vsphere volumes (<a href=https://github.com/kubernetes/kubernetes/pull/96533>#96533</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Storage]</p></li><li><p>Remove ready file and its directory (which is created during volume SetUp) during emptyDir volume TearDown. (<a href=https://github.com/kubernetes/kubernetes/pull/95770>#95770</a>, <a href=https://github.com/jingxu97>@jingxu97</a>) [SIG Storage]</p></li><li><p>Reorganized iptables rules to fix a performance issue (<a href=https://github.com/kubernetes/kubernetes/pull/95252>#95252</a>, <a href=https://github.com/tssurya>@tssurya</a>) [SIG Network]</p></li><li><p>Require feature flag CustomCPUCFSQuotaPeriod if setting a non-default cpuCFSQuotaPeriod in kubelet config. (<a href=https://github.com/kubernetes/kubernetes/pull/94687>#94687</a>, <a href=https://github.com/karan>@karan</a>) [SIG Node]</p></li><li><p>Resolves a regression in 1.19+ with workloads targeting deprecated beta os/arch labels getting stuck in NodeAffinity status on node startup. (<a href=https://github.com/kubernetes/kubernetes/pull/96810>#96810</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node]</p></li><li><p>Resolves non-deterministic behavior of the garbage collection controller when ownerReferences with incorrect data are encountered. Events with a reason of <code>OwnerRefInvalidNamespace</code> are recorded when namespace mismatches between child and owner objects are detected. The <a href=https://github.com/kubernetes-sigs/kubectl-check-ownerreferences>kubectl-check-ownerreferences</a> tool can be run prior to upgrading to locate existing objects with invalid ownerReferences.</p><ul><li>A namespaced object with an ownerReference referencing a uid of a namespaced kind which does not exist in the same namespace is now consistently treated as though that owner does not exist, and the child object is deleted.</li><li>A cluster-scoped object with an ownerReference referencing a uid of a namespaced kind is now consistently treated as though that owner is not resolvable, and the child object is ignored by the garbage collector. (<a href=https://github.com/kubernetes/kubernetes/pull/92743>#92743</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps and Testing]</li></ul></li><li><p>Skip [k8s.io/kubernetes@v1.19.0/test/e2e/storage/testsuites/base.go:162]: Driver azure-disk doesn't support snapshot type DynamicSnapshot -- skipping
skip [k8s.io/kubernetes@v1.19.0/test/e2e/storage/testsuites/base.go:185]: Driver azure-disk doesn't support ntfs -- skipping (<a href=https://github.com/kubernetes/kubernetes/pull/96144>#96144</a>, <a href=https://github.com/qinpingli>@qinpingli</a>) [SIG Storage and Testing]</p></li><li><p>StatefulSet Controller now waits for PersistentVolumeClaim deletion before creating pods. (<a href=https://github.com/kubernetes/kubernetes/pull/93457>#93457</a>, <a href=https://github.com/ymmt2005>@ymmt2005</a>)</p></li><li><p>StreamWatcher now calls HandleCrash at appropriate sequence. (<a href=https://github.com/kubernetes/kubernetes/pull/93108>#93108</a>, <a href=https://github.com/lixiaobing1>@lixiaobing1</a>)</p></li><li><p>Support the node label <code>node.kubernetes.io/exclude-from-external-load-balancers</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95542>#95542</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>The AWS network load balancer attributes can now be specified during service creation (<a href=https://github.com/kubernetes/kubernetes/pull/95247>#95247</a>, <a href=https://github.com/kishorj>@kishorj</a>) [SIG Cloud Provider]</p></li><li><p>The <code>/debug/api_priority_and_fairness/dump_requests</code> path at an apiserver will no longer return a phantom line for each exempt priority level. (<a href=https://github.com/kubernetes/kubernetes/pull/93406>#93406</a>, <a href=https://github.com/MikeSpreitzer>@MikeSpreitzer</a>) [SIG API Machinery]</p></li><li><p>The kube-apiserver will no longer serve APIs that should have been deleted in GA non-alpha levels. Alpha levels will continue to serve the removed APIs so that CI doesn't immediately break. (<a href=https://github.com/kubernetes/kubernetes/pull/96525>#96525</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery]</p></li><li><p>The kubelet recognizes the --containerd-namespace flag to configure the namespace used by cadvisor. (<a href=https://github.com/kubernetes/kubernetes/pull/87054>#87054</a>, <a href=https://github.com/changyaowei>@changyaowei</a>) [SIG Node]</p></li><li><p>Unhealthy pods covered by PDBs can be successfully evicted if enough healthy pods are available. (<a href=https://github.com/kubernetes/kubernetes/pull/94381>#94381</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG Apps]</p></li><li><p>Update Calico to v3.15.2 (<a href=https://github.com/kubernetes/kubernetes/pull/94241>#94241</a>, <a href=https://github.com/lmm>@lmm</a>) [SIG Cloud Provider]</p></li><li><p>Update default etcd server version to 3.4.13 (<a href=https://github.com/kubernetes/kubernetes/pull/94287>#94287</a>, <a href=https://github.com/jingyih>@jingyih</a>) [SIG API Machinery, Cloud Provider, Cluster Lifecycle and Testing]</p></li><li><p>Update max azure data disk count map (<a href=https://github.com/kubernetes/kubernetes/pull/96308>#96308</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Update the PIP when it is not in the Succeeded provisioning state during the LB update. (<a href=https://github.com/kubernetes/kubernetes/pull/95748>#95748</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Update the frontend IP config when the service's <code>pipName</code> annotation is changed (<a href=https://github.com/kubernetes/kubernetes/pull/95813>#95813</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Update the route table tag in the route reconcile loop (<a href=https://github.com/kubernetes/kubernetes/pull/96545>#96545</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Use NLB Subnet CIDRs instead of VPC CIDRs in Health Check SG Rules (<a href=https://github.com/kubernetes/kubernetes/pull/93515>#93515</a>, <a href=https://github.com/t0rr3sp3dr0>@t0rr3sp3dr0</a>) [SIG Cloud Provider]</p></li><li><p>Users will see increase in time for deletion of pods and also guarantee that removal of pod from api server would mean deletion of all the resources from container runtime. (<a href=https://github.com/kubernetes/kubernetes/pull/92817>#92817</a>, <a href=https://github.com/kmala>@kmala</a>) [SIG Node]</p></li><li><p>Very large patches may now be specified to <code>kubectl patch</code> with the <code>--patch-file</code> flag instead of including them directly on the command line. The <code>--patch</code> and <code>--patch-file</code> flags are mutually exclusive. (<a href=https://github.com/kubernetes/kubernetes/pull/93548>#93548</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG CLI]</p></li><li><p>Volume binding: report UnschedulableAndUnresolvable status instead of an error when bound PVs not found (<a href=https://github.com/kubernetes/kubernetes/pull/95541>#95541</a>, <a href=https://github.com/cofyc>@cofyc</a>) [SIG Apps, Scheduling and Storage]</p></li><li><p>Warn instead of fail when creating Roles and ClusterRoles with custom verbs via kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/92492>#92492</a>, <a href=https://github.com/eddiezane>@eddiezane</a>) [SIG CLI]</p></li><li><p>When creating a PVC with the volume.beta.kubernetes.io/storage-provisioner annotation already set, the PV controller might have incorrectly deleted the newly provisioned PV instead of binding it to the PVC, depending on timing and system load. (<a href=https://github.com/kubernetes/kubernetes/pull/95909>#95909</a>, <a href=https://github.com/pohly>@pohly</a>) [SIG Apps and Storage]</p></li><li><p>[kubectl] Fail when local source file doesn't exist (<a href=https://github.com/kubernetes/kubernetes/pull/90333>#90333</a>, <a href=https://github.com/bamarni>@bamarni</a>) [SIG CLI]</p></li></ul><h3 id=other-cleanup-or-flake>Other (Cleanup or Flake)</h3><ul><li><p><strong>Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.</strong>:</p>([#96443](https://github.com/kubernetes/kubernetes/pull/96443), [@alaypatel07](https://github.com/alaypatel07)) [SIG Apps]</li><li><p>--redirect-container-streaming is no longer functional. The flag will be removed in v1.22 (<a href=https://github.com/kubernetes/kubernetes/pull/95935>#95935</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG Node]</p></li><li><p>A new metric <code>requestAbortsTotal</code> has been introduced that counts aborted requests for each <code>group</code>, <code>version</code>, <code>verb</code>, <code>resource</code>, <code>subresource</code> and <code>scope</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/95002>#95002</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery, Cloud Provider, Instrumentation and Scheduling]</p></li><li><p>API priority and fairness metrics use snake_case in label names (<a href=https://github.com/kubernetes/kubernetes/pull/96236>#96236</a>, <a href=https://github.com/adtac>@adtac</a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Testing]</p></li><li><p>Add fine grained debugging to intra-pod conformance test to troubleshoot networking issues for potentially unhealthy nodes when running conformance or sonobuoy tests. (<a href=https://github.com/kubernetes/kubernetes/pull/93837>#93837</a>, <a href=https://github.com/jayunit100>@jayunit100</a>)</p></li><li><p>Add the following metrics:</p><ul><li>network_plugin_operations_total</li><li>network_plugin_operations_errors_total (<a href=https://github.com/kubernetes/kubernetes/pull/93066>#93066</a>, <a href=https://github.com/AnishShah>@AnishShah</a>)</li></ul></li><li><p>Adds a bootstrapping ClusterRole, ClusterRoleBinding and group for /metrics, /livez/<em>, /readyz/</em>, & /healthz/- endpoints. (<a href=https://github.com/kubernetes/kubernetes/pull/93311>#93311</a>, <a href=https://github.com/logicalhan>@logicalhan</a>) [SIG API Machinery, Auth, Cloud Provider and Instrumentation]</p></li><li><p>AdmissionReview objects sent for the creation of Namespace API objects now populate the <code>namespace</code> attribute consistently (previously the <code>namespace</code> attribute was empty for Namespace creation via POST requests, and populated for Namespace creation via server-side-apply PATCH requests) (<a href=https://github.com/kubernetes/kubernetes/pull/95012>#95012</a>, <a href=https://github.com/nodo>@nodo</a>) [SIG API Machinery and Testing]</p></li><li><p>Applies translations on all command descriptions (<a href=https://github.com/kubernetes/kubernetes/pull/95439>#95439</a>, <a href=https://github.com/HerrNaN>@HerrNaN</a>) [SIG CLI]</p></li><li><p>Base-images: Update to debian-iptables:buster-v1.3.0</p><ul><li>Uses iptables 1.8.5</li><li>base-images: Update to debian-base:buster-v1.2.0</li><li>cluster/images/etcd: Build etcd:3.4.13-1 image<ul><li>Uses debian-base:buster-v1.2.0 (<a href=https://github.com/kubernetes/kubernetes/pull/94733>#94733</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Release and Testing]</li></ul></li></ul></li><li><p>Changed: default "Accept-Encoding" header removed from HTTP probes. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes>https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes</a> (<a href=https://github.com/kubernetes/kubernetes/pull/96127>#96127</a>, <a href=https://github.com/fonsecas72>@fonsecas72</a>) [SIG Network and Node]</p></li><li><p>Client-go header logging (at verbosity levels >= 9) now masks <code>Authorization</code> header contents (<a href=https://github.com/kubernetes/kubernetes/pull/95316>#95316</a>, <a href=https://github.com/sfowl>@sfowl</a>) [SIG API Machinery]</p></li><li><p>Decrease warning message frequency on setting volume ownership for configmap/secret. (<a href=https://github.com/kubernetes/kubernetes/pull/92878>#92878</a>, <a href=https://github.com/jvanz>@jvanz</a>)</p></li><li><p>Enhance log information of verifyRunAsNonRoot, add pod, container information (<a href=https://github.com/kubernetes/kubernetes/pull/94911>#94911</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node]</p></li><li><p>Fix func name NewCreateCreateDeploymentOptions (<a href=https://github.com/kubernetes/kubernetes/pull/91931>#91931</a>, <a href=https://github.com/lixiaobing1>@lixiaobing1</a>) [SIG CLI]</p></li><li><p>Fix kubelet to properly log when a container is started. Previously, kubelet may log that container is dead and was restarted when it was actually started for the first time. This behavior only happened on pods with initContainers and regular containers. (<a href=https://github.com/kubernetes/kubernetes/pull/91469>#91469</a>, <a href=https://github.com/rata>@rata</a>)</p></li><li><p>Fixes the message about no auth for metrics in scheduler. (<a href=https://github.com/kubernetes/kubernetes/pull/94035>#94035</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG Scheduling]</p></li><li><p>Generators for services are removed from kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/95256>#95256</a>, <a href=https://github.com/Git-Jiro>@Git-Jiro</a>) [SIG CLI]</p></li><li><p>Introduce kubectl-convert plugin. (<a href=https://github.com/kubernetes/kubernetes/pull/96190>#96190</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI and Testing]</p></li><li><p>Kube-scheduler now logs processed component config at startup (<a href=https://github.com/kubernetes/kubernetes/pull/96426>#96426</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Scheduling]</p></li><li><p>Kubeadm: Separate argument key/value in log msg (<a href=https://github.com/kubernetes/kubernetes/pull/94016>#94016</a>, <a href=https://github.com/mrueg>@mrueg</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: remove the CoreDNS check for known image digests when applying the addon (<a href=https://github.com/kubernetes/kubernetes/pull/94506>#94506</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: update the default pause image version to 1.4.0 on Windows. With this update the image supports Windows versions 1809 (2019LTS), 1903, 1909, 2004 (<a href=https://github.com/kubernetes/kubernetes/pull/95419>#95419</a>, <a href=https://github.com/jsturtevant>@jsturtevant</a>) [SIG Cluster Lifecycle and Windows]</p></li><li><p>Kubectl: the <code>generator</code> flag of <code>kubectl autoscale</code> has been deprecated and has no effect, it will be removed in a feature release (<a href=https://github.com/kubernetes/kubernetes/pull/92998>#92998</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG CLI]</p></li><li><p>Lock ExternalPolicyForExternalIP to default, this feature gate will be removed in 1.22. (<a href=https://github.com/kubernetes/kubernetes/pull/94581>#94581</a>, <a href=https://github.com/knabben>@knabben</a>) [SIG Network]</p></li><li><p>Mask ceph RBD adminSecrets in logs when logLevel >= 4. (<a href=https://github.com/kubernetes/kubernetes/pull/95245>#95245</a>, <a href=https://github.com/sfowl>@sfowl</a>)</p></li><li><p>Remove offensive words from kubectl cluster-info command. (<a href=https://github.com/kubernetes/kubernetes/pull/95202>#95202</a>, <a href=https://github.com/rikatz>@rikatz</a>)</p></li><li><p>Remove support for "ci/k8s-master" version label in kubeadm, use "ci/latest" instead. See <a href=https://github.com/kubernetes/test-infra/pull/18517>kubernetes/test-infra#18517</a>. (<a href=https://github.com/kubernetes/kubernetes/pull/93626>#93626</a>, <a href=https://github.com/vikkyomkar>@vikkyomkar</a>)</p></li><li><p>Remove the dependency of csi-translation-lib module on apiserver/cloud-provider/controller-manager (<a href=https://github.com/kubernetes/kubernetes/pull/95543>#95543</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Release]</p></li><li><p>Scheduler framework interface moved from pkg/scheduler/framework/v1alpha to pkg/scheduler/framework (<a href=https://github.com/kubernetes/kubernetes/pull/95069>#95069</a>, <a href=https://github.com/farah>@farah</a>) [SIG Scheduling, Storage and Testing]</p></li><li><p>Service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset is removed. All Standard load balancers will always enable tcp resets. (<a href=https://github.com/kubernetes/kubernetes/pull/94297>#94297</a>, <a href=https://github.com/MarcPow>@MarcPow</a>) [SIG Cloud Provider]</p></li><li><p>Stop propagating SelfLink (deprecated in 1.16) in kube-apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/94397>#94397</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Testing]</p></li><li><p>Strip unnecessary security contexts on Windows (<a href=https://github.com/kubernetes/kubernetes/pull/93475>#93475</a>, <a href=https://github.com/ravisantoshgudimetla>@ravisantoshgudimetla</a>) [SIG Node, Testing and Windows]</p></li><li><p>To ensure the code be strong, add unit test for GetAddressAndDialer (<a href=https://github.com/kubernetes/kubernetes/pull/93180>#93180</a>, <a href=https://github.com/FreeZhang61>@FreeZhang61</a>) [SIG Node]</p></li><li><p>UDP and SCTP protocols can left stale connections that need to be cleared to avoid services disruption, but they can cause problems that are hard to debug.
Kubernetes components using a loglevel greater or equal than 4 will log the conntrack operations and its output, to show the entries that were deleted. (<a href=https://github.com/kubernetes/kubernetes/pull/95694>#95694</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</p></li><li><p>Update CNI plugins to v0.8.7 (<a href=https://github.com/kubernetes/kubernetes/pull/94367>#94367</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Cloud Provider, Network, Node, Release and Testing]</p></li><li><p>Update cri-tools to <a href=https://github.com/kubernetes-sigs/cri-tools/releases/tag/v1.19.0>v1.19.0</a> (<a href=https://github.com/kubernetes/kubernetes/pull/94307>#94307</a>, <a href=https://github.com/xmudrii>@xmudrii</a>) [SIG Cloud Provider]</p></li><li><p>Update etcd client side to v3.4.13 (<a href=https://github.com/kubernetes/kubernetes/pull/94259>#94259</a>, <a href=https://github.com/jingyih>@jingyih</a>) [SIG API Machinery and Cloud Provider]</p></li><li><p>Users will now be able to configure all supported values for AWS NLB health check interval and thresholds for new resources. (<a href=https://github.com/kubernetes/kubernetes/pull/96312>#96312</a>, <a href=https://github.com/kishorj>@kishorj</a>) [SIG Cloud Provider]</p></li><li><p>V1helpers.MatchNodeSelectorTerms now accepts just a Node and a list of Terms (<a href=https://github.com/kubernetes/kubernetes/pull/95871>#95871</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Scheduling and Storage]</p></li><li><p>Vsphere: improve logging message on node cache refresh event (<a href=https://github.com/kubernetes/kubernetes/pull/95236>#95236</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Cloud Provider]</p></li><li><p><code>MatchNodeSelectorTerms</code> function moved to <code>k8s.io/component-helpers</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95531>#95531</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Scheduling and Storage]</p></li><li><p><code>kubectl api-resources</code> now prints the API version (as 'API group/version', same as output of <code>kubectl api-versions</code>). The column APIGROUP is now APIVERSION (<a href=https://github.com/kubernetes/kubernetes/pull/95253>#95253</a>, <a href=https://github.com/sallyom>@sallyom</a>) [SIG CLI]</p></li><li><p><code>kubectl get ingress</code> now prefers the <code>networking.k8s.io/v1</code> over <code>extensions/v1beta1</code> (deprecated since v1.14). To explicitly request the deprecated version, use <code>kubectl get ingress.v1beta1.extensions</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94309>#94309</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and CLI]</p></li></ul><h2 id=dependencies>Dependencies</h2><h3 id=added>Added</h3><ul><li>cloud.google.com/go/firestore: v1.1.0</li><li>github.com/Azure/go-autorest: <a href=https://github.com/Azure/go-autorest/tree/v14.2.0>v14.2.0+incompatible</a></li><li>github.com/armon/go-metrics: <a href=https://github.com/armon/go-metrics/tree/f0300d1>f0300d1</a></li><li>github.com/armon/go-radix: <a href=https://github.com/armon/go-radix/tree/7fddfc3>7fddfc3</a></li><li>github.com/bketelsen/crypt: <a href=https://github.com/bketelsen/crypt/tree/5cbc8cc>5cbc8cc</a></li><li>github.com/form3tech-oss/jwt-go: <a href=https://github.com/form3tech-oss/jwt-go/tree/v3.2.2>v3.2.2+incompatible</a></li><li>github.com/fvbommel/sortorder: <a href=https://github.com/fvbommel/sortorder/tree/v1.0.1>v1.0.1</a></li><li>github.com/hashicorp/consul/api: <a href=https://github.com/hashicorp/consul/api/tree/v1.1.0>v1.1.0</a></li><li>github.com/hashicorp/consul/sdk: <a href=https://github.com/hashicorp/consul/sdk/tree/v0.1.1>v0.1.1</a></li><li>github.com/hashicorp/errwrap: <a href=https://github.com/hashicorp/errwrap/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-cleanhttp: <a href=https://github.com/hashicorp/go-cleanhttp/tree/v0.5.1>v0.5.1</a></li><li>github.com/hashicorp/go-immutable-radix: <a href=https://github.com/hashicorp/go-immutable-radix/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-msgpack: <a href=https://github.com/hashicorp/go-msgpack/tree/v0.5.3>v0.5.3</a></li><li>github.com/hashicorp/go-multierror: <a href=https://github.com/hashicorp/go-multierror/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-rootcerts: <a href=https://github.com/hashicorp/go-rootcerts/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-sockaddr: <a href=https://github.com/hashicorp/go-sockaddr/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-uuid: <a href=https://github.com/hashicorp/go-uuid/tree/v1.0.1>v1.0.1</a></li><li>github.com/hashicorp/go.net: <a href=https://github.com/hashicorp/go.net/tree/v0.0.1>v0.0.1</a></li><li>github.com/hashicorp/logutils: <a href=https://github.com/hashicorp/logutils/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/mdns: <a href=https://github.com/hashicorp/mdns/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/memberlist: <a href=https://github.com/hashicorp/memberlist/tree/v0.1.3>v0.1.3</a></li><li>github.com/hashicorp/serf: <a href=https://github.com/hashicorp/serf/tree/v0.8.2>v0.8.2</a></li><li>github.com/jmespath/go-jmespath/internal/testify: <a href=https://github.com/jmespath/go-jmespath/internal/testify/tree/v1.5.1>v1.5.1</a></li><li>github.com/mitchellh/cli: <a href=https://github.com/mitchellh/cli/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/go-testing-interface: <a href=https://github.com/mitchellh/go-testing-interface/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/gox: <a href=https://github.com/mitchellh/gox/tree/v0.4.0>v0.4.0</a></li><li>github.com/mitchellh/iochan: <a href=https://github.com/mitchellh/iochan/tree/v1.0.0>v1.0.0</a></li><li>github.com/pascaldekloe/goe: <a href=https://github.com/pascaldekloe/goe/tree/57f6aae>57f6aae</a></li><li>github.com/posener/complete: <a href=https://github.com/posener/complete/tree/v1.1.1>v1.1.1</a></li><li>github.com/ryanuber/columnize: <a href=https://github.com/ryanuber/columnize/tree/9b3edd6>9b3edd6</a></li><li>github.com/sean-/seed: <a href=https://github.com/sean-/seed/tree/e2103e2>e2103e2</a></li><li>github.com/subosito/gotenv: <a href=https://github.com/subosito/gotenv/tree/v1.2.0>v1.2.0</a></li><li>github.com/willf/bitset: <a href=https://github.com/willf/bitset/tree/d5bec33>d5bec33</a></li><li>gopkg.in/ini.v1: v1.51.0</li><li>gopkg.in/yaml.v3: 9f266ea</li><li>rsc.io/quote/v3: v3.1.0</li><li>rsc.io/sampler: v1.3.0</li></ul><h3 id=changed>Changed</h3><ul><li>cloud.google.com/go/bigquery: v1.0.1 → v1.4.0</li><li>cloud.google.com/go/datastore: v1.0.0 → v1.1.0</li><li>cloud.google.com/go/pubsub: v1.0.1 → v1.2.0</li><li>cloud.google.com/go/storage: v1.0.0 → v1.6.0</li><li>cloud.google.com/go: v0.51.0 → v0.54.0</li><li>github.com/Azure/go-autorest/autorest/adal: <a href=https://github.com/Azure/go-autorest/autorest/adal/compare/v0.8.2...v0.9.5>v0.8.2 → v0.9.5</a></li><li>github.com/Azure/go-autorest/autorest/date: <a href=https://github.com/Azure/go-autorest/autorest/date/compare/v0.2.0...v0.3.0>v0.2.0 → v0.3.0</a></li><li>github.com/Azure/go-autorest/autorest/mocks: <a href=https://github.com/Azure/go-autorest/autorest/mocks/compare/v0.3.0...v0.4.1>v0.3.0 → v0.4.1</a></li><li>github.com/Azure/go-autorest/autorest: <a href=https://github.com/Azure/go-autorest/autorest/compare/v0.9.6...v0.11.1>v0.9.6 → v0.11.1</a></li><li>github.com/Azure/go-autorest/logger: <a href=https://github.com/Azure/go-autorest/logger/compare/v0.1.0...v0.2.0>v0.1.0 → v0.2.0</a></li><li>github.com/Azure/go-autorest/tracing: <a href=https://github.com/Azure/go-autorest/tracing/compare/v0.5.0...v0.6.0>v0.5.0 → v0.6.0</a></li><li>github.com/Microsoft/go-winio: <a href=https://github.com/Microsoft/go-winio/compare/fc70bd9...v0.4.15>fc70bd9 → v0.4.15</a></li><li>github.com/aws/aws-sdk-go: <a href=https://github.com/aws/aws-sdk-go/compare/v1.28.2...v1.35.24>v1.28.2 → v1.35.24</a></li><li>github.com/blang/semver: <a href=https://github.com/blang/semver/compare/v3.5.0...v3.5.1>v3.5.0+incompatible → v3.5.1+incompatible</a></li><li>github.com/checkpoint-restore/go-criu/v4: <a href=https://github.com/checkpoint-restore/go-criu/v4/compare/v4.0.2...v4.1.0>v4.0.2 → v4.1.0</a></li><li>github.com/containerd/containerd: <a href=https://github.com/containerd/containerd/compare/v1.3.3...v1.4.1>v1.3.3 → v1.4.1</a></li><li>github.com/containerd/ttrpc: <a href=https://github.com/containerd/ttrpc/compare/v1.0.0...v1.0.2>v1.0.0 → v1.0.2</a></li><li>github.com/containerd/typeurl: <a href=https://github.com/containerd/typeurl/compare/v1.0.0...v1.0.1>v1.0.0 → v1.0.1</a></li><li>github.com/coreos/etcd: <a href=https://github.com/coreos/etcd/compare/v3.3.10...v3.3.13>v3.3.10+incompatible → v3.3.13+incompatible</a></li><li>github.com/docker/docker: <a href=https://github.com/docker/docker/compare/aa6a989...bd33bbf>aa6a989 → bd33bbf</a></li><li>github.com/go-gl/glfw/v3.3/glfw: <a href=https://github.com/go-gl/glfw/v3.3/glfw/compare/12ad95a...6f7a984>12ad95a → 6f7a984</a></li><li>github.com/golang/groupcache: <a href=https://github.com/golang/groupcache/compare/215e871...8c9f03a>215e871 → 8c9f03a</a></li><li>github.com/golang/mock: <a href=https://github.com/golang/mock/compare/v1.3.1...v1.4.1>v1.3.1 → v1.4.1</a></li><li>github.com/golang/protobuf: <a href=https://github.com/golang/protobuf/compare/v1.4.2...v1.4.3>v1.4.2 → v1.4.3</a></li><li>github.com/google/cadvisor: <a href=https://github.com/google/cadvisor/compare/v0.37.0...v0.38.5>v0.37.0 → v0.38.5</a></li><li>github.com/google/go-cmp: <a href=https://github.com/google/go-cmp/compare/v0.4.0...v0.5.2>v0.4.0 → v0.5.2</a></li><li>github.com/google/pprof: <a href=https://github.com/google/pprof/compare/d4f498a...1ebb73c>d4f498a → 1ebb73c</a></li><li>github.com/google/uuid: <a href=https://github.com/google/uuid/compare/v1.1.1...v1.1.2>v1.1.1 → v1.1.2</a></li><li>github.com/gorilla/mux: <a href=https://github.com/gorilla/mux/compare/v1.7.3...v1.8.0>v1.7.3 → v1.8.0</a></li><li>github.com/gorilla/websocket: <a href=https://github.com/gorilla/websocket/compare/v1.4.0...v1.4.2>v1.4.0 → v1.4.2</a></li><li>github.com/jmespath/go-jmespath: <a href=https://github.com/jmespath/go-jmespath/compare/c2b33e8...v0.4.0>c2b33e8 → v0.4.0</a></li><li>github.com/karrick/godirwalk: <a href=https://github.com/karrick/godirwalk/compare/v1.7.5...v1.16.1>v1.7.5 → v1.16.1</a></li><li>github.com/opencontainers/go-digest: <a href=https://github.com/opencontainers/go-digest/compare/v1.0.0-rc1...v1.0.0>v1.0.0-rc1 → v1.0.0</a></li><li>github.com/opencontainers/runc: <a href=https://github.com/opencontainers/runc/compare/819fcc6...v1.0.0-rc92>819fcc6 → v1.0.0-rc92</a></li><li>github.com/opencontainers/runtime-spec: <a href=https://github.com/opencontainers/runtime-spec/compare/237cc4f...4d89ac9>237cc4f → 4d89ac9</a></li><li>github.com/opencontainers/selinux: <a href=https://github.com/opencontainers/selinux/compare/v1.5.2...v1.6.0>v1.5.2 → v1.6.0</a></li><li>github.com/prometheus/procfs: <a href=https://github.com/prometheus/procfs/compare/v0.1.3...v0.2.0>v0.1.3 → v0.2.0</a></li><li>github.com/quobyte/api: <a href=https://github.com/quobyte/api/compare/v0.1.2...v0.1.8>v0.1.2 → v0.1.8</a></li><li>github.com/spf13/cobra: <a href=https://github.com/spf13/cobra/compare/v1.0.0...v1.1.1>v1.0.0 → v1.1.1</a></li><li>github.com/spf13/viper: <a href=https://github.com/spf13/viper/compare/v1.4.0...v1.7.0>v1.4.0 → v1.7.0</a></li><li>github.com/storageos/go-api: <a href=https://github.com/storageos/go-api/compare/343b3ef...v2.2.0>343b3ef → v2.2.0+incompatible</a></li><li>github.com/stretchr/testify: <a href=https://github.com/stretchr/testify/compare/v1.4.0...v1.6.1>v1.4.0 → v1.6.1</a></li><li>github.com/vishvananda/netns: <a href=https://github.com/vishvananda/netns/compare/52d707b...db3c7e5>52d707b → db3c7e5</a></li><li>go.etcd.io/etcd: 17cef6e → dd1b699</li><li>go.opencensus.io: v0.22.2 → v0.22.3</li><li>golang.org/x/crypto: 75b2880 → 7f63de1</li><li>golang.org/x/exp: da58074 → 6cc2880</li><li>golang.org/x/lint: fdd1cda → 738671d</li><li>golang.org/x/net: ab34263 → 69a7880</li><li>golang.org/x/oauth2: 858c2ad → bf48bf1</li><li>golang.org/x/sys: ed371f2 → 5cba982</li><li>golang.org/x/text: v0.3.3 → v0.3.4</li><li>golang.org/x/time: 555d28b → 3af7569</li><li>golang.org/x/xerrors: 9bdfabe → 5ec99f8</li><li>google.golang.org/api: v0.15.1 → v0.20.0</li><li>google.golang.org/genproto: cb27e3a → 8816d57</li><li>google.golang.org/grpc: v1.27.0 → v1.27.1</li><li>google.golang.org/protobuf: v1.24.0 → v1.25.0</li><li>honnef.co/go/tools: v0.0.1-2019.2.3 → v0.0.1-2020.1.3</li><li>k8s.io/gengo: 8167cfd → 83324d8</li><li>k8s.io/klog/v2: v2.2.0 → v2.4.0</li><li>k8s.io/kube-openapi: 6aeccd4 → d219536</li><li>k8s.io/system-validators: v1.1.2 → v1.2.0</li><li>k8s.io/utils: d5654de → 67b214c</li><li>sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.9 → v0.0.14</li><li>sigs.k8s.io/structured-merge-diff/v4: v4.0.1 → v4.0.2</li></ul><h3 id=removed>Removed</h3><ul><li>github.com/armon/consul-api: <a href=https://github.com/armon/consul-api/tree/eb2c6b5>eb2c6b5</a></li><li>github.com/go-ini/ini: <a href=https://github.com/go-ini/ini/tree/v1.9.0>v1.9.0</a></li><li>github.com/ugorji/go: <a href=https://github.com/ugorji/go/tree/v1.1.4>v1.1.4</a></li><li>github.com/xlab/handysort: <a href=https://github.com/xlab/handysort/tree/fb3537e>fb3537e</a></li><li>github.com/xordataexchange/crypt: <a href=https://github.com/xordataexchange/crypt/tree/b2862e3>b2862e3</a></li><li>vbom.ml/util: db5cfe1</li></ul><h2 id=dependencies-1>Dependencies</h2><h3 id=added-1>Added</h3><ul><li>cloud.google.com/go/firestore: v1.1.0</li><li>github.com/Azure/go-autorest: <a href=https://github.com/Azure/go-autorest/tree/v14.2.0>v14.2.0+incompatible</a></li><li>github.com/armon/go-metrics: <a href=https://github.com/armon/go-metrics/tree/f0300d1>f0300d1</a></li><li>github.com/armon/go-radix: <a href=https://github.com/armon/go-radix/tree/7fddfc3>7fddfc3</a></li><li>github.com/bketelsen/crypt: <a href=https://github.com/bketelsen/crypt/tree/5cbc8cc>5cbc8cc</a></li><li>github.com/form3tech-oss/jwt-go: <a href=https://github.com/form3tech-oss/jwt-go/tree/v3.2.2>v3.2.2+incompatible</a></li><li>github.com/fvbommel/sortorder: <a href=https://github.com/fvbommel/sortorder/tree/v1.0.1>v1.0.1</a></li><li>github.com/hashicorp/consul/api: <a href=https://github.com/hashicorp/consul/api/tree/v1.1.0>v1.1.0</a></li><li>github.com/hashicorp/consul/sdk: <a href=https://github.com/hashicorp/consul/sdk/tree/v0.1.1>v0.1.1</a></li><li>github.com/hashicorp/errwrap: <a href=https://github.com/hashicorp/errwrap/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-cleanhttp: <a href=https://github.com/hashicorp/go-cleanhttp/tree/v0.5.1>v0.5.1</a></li><li>github.com/hashicorp/go-immutable-radix: <a href=https://github.com/hashicorp/go-immutable-radix/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-msgpack: <a href=https://github.com/hashicorp/go-msgpack/tree/v0.5.3>v0.5.3</a></li><li>github.com/hashicorp/go-multierror: <a href=https://github.com/hashicorp/go-multierror/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-rootcerts: <a href=https://github.com/hashicorp/go-rootcerts/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-sockaddr: <a href=https://github.com/hashicorp/go-sockaddr/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-uuid: <a href=https://github.com/hashicorp/go-uuid/tree/v1.0.1>v1.0.1</a></li><li>github.com/hashicorp/go.net: <a href=https://github.com/hashicorp/go.net/tree/v0.0.1>v0.0.1</a></li><li>github.com/hashicorp/logutils: <a href=https://github.com/hashicorp/logutils/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/mdns: <a href=https://github.com/hashicorp/mdns/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/memberlist: <a href=https://github.com/hashicorp/memberlist/tree/v0.1.3>v0.1.3</a></li><li>github.com/hashicorp/serf: <a href=https://github.com/hashicorp/serf/tree/v0.8.2>v0.8.2</a></li><li>github.com/jmespath/go-jmespath/internal/testify: <a href=https://github.com/jmespath/go-jmespath/internal/testify/tree/v1.5.1>v1.5.1</a></li><li>github.com/mitchellh/cli: <a href=https://github.com/mitchellh/cli/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/go-testing-interface: <a href=https://github.com/mitchellh/go-testing-interface/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/gox: <a href=https://github.com/mitchellh/gox/tree/v0.4.0>v0.4.0</a></li><li>github.com/mitchellh/iochan: <a href=https://github.com/mitchellh/iochan/tree/v1.0.0>v1.0.0</a></li><li>github.com/pascaldekloe/goe: <a href=https://github.com/pascaldekloe/goe/tree/57f6aae>57f6aae</a></li><li>github.com/posener/complete: <a href=https://github.com/posener/complete/tree/v1.1.1>v1.1.1</a></li><li>github.com/ryanuber/columnize: <a href=https://github.com/ryanuber/columnize/tree/9b3edd6>9b3edd6</a></li><li>github.com/sean-/seed: <a href=https://github.com/sean-/seed/tree/e2103e2>e2103e2</a></li><li>github.com/subosito/gotenv: <a href=https://github.com/subosito/gotenv/tree/v1.2.0>v1.2.0</a></li><li>github.com/willf/bitset: <a href=https://github.com/willf/bitset/tree/d5bec33>d5bec33</a></li><li>gopkg.in/ini.v1: v1.51.0</li><li>gopkg.in/yaml.v3: 9f266ea</li><li>rsc.io/quote/v3: v3.1.0</li><li>rsc.io/sampler: v1.3.0</li></ul><h3 id=changed-1>Changed</h3><ul><li>cloud.google.com/go/bigquery: v1.0.1 → v1.4.0</li><li>cloud.google.com/go/datastore: v1.0.0 → v1.1.0</li><li>cloud.google.com/go/pubsub: v1.0.1 → v1.2.0</li><li>cloud.google.com/go/storage: v1.0.0 → v1.6.0</li><li>cloud.google.com/go: v0.51.0 → v0.54.0</li><li>github.com/Azure/go-autorest/autorest/adal: <a href=https://github.com/Azure/go-autorest/autorest/adal/compare/v0.8.2...v0.9.5>v0.8.2 → v0.9.5</a></li><li>github.com/Azure/go-autorest/autorest/date: <a href=https://github.com/Azure/go-autorest/autorest/date/compare/v0.2.0...v0.3.0>v0.2.0 → v0.3.0</a></li><li>github.com/Azure/go-autorest/autorest/mocks: <a href=https://github.com/Azure/go-autorest/autorest/mocks/compare/v0.3.0...v0.4.1>v0.3.0 → v0.4.1</a></li><li>github.com/Azure/go-autorest/autorest: <a href=https://github.com/Azure/go-autorest/autorest/compare/v0.9.6...v0.11.1>v0.9.6 → v0.11.1</a></li><li>github.com/Azure/go-autorest/logger: <a href=https://github.com/Azure/go-autorest/logger/compare/v0.1.0...v0.2.0>v0.1.0 → v0.2.0</a></li><li>github.com/Azure/go-autorest/tracing: <a href=https://github.com/Azure/go-autorest/tracing/compare/v0.5.0...v0.6.0>v0.5.0 → v0.6.0</a></li><li>github.com/Microsoft/go-winio: <a href=https://github.com/Microsoft/go-winio/compare/fc70bd9...v0.4.15>fc70bd9 → v0.4.15</a></li><li>github.com/aws/aws-sdk-go: <a href=https://github.com/aws/aws-sdk-go/compare/v1.28.2...v1.35.24>v1.28.2 → v1.35.24</a></li><li>github.com/blang/semver: <a href=https://github.com/blang/semver/compare/v3.5.0...v3.5.1>v3.5.0+incompatible → v3.5.1+incompatible</a></li><li>github.com/checkpoint-restore/go-criu/v4: <a href=https://github.com/checkpoint-restore/go-criu/v4/compare/v4.0.2...v4.1.0>v4.0.2 → v4.1.0</a></li><li>github.com/containerd/containerd: <a href=https://github.com/containerd/containerd/compare/v1.3.3...v1.4.1>v1.3.3 → v1.4.1</a></li><li>github.com/containerd/ttrpc: <a href=https://github.com/containerd/ttrpc/compare/v1.0.0...v1.0.2>v1.0.0 → v1.0.2</a></li><li>github.com/containerd/typeurl: <a href=https://github.com/containerd/typeurl/compare/v1.0.0...v1.0.1>v1.0.0 → v1.0.1</a></li><li>github.com/coreos/etcd: <a href=https://github.com/coreos/etcd/compare/v3.3.10...v3.3.13>v3.3.10+incompatible → v3.3.13+incompatible</a></li><li>github.com/docker/docker: <a href=https://github.com/docker/docker/compare/aa6a989...bd33bbf>aa6a989 → bd33bbf</a></li><li>github.com/go-gl/glfw/v3.3/glfw: <a href=https://github.com/go-gl/glfw/v3.3/glfw/compare/12ad95a...6f7a984>12ad95a → 6f7a984</a></li><li>github.com/golang/groupcache: <a href=https://github.com/golang/groupcache/compare/215e871...8c9f03a>215e871 → 8c9f03a</a></li><li>github.com/golang/mock: <a href=https://github.com/golang/mock/compare/v1.3.1...v1.4.1>v1.3.1 → v1.4.1</a></li><li>github.com/golang/protobuf: <a href=https://github.com/golang/protobuf/compare/v1.4.2...v1.4.3>v1.4.2 → v1.4.3</a></li><li>github.com/google/cadvisor: <a href=https://github.com/google/cadvisor/compare/v0.37.0...v0.38.5>v0.37.0 → v0.38.5</a></li><li>github.com/google/go-cmp: <a href=https://github.com/google/go-cmp/compare/v0.4.0...v0.5.2>v0.4.0 → v0.5.2</a></li><li>github.com/google/pprof: <a href=https://github.com/google/pprof/compare/d4f498a...1ebb73c>d4f498a → 1ebb73c</a></li><li>github.com/google/uuid: <a href=https://github.com/google/uuid/compare/v1.1.1...v1.1.2>v1.1.1 → v1.1.2</a></li><li>github.com/gorilla/mux: <a href=https://github.com/gorilla/mux/compare/v1.7.3...v1.8.0>v1.7.3 → v1.8.0</a></li><li>github.com/gorilla/websocket: <a href=https://github.com/gorilla/websocket/compare/v1.4.0...v1.4.2>v1.4.0 → v1.4.2</a></li><li>github.com/jmespath/go-jmespath: <a href=https://github.com/jmespath/go-jmespath/compare/c2b33e8...v0.4.0>c2b33e8 → v0.4.0</a></li><li>github.com/karrick/godirwalk: <a href=https://github.com/karrick/godirwalk/compare/v1.7.5...v1.16.1>v1.7.5 → v1.16.1</a></li><li>github.com/opencontainers/go-digest: <a href=https://github.com/opencontainers/go-digest/compare/v1.0.0-rc1...v1.0.0>v1.0.0-rc1 → v1.0.0</a></li><li>github.com/opencontainers/runc: <a href=https://github.com/opencontainers/runc/compare/819fcc6...v1.0.0-rc92>819fcc6 → v1.0.0-rc92</a></li><li>github.com/opencontainers/runtime-spec: <a href=https://github.com/opencontainers/runtime-spec/compare/237cc4f...4d89ac9>237cc4f → 4d89ac9</a></li><li>github.com/opencontainers/selinux: <a href=https://github.com/opencontainers/selinux/compare/v1.5.2...v1.6.0>v1.5.2 → v1.6.0</a></li><li>github.com/prometheus/procfs: <a href=https://github.com/prometheus/procfs/compare/v0.1.3...v0.2.0>v0.1.3 → v0.2.0</a></li><li>github.com/quobyte/api: <a href=https://github.com/quobyte/api/compare/v0.1.2...v0.1.8>v0.1.2 → v0.1.8</a></li><li>github.com/spf13/cobra: <a href=https://github.com/spf13/cobra/compare/v1.0.0...v1.1.1>v1.0.0 → v1.1.1</a></li><li>github.com/spf13/viper: <a href=https://github.com/spf13/viper/compare/v1.4.0...v1.7.0>v1.4.0 → v1.7.0</a></li><li>github.com/storageos/go-api: <a href=https://github.com/storageos/go-api/compare/343b3ef...v2.2.0>343b3ef → v2.2.0+incompatible</a></li><li>github.com/stretchr/testify: <a href=https://github.com/stretchr/testify/compare/v1.4.0...v1.6.1>v1.4.0 → v1.6.1</a></li><li>github.com/vishvananda/netns: <a href=https://github.com/vishvananda/netns/compare/52d707b...db3c7e5>52d707b → db3c7e5</a></li><li>go.etcd.io/etcd: 17cef6e → dd1b699</li><li>go.opencensus.io: v0.22.2 → v0.22.3</li><li>golang.org/x/crypto: 75b2880 → 7f63de1</li><li>golang.org/x/exp: da58074 → 6cc2880</li><li>golang.org/x/lint: fdd1cda → 738671d</li><li>golang.org/x/net: ab34263 → 69a7880</li><li>golang.org/x/oauth2: 858c2ad → bf48bf1</li><li>golang.org/x/sys: ed371f2 → 5cba982</li><li>golang.org/x/text: v0.3.3 → v0.3.4</li><li>golang.org/x/time: 555d28b → 3af7569</li><li>golang.org/x/xerrors: 9bdfabe → 5ec99f8</li><li>google.golang.org/api: v0.15.1 → v0.20.0</li><li>google.golang.org/genproto: cb27e3a → 8816d57</li><li>google.golang.org/grpc: v1.27.0 → v1.27.1</li><li>google.golang.org/protobuf: v1.24.0 → v1.25.0</li><li>honnef.co/go/tools: v0.0.1-2019.2.3 → v0.0.1-2020.1.3</li><li>k8s.io/gengo: 8167cfd → 83324d8</li><li>k8s.io/klog/v2: v2.2.0 → v2.4.0</li><li>k8s.io/kube-openapi: 6aeccd4 → d219536</li><li>k8s.io/system-validators: v1.1.2 → v1.2.0</li><li>k8s.io/utils: d5654de → 67b214c</li><li>sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.9 → v0.0.14</li><li>sigs.k8s.io/structured-merge-diff/v4: v4.0.1 → v4.0.2</li></ul><h3 id=removed-1>Removed</h3><ul><li>github.com/armon/consul-api: <a href=https://github.com/armon/consul-api/tree/eb2c6b5>eb2c6b5</a></li><li>github.com/go-ini/ini: <a href=https://github.com/go-ini/ini/tree/v1.9.0>v1.9.0</a></li><li>github.com/ugorji/go: <a href=https://github.com/ugorji/go/tree/v1.1.4>v1.1.4</a></li><li>github.com/xlab/handysort: <a href=https://github.com/xlab/handysort/tree/fb3537e>fb3537e</a></li><li>github.com/xordataexchange/crypt: <a href=https://github.com/xordataexchange/crypt/tree/b2862e3>b2862e3</a></li><li>vbom.ml/util: db5cfe1</li></ul><h1 id=v1-20-0-rc-0>v1.20.0-rc.0</h1><h2 id=downloads-for-v1-20-0-rc-0>Downloads for v1.20.0-rc.0</h2><h3 id=source-code>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>acfee8658831f9503fccda0904798405434f17be7064a361a9f34c6ed04f1c0f685e79ca40cef5fcf34e3193bacbf467665e8dc277e0562ebdc929170034b5ae</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>9d962f8845e1fa221649cf0c0e178f0f03808486c49ea15ab5ec67861ec5aa948cf18bc0ee9b2067643c8332227973dd592e6a4457456a9d9d80e8ef28d5f7c3</td></tr></tbody></table><h3 id=client-binaries-1>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>062b57f1a450fe01d6184f104d81d376bdf5720010412821e315fd9b1b622a400ac91f996540daa66cee172006f3efade4eccc19265494f1a1d7cc9450f0b50a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>86e96d2c2046c5e62e02bef30a6643f25e01f1b3eba256cab7dd61252908540c26cb058490e9cecc5a9bad97d2b577f5968884e9f1a90237e302419f39e068bc</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>619d3afb9ce902368390e71633396010e88e87c5fd848e3adc71571d1d4a25be002588415e5f83afee82460f8a7c9e0bd968335277cb8f8cb51e58d4bb43e64e</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>60965150a60ab3d05a248339786e0c7da4b89a04539c3719737b13d71302bac1dd9bcaa427d8a1f84a7b42d0c67801dce2de0005e9e47d21122868b32ac3d40f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>688e064f4ef6a17189dbb5af468c279b9de35e215c40500fb97b1d46692d222747023f9e07a7f7ba006400f9532a8912e69d7c5143f956b1dadca144c67ee711</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>47b8abc02b42b3b1de67da184921b5801d7e3cb09befac840c85913193fc5ac4e5e3ecfcb57da6b686ff21af9a3bd42ae6949d4744dbe6ad976794340e328b83</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>971b41d3169f30e6c412e0254c180636abb7ccc8dcee6641b0e9877b69752fc61aa30b76c19c108969df654fe385da3cb3a44dd59d3c28dc45561392d7e08874</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>2d34e8387e31531d9aca5655f2f0d18e75b01825dc1c39b7beb73a7b7b610e2ba429e5ca97d5c41a71b67e75e7096c86ab63fda9baab4c0878c1ccb3a1aefac8</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>f909640f4140693bb871936f10a40e79b43502105d0adb318b35bb7a64a770ad9d05a3a732368ccd3d15d496d75454789165bd1f5c2571da9a00569b3e6c007c</td></tr></tbody></table><h3 id=server-binaries-1>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>0ea4458ae34108c633b4d48f1f128c6274dbc82b613492e78b3e0a2f656ac0df0bb9a75124e15d67c8e81850adcecf19f4ab0234c17247ee7ddf84f2df3e5eaa</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>aef6a4d457faa29936603370f29a8523bb274211c3cb5101bd31aaf469c91ba6bd149ea99a4ccdd83352cf37e4d6508c5ee475ec10292bccd2f77ceea31e1c28</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>4829f473e9d60f9929ad17c70fdc2b6b6509ed75418be0b23a75b28580949736cb5b0bd6382070f93aa0a2a8863f0b1596daf965186ca749996c29d03ef7d8b8</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>9ab0790d382a3e28df1c013762c09da0085449cfd09d176d80be932806c24a715ea829be0075c3e221a2ad9cf06e726b4c39ab41987c1fb0fee2563e48206763</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>98670b587e299856dd9821b7517a35f9a65835b915b153de08b66c54d82160438b66f774bf5306c07bc956d70ff709860bc23162225da5e89f995d3fdc1f0122</td></tr></tbody></table><h3 id=node-binaries-1>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>699e9c8d1837198312eade8eb6fec390f6a2fea9e08207d2f58e8bb6e3e799028aca69e4670aac0a4ba7cf0af683aee2c158bf78cc520c80edc876c8d94d521a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>f3b5eab0669490e3cd7e802693daf3555d08323dfff6e73a881fce00fed4690e8bdaf1610278d9de74036ca37631016075e5695a02158b7d3e7582b20ef7fa35</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>e5012f77363561a609aaf791baaa17d09009819c4085a57132e5feb5366275a54640094e6ed1cba527f42b586c6d62999c2a5435edf5665ff0e114db4423c2ae</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>2a6d6501620b1a9838dff05c66a40260cc22154a28027813346eb16e18c386bc3865298a46a0f08da71cd55149c5e7d07c4c4c431b4fd231486dd9d716548adb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>5eca02777519e31428a1e5842fe540b813fb8c929c341bbc71dcfd60d98deb89060f8f37352e8977020e21e053379eead6478eb2d54ced66fb9d38d5f3142bf0</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-rc.0/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>8ace02e7623dff894e863a2e0fa7dfb916368431d1723170713fe82e334c0ae0481b370855b71e2561de0fb64fed124281be604761ec08607230b66fb9ed1c03</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-beta-2>Changelog since v1.20.0-beta.2</h2><h2 id=changes-by-kind-1>Changes by Kind</h2><h3 id=feature-1>Feature</h3><ul><li>Kubernetes is now built using go1.15.5<ul><li>build: Update to <a href=mailto:k/repo-infra@v0.1.2>k/repo-infra@v0.1.2</a> (supports go1.15.5) (<a href=https://github.com/kubernetes/kubernetes/pull/95776>#95776</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Cloud Provider, Instrumentation, Release and Testing]</li></ul></li></ul><h3 id=failing-test-1>Failing Test</h3><ul><li>Resolves an issue running Ingress conformance tests on clusters which use finalizers on Ingress objects to manage releasing load balancer resources (<a href=https://github.com/kubernetes/kubernetes/pull/96742>#96742</a>, <a href=https://github.com/spencerhance>@spencerhance</a>) [SIG Network and Testing]</li><li>The Conformance test "validates that there is no conflict between pods with same hostPort but different hostIP and protocol" now validates the connectivity to each hostPort, in addition to the functionality. (<a href=https://github.com/kubernetes/kubernetes/pull/96627>#96627</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Scheduling and Testing]</li></ul><h3 id=bug-or-regression-1>Bug or Regression</h3><ul><li>Bump node-problem-detector version to v0.8.5 to fix OOM detection in with Linux kernels 5.1+ (<a href=https://github.com/kubernetes/kubernetes/pull/96716>#96716</a>, <a href=https://github.com/tosi3k>@tosi3k</a>) [SIG Cloud Provider, Scalability and Testing]</li><li>Changes to timeout parameter handling in 1.20.0-beta.2 have been reverted to avoid breaking backwards compatibility with existing clients. (<a href=https://github.com/kubernetes/kubernetes/pull/96727>#96727</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Testing]</li><li>Duplicate owner reference entries in create/update/patch requests now get deduplicated by the API server. The client sending the request now receives a warning header in the API response. Clients should stop sending requests with duplicate owner references. The API server may reject such requests as early as 1.24. (<a href=https://github.com/kubernetes/kubernetes/pull/96185>#96185</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery and Testing]</li><li>Fix: resize Azure disk issue when it's in attached state (<a href=https://github.com/kubernetes/kubernetes/pull/96705>#96705</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fixed a bug where aggregator_unavailable_apiservice metrics were reported for deleted apiservices. (<a href=https://github.com/kubernetes/kubernetes/pull/96421>#96421</a>, <a href=https://github.com/dgrisonnet>@dgrisonnet</a>) [SIG API Machinery and Instrumentation]</li><li>Fixes code generation for non-namespaced create subresources fake client test. (<a href=https://github.com/kubernetes/kubernetes/pull/96586>#96586</a>, <a href=https://github.com/Doude>@Doude</a>) [SIG API Machinery]</li><li>HTTP/2 connection health check is enabled by default in all Kubernetes clients. The feature should work out-of-the-box. If needed, users can tune the feature via the HTTP2_READ_IDLE_TIMEOUT_SECONDS and HTTP2_PING_TIMEOUT_SECONDS environment variables. The feature is disabled if HTTP2_READ_IDLE_TIMEOUT_SECONDS is set to 0. (<a href=https://github.com/kubernetes/kubernetes/pull/95981>#95981</a>, <a href=https://github.com/caesarxuchao>@caesarxuchao</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Node]</li><li>Kubeadm: fix coredns migration should be triggered when there are newdefault configs during kubeadm upgrade (<a href=https://github.com/kubernetes/kubernetes/pull/96907>#96907</a>, <a href=https://github.com/pacoxu>@pacoxu</a>) [SIG Cluster Lifecycle]</li><li>Reduce volume name length for vsphere volumes (<a href=https://github.com/kubernetes/kubernetes/pull/96533>#96533</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Storage]</li><li>Resolves a regression in 1.19+ with workloads targeting deprecated beta os/arch labels getting stuck in NodeAffinity status on node startup. (<a href=https://github.com/kubernetes/kubernetes/pull/96810>#96810</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node]</li></ul><h2 id=dependencies-2>Dependencies</h2><h3 id=added-2>Added</h3><p><em>Nothing has changed.</em></p><h3 id=changed-2>Changed</h3><ul><li>github.com/google/cadvisor: <a href=https://github.com/google/cadvisor/compare/v0.38.4...v0.38.5>v0.38.4 → v0.38.5</a></li></ul><h3 id=removed-2>Removed</h3><p><em>Nothing has changed.</em></p><h1 id=v1-20-0-beta-2>v1.20.0-beta.2</h1><h2 id=downloads-for-v1-20-0-beta-2>Downloads for v1.20.0-beta.2</h2><h3 id=source-code-1>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>fe769280aa623802a949b6a35fbddadbba1d6f9933a54132a35625683719595ecf58096a9aa0f7456f8d4931774df21bfa98e148bc3d85913f1da915134f77bd</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>ce1c8d97c52e5189af335d673bd7e99c564816f6adebf249838f7e3f0e920f323b4e398a5d163ea767091497012ec38843c59ff14e6fdd07683b682135eed645</td></tr></tbody></table><h3 id=client-binaries-2>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>d6c14bd0f6702f4bbdf14a6abdfa4e5936de5b4efee38aa86c2bd7272967ec6d7868b88fc00ad4a7c3a20717a35e6be2b84e56dec04154fd702315f641409f7c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>b923c44cb0acb91a8f6fd442c2168aa6166c848f5d037ce50a7cb11502be3698db65836b373c916f75b648d6ac8d9158807a050eecc4e1c77cffa25b386c8cdb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>8cae14146a9034dcd4e9d69d5d700f195a77aac35f629a148960ae028ed8b4fe12213993fe3e6e464b4b3e111adebe6f3dd7ca0accc70c738ed5cfd8993edd7c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>1f54e5262a0432945ead57fcb924e6bfedd9ea76db1dd9ebd946787a2923c247cf16e10505307b47e365905a1b398678dac5af0f433c439c158a33e08362d97b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>31cf79c01e4878a231b4881fe3ed5ef790bd5fb5419388438d3f8c6a2129e655aba9e00b8e1d77e0bc5d05ecc75cf4ae02cf8266788822d0306c49c85ee584ed</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>2527948c40be2e16724d939316ad5363f15aa22ebf42d59359d8b6f757d30cfef6447434cc93bc5caa5a23a6a00a2da8d8191b6441e06bba469d9d4375989a97</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>b777ad764b3a46651ecb0846e5b7f860bb2c1c4bd4d0fcc468c6ccffb7d3b8dcb6dcdd73b13c16ded7219f91bba9f1e92f9258527fd3bb162b54d7901ac303ff</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>8a2f58aaab01be9fe298e4d01456536047cbdd39a37d3e325c1f69ceab3a0504998be41a9f41a894735dfc4ed22bed02591eea5f3c75ce12d9e95ba134e72ec5</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>2f69cda177a178df149f5de66b7dba7f5ce14c1ffeb7c8d7dc4130c701b47d89bb2fbe74e7a262f573e4d21dee2c92414d050d7829e7c6fc3637a9d6b0b9c5c1</td></tr></tbody></table><h3 id=server-binaries-2>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>3ecaac0213d369eab691ac55376821a80df5013cb12e1263f18d1c236a9e49d42b3cea422175556d8f929cdf3109b22c0b6212ac0f2e80cc7a5f4afa3aba5f24</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>580030b57ff207e177208fec0801a43389cae10cc2c9306327d354e7be6a055390184531d54b6742e0983550b7a76693cc4a705c2d2f4ac30495cf63cef26b9b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>3e3286bd54671549fbef0dfdaaf1da99bc5c3efb32cc8d1e1985d9926520cea0c43bcf7cbcbbc8b1c1a95eab961255693008af3bb1ba743362998b5f0017d6d7</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>9fa051e7e97648e97e26b09ab6d26be247b41b1a5938d2189204c9e6688e455afe76612bbcdd994ed5692935d0d960bd96dc222bce4b83f61d62557752b9d75b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>fa85d432eff586f30975c95664ac130b9f5ae02dc52b97613ed7a41324496631ea11d1a267daba564cf2485a9e49707814d86bbd3175486c7efc8b58a9314af5</td></tr></tbody></table><h3 id=node-binaries-2>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>86e631f95fe670b467ead2b88d34e0364eaa275935af433d27cc378d82dcaa22041ccce40f5fa9561b9656dadaa578dc018ad458a59b1690d35f86dca4776b5c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>a8754ff58a0e902397056b8615ab49af07aca347ba7cc4a812c238e3812234862270f25106b6a94753b157bb153b8eae8b39a01ed67384774d798598c243583b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>28d727d7d08e2c856c9b4a574ef2dbf9e37236a0555f7ec5258b4284fa0582fb94b06783aaf50bf661f7503d101fbd70808aba6de02a2f0af94db7d065d25947</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>a1283449f1a0b155c11449275e9371add544d0bdd4609d6dc737ed5f7dd228e84e24ff249613a2a153691627368dd894ad64f4e6c0010eecc6efd2c13d4fb133</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>5806028ba15a6a9c54a34f90117bc3181428dbb0e7ced30874c9f4a953ea5a0e9b2c73e6b1e2545e1b4e5253e9c7691588538b44cdfa666ce6865964b92d2fa8</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.2/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>d5327e3b7916c78777b9b69ba0f3758c3a8645c67af80114a0ae52babd7af27bb504febbaf51b1bfe5bd2d74c8c5c573471e1cb449f2429453f4b1be9d5e682a</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-beta-1>Changelog since v1.20.0-beta.1</h2><h2 id=urgent-upgrade-notes-1>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade-1>(No, really, you MUST read this before you upgrade)</h3><ul><li>A bug was fixed in kubelet where exec probe timeouts were not respected. Ensure that pods relying on this behavior are updated to correctly handle probe timeouts.</li></ul><p>This change in behavior may be unexpected for some clusters and can be disabled by turning off the ExecProbeTimeout feature gate. This gate will be locked and removed in future releases so that exec probe timeouts are always respected. (<a href=https://github.com/kubernetes/kubernetes/pull/94115>#94115</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Node and Testing]</p><ul><li><p>For CSI drivers, kubelet no longer creates the target_path for NodePublishVolume in accordance with the CSI spec. Kubelet also no longer checks if staging and target paths are mounts or corrupted. CSI drivers need to be idempotent and do any necessary mount verification. (<a href=https://github.com/kubernetes/kubernetes/pull/88759>#88759</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</p></li><li><p>Kubeadm:</p></li><li><p>The label applied to control-plane nodes "node-role.kubernetes.io/master" is now deprecated and will be removed in a future release after a GA deprecation period.</p></li><li><p>Introduce a new label "node-role.kubernetes.io/control-plane" that will be applied in parallel to "node-role.kubernetes.io/master" until the removal of the "node-role.kubernetes.io/master" label.</p></li><li><p>Make "kubeadm upgrade apply" add the "node-role.kubernetes.io/control-plane" label on existing nodes that only have the "node-role.kubernetes.io/master" label during upgrade.</p></li><li><p>Please adapt your tooling built on top of kubeadm to use the "node-role.kubernetes.io/control-plane" label.</p></li><li><p>The taint applied to control-plane nodes "node-role.kubernetes.io/master:NoSchedule" is now deprecated and will be removed in a future release after a GA deprecation period.</p></li><li><p>Apply toleration for a new, future taint "node-role.kubernetes.io/control-plane:NoSchedule" to the kubeadm CoreDNS / kube-dns managed manifests. Note that this taint is not yet applied to kubeadm control-plane nodes.</p></li><li><p>Please adapt your workloads to tolerate the same future taint preemptively.</p></li></ul><p>For more details see: <a href=http://git.k8s.io/enhancements/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint/README.md>http://git.k8s.io/enhancements/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint/README.md</a> (<a href=https://github.com/kubernetes/kubernetes/pull/95382>#95382</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p><h2 id=changes-by-kind-2>Changes by Kind</h2><h3 id=deprecation-1>Deprecation</h3><ul><li>Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet uses a module called "dockershim" which implements CRI support for Docker and it has seen maintenance issues in the Kubernetes community. We encourage you to evaluate moving to a container runtime that is a full-fledged implementation of CRI (v1alpha1 or v1 compliant) as they become available. (<a href=https://github.com/kubernetes/kubernetes/pull/94624>#94624</a>, <a href=https://github.com/dims>@dims</a>) [SIG Node]</li><li>Kubectl: deprecate --delete-local-data (<a href=https://github.com/kubernetes/kubernetes/pull/95076>#95076</a>, <a href=https://github.com/dougsland>@dougsland</a>) [SIG CLI, Cloud Provider and Scalability]</li></ul><h3 id=api-change-1>API Change</h3><ul><li><p>API priority and fairness graduated to beta
1.19 servers with APF turned on should not be run in a multi-server cluster with 1.20+ servers. (<a href=https://github.com/kubernetes/kubernetes/pull/96527>#96527</a>, <a href=https://github.com/adtac>@adtac</a>) [SIG API Machinery and Testing]</p></li><li><p>Add LoadBalancerIPMode feature gate (<a href=https://github.com/kubernetes/kubernetes/pull/92312>#92312</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG Apps, CLI, Cloud Provider and Network]</p></li><li><p>Add WindowsContainerResources and Annotations to CRI-API UpdateContainerResourcesRequest (<a href=https://github.com/kubernetes/kubernetes/pull/95741>#95741</a>, <a href=https://github.com/katiewasnothere>@katiewasnothere</a>) [SIG Node]</p></li><li><p>Add a 'serving' and <code>terminating</code> condition to the EndpointSlice API.</p><p><code>serving</code> tracks the readiness of endpoints regardless of their terminating state. This is distinct from <code>ready</code> since <code>ready</code> is only true when pods are not terminating.
<code>terminating</code> is true when an endpoint is terminating. For pods this is any endpoint with a deletion timestamp. (<a href=https://github.com/kubernetes/kubernetes/pull/92968>#92968</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps and Network]</p></li><li><p>Add support for hugepages to downward API (<a href=https://github.com/kubernetes/kubernetes/pull/86102>#86102</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>) [SIG API Machinery, Apps, CLI, Network, Node, Scheduling and Testing]</p></li><li><p>Adds kubelet alpha feature, <code>GracefulNodeShutdown</code> which makes kubelet aware of node system shutdowns and result in graceful termination of pods during a system shutdown. (<a href=https://github.com/kubernetes/kubernetes/pull/96129>#96129</a>, <a href=https://github.com/bobbypage>@bobbypage</a>) [SIG Node]</p></li><li><p>AppProtocol is now GA for Endpoints and Services. The ServiceAppProtocol feature gate will be deprecated in 1.21. (<a href=https://github.com/kubernetes/kubernetes/pull/96327>#96327</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</p></li><li><p>Automatic allocation of NodePorts for services with type LoadBalancer can now be disabled by setting the (new) parameter
Service.spec.allocateLoadBalancerNodePorts=false. The default is to allocate NodePorts for services with type LoadBalancer which is the existing behavior. (<a href=https://github.com/kubernetes/kubernetes/pull/92744>#92744</a>, <a href=https://github.com/uablrek>@uablrek</a>) [SIG Apps and Network]</p></li><li><p>Document that ServiceTopology feature is required to use <code>service.spec.topologyKeys</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/96528>#96528</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps]</p></li><li><p>EndpointSlice has a new NodeName field guarded by the EndpointSliceNodeName feature gate.</p><ul><li>EndpointSlice topology field will be deprecated in an upcoming release.</li><li>EndpointSlice "IP" address type is formally removed after being deprecated in Kubernetes 1.17.</li><li>The discovery.k8s.io/v1alpha1 API is deprecated and will be removed in Kubernetes 1.21. (<a href=https://github.com/kubernetes/kubernetes/pull/96440>#96440</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps and Network]</li></ul></li><li><p>Fewer candidates are enumerated for preemption to improve performance in large clusters (<a href=https://github.com/kubernetes/kubernetes/pull/94814>#94814</a>, <a href=https://github.com/adtac>@adtac</a>) [SIG Scheduling]</p></li><li><p>If BoundServiceAccountTokenVolume is enabled, cluster admins can use metric <code>serviceaccount_stale_tokens_total</code> to monitor workloads that are depending on the extended tokens. If there are no such workloads, turn off extended tokens by starting <code>kube-apiserver</code> with flag <code>--service-account-extend-token-expiration=false</code> (<a href=https://github.com/kubernetes/kubernetes/pull/96273>#96273</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery and Auth]</p></li><li><p>Introduce alpha support for exec-based container registry credential provider plugins in the kubelet. (<a href=https://github.com/kubernetes/kubernetes/pull/94196>#94196</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Node and Release]</p></li><li><p>Kube-apiserver now deletes expired kube-apiserver Lease objects:</p><ul><li>The feature is under feature gate <code>APIServerIdentity</code>.</li><li>A flag is added to kube-apiserver: <code>identity-lease-garbage-collection-check-period-seconds</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95895>#95895</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery, Apps, Auth and Testing]</li></ul></li><li><p>Move configurable fsgroup change policy for pods to beta (<a href=https://github.com/kubernetes/kubernetes/pull/96376>#96376</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Apps and Storage]</p></li><li><p>New flag is introduced, i.e. --topology-manager-scope=container|pod.
The default value is the "container" scope. (<a href=https://github.com/kubernetes/kubernetes/pull/92967>#92967</a>, <a href=https://github.com/cezaryzukowski>@cezaryzukowski</a>) [SIG Instrumentation, Node and Testing]</p></li><li><p>NodeAffinity plugin can be configured with AddedAffinity. (<a href=https://github.com/kubernetes/kubernetes/pull/96202>#96202</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Node, Scheduling and Testing]</p></li><li><p>Promote RuntimeClass feature to GA.
Promote node.k8s.io API groups from v1beta1 to v1. (<a href=https://github.com/kubernetes/kubernetes/pull/95718>#95718</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>) [SIG Apps, Auth, Node, Scheduling and Testing]</p></li><li><p>Reminder: The labels "failure-domain.beta.kubernetes.io/zone" and "failure-domain.beta.kubernetes.io/region" are deprecated in favor of "topology.kubernetes.io/zone" and "topology.kubernetes.io/region" respectively. All users of the "failure-domain.beta..." labels should switch to the "topology..." equivalents. (<a href=https://github.com/kubernetes/kubernetes/pull/96033>#96033</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG API Machinery, Apps, CLI, Cloud Provider, Network, Node, Scheduling, Storage and Testing]</p></li><li><p>The usage of mixed protocol values in the same LoadBalancer Service is possible if the new feature gate MixedProtocolLBSVC is enabled.
"action required"
The feature gate is disabled by default. The user has to enable it for the API Server. (<a href=https://github.com/kubernetes/kubernetes/pull/94028>#94028</a>, <a href=https://github.com/janosi>@janosi</a>) [SIG API Machinery and Apps]</p></li><li><p>This PR will introduce a feature gate CSIServiceAccountToken with two additional fields in <code>CSIDriverSpec</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/93130>#93130</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Apps, Auth, CLI, Network, Node, Storage and Testing]</p></li><li><p>Users can try the cronjob controller v2 using the feature gate. This will be the default controller in future releases. (<a href=https://github.com/kubernetes/kubernetes/pull/93370>#93370</a>, <a href=https://github.com/alaypatel07>@alaypatel07</a>) [SIG API Machinery, Apps, Auth and Testing]</p></li><li><p>VolumeSnapshotDataSource moves to GA in 1.20 release (<a href=https://github.com/kubernetes/kubernetes/pull/95282>#95282</a>, <a href=https://github.com/xing-yang>@xing-yang</a>) [SIG Apps]</p></li></ul><h3 id=feature-2>Feature</h3><ul><li><strong>Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.</strong>: (<a href=https://github.com/kubernetes/kubernetes/pull/95896>#95896</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery and Cluster Lifecycle]</li><li>A new set of alpha metrics are reported by the Kubernetes scheduler under the <code>/metrics/resources</code> endpoint that allow administrators to easily see the resource consumption (requests and limits for all resources on the pods) and compare it to actual pod usage or node capacity. (<a href=https://github.com/kubernetes/kubernetes/pull/94866>#94866</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG API Machinery, Instrumentation, Node and Scheduling]</li><li>Add --experimental-logging-sanitization flag enabling runtime protection from leaking sensitive data in logs (<a href=https://github.com/kubernetes/kubernetes/pull/96370>#96370</a>, <a href=https://github.com/serathius>@serathius</a>) [SIG API Machinery, Cluster Lifecycle and Instrumentation]</li><li>Add a StorageVersionAPI feature gate that makes API server update storageversions before serving certain write requests.
This feature allows the storage migrator to manage storage migration for built-in resources.
Enabling internal.apiserver.k8s.io/v1alpha1 API and APIServerIdentity feature gate are required to use this feature. (<a href=https://github.com/kubernetes/kubernetes/pull/93873>#93873</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery, Auth and Testing]</li><li>Add a new <code>vSphere</code> metric: <code>cloudprovider_vsphere_vcenter_versions</code>. It's content show <code>vCenter</code> hostnames with the associated server version. (<a href=https://github.com/kubernetes/kubernetes/pull/94526>#94526</a>, <a href=https://github.com/Danil-Grigorev>@Danil-Grigorev</a>) [SIG Cloud Provider and Instrumentation]</li><li>Add feature to size memory backed volumes (<a href=https://github.com/kubernetes/kubernetes/pull/94444>#94444</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>) [SIG Storage and Testing]</li><li>Add node_authorizer_actions_duration_seconds metric that can be used to estimate load to node authorizer. (<a href=https://github.com/kubernetes/kubernetes/pull/92466>#92466</a>, <a href=https://github.com/mborsz>@mborsz</a>) [SIG API Machinery, Auth and Instrumentation]</li><li>Add pod_ based CPU and memory metrics to Kubelet's /metrics/resource endpoint (<a href=https://github.com/kubernetes/kubernetes/pull/95839>#95839</a>, <a href=https://github.com/egernst>@egernst</a>) [SIG Instrumentation, Node and Testing]</li><li>Adds a headless service on node-local-cache addon. (<a href=https://github.com/kubernetes/kubernetes/pull/88412>#88412</a>, <a href=https://github.com/stafot>@stafot</a>) [SIG Cloud Provider and Network]</li><li>CRDs: For structural schemas, non-nullable null map fields will now be dropped and defaulted if a default is available. null items in list will continue being preserved, and fail validation if not nullable. (<a href=https://github.com/kubernetes/kubernetes/pull/95423>#95423</a>, <a href=https://github.com/apelisse>@apelisse</a>) [SIG API Machinery]</li><li>E2e test for PodFsGroupChangePolicy (<a href=https://github.com/kubernetes/kubernetes/pull/96247>#96247</a>, <a href=https://github.com/saikat-royc>@saikat-royc</a>) [SIG Storage and Testing]</li><li>Gradudate the Pod Resources API to G.A
Introduces the pod_resources_endpoint_requests_total metric which tracks the total number of requests to the pod resources API (<a href=https://github.com/kubernetes/kubernetes/pull/92165>#92165</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Instrumentation, Node and Testing]</li><li>Introduce api-extensions category which will return: mutating admission configs, validating admission configs, CRDs and APIServices when used in kubectl get, for example. (<a href=https://github.com/kubernetes/kubernetes/pull/95603>#95603</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG API Machinery]</li><li>Kube-apiserver now maintains a Lease object to identify itself:<ul><li>The feature is under feature gate <code>APIServerIdentity</code>.</li><li>Two flags are added to kube-apiserver: <code>identity-lease-duration-seconds</code>, <code>identity-lease-renew-interval-seconds</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95533>#95533</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery]</li></ul></li><li>Kube-apiserver: The timeout used when making health check calls to etcd can now be configured with <code>--etcd-healthcheck-timeout</code>. The default timeout is 2 seconds, matching the previous behavior. (<a href=https://github.com/kubernetes/kubernetes/pull/93244>#93244</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG API Machinery]</li><li>Kubectl: Previously users cannot provide arguments to a external diff tool via KUBECTL_EXTERNAL_DIFF env. This release now allow users to specify args to KUBECTL_EXTERNAL_DIFF env. (<a href=https://github.com/kubernetes/kubernetes/pull/95292>#95292</a>, <a href=https://github.com/dougsland>@dougsland</a>) [SIG CLI]</li><li>Scheduler now ignores Pod update events if the resourceVersion of old and new Pods are identical. (<a href=https://github.com/kubernetes/kubernetes/pull/96071>#96071</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li><li>Support custom tags for cloud provider managed resources (<a href=https://github.com/kubernetes/kubernetes/pull/96450>#96450</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Support customize load balancer health probe protocol and request path (<a href=https://github.com/kubernetes/kubernetes/pull/96338>#96338</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Support multiple standard load balancers in one cluster (<a href=https://github.com/kubernetes/kubernetes/pull/96111>#96111</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>The beta <code>RootCAConfigMap</code> feature gate is enabled by default and causes kube-controller-manager to publish a "kube-root-ca.crt" ConfigMap to every namespace. This ConfigMap contains a CA bundle used for verifying connections to the kube-apiserver. (<a href=https://github.com/kubernetes/kubernetes/pull/96197>#96197</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Apps, Auth and Testing]</li><li>The kubelet_runtime_operations_duration_seconds metric got additional buckets of 60, 300, 600, 900 and 1200 seconds (<a href=https://github.com/kubernetes/kubernetes/pull/96054>#96054</a>, <a href=https://github.com/alvaroaleman>@alvaroaleman</a>) [SIG Instrumentation and Node]</li><li>There is a new pv_collector_total_pv_count metric that counts persistent volumes by the volume plugin name and volume mode. (<a href=https://github.com/kubernetes/kubernetes/pull/95719>#95719</a>, <a href=https://github.com/tsmetana>@tsmetana</a>) [SIG Apps, Instrumentation, Storage and Testing]</li><li>Volume snapshot e2e test to validate PVC and VolumeSnapshotContent finalizer (<a href=https://github.com/kubernetes/kubernetes/pull/95863>#95863</a>, <a href=https://github.com/RaunakShah>@RaunakShah</a>) [SIG Cloud Provider, Storage and Testing]</li><li>Warns user when executing kubectl apply/diff to resource currently being deleted. (<a href=https://github.com/kubernetes/kubernetes/pull/95544>#95544</a>, <a href=https://github.com/SaiHarshaK>@SaiHarshaK</a>) [SIG CLI]</li><li><code>kubectl alpha debug</code> has graduated to beta and is now <code>kubectl debug</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/96138>#96138</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI and Testing]</li><li><code>kubectl debug</code> gains support for changing container images when copying a pod for debugging, similar to how <code>kubectl set image</code> works. See <code>kubectl help debug</code> for more information. (<a href=https://github.com/kubernetes/kubernetes/pull/96058>#96058</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI]</li></ul><h3 id=documentation-1>Documentation</h3><ul><li>Updates docs and guidance on cloud provider InstancesV2 and Zones interface for external cloud providers:<ul><li>removes experimental warning for InstancesV2</li><li>document that implementation of InstancesV2 will disable calls to Zones</li><li>deprecate Zones in favor of InstancesV2 (<a href=https://github.com/kubernetes/kubernetes/pull/96397>#96397</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Cloud Provider]</li></ul></li></ul><h3 id=bug-or-regression-2>Bug or Regression</h3><ul><li>Change plugin name in fsgroupapplymetrics of csi and flexvolume to distinguish different driver (<a href=https://github.com/kubernetes/kubernetes/pull/95892>#95892</a>, <a href=https://github.com/JornShen>@JornShen</a>) [SIG Instrumentation, Storage and Testing]</li><li>Clear UDP conntrack entry on endpoint changes when using nodeport (<a href=https://github.com/kubernetes/kubernetes/pull/71573>#71573</a>, <a href=https://github.com/JacobTanenbaum>@JacobTanenbaum</a>) [SIG Network]</li><li>Exposes and sets a default timeout for the TokenReview client for DelegatingAuthenticationOptions (<a href=https://github.com/kubernetes/kubernetes/pull/96217>#96217</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery and Cloud Provider]</li><li>Fix CVE-2020-8555 for Quobyte client connections. (<a href=https://github.com/kubernetes/kubernetes/pull/95206>#95206</a>, <a href=https://github.com/misterikkit>@misterikkit</a>) [SIG Storage]</li><li>Fix IP fragmentation of UDP and TCP packets not supported issues on LoadBalancer rules (<a href=https://github.com/kubernetes/kubernetes/pull/96464>#96464</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Fix a bug that DefaultPreemption plugin is disabled when using (legacy) scheduler policy. (<a href=https://github.com/kubernetes/kubernetes/pull/96439>#96439</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling and Testing]</li><li>Fix bug in JSON path parser where an error occurs when a range is empty (<a href=https://github.com/kubernetes/kubernetes/pull/95933>#95933</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG API Machinery]</li><li>Fix client-go prometheus metrics to correctly present the API path accessed in some environments. (<a href=https://github.com/kubernetes/kubernetes/pull/74363>#74363</a>, <a href=https://github.com/aanm>@aanm</a>) [SIG API Machinery]</li><li>Fix memory leak in kube-apiserver when underlying time goes forth and back. (<a href=https://github.com/kubernetes/kubernetes/pull/96266>#96266</a>, <a href=https://github.com/chenyw1990>@chenyw1990</a>) [SIG API Machinery]</li><li>Fix paging issues when Azure API returns empty values with non-empty nextLink (<a href=https://github.com/kubernetes/kubernetes/pull/96211>#96211</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fix pull image error from multiple ACRs using azure managed identity (<a href=https://github.com/kubernetes/kubernetes/pull/96355>#96355</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix vSphere volumes that could be erroneously attached to wrong node (<a href=https://github.com/kubernetes/kubernetes/pull/96224>#96224</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider and Storage]</li><li>Fixed a bug that prevents kubectl to validate CRDs with schema using x-kubernetes-preserve-unknown-fields on object fields. (<a href=https://github.com/kubernetes/kubernetes/pull/96369>#96369</a>, <a href=https://github.com/gautierdelorme>@gautierdelorme</a>) [SIG API Machinery and Testing]</li><li>For vSphere Cloud Provider, If VM of worker node is deleted, the node will also be deleted by node controller (<a href=https://github.com/kubernetes/kubernetes/pull/92608>#92608</a>, <a href=https://github.com/lubronzhan>@lubronzhan</a>) [SIG Cloud Provider]</li><li>HTTP/2 connection health check is enabled by default in all Kubernetes clients. The feature should work out-of-the-box. If needed, users can tune the feature via the HTTP2_READ_IDLE_TIMEOUT_SECONDS and HTTP2_PING_TIMEOUT_SECONDS environment variables. The feature is disabled if HTTP2_READ_IDLE_TIMEOUT_SECONDS is set to 0. (<a href=https://github.com/kubernetes/kubernetes/pull/95981>#95981</a>, <a href=https://github.com/caesarxuchao>@caesarxuchao</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Node]</li><li>If the user specifies an invalid timeout in the request URL, the request will be aborted with an HTTP 400.<ul><li>If the user specifies a timeout in the request URL that exceeds the maximum request deadline allowed by the apiserver, the request will be aborted with an HTTP 400. (<a href=https://github.com/kubernetes/kubernetes/pull/96061>#96061</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery, Network and Testing]</li></ul></li><li>Improve error messages related to nodePort endpoint changes conntrack entries cleanup. (<a href=https://github.com/kubernetes/kubernetes/pull/96251>#96251</a>, <a href=https://github.com/ravens>@ravens</a>) [SIG Network]</li><li>Print go stack traces at -v=4 and not -v=2 (<a href=https://github.com/kubernetes/kubernetes/pull/94663>#94663</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</li><li>Remove ready file and its directory (which is created during volume SetUp) during emptyDir volume TearDown. (<a href=https://github.com/kubernetes/kubernetes/pull/95770>#95770</a>, <a href=https://github.com/jingxu97>@jingxu97</a>) [SIG Storage]</li><li>Resolves non-deterministic behavior of the garbage collection controller when ownerReferences with incorrect data are encountered. Events with a reason of <code>OwnerRefInvalidNamespace</code> are recorded when namespace mismatches between child and owner objects are detected.<ul><li>A namespaced object with an ownerReference referencing a uid of a namespaced kind which does not exist in the same namespace is now consistently treated as though that owner does not exist, and the child object is deleted.</li><li>A cluster-scoped object with an ownerReference referencing a uid of a namespaced kind is now consistently treated as though that owner is not resolvable, and the child object is ignored by the garbage collector. (<a href=https://github.com/kubernetes/kubernetes/pull/92743>#92743</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps and Testing]</li></ul></li><li>Skip [k8s.io/kubernetes@v1.19.0/test/e2e/storage/testsuites/base.go:162]: Driver azure-disk doesn't support snapshot type DynamicSnapshot -- skipping
skip [k8s.io/kubernetes@v1.19.0/test/e2e/storage/testsuites/base.go:185]: Driver azure-disk doesn't support ntfs -- skipping (<a href=https://github.com/kubernetes/kubernetes/pull/96144>#96144</a>, <a href=https://github.com/qinpingli>@qinpingli</a>) [SIG Storage and Testing]</li><li>The AWS network load balancer attributes can now be specified during service creation (<a href=https://github.com/kubernetes/kubernetes/pull/95247>#95247</a>, <a href=https://github.com/kishorj>@kishorj</a>) [SIG Cloud Provider]</li><li>The kube-apiserver will no longer serve APIs that should have been deleted in GA non-alpha levels. Alpha levels will continue to serve the removed APIs so that CI doesn't immediately break. (<a href=https://github.com/kubernetes/kubernetes/pull/96525>#96525</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery]</li><li>Update max azure data disk count map (<a href=https://github.com/kubernetes/kubernetes/pull/96308>#96308</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Update the route table tag in the route reconcile loop (<a href=https://github.com/kubernetes/kubernetes/pull/96545>#96545</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Volume binding: report UnschedulableAndUnresolvable status instead of an error when bound PVs not found (<a href=https://github.com/kubernetes/kubernetes/pull/95541>#95541</a>, <a href=https://github.com/cofyc>@cofyc</a>) [SIG Apps, Scheduling and Storage]</li><li>[kubectl] Fail when local source file doesn't exist (<a href=https://github.com/kubernetes/kubernetes/pull/90333>#90333</a>, <a href=https://github.com/bamarni>@bamarni</a>) [SIG CLI]</li></ul><h3 id=other-cleanup-or-flake-1>Other (Cleanup or Flake)</h3><ul><li>Handle slow cronjob lister in cronjob controller v2 and improve memory footprint. (<a href=https://github.com/kubernetes/kubernetes/pull/96443>#96443</a>, <a href=https://github.com/alaypatel07>@alaypatel07</a>) [SIG Apps]</li><li>--redirect-container-streaming is no longer functional. The flag will be removed in v1.22 (<a href=https://github.com/kubernetes/kubernetes/pull/95935>#95935</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG Node]</li><li>A new metric <code>requestAbortsTotal</code> has been introduced that counts aborted requests for each <code>group</code>, <code>version</code>, <code>verb</code>, <code>resource</code>, <code>subresource</code> and <code>scope</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/95002>#95002</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery, Cloud Provider, Instrumentation and Scheduling]</li><li>API priority and fairness metrics use snake_case in label names (<a href=https://github.com/kubernetes/kubernetes/pull/96236>#96236</a>, <a href=https://github.com/adtac>@adtac</a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Testing]</li><li>Applies translations on all command descriptions (<a href=https://github.com/kubernetes/kubernetes/pull/95439>#95439</a>, <a href=https://github.com/HerrNaN>@HerrNaN</a>) [SIG CLI]</li><li>Changed: default "Accept-Encoding" header removed from HTTP probes. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes>https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes</a> (<a href=https://github.com/kubernetes/kubernetes/pull/96127>#96127</a>, <a href=https://github.com/fonsecas72>@fonsecas72</a>) [SIG Network and Node]</li><li>Generators for services are removed from kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/95256>#95256</a>, <a href=https://github.com/Git-Jiro>@Git-Jiro</a>) [SIG CLI]</li><li>Introduce kubectl-convert plugin. (<a href=https://github.com/kubernetes/kubernetes/pull/96190>#96190</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI and Testing]</li><li>Kube-scheduler now logs processed component config at startup (<a href=https://github.com/kubernetes/kubernetes/pull/96426>#96426</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Scheduling]</li><li>NONE (<a href=https://github.com/kubernetes/kubernetes/pull/96179>#96179</a>, <a href=https://github.com/bbyrne5>@bbyrne5</a>) [SIG Network]</li><li>Users will now be able to configure all supported values for AWS NLB health check interval and thresholds for new resources. (<a href=https://github.com/kubernetes/kubernetes/pull/96312>#96312</a>, <a href=https://github.com/kishorj>@kishorj</a>) [SIG Cloud Provider]</li></ul><h2 id=dependencies-3>Dependencies</h2><h3 id=added-3>Added</h3><ul><li>cloud.google.com/go/firestore: v1.1.0</li><li>github.com/armon/go-metrics: <a href=https://github.com/armon/go-metrics/tree/f0300d1>f0300d1</a></li><li>github.com/armon/go-radix: <a href=https://github.com/armon/go-radix/tree/7fddfc3>7fddfc3</a></li><li>github.com/bketelsen/crypt: <a href=https://github.com/bketelsen/crypt/tree/5cbc8cc>5cbc8cc</a></li><li>github.com/hashicorp/consul/api: <a href=https://github.com/hashicorp/consul/api/tree/v1.1.0>v1.1.0</a></li><li>github.com/hashicorp/consul/sdk: <a href=https://github.com/hashicorp/consul/sdk/tree/v0.1.1>v0.1.1</a></li><li>github.com/hashicorp/errwrap: <a href=https://github.com/hashicorp/errwrap/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-cleanhttp: <a href=https://github.com/hashicorp/go-cleanhttp/tree/v0.5.1>v0.5.1</a></li><li>github.com/hashicorp/go-immutable-radix: <a href=https://github.com/hashicorp/go-immutable-radix/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-msgpack: <a href=https://github.com/hashicorp/go-msgpack/tree/v0.5.3>v0.5.3</a></li><li>github.com/hashicorp/go-multierror: <a href=https://github.com/hashicorp/go-multierror/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-rootcerts: <a href=https://github.com/hashicorp/go-rootcerts/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-sockaddr: <a href=https://github.com/hashicorp/go-sockaddr/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/go-uuid: <a href=https://github.com/hashicorp/go-uuid/tree/v1.0.1>v1.0.1</a></li><li>github.com/hashicorp/go.net: <a href=https://github.com/hashicorp/go.net/tree/v0.0.1>v0.0.1</a></li><li>github.com/hashicorp/logutils: <a href=https://github.com/hashicorp/logutils/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/mdns: <a href=https://github.com/hashicorp/mdns/tree/v1.0.0>v1.0.0</a></li><li>github.com/hashicorp/memberlist: <a href=https://github.com/hashicorp/memberlist/tree/v0.1.3>v0.1.3</a></li><li>github.com/hashicorp/serf: <a href=https://github.com/hashicorp/serf/tree/v0.8.2>v0.8.2</a></li><li>github.com/mitchellh/cli: <a href=https://github.com/mitchellh/cli/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/go-testing-interface: <a href=https://github.com/mitchellh/go-testing-interface/tree/v1.0.0>v1.0.0</a></li><li>github.com/mitchellh/gox: <a href=https://github.com/mitchellh/gox/tree/v0.4.0>v0.4.0</a></li><li>github.com/mitchellh/iochan: <a href=https://github.com/mitchellh/iochan/tree/v1.0.0>v1.0.0</a></li><li>github.com/pascaldekloe/goe: <a href=https://github.com/pascaldekloe/goe/tree/57f6aae>57f6aae</a></li><li>github.com/posener/complete: <a href=https://github.com/posener/complete/tree/v1.1.1>v1.1.1</a></li><li>github.com/ryanuber/columnize: <a href=https://github.com/ryanuber/columnize/tree/9b3edd6>9b3edd6</a></li><li>github.com/sean-/seed: <a href=https://github.com/sean-/seed/tree/e2103e2>e2103e2</a></li><li>github.com/subosito/gotenv: <a href=https://github.com/subosito/gotenv/tree/v1.2.0>v1.2.0</a></li><li>github.com/willf/bitset: <a href=https://github.com/willf/bitset/tree/d5bec33>d5bec33</a></li><li>gopkg.in/ini.v1: v1.51.0</li><li>gopkg.in/yaml.v3: 9f266ea</li><li>rsc.io/quote/v3: v3.1.0</li><li>rsc.io/sampler: v1.3.0</li></ul><h3 id=changed-3>Changed</h3><ul><li>cloud.google.com/go/bigquery: v1.0.1 → v1.4.0</li><li>cloud.google.com/go/datastore: v1.0.0 → v1.1.0</li><li>cloud.google.com/go/pubsub: v1.0.1 → v1.2.0</li><li>cloud.google.com/go/storage: v1.0.0 → v1.6.0</li><li>cloud.google.com/go: v0.51.0 → v0.54.0</li><li>github.com/Microsoft/go-winio: <a href=https://github.com/Microsoft/go-winio/compare/fc70bd9...v0.4.15>fc70bd9 → v0.4.15</a></li><li>github.com/aws/aws-sdk-go: <a href=https://github.com/aws/aws-sdk-go/compare/v1.35.5...v1.35.24>v1.35.5 → v1.35.24</a></li><li>github.com/blang/semver: <a href=https://github.com/blang/semver/compare/v3.5.0...v3.5.1>v3.5.0+incompatible → v3.5.1+incompatible</a></li><li>github.com/checkpoint-restore/go-criu/v4: <a href=https://github.com/checkpoint-restore/go-criu/v4/compare/v4.0.2...v4.1.0>v4.0.2 → v4.1.0</a></li><li>github.com/containerd/containerd: <a href=https://github.com/containerd/containerd/compare/v1.3.3...v1.4.1>v1.3.3 → v1.4.1</a></li><li>github.com/containerd/ttrpc: <a href=https://github.com/containerd/ttrpc/compare/v1.0.0...v1.0.2>v1.0.0 → v1.0.2</a></li><li>github.com/containerd/typeurl: <a href=https://github.com/containerd/typeurl/compare/v1.0.0...v1.0.1>v1.0.0 → v1.0.1</a></li><li>github.com/coreos/etcd: <a href=https://github.com/coreos/etcd/compare/v3.3.10...v3.3.13>v3.3.10+incompatible → v3.3.13+incompatible</a></li><li>github.com/docker/docker: <a href=https://github.com/docker/docker/compare/aa6a989...bd33bbf>aa6a989 → bd33bbf</a></li><li>github.com/go-gl/glfw/v3.3/glfw: <a href=https://github.com/go-gl/glfw/v3.3/glfw/compare/12ad95a...6f7a984>12ad95a → 6f7a984</a></li><li>github.com/golang/groupcache: <a href=https://github.com/golang/groupcache/compare/215e871...8c9f03a>215e871 → 8c9f03a</a></li><li>github.com/golang/mock: <a href=https://github.com/golang/mock/compare/v1.3.1...v1.4.1>v1.3.1 → v1.4.1</a></li><li>github.com/golang/protobuf: <a href=https://github.com/golang/protobuf/compare/v1.4.2...v1.4.3>v1.4.2 → v1.4.3</a></li><li>github.com/google/cadvisor: <a href=https://github.com/google/cadvisor/compare/v0.37.0...v0.38.4>v0.37.0 → v0.38.4</a></li><li>github.com/google/go-cmp: <a href=https://github.com/google/go-cmp/compare/v0.4.0...v0.5.2>v0.4.0 → v0.5.2</a></li><li>github.com/google/pprof: <a href=https://github.com/google/pprof/compare/d4f498a...1ebb73c>d4f498a → 1ebb73c</a></li><li>github.com/google/uuid: <a href=https://github.com/google/uuid/compare/v1.1.1...v1.1.2>v1.1.1 → v1.1.2</a></li><li>github.com/gorilla/mux: <a href=https://github.com/gorilla/mux/compare/v1.7.3...v1.8.0>v1.7.3 → v1.8.0</a></li><li>github.com/gorilla/websocket: <a href=https://github.com/gorilla/websocket/compare/v1.4.0...v1.4.2>v1.4.0 → v1.4.2</a></li><li>github.com/karrick/godirwalk: <a href=https://github.com/karrick/godirwalk/compare/v1.7.5...v1.16.1>v1.7.5 → v1.16.1</a></li><li>github.com/opencontainers/runc: <a href=https://github.com/opencontainers/runc/compare/819fcc6...v1.0.0-rc92>819fcc6 → v1.0.0-rc92</a></li><li>github.com/opencontainers/runtime-spec: <a href=https://github.com/opencontainers/runtime-spec/compare/237cc4f...4d89ac9>237cc4f → 4d89ac9</a></li><li>github.com/opencontainers/selinux: <a href=https://github.com/opencontainers/selinux/compare/v1.5.2...v1.6.0>v1.5.2 → v1.6.0</a></li><li>github.com/prometheus/procfs: <a href=https://github.com/prometheus/procfs/compare/v0.1.3...v0.2.0>v0.1.3 → v0.2.0</a></li><li>github.com/quobyte/api: <a href=https://github.com/quobyte/api/compare/v0.1.2...v0.1.8>v0.1.2 → v0.1.8</a></li><li>github.com/spf13/cobra: <a href=https://github.com/spf13/cobra/compare/v1.0.0...v1.1.1>v1.0.0 → v1.1.1</a></li><li>github.com/spf13/viper: <a href=https://github.com/spf13/viper/compare/v1.4.0...v1.7.0>v1.4.0 → v1.7.0</a></li><li>github.com/stretchr/testify: <a href=https://github.com/stretchr/testify/compare/v1.4.0...v1.6.1>v1.4.0 → v1.6.1</a></li><li>github.com/vishvananda/netns: <a href=https://github.com/vishvananda/netns/compare/52d707b...db3c7e5>52d707b → db3c7e5</a></li><li>go.opencensus.io: v0.22.2 → v0.22.3</li><li>golang.org/x/exp: da58074 → 6cc2880</li><li>golang.org/x/lint: fdd1cda → 738671d</li><li>golang.org/x/net: ab34263 → 69a7880</li><li>golang.org/x/oauth2: 858c2ad → bf48bf1</li><li>golang.org/x/sys: ed371f2 → 5cba982</li><li>golang.org/x/text: v0.3.3 → v0.3.4</li><li>golang.org/x/time: 555d28b → 3af7569</li><li>golang.org/x/xerrors: 9bdfabe → 5ec99f8</li><li>google.golang.org/api: v0.15.1 → v0.20.0</li><li>google.golang.org/genproto: cb27e3a → 8816d57</li><li>google.golang.org/grpc: v1.27.0 → v1.27.1</li><li>google.golang.org/protobuf: v1.24.0 → v1.25.0</li><li>honnef.co/go/tools: v0.0.1-2019.2.3 → v0.0.1-2020.1.3</li><li>k8s.io/gengo: 8167cfd → 83324d8</li><li>k8s.io/klog/v2: v2.2.0 → v2.4.0</li><li>k8s.io/kube-openapi: 8b50664 → d219536</li><li>k8s.io/utils: d5654de → 67b214c</li><li>sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.12 → v0.0.14</li><li>sigs.k8s.io/structured-merge-diff/v4: b3cf1e8 → v4.0.2</li></ul><h3 id=removed-3>Removed</h3><ul><li>github.com/armon/consul-api: <a href=https://github.com/armon/consul-api/tree/eb2c6b5>eb2c6b5</a></li><li>github.com/go-ini/ini: <a href=https://github.com/go-ini/ini/tree/v1.9.0>v1.9.0</a></li><li>github.com/ugorji/go: <a href=https://github.com/ugorji/go/tree/v1.1.4>v1.1.4</a></li><li>github.com/xordataexchange/crypt: <a href=https://github.com/xordataexchange/crypt/tree/b2862e3>b2862e3</a></li></ul><h1 id=v1-20-0-beta-1>v1.20.0-beta.1</h1><h2 id=downloads-for-v1-20-0-beta-1>Downloads for v1.20.0-beta.1</h2><h3 id=source-code-2>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>4eddf4850c2d57751696f352d0667309339090aeb30ff93e8db8a22c6cdebf74cb2d5dc78d4ae384c4e25491efc39413e2e420a804b76b421a9ad934e56b0667</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>59de5221162e9b6d88f5abbdb99765cb2b2e501498ea853fb65f2abe390211e28d9f21e0d87be3ade550a5ea6395d04552cf093d2ce2f99fd45ad46545dd13cb</td></tr></tbody></table><h3 id=client-binaries-3>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>d69ffed19b034a4221fc084e43ac293cf392e98febf5bf580f8d92307a8421d8b3aab18f9ca70608937e836b42c7a34e829f88eba6e040218a4486986e2fca21</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>1b542e165860c4adcd4550adc19b86c3db8cd75d2a1b8db17becc752da78b730ee48f1b0aaf8068d7bfbb1d8e023741ec293543bc3dd0f4037172a6917db8169</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>90ad52785eecb43a6f9035b92b6ba39fc84e67f8bc91cf098e70f8cfdd405c4b9d5c02dccb21022f21bb5b6ce92fdef304def1da0a7255c308e2c5fb3a9cdaab</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>d0cb3322b056e1821679afa70728ffc0d3375e8f3326dabbe8185be2e60f665ab8985b13a1a432e10281b84a929e0f036960253ac0dd6e0b44677d539e98e61b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>3aecc8197e0aa368408624add28a2dd5e73f0d8a48e5e33c19edf91d5323071d16a27353a6f3e22df4f66ed7bfbae8e56e0a9050f7bbdf927ce6aeb29bba6374</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>6ff145058f62d478b98f1e418e272555bfb5c7861834fbbf10a8fb334cc7ff09b32f2666a54b230932ba71d2fc7d3b1c1f5e99e6fe6d6ec83926a9b931cd2474</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>ff7b8bb894076e05a3524f6327a4a6353b990466f3292e84c92826cb64b5c82b3855f48b8e297ccadc8bcc15552bc056419ff6ff8725fc4e640828af9cc1331b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>6c6dcac9c725605763a130b5a975f2b560aa976a5c809d4e0887900701b707baccb9ca1aebc10a03cfa7338a6f42922bbf838ccf6800fc2a3e231686a72568b6</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>d12e3a29c960f0ddd1b9aabf5426ac1259863ac6c8f2be1736ebeb57ddca6b1c747ee2c363be19e059e38cf71488c5ea3509ad4d0e67fd5087282a5ad0ae9a48</td></tr></tbody></table><h3 id=server-binaries-3>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>904e8c049179e071c6caa65f525f465260bb4d4318a6dd9cc05be2172f39f7cfc69d1672736e01d926045764fe8872e806444e3af77ffef823ede769537b7d20</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>5934959374868aed8d4294de84411972660bca7b2e952201a9403f37e40c60a5c53eaea8001344d0bf4a00c8cd27de6324d88161388de27f263a5761357cb82b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>4c884585970f80dc5462d9a734d7d5be9558b36c6e326a8a3139423efbd7284fa9f53fb077983647e17e19f03f5cb9bf26201450c78daecf10afa5a1ab5f9efc</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>235b78b08440350dcb9f13b63f7722bd090c672d8e724ca5d409256e5a5d4f46d431652a1aa908c3affc5b1e162318471de443d38b93286113e79e7f90501a9b</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>220fc9351702b3ecdcf79089892ceb26753a8a1deaf46922ffb3d3b62b999c93fef89440e779ca6043372b963081891b3a966d1a5df0cf261bdd44395fd28dce</td></tr></tbody></table><h3 id=node-binaries-3>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>fe59d3a1f21c47bab126f689687657f77fbcb46a2caeef48eecd073b2b22879f997a466911b5c5c829e9cf27e68a36ecdf18686d42714839d4b97d6c7281578d</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>93e545aa963cfd11e0b2c6d47669b5ef70c5a86ef80c3353c1a074396bff1e8e7371dda25c39d78c7a9e761f2607b8b5ab843fa0c10b8ff9663098fae8d25725</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>5e0f177f9bec406a668d4b37e69b191208551fdf289c82b5ec898959da4f8a00a2b0695cbf1d2de5acb809321c6e5604f5483d33556543d92b96dcf80e814dd3</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>574412059e4d257eb904cd4892a075b6a2cde27adfa4976ee64c46d6768facece338475f1b652ad94c8df7cfcbb70ebdf0113be109c7099ab76ffdb6f023eefd</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>b1ffaa6d7f77d89885c642663cb14a86f3e2ec2afd223e3bb2000962758cf0f15320969ffc4be93b5826ff22d54fdbae0dbea09f9d8228eda6da50b6fdc88758</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.1/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>388983765213cf3bdc1f8b27103ed79e39028767e5f1571e35ed1f91ed100e49f3027f7b7ff19b53fab7fbb6d723c0439f21fc6ed62be64532c25f5bfa7ee265</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-beta-0>Changelog since v1.20.0-beta.0</h2><h2 id=changes-by-kind-3>Changes by Kind</h2><h3 id=deprecation-2>Deprecation</h3><ul><li>ACTION REQUIRED: The kube-apiserver ability to serve on an insecure port, deprecated since v1.10, has been removed. The insecure address flags <code>--address</code> and <code>--insecure-bind-address</code> have no effect in kube-apiserver and will be removed in v1.24. The insecure port flags <code>--port</code> and <code>--insecure-port</code> may only be set to 0 and will be removed in v1.24. (<a href=https://github.com/kubernetes/kubernetes/pull/95856>#95856</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG API Machinery, Node and Testing]</li></ul><h3 id=api-change-2>API Change</h3><ul><li><ul><li><code>TokenRequest</code> and <code>TokenRequestProjection</code> features have been promoted to GA. This feature allows generating service account tokens that are not visible in Secret objects and are tied to the lifetime of a Pod object. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection</a> for details on configuring and using this feature. The <code>TokenRequest</code> and <code>TokenRequestProjection</code> feature gates will be removed in v1.21.</li><li>kubeadm's kube-apiserver Pod manifest now includes the following flags by default "--service-account-key-file", "--service-account-signing-key-file", "--service-account-issuer". (<a href=https://github.com/kubernetes/kubernetes/pull/93258>#93258</a>, <a href=https://github.com/zshihang>@zshihang</a>) [SIG API Machinery, Auth, Cluster Lifecycle, Storage and Testing]</li></ul></li><li>Certain fields on Service objects will be automatically cleared when changing the service's <code>type</code> to a mode that does not need those fields. For example, changing from type=LoadBalancer to type=ClusterIP will clear the NodePort assignments, rather than forcing the user to clear them. (<a href=https://github.com/kubernetes/kubernetes/pull/95196>#95196</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG API Machinery, Apps, Network and Testing]</li><li>Services will now have a <code>clusterIPs</code> field to go with <code>clusterIP</code>. <code>clusterIPs[0]</code> is a synonym for <code>clusterIP</code> and will be syncronized on create and update operations. (<a href=https://github.com/kubernetes/kubernetes/pull/95894>#95894</a>, <a href=https://github.com/thockin>@thockin</a>) [SIG Network]</li></ul><h3 id=feature-3>Feature</h3><ul><li>A new metric <code>apiserver_request_filter_duration_seconds</code> has been introduced that
measures request filter latency in seconds. (<a href=https://github.com/kubernetes/kubernetes/pull/95207>#95207</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery and Instrumentation]</li><li>Add a new flag to set priority for the kubelet on Windows nodes so that workloads cannot overwhelm the node there by disrupting kubelet process. (<a href=https://github.com/kubernetes/kubernetes/pull/96051>#96051</a>, <a href=https://github.com/ravisantoshgudimetla>@ravisantoshgudimetla</a>) [SIG Node and Windows]</li><li>Changed: default "Accept: <em>/</em>" header added to HTTP probes. See <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes>https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#http-probes</a> (<a href=https://github.com/kubernetes/website/pull/24756>https://github.com/kubernetes/website/pull/24756</a>) (<a href=https://github.com/kubernetes/kubernetes/pull/95641>#95641</a>, <a href=https://github.com/fonsecas72>@fonsecas72</a>) [SIG Network and Node]</li><li>Client-go credential plugins can now be passed in the current cluster information via the KUBERNETES_EXEC_INFO environment variable. (<a href=https://github.com/kubernetes/kubernetes/pull/95489>#95489</a>, <a href=https://github.com/ankeesler>@ankeesler</a>) [SIG API Machinery and Auth]</li><li>Kube-apiserver: added support for compressing rotated audit log files with <code>--audit-log-compress</code> (<a href=https://github.com/kubernetes/kubernetes/pull/94066>#94066</a>, <a href=https://github.com/lojies>@lojies</a>) [SIG API Machinery and Auth]</li></ul><h3 id=documentation-2>Documentation</h3><ul><li>Fake dynamic client: document that List does not preserve TypeMeta in UnstructuredList (<a href=https://github.com/kubernetes/kubernetes/pull/95117>#95117</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery]</li></ul><h3 id=bug-or-regression-3>Bug or Regression</h3><ul><li>Added support to kube-proxy for externalTrafficPolicy=Local setting via Direct Server Return (DSR) load balancers on Windows. (<a href=https://github.com/kubernetes/kubernetes/pull/93166>#93166</a>, <a href=https://github.com/elweb9858>@elweb9858</a>) [SIG Network]</li><li>Disable watchcache for events (<a href=https://github.com/kubernetes/kubernetes/pull/96052>#96052</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery]</li><li>Disabled <code>LocalStorageCapacityIsolation</code> feature gate is honored during scheduling. (<a href=https://github.com/kubernetes/kubernetes/pull/96092>#96092</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li><li>Fix bug in JSON path parser where an error occurs when a range is empty (<a href=https://github.com/kubernetes/kubernetes/pull/95933>#95933</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG API Machinery]</li><li>Fix k8s.io/apimachinery/pkg/api/meta.SetStatusCondition to update ObservedGeneration (<a href=https://github.com/kubernetes/kubernetes/pull/95961>#95961</a>, <a href=https://github.com/KnicKnic>@KnicKnic</a>) [SIG API Machinery]</li><li>Fixed a regression which prevented pods with <code>docker/default</code> seccomp annotations from being created in 1.19 if a PodSecurityPolicy was in place which did not allow <code>runtime/default</code> seccomp profiles. (<a href=https://github.com/kubernetes/kubernetes/pull/95985>#95985</a>, <a href=https://github.com/saschagrunert>@saschagrunert</a>) [SIG Auth]</li><li>Kubectl: print error if users place flags before plugin name (<a href=https://github.com/kubernetes/kubernetes/pull/92343>#92343</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG CLI]</li><li>When creating a PVC with the volume.beta.kubernetes.io/storage-provisioner annotation already set, the PV controller might have incorrectly deleted the newly provisioned PV instead of binding it to the PVC, depending on timing and system load. (<a href=https://github.com/kubernetes/kubernetes/pull/95909>#95909</a>, <a href=https://github.com/pohly>@pohly</a>) [SIG Apps and Storage]</li></ul><h3 id=other-cleanup-or-flake-2>Other (Cleanup or Flake)</h3><ul><li>Kubectl: the <code>generator</code> flag of <code>kubectl autoscale</code> has been deprecated and has no effect, it will be removed in a feature release (<a href=https://github.com/kubernetes/kubernetes/pull/92998>#92998</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG CLI]</li><li>V1helpers.MatchNodeSelectorTerms now accepts just a Node and a list of Terms (<a href=https://github.com/kubernetes/kubernetes/pull/95871>#95871</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Scheduling and Storage]</li><li><code>MatchNodeSelectorTerms</code> function moved to <code>k8s.io/component-helpers</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95531>#95531</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Scheduling and Storage]</li></ul><h2 id=dependencies-4>Dependencies</h2><h3 id=added-4>Added</h3><p><em>Nothing has changed.</em></p><h3 id=changed-4>Changed</h3><p><em>Nothing has changed.</em></p><h3 id=removed-4>Removed</h3><p><em>Nothing has changed.</em></p><h1 id=v1-20-0-beta-0>v1.20.0-beta.0</h1><h2 id=downloads-for-v1-20-0-beta-0>Downloads for v1.20.0-beta.0</h2><h3 id=source-code-3>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>385e49e32bbd6996f07bcadbf42285755b8a8ef9826ee1ba42bd82c65827cf13f63e5634b834451b263a93b708299cbb4b4b0b8ddbc688433deaf6bec240aa67</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>842e80f6dcad461426fb699de8a55fde8621d76a94e54288fe9939cc1a3bbd0f4799abadac2c59bcf3f91d743726dbd17e1755312ae7fec482ef560f336dbcbb</td></tr></tbody></table><h3 id=client-binaries-4>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>bde5e7d9ee3e79d1e69465a3ddb4bb36819a4f281b5c01a7976816d7c784410812dde133cdf941c47e5434e9520701b9c5e8b94d61dca77c172f87488dfaeb26</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>721bb8444c9e0d7a9f8461e3f5428882d76fcb3def6eb11b8e8e08fae7f7383630699248660d69d4f6a774124d6437888666e1fa81298d5b5518bc4a6a6b2c92</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>71e4edc41afbd65f813e7ecbc22b27c95f248446f005e288d758138dc4cc708735be7218af51bcf15e8b9893a3598c45d6a685f605b46f50af3762b02c32ed76</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>bbefc749156f63898973f2f7c7a6f1467481329fb430d641fe659b497e64d679886482d557ebdddb95932b93de8d1e3e365c91d4bf9f110b68bd94b0ba702ded</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>9803190685058b4b64d002c2fbfb313308bcea4734ed53a8c340cfdae4894d8cb13b3e819ae64051bafe0fbf8b6ecab53a6c1dcf661c57640c75b0eb60041113</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>bcdceea64cba1ae38ea2bab50d8fd77c53f6d673de12566050b0e3c204334610e6c19e4ace763e68b5e48ab9e811521208b852b1741627be30a2b17324fc1daf</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>41e36d00867e90012d5d5adfabfaae8d9f5a9fd32f290811e3c368e11822916b973afaaf43961081197f2cbab234090d97d89774e674aeadc1da61f7a64708a9</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>c50fec5aec2d0e742f851f25c236cb73e76f8fc73b0908049a10ae736c0205b8fff83eb3d29b1748412edd942da00dd738195d9003f25b577d6af8359d84fb2f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>0fd6777c349908b6d627e849ea2d34c048b8de41f7df8a19898623f597e6debd35b7bcbf8e1d43a1be3a9abb45e4810bc498a0963cf780b109e93211659e9c7e</td></tr></tbody></table><h3 id=server-binaries-4>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>30d982424ca64bf0923503ae8195b2e2a59497096b2d9e58dfd491cd6639633027acfa9750bc7bccf34e1dc116d29d2f87cbd7ae713db4210ce9ac16182f0576</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>f08b62be9bc6f0745f820b0083c7a31eedb2ce370a037c768459a59192107b944c8f4345d0bb88fc975f2e7a803ac692c9ac3e16d4a659249d4600e84ff75d9e</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>e3472b5b3dfae0a56e5363d52062b1e4a9fc227a05e0cf5ece38233b2c442f427970aab94a52377fb87e583663c120760d154bc1c4ac22dca1f4d0d1ebb96088</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>06c254e0a62f755d31bc40093d86c44974f0a60308716cc3214a6b3c249a4d74534d909b82f8a3dd3a3c9720e61465b45d2bb3a327ef85d3caba865750020dfb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>2edeb4411c26a0de057a66787091ab1044f71774a464aed898ffee26634a40127181c2edddb38e786b6757cca878fd0c3a885880eec6c3448b93c645770abb12</td></tr></tbody></table><h3 id=node-binaries-4>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>cc1d5b94b86070b5e7746d7aaeaeac3b3a5e5ebbff1ec33885f7eeab270a6177d593cb1975b2e56f4430b7859ad42da76f266629f9313e0f688571691ac448ed</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>75e82c7c9122add3b24695b94dcb0723c52420c3956abf47511e37785aa48a1fa8257db090c6601010c4475a325ccfff13eb3352b65e3aa1774f104b09b766b0</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>16ef27c40bf4d678a55fcd3d3f7d09f1597eec2cc58f9950946f0901e52b82287be397ad7f65e8d162d8a9cdb4a34a610b6db8b5d0462be8e27c4b6eb5d6e5e7</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>939865f2c4cb6a8934f22a06223e416dec5f768ffc1010314586149470420a1d62aef97527c34d8a636621c9669d6489908ce1caf96f109e8d073cee1c030b50</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>bbfdd844075fb816079af7b73d99bc1a78f41717cdbadb043f6f5872b4dc47bc619f7f95e2680d4b516146db492c630c17424e36879edb45e40c91bc2ae4493c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-beta.0/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>a2b3ea40086fd71aed71a4858fd3fc79fd1907bc9ea8048ff3c82ec56477b0a791b724e5a52d79b3b36338c7fbd93dfd3d03b00ccea9042bda0d270fc891e4ec</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-alpha-3>Changelog since v1.20.0-alpha.3</h2><h2 id=urgent-upgrade-notes-2>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade-2>(No, really, you MUST read this before you upgrade)</h3><ul><li>Kubeadm: improve the validation of serviceSubnet and podSubnet.
ServiceSubnet has to be limited in size, due to implementation details, and the mask can not allocate more than 20 bits.
PodSubnet validates against the corresponding cluster "--node-cidr-mask-size" of the kube-controller-manager, it fail if the values are not compatible.
kubeadm no longer sets the node-mask automatically on IPv6 deployments, you must check that your IPv6 service subnet mask is compatible with the default node mask /64 or set it accordenly.
Previously, for IPv6, if the podSubnet had a mask lower than /112, kubeadm calculated a node-mask to be multiple of eight and splitting the available bits to maximise the number used for nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/95723>#95723</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Cluster Lifecycle]</li><li>Windows hyper-v container featuregate is deprecated in 1.20 and will be removed in 1.21 (<a href=https://github.com/kubernetes/kubernetes/pull/95505>#95505</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</li></ul><h2 id=changes-by-kind-4>Changes by Kind</h2><h3 id=deprecation-3>Deprecation</h3><ul><li>Support 'controlplane' as a valid EgressSelection type in the EgressSelectorConfiguration API. 'Master' is deprecated and will be removed in v1.22. (<a href=https://github.com/kubernetes/kubernetes/pull/95235>#95235</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery]</li></ul><h3 id=api-change-3>API Change</h3><ul><li>Add dual-stack Services (alpha). This is a BREAKING CHANGE to an alpha API.
It changes the dual-stack API wrt Service from a single ipFamily field to 3
fields: ipFamilyPolicy (SingleStack, PreferDualStack, RequireDualStack),
ipFamilies (a list of families assigned), and clusterIPs (inclusive of
clusterIP). Most users do not need to set anything at all, defaulting will
handle it for them. Services are single-stack unless the user asks for
dual-stack. This is all gated by the "IPv6DualStack" feature gate. (<a href=https://github.com/kubernetes/kubernetes/pull/91824>#91824</a>, <a href=https://github.com/khenidak>@khenidak</a>) [SIG API Machinery, Apps, CLI, Network, Node, Scheduling and Testing]</li><li>Introduces a metric source for HPAs which allows scaling based on container resource usage. (<a href=https://github.com/kubernetes/kubernetes/pull/90691>#90691</a>, <a href=https://github.com/arjunrn>@arjunrn</a>) [SIG API Machinery, Apps, Autoscaling and CLI]</li></ul><h3 id=feature-4>Feature</h3><ul><li>Add a metric for time taken to perform recursive permission change (<a href=https://github.com/kubernetes/kubernetes/pull/95866>#95866</a>, <a href=https://github.com/JornShen>@JornShen</a>) [SIG Instrumentation and Storage]</li><li>Allow cross compilation of kubernetes on different platforms. (<a href=https://github.com/kubernetes/kubernetes/pull/94403>#94403</a>, <a href=https://github.com/bnrjee>@bnrjee</a>) [SIG Release]</li><li>Command to start network proxy changes from 'KUBE_ENABLE_EGRESS_VIA_KONNECTIVITY_SERVICE ./cluster/kube-up.sh' to 'KUBE_ENABLE_KONNECTIVITY_SERVICE=true ./hack/kube-up.sh' (<a href=https://github.com/kubernetes/kubernetes/pull/92669>#92669</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG Cloud Provider]</li><li>DefaultPodTopologySpread graduated to Beta. The feature gate is enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/95631>#95631</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling and Testing]</li><li>Kubernetes E2E test image manifest lists now contain Windows images. (<a href=https://github.com/kubernetes/kubernetes/pull/77398>#77398</a>, <a href=https://github.com/claudiubelu>@claudiubelu</a>) [SIG Testing and Windows]</li><li>Support for Windows container images (OS Versions: 1809, 1903, 1909, 2004) was added the pause:3.4 image. (<a href=https://github.com/kubernetes/kubernetes/pull/91452>#91452</a>, <a href=https://github.com/claudiubelu>@claudiubelu</a>) [SIG Node, Release and Windows]</li></ul><h3 id=documentation-3>Documentation</h3><ul><li>Fake dynamic client: document that List does not preserve TypeMeta in UnstructuredList (<a href=https://github.com/kubernetes/kubernetes/pull/95117>#95117</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery]</li></ul><h3 id=bug-or-regression-4>Bug or Regression</h3><ul><li>Exposes and sets a default timeout for the SubjectAccessReview client for DelegatingAuthorizationOptions. (<a href=https://github.com/kubernetes/kubernetes/pull/95725>#95725</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery and Cloud Provider]</li><li>Alter wording to describe pods using a pvc (<a href=https://github.com/kubernetes/kubernetes/pull/95635>#95635</a>, <a href=https://github.com/RaunakShah>@RaunakShah</a>) [SIG CLI]</li><li>If we set SelectPolicy MinPolicySelect on scaleUp behavior or scaleDown behavior,Horizontal Pod Autoscaler doesn`t automatically scale the number of pods correctly (<a href=https://github.com/kubernetes/kubernetes/pull/95647>#95647</a>, <a href=https://github.com/JoshuaAndrew>@JoshuaAndrew</a>) [SIG Apps and Autoscaling]</li><li>Ignore apparmor for non-linux operating systems (<a href=https://github.com/kubernetes/kubernetes/pull/93220>#93220</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</li><li>Ipvs: ensure selected scheduler kernel modules are loaded (<a href=https://github.com/kubernetes/kubernetes/pull/93040>#93040</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Network]</li><li>Kubeadm: add missing "--experimental-patches" flag to "kubeadm init phase control-plane" (<a href=https://github.com/kubernetes/kubernetes/pull/95786>#95786</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG Cluster Lifecycle]</li><li>Reorganized iptables rules to fix a performance issue (<a href=https://github.com/kubernetes/kubernetes/pull/95252>#95252</a>, <a href=https://github.com/tssurya>@tssurya</a>) [SIG Network]</li><li>Unhealthy pods covered by PDBs can be successfully evicted if enough healthy pods are available. (<a href=https://github.com/kubernetes/kubernetes/pull/94381>#94381</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG Apps]</li><li>Update the PIP when it is not in the Succeeded provisioning state during the LB update. (<a href=https://github.com/kubernetes/kubernetes/pull/95748>#95748</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Update the frontend IP config when the service's <code>pipName</code> annotation is changed (<a href=https://github.com/kubernetes/kubernetes/pull/95813>#95813</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li></ul><h3 id=other-cleanup-or-flake-3>Other (Cleanup or Flake)</h3><ul><li>NO (<a href=https://github.com/kubernetes/kubernetes/pull/95690>#95690</a>, <a href=https://github.com/nikhita>@nikhita</a>) [SIG Release]</li></ul><h2 id=dependencies-5>Dependencies</h2><h3 id=added-5>Added</h3><ul><li>github.com/form3tech-oss/jwt-go: <a href=https://github.com/form3tech-oss/jwt-go/tree/v3.2.2>v3.2.2+incompatible</a></li></ul><h3 id=changed-5>Changed</h3><ul><li>github.com/Azure/go-autorest/autorest/adal: <a href=https://github.com/Azure/go-autorest/autorest/adal/compare/v0.9.0...v0.9.5>v0.9.0 → v0.9.5</a></li><li>github.com/Azure/go-autorest/autorest/mocks: <a href=https://github.com/Azure/go-autorest/autorest/mocks/compare/v0.4.0...v0.4.1>v0.4.0 → v0.4.1</a></li><li>golang.org/x/crypto: 75b2880 → 7f63de1</li></ul><h3 id=removed-5>Removed</h3><p><em>Nothing has changed.</em></p><h1 id=v1-20-0-alpha-3>v1.20.0-alpha.3</h1><h2 id=downloads-for-v1-20-0-alpha-3>Downloads for v1.20.0-alpha.3</h2><h3 id=source-code-4>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>542cc9e0cd97732020491456402b6e2b4f54f2714007ee1374a7d363663a1b41e82b50886176a5313aaccfbfd4df2bc611d6b32d19961cdc98b5821b75d6b17c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>5e5d725294e552fd1d14fd6716d013222827ac2d4e2d11a7a1fdefb77b3459bbeb69931f38e1597de205dd32a1c9763ab524c2af1551faef4f502ef0890f7fbf</td></tr></tbody></table><h3 id=client-binaries-5>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>60004939727c75d0f06adc4449e16b43303941937c0e9ea9aca7d947e93a5aed5d11e53d1fc94caeb988be66d39acab118d406dc2d6cead61181e1ced6d2be1a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>7edba9c4f1bf38fdf1fa5bff2856c05c0e127333ce19b17edf3119dc9b80462c027404a1f58a5eabf1de73a8f2f20aced043dda1fafd893619db1a188cda550c</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>db1818aa82d072cb3e32a2a988e66d76ecf7cebc6b8a29845fa2d6ec27f14a36e4b9839b1b7ed8c43d2da9cde00215eb672a7e8ee235d2e3107bc93c22e58d38</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>d2922e70d22364b1f5a1e94a0c115f849fe2575b231b1ba268f73a9d86fc0a9fbb78dc713446839a2593acf1341cb5a115992f350870f13c1a472bb107b75af7</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>2e3ae20e554c7d4fc3a8afdfcafe6bbc81d4c5e9aea036357baac7a3fdc2e8098aa8a8c3dded3951667d57f667ce3fbf37ec5ae5ceb2009a569dc9002d3a92f9</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>b54a34e572e6a86221577de376e6f7f9fcd82327f7fe94f2fc8d21f35d302db8a0f3d51e60dc89693999f5df37c96d0c3649a29f07f095efcdd59923ae285c95</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>5be1b70dc437d3ba88cb0b89cd1bc555f79896c3f5b5f4fa0fb046a0d09d758b994d622ebe5cef8e65bba938c5ae945b81dc297f9dfa0d98f82ea75f344a3a0d</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>88cf3f66168ef3bf9a5d3d2275b7f33799406e8205f2c202997ebec23d449aa4bb48b010356ab1cf52ff7b527b8df7c8b9947a43a82ebe060df83c3d21b7223a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>87d2d4ea1829da8cfa1a705a03ea26c759a03bd1c4d8b96f2c93264c4d172bb63a91d9ddda65cdc5478b627c30ae8993db5baf8be262c157d83bffcebe85474e</td></tr></tbody></table><h3 id=server-binaries-5>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>7af691fc0b13a937797912374e3b3eeb88d5262e4eb7d4ebe92a3b64b3c226cb049aedfd7e39f639f6990444f7bcf2fe58699cf0c29039daebe100d7eebf60de</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>557c47870ecf5c2090b2694c8f0c8e3b4ca23df5455a37945bd037bc6fb5b8f417bf737bb66e6336b285112cb52de0345240fdb2f3ce1c4fb335ca7ef1197f99</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>981de6cf7679d743cdeef1e894314357b68090133814801870504ef30564e32b5675e270db20961e9a731e35241ad9b037bdaf749da87b6c4ce8889eeb1c5855</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>506578a21601ccff609ae757a55e68634c15cbfecbf13de972c96b32a155ded29bd71aee069c77f5f721416672c7a7ac0b8274de22bfd28e1ecae306313d96c5</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>af0cdcd4a77a7cc8060a076641615730a802f1f02dab084e41926023489efec6102d37681c70ab0dbe7440cd3e72ea0443719a365467985360152b9aae657375</td></tr></tbody></table><h3 id=node-binaries-5>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>2d92c61596296279de1efae23b2b707415565d9d50cd61a7231b8d10325732b059bcb90f3afb36bef2575d203938c265572721e38df408e8792d3949523bd5d9</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>c298de9b5ac1b8778729a2d8e2793ff86743033254fbc27014333880b03c519de81691caf03aa418c729297ee8942ce9ec89d11b0e34a80576b9936015dc1519</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>daa3c65afda6d7aff206c1494390bbcc205c2c6f8db04c10ca967a690578a01c49d49c6902b85e7158f79fd4d2a87c5d397d56524a75991c9d7db85ac53059a7</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>05661908bb73bfcaf9c2eae96e9a6a793db5a7a100bce6df9e057985dd53a7a5248d72e81b6d13496bd38b9326c17cdb2edaf0e982b6437507245fb846e1efc6</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>845e518e2c4ef0cef2c3b58f0b9ea5b5fe9b8a249717f789607752484c424c26ae854b263b7c0a004a8426feb9aa3683c177a9ed2567e6c3521f4835ea08c24a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.3/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>530e536574ed2c3e5973d3c0f0fdd2b4d48ef681a7a7c02db13e605001669eeb4f4b8a856fc08fc21436658c27b377f5d04dbcb3aae438098abc953b6eaf5712</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-alpha-2>Changelog since v1.20.0-alpha.2</h2><h2 id=changes-by-kind-5>Changes by Kind</h2><h3 id=api-change-4>API Change</h3><ul><li>New parameter <code>defaultingType</code> for <code>PodTopologySpread</code> plugin allows to use k8s defined or user provided default constraints (<a href=https://github.com/kubernetes/kubernetes/pull/95048>#95048</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li></ul><h3 id=feature-5>Feature</h3><ul><li>Added new k8s.io/component-helpers repository providing shared helper code for (core) components. (<a href=https://github.com/kubernetes/kubernetes/pull/92507>#92507</a>, <a href=https://github.com/ingvagabund>@ingvagabund</a>) [SIG Apps, Node, Release and Scheduling]</li><li>Adds <code>create ingress</code> command to <code>kubectl</code> (<a href=https://github.com/kubernetes/kubernetes/pull/78153>#78153</a>, <a href=https://github.com/amimof>@amimof</a>) [SIG CLI and Network]</li><li>Kubectl create now supports creating ingress objects. (<a href=https://github.com/kubernetes/kubernetes/pull/94327>#94327</a>, <a href=https://github.com/rikatz>@rikatz</a>) [SIG CLI and Network]</li><li>New default scheduling plugins order reduces scheduling and preemption latency when taints and node affinity are used (<a href=https://github.com/kubernetes/kubernetes/pull/95539>#95539</a>, <a href=https://github.com/soulxu>@soulxu</a>) [SIG Scheduling]</li><li>SCTP support in API objects (Pod, Service, NetworkPolicy) is now GA.
Note that this has no effect on whether SCTP is enabled on nodes at the kernel level,
and note that some cloud platforms and network plugins do not support SCTP traffic. (<a href=https://github.com/kubernetes/kubernetes/pull/95566>#95566</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Apps and Network]</li><li>Scheduling Framework: expose Run[Pre]ScorePlugins functions to PreemptionHandle which can be used in PostFilter extention point. (<a href=https://github.com/kubernetes/kubernetes/pull/93534>#93534</a>, <a href=https://github.com/everpeace>@everpeace</a>) [SIG Scheduling and Testing]</li><li>SelectorSpreadPriority maps to PodTopologySpread plugin when DefaultPodTopologySpread feature is enabled (<a href=https://github.com/kubernetes/kubernetes/pull/95448>#95448</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>SetHostnameAsFQDN has been graduated to Beta and therefore it is enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/95267>#95267</a>, <a href=https://github.com/javidiaz>@javidiaz</a>) [SIG Node]</li></ul><h3 id=bug-or-regression-5>Bug or Regression</h3><ul><li>An issues preventing volume expand controller to annotate the PVC with <code>volume.kubernetes.io/storage-resizer</code> when the PVC StorageClass is already updated to the out-of-tree provisioner is now fixed. (<a href=https://github.com/kubernetes/kubernetes/pull/94489>#94489</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG API Machinery, Apps and Storage]</li><li>Change the mount way from systemd to normal mount except ceph and glusterfs intree-volume. (<a href=https://github.com/kubernetes/kubernetes/pull/94916>#94916</a>, <a href=https://github.com/smileusd>@smileusd</a>) [SIG Apps, Cloud Provider, Network, Node, Storage and Testing]</li><li>Fix azure disk attach failure for disk size bigger than 4TB (<a href=https://github.com/kubernetes/kubernetes/pull/95463>#95463</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix azure disk data loss issue on Windows when unmount disk (<a href=https://github.com/kubernetes/kubernetes/pull/95456>#95456</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix verb & scope reporting for kube-apiserver metrics (LIST reported instead of GET) (<a href=https://github.com/kubernetes/kubernetes/pull/95562>#95562</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Testing]</li><li>Fix vsphere detach failure for static PVs (<a href=https://github.com/kubernetes/kubernetes/pull/95447>#95447</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider and Storage]</li><li>Fix: smb valid path error (<a href=https://github.com/kubernetes/kubernetes/pull/95583>#95583</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</li><li>Fixed a bug causing incorrect formatting of <code>kubectl describe ingress</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94985>#94985</a>, <a href=https://github.com/howardjohn>@howardjohn</a>) [SIG CLI and Network]</li><li>Fixed a bug in client-go where new clients with customized <code>Dial</code>, <code>Proxy</code>, <code>GetCert</code> config may get stale HTTP transports. (<a href=https://github.com/kubernetes/kubernetes/pull/95427>#95427</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG API Machinery]</li><li>Fixes high CPU usage in kubectl drain (<a href=https://github.com/kubernetes/kubernetes/pull/95260>#95260</a>, <a href=https://github.com/amandahla>@amandahla</a>) [SIG CLI]</li><li>Support the node label <code>node.kubernetes.io/exclude-from-external-load-balancers</code> (<a href=https://github.com/kubernetes/kubernetes/pull/95542>#95542</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li></ul><h3 id=other-cleanup-or-flake-4>Other (Cleanup or Flake)</h3><ul><li>Fix func name NewCreateCreateDeploymentOptions (<a href=https://github.com/kubernetes/kubernetes/pull/91931>#91931</a>, <a href=https://github.com/lixiaobing1>@lixiaobing1</a>) [SIG CLI]</li><li>Kubeadm: update the default pause image version to 1.4.0 on Windows. With this update the image supports Windows versions 1809 (2019LTS), 1903, 1909, 2004 (<a href=https://github.com/kubernetes/kubernetes/pull/95419>#95419</a>, <a href=https://github.com/jsturtevant>@jsturtevant</a>) [SIG Cluster Lifecycle and Windows]</li><li>Upgrade snapshot controller to 3.0.0 (<a href=https://github.com/kubernetes/kubernetes/pull/95412>#95412</a>, <a href=https://github.com/saikat-royc>@saikat-royc</a>) [SIG Cloud Provider]</li><li>Remove the dependency of csi-translation-lib module on apiserver/cloud-provider/controller-manager (<a href=https://github.com/kubernetes/kubernetes/pull/95543>#95543</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Release]</li><li>Scheduler framework interface moved from pkg/scheduler/framework/v1alpha to pkg/scheduler/framework (<a href=https://github.com/kubernetes/kubernetes/pull/95069>#95069</a>, <a href=https://github.com/farah>@farah</a>) [SIG Scheduling, Storage and Testing]</li><li>UDP and SCTP protocols can left stale connections that need to be cleared to avoid services disruption, but they can cause problems that are hard to debug.
Kubernetes components using a loglevel greater or equal than 4 will log the conntrack operations and its output, to show the entries that were deleted. (<a href=https://github.com/kubernetes/kubernetes/pull/95694>#95694</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li></ul><h2 id=dependencies-6>Dependencies</h2><h3 id=added-6>Added</h3><p><em>Nothing has changed.</em></p><h3 id=changed-6>Changed</h3><p><em>Nothing has changed.</em></p><h3 id=removed-6>Removed</h3><p><em>Nothing has changed.</em></p><h1 id=v1-20-0-alpha-2>v1.20.0-alpha.2</h1><h2 id=downloads-for-v1-20-0-alpha-2>Downloads for v1.20.0-alpha.2</h2><h3 id=source-code-5>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>45089a4d26d56a5d613ecbea64e356869ac738eca3cc71d16b74ea8ae1b4527bcc32f1dc35ff7aa8927e138083c7936603faf063121d965a2f0f8ba28fa128d8</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>646edd890d6df5858b90aaf68cc6e1b4589b8db09396ae921b5c400f2188234999e6c9633906692add08c6e8b4b09f12b2099132b0a7533443fb2a01cfc2bf81</td></tr></tbody></table><h3 id=client-binaries-6>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>c136273883e24a2a50b5093b9654f01cdfe57b97461d34885af4a68c2c4d108c07583c02b1cdf7f57f82e91306e542ce8f3bddb12fcce72b744458bc4796f8eb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>6ec59f1ed30569fa64ddb2d0de32b1ae04cda4ffe13f339050a7c9d7c63d425ee6f6d963dcf82c17281c4474da3eaf32c08117669052872a8c81bdce2c8a5415</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>7b40a4c087e2ea7f8d055f297fcd39a3f1cb6c866e7a3981a9408c3c3eb5363c648613491aad11bc7d44d5530b20832f8f96f6ceff43deede911fb74aafad35f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>cda9955feebea5acb8f2b5b87895d24894bbbbde47041453b1f926ebdf47a258ce0496aa27d06bcbf365b5615ce68a20d659b64410c54227216726e2ee432fca</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>f65bd9241c7eb88a4886a285330f732448570aea4ededaebeabcf70d17ea185f51bf8a7218f146ee09fb1adceca7ee71fb3c3683834f2c415163add820fba96e</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>1e377599af100a81d027d9199365fb8208d443a8e0a97affff1a79dc18796e14b78cb53d6e245c1c1e8defd0e050e37bf5f2a23c8a3ff45a6d18d03619709bf5</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>1cdee81478246aa7e7b80ae4efc7f070a5b058083ae278f59fad088b75a8052761b0e15ab261a6e667ddafd6a69fb424fc307072ed47941cad89a85af7aee93d</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>d8774167c87b6844c348aa15e92d5033c528d6ab9e95d08a7cb22da68bafd8e46d442cf57a5f6affad62f674c10ae6947d524b94108b5e450ca78f92656d63c0</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>f664b47d8daa6036f8154c1dc1f881bfe683bf57c39d9b491de3848c03d051c50c6644d681baf7f9685eae45f9ce62e4c6dfea2853763cfe8256a61bdd59d894</td></tr></tbody></table><h3 id=server-binaries-6>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>d6fcb4600be0beb9de222a8da64c35fe22798a0da82d41401d34d0f0fc7e2817512169524c281423d8f4a007cd77452d966317d5a1b67d2717a05ff346e8aa7d</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>022a76cf10801f8afbabb509572479b68fdb4e683526fa0799cdbd9bab4d3f6ecb76d1d63d0eafee93e3edf6c12892d84b9c771ef2325663b95347728fa3d6c0</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>0679aadd60bbf6f607e5befad74b5267eb2d4c1b55985cc25a97e0f4c5efb7acbb3ede91bfa6a5a5713dae4d7a302f6faaf678fd6b359284c33d9a6aca2a08bb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>9f2cfeed543b515eafb60d9765a3afff4f3d323c0a5c8a0d75e3de25985b2627817bfcbe59a9a61d969e026e2b861adb974a09eae75b58372ed736ceaaed2a82</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>937258704d7b9dcd91f35f2d34ee9dd38c18d9d4e867408c05281bfbbb919ad012c95880bee84d2674761aa44cc617fb2fae1124cf63b689289286d6eac1c407</td></tr></tbody></table><h3 id=node-binaries-6>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>076165d745d47879de68f4404eaf432920884be48277eb409e84bf2c61759633bf3575f46b0995f1fc693023d76c0921ed22a01432e756d7f8d9e246a243b126</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>1ff2e2e3e43af41118cdfb70c778e15035bbb1aca833ffd2db83c4bcd44f55693e956deb9e65017ebf3c553f2820ad5cd05f5baa33f3d63f3e00ed980ea4dfed</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>b232c7359b8c635126899beee76998078eec7a1ef6758d92bcdebe8013b0b1e4d7b33ecbf35e3f82824fe29493400845257e70ed63c1635bfa36c8b3b4969f6f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>51d415a068f554840f4c78d11a4fedebd7cb03c686b0ec864509b24f7a8667ebf54bb0a25debcf2b70f38be1e345e743f520695b11806539a55a3620ce21946f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>b51c082d8af358233a088b632cf2f6c8cfe5421471c27f5dc9ba4839ae6ea75df25d84298f2042770097554c01742bb7686694b331ad9bafc93c86317b867728</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.2/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>91b9d26620a2dde67a0edead0039814efccbdfd54594dda3597aaced6d89140dc92612ed0727bc21d63468efeef77c845e640153b09e39d8b736062e6eee0c76</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-alpha-1>Changelog since v1.20.0-alpha.1</h2><h2 id=changes-by-kind-6>Changes by Kind</h2><h3 id=deprecation-4>Deprecation</h3><ul><li>Action-required: kubeadm: graduate the "kubeadm alpha certs" command to a parent command "kubeadm certs". The command "kubeadm alpha certs" is deprecated and will be removed in a future release. Please migrate. (<a href=https://github.com/kubernetes/kubernetes/pull/94938>#94938</a>, <a href=https://github.com/yagonobre>@yagonobre</a>) [SIG Cluster Lifecycle]</li><li>Action-required: kubeadm: remove the deprecated feature --experimental-kustomize from kubeadm commands. The feature was replaced with --experimental-patches in 1.19. To migrate see the --help description for the --experimental-patches flag. (<a href=https://github.com/kubernetes/kubernetes/pull/94871>#94871</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: deprecate self-hosting support. The experimental command "kubeadm alpha self-hosting" is now deprecated and will be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/95125>#95125</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Removes deprecated scheduler metrics DeprecatedSchedulingDuration, DeprecatedSchedulingAlgorithmPredicateEvaluationSecondsDuration, DeprecatedSchedulingAlgorithmPriorityEvaluationSecondsDuration (<a href=https://github.com/kubernetes/kubernetes/pull/94884>#94884</a>, <a href=https://github.com/arghya88>@arghya88</a>) [SIG Instrumentation and Scheduling]</li><li>Scheduler alpha metrics binding_duration_seconds and scheduling_algorithm_preemption_evaluation_seconds are deprecated, Both of those metrics are now covered as part of framework_extension_point_duration_seconds, the former as a PostFilter the latter and a Bind plugin. The plan is to remove both in 1.21 (<a href=https://github.com/kubernetes/kubernetes/pull/95001>#95001</a>, <a href=https://github.com/arghya88>@arghya88</a>) [SIG Instrumentation and Scheduling]</li></ul><h3 id=api-change-5>API Change</h3><ul><li>GPU metrics provided by kubelet are now disabled by default (<a href=https://github.com/kubernetes/kubernetes/pull/95184>#95184</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Node]</li><li>New parameter <code>defaultingType</code> for <code>PodTopologySpread</code> plugin allows to use k8s defined or user provided default constraints (<a href=https://github.com/kubernetes/kubernetes/pull/95048>#95048</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Server Side Apply now treats LabelSelector fields as atomic (meaning the entire selector is managed by a single writer and updated together), since they contain interrelated and inseparable fields that do not merge in intuitive ways. (<a href=https://github.com/kubernetes/kubernetes/pull/93901>#93901</a>, <a href=https://github.com/jpbetz>@jpbetz</a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Storage and Testing]</li><li>Status of v1beta1 CRDs without "preserveUnknownFields:false" will show violation "spec.preserveUnknownFields: Invalid value: true: must be false" (<a href=https://github.com/kubernetes/kubernetes/pull/93078>#93078</a>, <a href=https://github.com/vareti>@vareti</a>) [SIG API Machinery]</li></ul><h3 id=feature-6>Feature</h3><ul><li><p>Added <code>get-users</code> and <code>delete-user</code> to the <code>kubectl config</code> subcommand (<a href=https://github.com/kubernetes/kubernetes/pull/89840>#89840</a>, <a href=https://github.com/eddiezane>@eddiezane</a>) [SIG CLI]</p></li><li><p>Added counter metric "apiserver_request_self" to count API server self-requests with labels for verb, resource, and subresource. (<a href=https://github.com/kubernetes/kubernetes/pull/94288>#94288</a>, <a href=https://github.com/LogicalShark>@LogicalShark</a>) [SIG API Machinery, Auth, Instrumentation and Scheduling]</p></li><li><p>Added new k8s.io/component-helpers repository providing shared helper code for (core) components. (<a href=https://github.com/kubernetes/kubernetes/pull/92507>#92507</a>, <a href=https://github.com/ingvagabund>@ingvagabund</a>) [SIG Apps, Node, Release and Scheduling]</p></li><li><p>Adds <code>create ingress</code> command to <code>kubectl</code> (<a href=https://github.com/kubernetes/kubernetes/pull/78153>#78153</a>, <a href=https://github.com/amimof>@amimof</a>) [SIG CLI and Network]</p></li><li><p>Allow configuring AWS LoadBalancer health check protocol via service annotations (<a href=https://github.com/kubernetes/kubernetes/pull/94546>#94546</a>, <a href=https://github.com/kishorj>@kishorj</a>) [SIG Cloud Provider]</p></li><li><p>Azure: Support multiple services sharing one IP address (<a href=https://github.com/kubernetes/kubernetes/pull/94991>#94991</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</p></li><li><p>Ephemeral containers now apply the same API defaults as initContainers and containers (<a href=https://github.com/kubernetes/kubernetes/pull/94896>#94896</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Apps and CLI]</p></li><li><p>In dual-stack bare-metal clusters, you can now pass dual-stack IPs to <code>kubelet --node-ip</code>.
eg: <code>kubelet --node-ip 10.1.0.5,fd01::0005</code>. This is not yet supported for non-bare-metal
clusters.</p><p>In dual-stack clusters where nodes have dual-stack addresses, hostNetwork pods
will now get dual-stack PodIPs. (<a href=https://github.com/kubernetes/kubernetes/pull/95239>#95239</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Network and Node]</p></li><li><p>Introduces a new GCE specific cluster creation variable KUBE_PROXY_DISABLE. When set to true, this will skip over the creation of kube-proxy (whether the daemonset or static pod). This can be used to control the lifecycle of kube-proxy separately from the lifecycle of the nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/91977>#91977</a>, <a href=https://github.com/varunmar>@varunmar</a>) [SIG Cloud Provider]</p></li><li><p>Kubeadm: do not throw errors if the current system time is outside of the NotBefore and NotAfter bounds of a loaded certificate. Print warnings instead. (<a href=https://github.com/kubernetes/kubernetes/pull/94504>#94504</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make the command "kubeadm alpha kubeconfig user" accept a "--config" flag and remove the following flags:</p><ul><li>apiserver-advertise-address / apiserver-bind-port: use either localAPIEndpoint from InitConfiguration or controlPlaneEndpoint from ClusterConfiguration.</li><li>cluster-name: use clusterName from ClusterConfiguration</li><li>cert-dir: use certificatesDir from ClusterConfiguration (<a href=https://github.com/kubernetes/kubernetes/pull/94879>#94879</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cluster Lifecycle]</li></ul></li><li><p>Kubectl rollout history sts/sts-name --revision=some-revision will start showing the detailed view of the sts on that specified revision (<a href=https://github.com/kubernetes/kubernetes/pull/86506>#86506</a>, <a href=https://github.com/dineshba>@dineshba</a>) [SIG CLI]</p></li><li><p>Scheduling Framework: expose Run[Pre]ScorePlugins functions to PreemptionHandle which can be used in PostFilter extention point. (<a href=https://github.com/kubernetes/kubernetes/pull/93534>#93534</a>, <a href=https://github.com/everpeace>@everpeace</a>) [SIG Scheduling and Testing]</p></li><li><p>Send gce node startup scripts logs to console and journal (<a href=https://github.com/kubernetes/kubernetes/pull/95311>#95311</a>, <a href=https://github.com/karan>@karan</a>) [SIG Cloud Provider and Node]</p></li><li><p>Support kubectl delete orphan/foreground/background options (<a href=https://github.com/kubernetes/kubernetes/pull/93384>#93384</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI and Testing]</p></li></ul><h3 id=bug-or-regression-6>Bug or Regression</h3><ul><li>Change the mount way from systemd to normal mount except ceph and glusterfs intree-volume. (<a href=https://github.com/kubernetes/kubernetes/pull/94916>#94916</a>, <a href=https://github.com/smileusd>@smileusd</a>) [SIG Apps, Cloud Provider, Network, Node, Storage and Testing]</li><li>Cloud node controller: handle empty providerID from getProviderID (<a href=https://github.com/kubernetes/kubernetes/pull/95342>#95342</a>, <a href=https://github.com/nicolehanjing>@nicolehanjing</a>) [SIG Cloud Provider]</li><li>Fix a bug where the endpoint slice controller was not mirroring the parent service labels to its corresponding endpoint slices (<a href=https://github.com/kubernetes/kubernetes/pull/94443>#94443</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Apps and Network]</li><li>Fix azure disk attach failure for disk size bigger than 4TB (<a href=https://github.com/kubernetes/kubernetes/pull/95463>#95463</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix azure disk data loss issue on Windows when unmount disk (<a href=https://github.com/kubernetes/kubernetes/pull/95456>#95456</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix detach azure disk issue when vm not exist (<a href=https://github.com/kubernetes/kubernetes/pull/95177>#95177</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix network_programming_latency metric reporting for Endpoints/EndpointSlice deletions, where we don't have correct timestamp (<a href=https://github.com/kubernetes/kubernetes/pull/95363>#95363</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG Network and Scalability]</li><li>Fix scheduler cache snapshot when a Node is deleted before its Pods (<a href=https://github.com/kubernetes/kubernetes/pull/95130>#95130</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Fix vsphere detach failure for static PVs (<a href=https://github.com/kubernetes/kubernetes/pull/95447>#95447</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider and Storage]</li><li>Fixed a bug that prevents the use of ephemeral containers in the presence of a validating admission webhook. (<a href=https://github.com/kubernetes/kubernetes/pull/94685>#94685</a>, <a href=https://github.com/verb>@verb</a>) [SIG Node and Testing]</li><li>Gracefully delete nodes when their parent scale set went missing (<a href=https://github.com/kubernetes/kubernetes/pull/95289>#95289</a>, <a href=https://github.com/bpineau>@bpineau</a>) [SIG Cloud Provider]</li><li>In dual-stack clusters, kubelet will now set up both IPv4 and IPv6 iptables rules, which may
fix some problems, eg with HostPorts. (<a href=https://github.com/kubernetes/kubernetes/pull/94474>#94474</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Network and Node]</li><li>Kubeadm: for Docker as the container runtime, make the "kubeadm reset" command stop containers before removing them (<a href=https://github.com/kubernetes/kubernetes/pull/94586>#94586</a>, <a href=https://github.com/BedivereZero>@BedivereZero</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: warn but do not error out on missing "ca.key" files for root CA, front-proxy CA and etcd CA, during "kubeadm join --control-plane" if the user has provided all certificates, keys and kubeconfig files which require signing with the given CA keys. (<a href=https://github.com/kubernetes/kubernetes/pull/94988>#94988</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Port mapping allows to map the same <code>containerPort</code> to multiple <code>hostPort</code> without naming the mapping explicitly. (<a href=https://github.com/kubernetes/kubernetes/pull/94494>#94494</a>, <a href=https://github.com/SergeyKanzhelev>@SergeyKanzhelev</a>) [SIG Network and Node]</li><li>Warn instead of fail when creating Roles and ClusterRoles with custom verbs via kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/92492>#92492</a>, <a href=https://github.com/eddiezane>@eddiezane</a>) [SIG CLI]</li></ul><h3 id=other-cleanup-or-flake-5>Other (Cleanup or Flake)</h3><ul><li>Added fine grained debugging to the intra-pod conformance test for helping easily resolve networking issues for nodes that might be unhealthy when running conformance or sonobuoy tests. (<a href=https://github.com/kubernetes/kubernetes/pull/93837>#93837</a>, <a href=https://github.com/jayunit100>@jayunit100</a>) [SIG Network and Testing]</li><li>AdmissionReview objects sent for the creation of Namespace API objects now populate the <code>namespace</code> attribute consistently (previously the <code>namespace</code> attribute was empty for Namespace creation via POST requests, and populated for Namespace creation via server-side-apply PATCH requests) (<a href=https://github.com/kubernetes/kubernetes/pull/95012>#95012</a>, <a href=https://github.com/nodo>@nodo</a>) [SIG API Machinery and Testing]</li><li>Client-go header logging (at verbosity levels >= 9) now masks <code>Authorization</code> header contents (<a href=https://github.com/kubernetes/kubernetes/pull/95316>#95316</a>, <a href=https://github.com/sfowl>@sfowl</a>) [SIG API Machinery]</li><li>Enhance log information of verifyRunAsNonRoot, add pod, container information (<a href=https://github.com/kubernetes/kubernetes/pull/94911>#94911</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node]</li><li>Errors from staticcheck:<br>vendor/k8s.io/client-go/discovery/cached/memory/memcache_test.go:94:2: this value of g is never used (SA4006) (<a href=https://github.com/kubernetes/kubernetes/pull/95098>#95098</a>, <a href=https://github.com/phunziker>@phunziker</a>) [SIG API Machinery]</li><li>Kubeadm: update the default pause image version to 1.4.0 on Windows. With this update the image supports Windows versions 1809 (2019LTS), 1903, 1909, 2004 (<a href=https://github.com/kubernetes/kubernetes/pull/95419>#95419</a>, <a href=https://github.com/jsturtevant>@jsturtevant</a>) [SIG Cluster Lifecycle and Windows]</li><li>Masks ceph RBD adminSecrets in logs when logLevel >= 4 (<a href=https://github.com/kubernetes/kubernetes/pull/95245>#95245</a>, <a href=https://github.com/sfowl>@sfowl</a>) [SIG Storage]</li><li>Upgrade snapshot controller to 3.0.0 (<a href=https://github.com/kubernetes/kubernetes/pull/95412>#95412</a>, <a href=https://github.com/saikat-royc>@saikat-royc</a>) [SIG Cloud Provider]</li><li>Remove offensive words from kubectl cluster-info command (<a href=https://github.com/kubernetes/kubernetes/pull/95202>#95202</a>, <a href=https://github.com/rikatz>@rikatz</a>) [SIG Architecture, CLI and Testing]</li><li>The following new metrics are available.<ul><li>network_plugin_operations_total</li><li>network_plugin_operations_errors_total (<a href=https://github.com/kubernetes/kubernetes/pull/93066>#93066</a>, <a href=https://github.com/AnishShah>@AnishShah</a>) [SIG Instrumentation, Network and Node]</li></ul></li><li>Vsphere: improve logging message on node cache refresh event (<a href=https://github.com/kubernetes/kubernetes/pull/95236>#95236</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Cloud Provider]</li><li><code>kubectl api-resources</code> now prints the API version (as 'API group/version', same as output of <code>kubectl api-versions</code>). The column APIGROUP is now APIVERSION (<a href=https://github.com/kubernetes/kubernetes/pull/95253>#95253</a>, <a href=https://github.com/sallyom>@sallyom</a>) [SIG CLI]</li></ul><h2 id=dependencies-7>Dependencies</h2><h3 id=added-7>Added</h3><ul><li>github.com/jmespath/go-jmespath/internal/testify: <a href=https://github.com/jmespath/go-jmespath/internal/testify/tree/v1.5.1>v1.5.1</a></li></ul><h3 id=changed-7>Changed</h3><ul><li>github.com/aws/aws-sdk-go: <a href=https://github.com/aws/aws-sdk-go/compare/v1.28.2...v1.35.5>v1.28.2 → v1.35.5</a></li><li>github.com/jmespath/go-jmespath: <a href=https://github.com/jmespath/go-jmespath/compare/c2b33e8...v0.4.0>c2b33e8 → v0.4.0</a></li><li>k8s.io/kube-openapi: 6aeccd4 → 8b50664</li><li>sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.9 → v0.0.12</li><li>sigs.k8s.io/structured-merge-diff/v4: v4.0.1 → b3cf1e8</li></ul><h3 id=removed-7>Removed</h3><p><em>Nothing has changed.</em></p><h1 id=v1-20-0-alpha-1>v1.20.0-alpha.1</h1><h2 id=downloads-for-v1-20-0-alpha-1>Downloads for v1.20.0-alpha.1</h2><h3 id=source-code-6>Source Code</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td>e7daed6502ea07816274f2371f96fe1a446d0d7917df4454b722d9eb3b5ff6163bfbbd5b92dfe7a0c1d07328b8c09c4ae966e482310d6b36de8813aaf87380b5</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td>e91213a0919647a1215d4691a63b12d89a3e74055463a8ebd71dc1a4cabf4006b3660881067af0189960c8dab74f4a7faf86f594df69021901213ee5b56550ea</td></tr></tbody></table><h3 id=client-binaries-7>Client binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td>1f3add5f826fa989820d715ca38e8864b66f30b59c1abeacbb4bfb96b4e9c694eac6b3f4c1c81e0ee3451082d44828cb7515315d91ad68116959a5efbdaef1e1</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td>c62acdc8993b0a950d4b0ce0b45473bf96373d501ce61c88adf4007afb15c1d53da8d53b778a7eccac6c1624f7fdda322be9f3a8bc2d80aaad7b4237c39f5eaf</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td>1203ababfe00f9bc5be5c059324c17160a96530c1379a152db33564bbe644ccdb94b30eea15a0655bd652efb17895a46c31bbba19d4f5f473c2a0ff62f6e551f</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td>31860088596e12d739c7aed94556c2d1e217971699b950c8417a3cea1bed4e78c9ff1717b9f3943354b75b4641d4b906cd910890dbf4278287c0d224837d9a7d</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td>8d469f37fe20d6e15b5debc13cce4c22e8b7a4f6a4ac787006b96507a85ce761f63b28140d692c54b5f7deb08697f8d5ddb9bbfa8f5ac0d9241fc7de3a3fe3cd</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td>0d62ee1729cd5884946b6c73701ad3a570fa4d642190ca0fe5c1db0fb0cba9da3ac86a948788d915b9432d28ab8cc499e28aadc64530b7d549ee752a6ed93ec1</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td>0fc0420e134ec0b8e0ab2654e1e102cebec47b48179703f1e1b79d51ee0d6da55a4e7304d8773d3cf830341ac2fe3cede1e6b0460fd88f7595534e0730422d5a</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td>3fb53b5260f4888c77c0e4ff602bbcf6bf38c364d2769850afe2b8d8e8b95f7024807c15e2b0d5603e787c46af8ac53492be9e88c530f578b8a389e3bd50c099</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td>2f44c93463d6b5244ce0c82f147e7f32ec2233d0e29c64c3c5759e23533aebd12671bf63e986c0861e9736f9b5259bb8d138574a7c8c8efc822e35cd637416c0</td></tr></tbody></table><h3 id=server-binaries-7>Server binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td>ae82d14b1214e4100f0cc2c988308b3e1edd040a65267d0eddb9082409f79644e55387889e3c0904a12c710f91206e9383edf510990bee8c9ea2e297b6472551</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td>9a2a5828b7d1ddb16cc19d573e99a4af642f84129408e6203eeeb0558e7b8db77f3269593b5770b6a976fe9df4a64240ed27ad05a4bd43719e55fce1db0abf58</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td>ed700dd226c999354ce05b73927388d36d08474c15333ae689427de15de27c84feb6b23c463afd9dd81993315f31eb8265938cfc7ecf6f750247aa42b9b33fa9</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td>abb7a9d726538be3ccf5057a0c63ff9732b616e213c6ebb81363f0c49f1e168ce8068b870061ad7cba7ba1d49252f94cf00a5f68cec0f38dc8fce4e24edc5ca6</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td>3a51888af1bfdd2d5b0101d173ee589c1f39240e4428165f5f85c610344db219625faa42f00a49a83ce943fb079be873b1a114a62003fae2f328f9bf9d1227a4</td></tr></tbody></table><h3 id=node-binaries-7>Node binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td>d0f28e3c38ca59a7ff1bfecb48a1ce97116520355d9286afdca1200d346c10018f5bbdf890f130a388654635a2e83e908b263ed45f8a88defca52a7c1d0a7984</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td>ed9d3f13028beb3be39bce980c966f82c4b39dc73beaae38cc075fea5be30b0309e555cb2af8196014f2cc9f0df823354213c314b4d6545ff6e30dd2d00ec90e</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td>ad5b3268db365dcdded9a9a4bffc90c7df0f844000349accdf2b8fb5f1081e553de9b9e9fb25d5e8a4ef7252d51fa94ef94d36d2ab31d157854e164136f662c2</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td>c4de2524e513996def5eeba7b83f7b406f17eaf89d4d557833a93bd035348c81fa9375dcd5c27cfcc55d73995449fc8ee504be1b3bd7b9f108b0b2f153cb05ae</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td>9157b44e3e7bd5478af9f72014e54d1afa5cd19b984b4cd8b348b312c385016bb77f29db47f44aea08b58abf47d8a396b92a2d0e03f2fe8acdd30f4f9466cbdb</td></tr><tr><td><a href=https://dl.k8s.io/v1.20.0-alpha.1/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td>8b40a43c5e6447379ad2ee8aac06e8028555e1b370a995f6001018a62411abe5fbbca6060b3d1682c5cadc07a27d49edd3204e797af46368800d55f4ca8aa1de</td></tr></tbody></table><h2 id=changelog-since-v1-20-0-alpha-0>Changelog since v1.20.0-alpha.0</h2><h2 id=urgent-upgrade-notes-3>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade-3>(No, really, you MUST read this before you upgrade)</h3><ul><li>Azure blob disk feature(<code>kind</code>: <code>Shared</code>, <code>Dedicated</code>) has been deprecated, you should use <code>kind</code>: <code>Managed</code> in <code>kubernetes.io/azure-disk</code> storage class. (<a href=https://github.com/kubernetes/kubernetes/pull/92905>#92905</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>CVE-2020-8559 (Medium): Privilege escalation from compromised node to cluster. See <a href=https://github.com/kubernetes/kubernetes/issues/92914>https://github.com/kubernetes/kubernetes/issues/92914</a> for more details.
The API Server will no longer proxy non-101 responses for upgrade requests. This could break proxied backends (such as an extension API server) that respond to upgrade requests with a non-101 response code. (<a href=https://github.com/kubernetes/kubernetes/pull/92941>#92941</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG API Machinery]</li></ul><h2 id=changes-by-kind-7>Changes by Kind</h2><h3 id=deprecation-5>Deprecation</h3><ul><li>Kube-apiserver: the componentstatus API is deprecated. This API provided status of etcd, kube-scheduler, and kube-controller-manager components, but only worked when those components were local to the API server, and when kube-scheduler and kube-controller-manager exposed unsecured health endpoints. Instead of this API, etcd health is included in the kube-apiserver health check and kube-scheduler/kube-controller-manager health checks can be made directly against those components' health endpoints. (<a href=https://github.com/kubernetes/kubernetes/pull/93570>#93570</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps and Cluster Lifecycle]</li><li>Kubeadm: deprecate the "kubeadm alpha kubelet config enable-dynamic" command. To continue using the feature please defer to the guide for "Dynamic Kubelet Configuration" at k8s.io. (<a href=https://github.com/kubernetes/kubernetes/pull/92881>#92881</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the deprecated "kubeadm alpha kubelet config enable-dynamic" command. To continue using the feature please defer to the guide for "Dynamic Kubelet Configuration" at k8s.io. This change also removes the parent command "kubeadm alpha kubelet" as there are no more sub-commands under it for the time being. (<a href=https://github.com/kubernetes/kubernetes/pull/94668>#94668</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the deprecated --kubelet-config flag for the command "kubeadm upgrade node" (<a href=https://github.com/kubernetes/kubernetes/pull/94869>#94869</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubelet's deprecated endpoint <code>metrics/resource/v1alpha1</code> has been removed, please adopt to <code>metrics/resource</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94272>#94272</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG Instrumentation and Node]</li><li>The v1alpha1 PodPreset API and admission plugin has been removed with no built-in replacement. Admission webhooks can be used to modify pods on creation. (<a href=https://github.com/kubernetes/kubernetes/pull/94090>#94090</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Apps, CLI, Cloud Provider, Scalability and Testing]</li></ul><h3 id=api-change-6>API Change</h3><ul><li>A new <code>nofuzz</code> go build tag now disables gofuzz support. Release binaries enable this. (<a href=https://github.com/kubernetes/kubernetes/pull/92491>#92491</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG API Machinery]</li><li>A new alpha-level field, <code>SupportsFsGroup</code>, has been introduced for CSIDrivers to allow them to specify whether they support volume ownership and permission modifications. The <code>CSIVolumeSupportFSGroup</code> feature gate must be enabled to allow this field to be used. (<a href=https://github.com/kubernetes/kubernetes/pull/92001>#92001</a>, <a href=https://github.com/huffmanca>@huffmanca</a>) [SIG API Machinery, CLI and Storage]</li><li>Added pod version skew strategy for seccomp profile to synchronize the deprecated annotations with the new API Server fields. Please see the corresponding section <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190717-seccomp-ga.md#version-skew-strategy>in the KEP</a> for more detailed explanations. (<a href=https://github.com/kubernetes/kubernetes/pull/91408>#91408</a>, <a href=https://github.com/saschagrunert>@saschagrunert</a>) [SIG Apps, Auth, CLI and Node]</li><li>Adds the ability to disable Accelerator/GPU metrics collected by Kubelet (<a href=https://github.com/kubernetes/kubernetes/pull/91930>#91930</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Node]</li><li>Custom Endpoints are now mirrored to EndpointSlices by a new EndpointSliceMirroring controller. (<a href=https://github.com/kubernetes/kubernetes/pull/91637>#91637</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps, Auth, Cloud Provider, Instrumentation, Network and Testing]</li><li>External facing API podresources is now available under k8s.io/kubelet/pkg/apis/ (<a href=https://github.com/kubernetes/kubernetes/pull/92632>#92632</a>, <a href=https://github.com/RenaudWasTaken>@RenaudWasTaken</a>) [SIG Node and Testing]</li><li>Fix conversions for custom metrics. (<a href=https://github.com/kubernetes/kubernetes/pull/94481>#94481</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Instrumentation]</li><li>Generic ephemeral volumes, a new alpha feature under the <code>GenericEphemeralVolume</code> feature gate, provide a more flexible alternative to <code>EmptyDir</code> volumes: as with <code>EmptyDir</code>, volumes are created and deleted for each pod automatically by Kubernetes. But because the normal provisioning process is used (<code>PersistentVolumeClaim</code>), storage can be provided by third-party storage vendors and all of the usual volume features work. Volumes don't need to be empt; for example, restoring from snapshot is supported. (<a href=https://github.com/kubernetes/kubernetes/pull/92784>#92784</a>, <a href=https://github.com/pohly>@pohly</a>) [SIG API Machinery, Apps, Auth, CLI, Instrumentation, Node, Scheduling, Storage and Testing]</li><li>Kube-controller-manager: volume plugins can be restricted from contacting local and loopback addresses by setting <code>--volume-host-allow-local-loopback=false</code>, or from contacting specific CIDR ranges by setting <code>--volume-host-cidr-denylist</code> (for example, <code>--volume-host-cidr-denylist=127.0.0.1/28,feed::/16</code>) (<a href=https://github.com/kubernetes/kubernetes/pull/91785>#91785</a>, <a href=https://github.com/mattcary>@mattcary</a>) [SIG API Machinery, Apps, Auth, CLI, Network, Node, Storage and Testing]</li><li>Kubernetes is now built with golang 1.15.0-rc.1.<ul><li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=https://github.com/kubernetes/kubernetes/pull/93264>#93264</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li></ul></li><li>Migrate scheduler, controller-manager and cloud-controller-manager to use LeaseLock (<a href=https://github.com/kubernetes/kubernetes/pull/94603>#94603</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery, Apps, Cloud Provider and Scheduling]</li><li>Modify DNS-1123 error messages to indicate that RFC 1123 is not followed exactly (<a href=https://github.com/kubernetes/kubernetes/pull/94182>#94182</a>, <a href=https://github.com/mattfenwick>@mattfenwick</a>) [SIG API Machinery, Apps, Auth, Network and Node]</li><li>The ServiceAccountIssuerDiscovery feature gate is now Beta and enabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/91921>#91921</a>, <a href=https://github.com/mtaufen>@mtaufen</a>) [SIG Auth]</li><li>The kube-controller-manager managed signers can now have distinct signing certificates and keys. See the help about <code>--cluster-signing-[signer-name]-{cert,key}-file</code>. <code>--cluster-signing-{cert,key}-file</code> is still the default. (<a href=https://github.com/kubernetes/kubernetes/pull/90822>#90822</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Apps and Auth]</li><li>When creating a networking.k8s.io/v1 Ingress API object, <code>spec.tls[*].secretName</code> values are required to pass validation rules for Secret API object names. (<a href=https://github.com/kubernetes/kubernetes/pull/93929>#93929</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Network]</li><li>WinOverlay feature graduated to beta (<a href=https://github.com/kubernetes/kubernetes/pull/94807>#94807</a>, <a href=https://github.com/ksubrmnn>@ksubrmnn</a>) [SIG Windows]</li></ul><h3 id=feature-7>Feature</h3><ul><li><p>ACTION REQUIRED : In CoreDNS v1.7.0, <a href=https://github.com/coredns/coredns/blob/master/notes/coredns-1.7.0.md#metric-changes>metrics names have been changed</a> which will be backward incompatible with existing reporting formulas that use the old metrics' names. Adjust your formulas to the new names before upgrading.</p><p>Kubeadm now includes CoreDNS version v1.7.0. Some of the major changes include:</p><ul><li>Fixed a bug that could cause CoreDNS to stop updating service records.</li><li>Fixed a bug in the forward plugin where only the first upstream server is always selected no matter which policy is set.</li><li>Remove already deprecated options <code>resyncperiod</code> and <code>upstream</code> in the Kubernetes plugin.</li><li>Includes Prometheus metrics name changes (to bring them in line with standard Prometheus metrics naming convention). They will be backward incompatible with existing reporting formulas that use the old metrics' names.</li><li>The federation plugin (allows for v1 Kubernetes federation) has been removed.
More details are available in <a href=https://coredns.io/2020/06/15/coredns-1.7.0-release/>https://coredns.io/2020/06/15/coredns-1.7.0-release/</a> (<a href=https://github.com/kubernetes/kubernetes/pull/92651>#92651</a>, <a href=https://github.com/rajansandeep>@rajansandeep</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle and Instrumentation]</li></ul></li><li><p>Add metrics for azure service operations (route and loadbalancer). (<a href=https://github.com/kubernetes/kubernetes/pull/94124>#94124</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider and Instrumentation]</p></li><li><p>Add network rule support in Azure account creation (<a href=https://github.com/kubernetes/kubernetes/pull/94239>#94239</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Add tags support for Azure File Driver (<a href=https://github.com/kubernetes/kubernetes/pull/92825>#92825</a>, <a href=https://github.com/ZeroMagic>@ZeroMagic</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Added kube-apiserver metrics: apiserver_current_inflight_request_measures and, when API Priority and Fairness is enable, windowed_request_stats. (<a href=https://github.com/kubernetes/kubernetes/pull/91177>#91177</a>, <a href=https://github.com/MikeSpreitzer>@MikeSpreitzer</a>) [SIG API Machinery, Instrumentation and Testing]</p></li><li><p>Audit events for API requests to deprecated API versions now include a <code>"k8s.io/deprecated": "true"</code> audit annotation. If a target removal release is identified, the audit event includes a <code>"k8s.io/removal-release": "&lt;majorVersion>.&lt;minorVersion>"</code> audit annotation as well. (<a href=https://github.com/kubernetes/kubernetes/pull/92842>#92842</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Instrumentation]</p></li><li><p>Cloud node-controller use InstancesV2 (<a href=https://github.com/kubernetes/kubernetes/pull/91319>#91319</a>, <a href=https://github.com/gongguan>@gongguan</a>) [SIG Apps, Cloud Provider, Scalability and Storage]</p></li><li><p>Kubeadm: Add a preflight check that the control-plane node has at least 1700MB of RAM (<a href=https://github.com/kubernetes/kubernetes/pull/93275>#93275</a>, <a href=https://github.com/xlgao-zju>@xlgao-zju</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: add the "--cluster-name" flag to the "kubeadm alpha kubeconfig user" to allow configuring the cluster name in the generated kubeconfig file (<a href=https://github.com/kubernetes/kubernetes/pull/93992>#93992</a>, <a href=https://github.com/prabhu43>@prabhu43</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: add the "--kubeconfig" flag to the "kubeadm init phase upload-certs" command to allow users to pass a custom location for a kubeconfig file. (<a href=https://github.com/kubernetes/kubernetes/pull/94765>#94765</a>, <a href=https://github.com/zhanw15>@zhanw15</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: deprecate the "--csr-only" and "--csr-dir" flags of the "kubeadm init phase certs" subcommands. Please use "kubeadm alpha certs generate-csr" instead. This new command allows you to generate new private keys and certificate signing requests for all the control-plane components, so that the certificates can be signed by an external CA. (<a href=https://github.com/kubernetes/kubernetes/pull/92183>#92183</a>, <a href=https://github.com/wallrj>@wallrj</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make etcd pod request 100m CPU, 100Mi memory and 100Mi ephemeral_storage by default (<a href=https://github.com/kubernetes/kubernetes/pull/94479>#94479</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubemark now supports both real and hollow nodes in a single cluster. (<a href=https://github.com/kubernetes/kubernetes/pull/93201>#93201</a>, <a href=https://github.com/ellistarn>@ellistarn</a>) [SIG Scalability]</p></li><li><p>Kubernetes is now built using go1.15.2</p><ul><li><p>build: Update to <a href=mailto:k/repo-infra@v0.1.1>k/repo-infra@v0.1.1</a> (supports go1.15.2)</p></li><li><p>build: Use go-runner:buster-v2.0.1 (built using go1.15.1)</p></li><li><p>bazel: Replace --features with Starlark build settings flag</p></li><li><p>hack/lib/util.sh: some bash cleanups</p><ul><li>switched one spot to use kube::logging</li><li>make kube::util::find-binary return an error when it doesn't find
anything so that hack scripts fail fast instead of with '' binary not
found errors.</li><li>this required deleting some genfeddoc stuff. the binary no longer
exists in k/k repo since we removed federation/, and I don't see it
in <a href=https://github.com/kubernetes-sigs/kubefed/>https://github.com/kubernetes-sigs/kubefed/</a> either. I'm assuming
that it's gone for good now.</li></ul></li><li><p>bazel: output go_binary rule directly from go_binary_conditional_pure</p><p>From: @mikedanese:
Instead of aliasing. Aliases are annoying in a number of ways. This is
specifically bugging me now because they make the action graph harder to
analyze programmatically. By using aliases here, we would need to handle
potentially aliased go_binary targets and dereference to the effective
target.</p><p>The comment references an issue with <code>pure = select(...)</code> which appears
to be resolved considering this now builds.</p></li><li><p>make kube::util::find-binary not dependent on bazel-out/ structure</p><p>Implement an aspect that outputs go_build_mode metadata for go binaries,
and use that during binary selection. (<a href=https://github.com/kubernetes/kubernetes/pull/94449>#94449</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Architecture, CLI, Cluster Lifecycle, Node, Release and Testing]</p></li></ul></li><li><p>Only update Azure data disks when attach/detach (<a href=https://github.com/kubernetes/kubernetes/pull/94265>#94265</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Promote SupportNodePidsLimit to GA to provide node to pod pid isolation
Promote SupportPodPidsLimit to GA to provide ability to limit pids per pod (<a href=https://github.com/kubernetes/kubernetes/pull/94140>#94140</a>, <a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>) [SIG Node and Testing]</p></li><li><p>Rename pod_preemption_metrics to preemption_metrics. (<a href=https://github.com/kubernetes/kubernetes/pull/93256>#93256</a>, <a href=https://github.com/ahg-g>@ahg-g</a>) [SIG Instrumentation and Scheduling]</p></li><li><p>Server-side apply behavior has been regularized in the case where a field is removed from the applied configuration. Removed fields which have no other owners are deleted from the live object, or reset to their default value if they have one. Safe ownership transfers, such as the transfer of a <code>replicas</code> field from a user to an HPA without resetting to the default value are documented in <a href=/docs/reference/using-api/server-side-apply/#transferring-ownership>Transferring Ownership</a> (<a href=https://github.com/kubernetes/kubernetes/pull/92661>#92661</a>, <a href=https://github.com/jpbetz>@jpbetz</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Testing]</p></li><li><p>Set CSIMigrationvSphere feature gates to beta.
Users should enable CSIMigration + CSIMigrationvSphere features and install the vSphere CSI Driver (<a href=https://github.com/kubernetes-sigs/vsphere-csi-driver>https://github.com/kubernetes-sigs/vsphere-csi-driver</a>) to move workload from the in-tree vSphere plugin "kubernetes.io/vsphere-volume" to vSphere CSI Driver.</p><p>Requires: vSphere vCenter/ESXi Version: 7.0u1, HW Version: VM version 15 (<a href=https://github.com/kubernetes/kubernetes/pull/92816>#92816</a>, <a href=https://github.com/divyenpatel>@divyenpatel</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Support [service.beta.kubernetes.io/azure-pip-ip-tags] annotations to allow customers to specify ip-tags to influence public-ip creation in Azure [Tag1=Value1, Tag2=Value2, etc.] (<a href=https://github.com/kubernetes/kubernetes/pull/94114>#94114</a>, <a href=https://github.com/MarcPow>@MarcPow</a>) [SIG Cloud Provider]</p></li><li><p>Support a smooth upgrade from client-side apply to server-side apply without conflicts, as well as support the corresponding downgrade. (<a href=https://github.com/kubernetes/kubernetes/pull/90187>#90187</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG API Machinery and Testing]</p></li><li><p>Trace output in apiserver logs is more organized and comprehensive. Traces are nested, and for all non-long running request endpoints, the entire filter chain is instrumented (e.g. authentication check is included). (<a href=https://github.com/kubernetes/kubernetes/pull/88936>#88936</a>, <a href=https://github.com/jpbetz>@jpbetz</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Scheduling]</p></li><li><p><code>kubectl alpha debug</code> now supports debugging nodes by creating a debugging container running in the node's host namespaces. (<a href=https://github.com/kubernetes/kubernetes/pull/92310>#92310</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI]</p></li></ul><h3 id=documentation-4>Documentation</h3><ul><li>Kubelet: remove alpha warnings for CNI flags. (<a href=https://github.com/kubernetes/kubernetes/pull/94508>#94508</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Network and Node]</li></ul><h3 id=failing-test-2>Failing Test</h3><ul><li>Kube-proxy iptables min-sync-period defaults to 1 sec. Previously, it was 0. (<a href=https://github.com/kubernetes/kubernetes/pull/92836>#92836</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li></ul><h3 id=bug-or-regression-7>Bug or Regression</h3><ul><li><p>A panic in the apiserver caused by the <code>informer-sync</code> health checker is now fixed. (<a href=https://github.com/kubernetes/kubernetes/pull/93600>#93600</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG API Machinery]</p></li><li><p>Add kubectl wait --ignore-not-found flag (<a href=https://github.com/kubernetes/kubernetes/pull/90969>#90969</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</p></li><li><p>Adding fix to the statefulset controller to wait for pvc deletion before creating pods. (<a href=https://github.com/kubernetes/kubernetes/pull/93457>#93457</a>, <a href=https://github.com/ymmt2005>@ymmt2005</a>) [SIG Apps]</p></li><li><p>Azure ARM client: don't segfault on empty response and http error (<a href=https://github.com/kubernetes/kubernetes/pull/94078>#94078</a>, <a href=https://github.com/bpineau>@bpineau</a>) [SIG Cloud Provider]</p></li><li><p>Azure: fix a bug that kube-controller-manager would panic if wrong Azure VMSS name is configured (<a href=https://github.com/kubernetes/kubernetes/pull/94306>#94306</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Cloud Provider]</p></li><li><p>Azure: per VMSS VMSS VMs cache to prevent throttling on clusters having many attached VMSS (<a href=https://github.com/kubernetes/kubernetes/pull/93107>#93107</a>, <a href=https://github.com/bpineau>@bpineau</a>) [SIG Cloud Provider]</p></li><li><p>Both apiserver_request_duration_seconds metrics and RequestReceivedTimestamp field of an audit event take
into account the time a request spends in the apiserver request filters. (<a href=https://github.com/kubernetes/kubernetes/pull/94903>#94903</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery, Auth and Instrumentation]</p></li><li><p>Build/lib/release: Explicitly use '--platform' in building server images</p><p>When we switched to go-runner for building the apiserver,
controller-manager, and scheduler server components, we no longer
reference the individual architectures in the image names, specifically
in the 'FROM' directive of the server image Dockerfiles.</p><p>As a result, server images for non-amd64 images copy in the go-runner
amd64 binary instead of the go-runner that matches that architecture.</p><p>This commit explicitly sets the '--platform=linux/${arch}' to ensure
we're pulling the correct go-runner arch from the manifest list.</p><p>Before:
<code>FROM ${base_image}</code></p><p>After:
<code>FROM --platform=linux/${arch} ${base_image}</code> (<a href=https://github.com/kubernetes/kubernetes/pull/94552>#94552</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release]</p></li><li><p>CSIDriver object can be deployed during volume attachment. (<a href=https://github.com/kubernetes/kubernetes/pull/93710>#93710</a>, <a href=https://github.com/Jiawei0227>@Jiawei0227</a>) [SIG Apps, Node, Storage and Testing]</p></li><li><p>CVE-2020-8557 (Medium): Node-local denial of service via container /etc/hosts file. See <a href=https://github.com/kubernetes/kubernetes/issues/93032>https://github.com/kubernetes/kubernetes/issues/93032</a> for more details. (<a href=https://github.com/kubernetes/kubernetes/pull/92916>#92916</a>, <a href=https://github.com/joelsmith>@joelsmith</a>) [SIG Node]</p></li><li><p>Do not add nodes labeled with kubernetes.azure.com/managed=false to backend pool of load balancer. (<a href=https://github.com/kubernetes/kubernetes/pull/93034>#93034</a>, <a href=https://github.com/matthias50>@matthias50</a>) [SIG Cloud Provider]</p></li><li><p>Do not fail sorting empty elements. (<a href=https://github.com/kubernetes/kubernetes/pull/94666>#94666</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</p></li><li><p>Do not retry volume expansion if CSI driver returns FailedPrecondition error (<a href=https://github.com/kubernetes/kubernetes/pull/92986>#92986</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Node and Storage]</p></li><li><p>Dockershim security: pod sandbox now always run with <code>no-new-privileges</code> and <code>runtime/default</code> seccomp profile
dockershim seccomp: custom profiles can now have smaller seccomp profiles when set at pod level (<a href=https://github.com/kubernetes/kubernetes/pull/90948>#90948</a>, <a href=https://github.com/pjbgf>@pjbgf</a>) [SIG Node]</p></li><li><p>Dual-stack: make nodeipam compatible with existing single-stack clusters when dual-stack feature gate become enabled by default (<a href=https://github.com/kubernetes/kubernetes/pull/90439>#90439</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG API Machinery]</p></li><li><p>Endpoint controller requeues service after an endpoint deletion event occurs to confirm that deleted endpoints are undesired to mitigate the effects of an out of sync endpoint cache. (<a href=https://github.com/kubernetes/kubernetes/pull/93030>#93030</a>, <a href=https://github.com/swetharepakula>@swetharepakula</a>) [SIG Apps and Network]</p></li><li><p>EndpointSlice controllers now return immediately if they encounter an error creating, updating, or deleting resources. (<a href=https://github.com/kubernetes/kubernetes/pull/93908>#93908</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</p></li><li><p>EndpointSliceMirroring controller now copies labels from Endpoints to EndpointSlices. (<a href=https://github.com/kubernetes/kubernetes/pull/93442>#93442</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</p></li><li><p>EndpointSliceMirroring controller now mirrors Endpoints that do not have a Service associated with them. (<a href=https://github.com/kubernetes/kubernetes/pull/94171>#94171</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps, Network and Testing]</p></li><li><p>Ensure backoff step is set to 1 for Azure armclient. (<a href=https://github.com/kubernetes/kubernetes/pull/94180>#94180</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>Ensure getPrimaryInterfaceID not panic when network interfaces for Azure VMSS are null (<a href=https://github.com/kubernetes/kubernetes/pull/94355>#94355</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>Eviction requests for pods that have a non-zero DeletionTimestamp will always succeed (<a href=https://github.com/kubernetes/kubernetes/pull/91342>#91342</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG Apps]</p></li><li><p>Extended DSR loadbalancer feature in winkernel kube-proxy to HNS versions 9.3-9.max, 10.2+ (<a href=https://github.com/kubernetes/kubernetes/pull/93080>#93080</a>, <a href=https://github.com/elweb9858>@elweb9858</a>) [SIG Network]</p></li><li><p>Fix HandleCrash order (<a href=https://github.com/kubernetes/kubernetes/pull/93108>#93108</a>, <a href=https://github.com/lixiaobing1>@lixiaobing1</a>) [SIG API Machinery]</p></li><li><p>Fix a concurrent map writes error in kubelet (<a href=https://github.com/kubernetes/kubernetes/pull/93773>#93773</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Node]</p></li><li><p>Fix a regression where kubeadm bails out with a fatal error when an optional version command line argument is supplied to the "kubeadm upgrade plan" command (<a href=https://github.com/kubernetes/kubernetes/pull/94421>#94421</a>, <a href=https://github.com/rosti>@rosti</a>) [SIG Cluster Lifecycle]</p></li><li><p>Fix azure file migration panic (<a href=https://github.com/kubernetes/kubernetes/pull/94853>#94853</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix bug where loadbalancer deletion gets stuck because of missing resource group #75198 (<a href=https://github.com/kubernetes/kubernetes/pull/93962>#93962</a>, <a href=https://github.com/phiphi282>@phiphi282</a>) [SIG Cloud Provider]</p></li><li><p>Fix calling AttachDisk on a previously attached EBS volume (<a href=https://github.com/kubernetes/kubernetes/pull/93567>#93567</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Cloud Provider, Storage and Testing]</p></li><li><p>Fix detection of image filesystem, disk metrics for devicemapper, detection of OOM Kills on 5.0+ linux kernels. (<a href=https://github.com/kubernetes/kubernetes/pull/92919>#92919</a>, <a href=https://github.com/dashpole>@dashpole</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Node]</p></li><li><p>Fix etcd_object_counts metric reported by kube-apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/94773>#94773</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery]</p></li><li><p>Fix incorrectly reported verbs for kube-apiserver metrics for CRD objects (<a href=https://github.com/kubernetes/kubernetes/pull/93523>#93523</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Instrumentation]</p></li><li><p>Fix instance not found issues when an Azure Node is recreated in a short time (<a href=https://github.com/kubernetes/kubernetes/pull/93316>#93316</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>Fix kube-apiserver /readyz to contain "informer-sync" check ensuring that internal informers are synced. (<a href=https://github.com/kubernetes/kubernetes/pull/93670>#93670</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Testing]</p></li><li><p>Fix kubectl SchemaError on CRDs with schema using x-kubernetes-preserve-unknown-fields on array types. (<a href=https://github.com/kubernetes/kubernetes/pull/94888>#94888</a>, <a href=https://github.com/sttts>@sttts</a>) [SIG API Machinery]</p></li><li><p>Fix memory leak in EndpointSliceTracker for EndpointSliceMirroring controller. (<a href=https://github.com/kubernetes/kubernetes/pull/93441>#93441</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</p></li><li><p>Fix missing csi annotations on node during parallel csinode update. (<a href=https://github.com/kubernetes/kubernetes/pull/94389>#94389</a>, <a href=https://github.com/pacoxu>@pacoxu</a>) [SIG Storage]</p></li><li><p>Fix the <code>cloudprovider_azure_api_request_duration_seconds</code> metric buckets to correctly capture the latency metrics. Previously, the majority of the calls would fall in the "+Inf" bucket. (<a href=https://github.com/kubernetes/kubernetes/pull/94873>#94873</a>, <a href=https://github.com/marwanad>@marwanad</a>) [SIG Cloud Provider and Instrumentation]</p></li><li><p>Fix: azure disk resize error if source does not exist (<a href=https://github.com/kubernetes/kubernetes/pull/93011>#93011</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix: detach azure disk broken on Azure Stack (<a href=https://github.com/kubernetes/kubernetes/pull/94885>#94885</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</p></li><li><p>Fix: determine the correct ip config based on ip family (<a href=https://github.com/kubernetes/kubernetes/pull/93043>#93043</a>, <a href=https://github.com/aramase>@aramase</a>) [SIG Cloud Provider]</p></li><li><p>Fix: initial delay in mounting azure disk & file (<a href=https://github.com/kubernetes/kubernetes/pull/93052>#93052</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fix: use sensitiveOptions on Windows mount (<a href=https://github.com/kubernetes/kubernetes/pull/94126>#94126</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fixed Ceph RBD volume expansion when no ceph.conf exists (<a href=https://github.com/kubernetes/kubernetes/pull/92027>#92027</a>, <a href=https://github.com/juliantaylor>@juliantaylor</a>) [SIG Storage]</p></li><li><p>Fixed a bug where improper storage and comparison of endpoints led to excessive API traffic from the endpoints controller (<a href=https://github.com/kubernetes/kubernetes/pull/94112>#94112</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Apps, Network and Testing]</p></li><li><p>Fixed a bug whereby the allocation of reusable CPUs and devices was not being honored when the TopologyManager was enabled (<a href=https://github.com/kubernetes/kubernetes/pull/93189>#93189</a>, <a href=https://github.com/klueska>@klueska</a>) [SIG Node]</p></li><li><p>Fixed a panic in kubectl debug when pod has multiple init containers or ephemeral containers (<a href=https://github.com/kubernetes/kubernetes/pull/94580>#94580</a>, <a href=https://github.com/kiyoshim55>@kiyoshim55</a>) [SIG CLI]</p></li><li><p>Fixed a regression that sometimes prevented <code>kubectl portforward</code> to work when TCP and UDP services were configured on the same port (<a href=https://github.com/kubernetes/kubernetes/pull/94728>#94728</a>, <a href=https://github.com/amorenoz>@amorenoz</a>) [SIG CLI]</p></li><li><p>Fixed bug in reflector that couldn't recover from "Too large resource version" errors with API servers 1.17.0-1.18.5 (<a href=https://github.com/kubernetes/kubernetes/pull/94316>#94316</a>, <a href=https://github.com/janeczku>@janeczku</a>) [SIG API Machinery]</p></li><li><p>Fixed bug where kubectl top pod output is not sorted when --sort-by and --containers flags are used together (<a href=https://github.com/kubernetes/kubernetes/pull/93692>#93692</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</p></li><li><p>Fixed kubelet creating extra sandbox for pods with RestartPolicyOnFailure after all containers succeeded (<a href=https://github.com/kubernetes/kubernetes/pull/92614>#92614</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Node and Testing]</p></li><li><p>Fixed memory leak in endpointSliceTracker (<a href=https://github.com/kubernetes/kubernetes/pull/92838>#92838</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Apps and Network]</p></li><li><p>Fixed node data lost in kube-scheduler for clusters with imbalance on number of nodes across zones (<a href=https://github.com/kubernetes/kubernetes/pull/93355>#93355</a>, <a href=https://github.com/maelk>@maelk</a>) [SIG Scheduling]</p></li><li><p>Fixed the EndpointSliceController to correctly create endpoints for IPv6-only pods.</p><p>Fixed the EndpointController to allow IPv6 headless services, if the IPv6DualStack
feature gate is enabled, by specifying <code>ipFamily: IPv6</code> on the service. (This already
worked with the EndpointSliceController.) (<a href=https://github.com/kubernetes/kubernetes/pull/91399>#91399</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Apps and Network]</p></li><li><p>Fixes a bug evicting pods after a taint with a limited tolerationSeconds toleration is removed from a node (<a href=https://github.com/kubernetes/kubernetes/pull/93722>#93722</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Apps and Node]</p></li><li><p>Fixes a bug where EndpointSlices would not be recreated after rapid Service recreation. (<a href=https://github.com/kubernetes/kubernetes/pull/94730>#94730</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps, Network and Testing]</p></li><li><p>Fixes a race condition in kubelet pod handling (<a href=https://github.com/kubernetes/kubernetes/pull/94751>#94751</a>, <a href=https://github.com/auxten>@auxten</a>) [SIG Node]</p></li><li><p>Fixes an issue proxying to ipv6 pods without specifying a port (<a href=https://github.com/kubernetes/kubernetes/pull/94834>#94834</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Network]</p></li><li><p>Fixes an issue that can result in namespaced custom resources being orphaned when their namespace is deleted, if the CRD defining the custom resource is removed concurrently with namespaces being deleted, then recreated. (<a href=https://github.com/kubernetes/kubernetes/pull/93790>#93790</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Apps]</p></li><li><p>Ignore root user check when windows pod starts (<a href=https://github.com/kubernetes/kubernetes/pull/92355>#92355</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Node and Windows]</p></li><li><p>Increased maximum IOPS of AWS EBS io1 volumes to 64,000 (current AWS maximum). (<a href=https://github.com/kubernetes/kubernetes/pull/90014>#90014</a>, <a href=https://github.com/jacobmarble>@jacobmarble</a>) [SIG Cloud Provider and Storage]</p></li><li><p>K8s.io/apimachinery: runtime.DefaultUnstructuredConverter.FromUnstructured now handles converting integer fields to typed float values (<a href=https://github.com/kubernetes/kubernetes/pull/93250>#93250</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</p></li><li><p>Kube-aggregator certificates are dynamically loaded on change from disk (<a href=https://github.com/kubernetes/kubernetes/pull/92791>#92791</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery]</p></li><li><p>Kube-apiserver: fixed a bug returning inconsistent results from list requests which set a field or label selector and set a paging limit (<a href=https://github.com/kubernetes/kubernetes/pull/94002>#94002</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery]</p></li><li><p>Kube-apiserver: jsonpath expressions with consecutive recursive descent operators are no longer evaluated for custom resource printer columns (<a href=https://github.com/kubernetes/kubernetes/pull/93408>#93408</a>, <a href=https://github.com/joelsmith>@joelsmith</a>) [SIG API Machinery]</p></li><li><p>Kube-proxy now trims extra spaces found in loadBalancerSourceRanges to match Service validation. (<a href=https://github.com/kubernetes/kubernetes/pull/94107>#94107</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Network]</p></li><li><p>Kube-up now includes CoreDNS version v1.7.0. Some of the major changes include:</p><ul><li>Fixed a bug that could cause CoreDNS to stop updating service records.</li><li>Fixed a bug in the forward plugin where only the first upstream server is always selected no matter which policy is set.</li><li>Remove already deprecated options <code>resyncperiod</code> and <code>upstream</code> in the Kubernetes plugin.</li><li>Includes Prometheus metrics name changes (to bring them in line with standard Prometheus metrics naming convention). They will be backward incompatible with existing reporting formulas that use the old metrics' names.</li><li>The federation plugin (allows for v1 Kubernetes federation) has been removed.
More details are available in <a href=https://coredns.io/2020/06/15/coredns-1.7.0-release/>https://coredns.io/2020/06/15/coredns-1.7.0-release/</a> (<a href=https://github.com/kubernetes/kubernetes/pull/92718>#92718</a>, <a href=https://github.com/rajansandeep>@rajansandeep</a>) [SIG Cloud Provider]</li></ul></li><li><p>Kubeadm now makes sure the etcd manifest is regenerated upon upgrade even when no etcd version change takes place (<a href=https://github.com/kubernetes/kubernetes/pull/94395>#94395</a>, <a href=https://github.com/rosti>@rosti</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: avoid a panic when determining if the running version of CoreDNS is supported during upgrades (<a href=https://github.com/kubernetes/kubernetes/pull/94299>#94299</a>, <a href=https://github.com/zouyee>@zouyee</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: ensure "kubeadm reset" does not unmount the root "/var/lib/kubelet" directory if it is mounted by the user (<a href=https://github.com/kubernetes/kubernetes/pull/93702>#93702</a>, <a href=https://github.com/thtanaka>@thtanaka</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: ensure the etcd data directory is created with 0700 permissions during control-plane init and join (<a href=https://github.com/kubernetes/kubernetes/pull/94102>#94102</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: fix the bug that kubeadm tries to call 'docker info' even if the CRI socket was for another CR (<a href=https://github.com/kubernetes/kubernetes/pull/94555>#94555</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: make the kubeconfig files for the kube-controller-manager and kube-scheduler use the LocalAPIEndpoint instead of the ControlPlaneEndpoint. This makes kubeadm clusters more reseliant to version skew problems during immutable upgrades: <a href=https://kubernetes.io/docs/setup/release/version-skew-policy/#kube-controller-manager-kube-scheduler-and-cloud-controller-manager>https://kubernetes.io/docs/setup/release/version-skew-policy/#kube-controller-manager-kube-scheduler-and-cloud-controller-manager</a> (<a href=https://github.com/kubernetes/kubernetes/pull/94398>#94398</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: relax the validation of kubeconfig server URLs. Allow the user to define custom kubeconfig server URLs without erroring out during validation of existing kubeconfig files (e.g. when using external CA mode). (<a href=https://github.com/kubernetes/kubernetes/pull/94816>#94816</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubeadm: remove duplicate DNS names and IP addresses from generated certificates (<a href=https://github.com/kubernetes/kubernetes/pull/92753>#92753</a>, <a href=https://github.com/QianChenglong>@QianChenglong</a>) [SIG Cluster Lifecycle]</p></li><li><p>Kubelet: assume that swap is disabled when <code>/proc/swaps</code> does not exist (<a href=https://github.com/kubernetes/kubernetes/pull/93931>#93931</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Node]</p></li><li><p>Kubelet: fix race condition in pluginWatcher (<a href=https://github.com/kubernetes/kubernetes/pull/93622>#93622</a>, <a href=https://github.com/knight42>@knight42</a>) [SIG Node]</p></li><li><p>Kuberuntime security: pod sandbox now always runs with <code>runtime/default</code> seccomp profile
kuberuntime seccomp: custom profiles can now have smaller seccomp profiles when set at pod level (<a href=https://github.com/kubernetes/kubernetes/pull/90949>#90949</a>, <a href=https://github.com/pjbgf>@pjbgf</a>) [SIG Node]</p></li><li><p>NONE (<a href=https://github.com/kubernetes/kubernetes/pull/71269>#71269</a>, <a href=https://github.com/DeliangFan>@DeliangFan</a>) [SIG Node]</p></li><li><p>New Azure instance types do now have correct max data disk count information. (<a href=https://github.com/kubernetes/kubernetes/pull/94340>#94340</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Pods with invalid Affinity/AntiAffinity LabelSelectors will now fail scheduling when these plugins are enabled (<a href=https://github.com/kubernetes/kubernetes/pull/93660>#93660</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Scheduling]</p></li><li><p>Require feature flag CustomCPUCFSQuotaPeriod if setting a non-default cpuCFSQuotaPeriod in kubelet config. (<a href=https://github.com/kubernetes/kubernetes/pull/94687>#94687</a>, <a href=https://github.com/karan>@karan</a>) [SIG Node]</p></li><li><p>Reverted devicemanager for Windows node added in 1.19rc1. (<a href=https://github.com/kubernetes/kubernetes/pull/93263>#93263</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node and Windows]</p></li><li><p>Scheduler bugfix: Scheduler doesn't lose pod information when nodes are quickly recreated. This could happen when nodes are restarted or quickly recreated reusing a nodename. (<a href=https://github.com/kubernetes/kubernetes/pull/93938>#93938</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scalability, Scheduling and Testing]</p></li><li><p>The EndpointSlice controller now waits for EndpointSlice and Node caches to be synced before starting. (<a href=https://github.com/kubernetes/kubernetes/pull/94086>#94086</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</p></li><li><p>The <code>/debug/api_priority_and_fairness/dump_requests</code> path at an apiserver will no longer return a phantom line for each exempt priority level. (<a href=https://github.com/kubernetes/kubernetes/pull/93406>#93406</a>, <a href=https://github.com/MikeSpreitzer>@MikeSpreitzer</a>) [SIG API Machinery]</p></li><li><p>The kubelet recognizes the --containerd-namespace flag to configure the namespace used by cadvisor. (<a href=https://github.com/kubernetes/kubernetes/pull/87054>#87054</a>, <a href=https://github.com/changyaowei>@changyaowei</a>) [SIG Node]</p></li><li><p>The terminationGracePeriodSeconds from pod spec is respected for the mirror pod. (<a href=https://github.com/kubernetes/kubernetes/pull/92442>#92442</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node and Testing]</p></li><li><p>Update Calico to v3.15.2 (<a href=https://github.com/kubernetes/kubernetes/pull/94241>#94241</a>, <a href=https://github.com/lmm>@lmm</a>) [SIG Cloud Provider]</p></li><li><p>Update default etcd server version to 3.4.13 (<a href=https://github.com/kubernetes/kubernetes/pull/94287>#94287</a>, <a href=https://github.com/jingyih>@jingyih</a>) [SIG API Machinery, Cloud Provider, Cluster Lifecycle and Testing]</p></li><li><p>Updated Cluster Autoscaler to 1.19.0; (<a href=https://github.com/kubernetes/kubernetes/pull/93577>#93577</a>, <a href=https://github.com/vivekbagade>@vivekbagade</a>) [SIG Autoscaling and Cloud Provider]</p></li><li><p>Use NLB Subnet CIDRs instead of VPC CIDRs in Health Check SG Rules (<a href=https://github.com/kubernetes/kubernetes/pull/93515>#93515</a>, <a href=https://github.com/t0rr3sp3dr0>@t0rr3sp3dr0</a>) [SIG Cloud Provider]</p></li><li><p>Users will see increase in time for deletion of pods and also guarantee that removal of pod from api server would mean deletion of all the resources from container runtime. (<a href=https://github.com/kubernetes/kubernetes/pull/92817>#92817</a>, <a href=https://github.com/kmala>@kmala</a>) [SIG Node]</p></li><li><p>Very large patches may now be specified to <code>kubectl patch</code> with the <code>--patch-file</code> flag instead of including them directly on the command line. The <code>--patch</code> and <code>--patch-file</code> flags are mutually exclusive. (<a href=https://github.com/kubernetes/kubernetes/pull/93548>#93548</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG CLI]</p></li><li><p>When creating a networking.k8s.io/v1 Ingress API object, <code>spec.rules[*].http</code> values are now validated consistently when the <code>host</code> field contains a wildcard. (<a href=https://github.com/kubernetes/kubernetes/pull/93954>#93954</a>, <a href=https://github.com/Miciah>@Miciah</a>) [SIG CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Storage and Testing]</p></li></ul><h3 id=other-cleanup-or-flake-6>Other (Cleanup or Flake)</h3><ul><li>--cache-dir sets cache directory for both http and discovery, defaults to $HOME/.kube/cache (<a href=https://github.com/kubernetes/kubernetes/pull/92910>#92910</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG API Machinery and CLI]</li><li>Adds a bootstrapping ClusterRole, ClusterRoleBinding and group for /metrics, /livez/<em>, /readyz/</em>, & /healthz/- endpoints. (<a href=https://github.com/kubernetes/kubernetes/pull/93311>#93311</a>, <a href=https://github.com/logicalhan>@logicalhan</a>) [SIG API Machinery, Auth, Cloud Provider and Instrumentation]</li><li>Base-images: Update to debian-iptables:buster-v1.3.0<ul><li>Uses iptables 1.8.5</li><li>base-images: Update to debian-base:buster-v1.2.0</li><li>cluster/images/etcd: Build etcd:3.4.13-1 image<ul><li>Uses debian-base:buster-v1.2.0 (<a href=https://github.com/kubernetes/kubernetes/pull/94733>#94733</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Release and Testing]</li></ul></li></ul></li><li>Build: Update to <a href=mailto:debian-base@v2.1.2>debian-base@v2.1.2</a> and <a href=mailto:debian-iptables@v12.1.1>debian-iptables@v12.1.1</a> (<a href=https://github.com/kubernetes/kubernetes/pull/93667>#93667</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Release and Testing]</li><li>Build: Update to <a href=mailto:debian-base@v2.1.3>debian-base@v2.1.3</a> and <a href=mailto:debian-iptables@v12.1.2>debian-iptables@v12.1.2</a> (<a href=https://github.com/kubernetes/kubernetes/pull/93916>#93916</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Release and Testing]</li><li>Build: Update to go-runner:buster-v2.0.0 (<a href=https://github.com/kubernetes/kubernetes/pull/94167>#94167</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release]</li><li>Fix kubelet to properly log when a container is started. Before, sometimes the log said that a container is dead and was restarted when it was started for the first time. This only happened when using pods with initContainers and regular containers. (<a href=https://github.com/kubernetes/kubernetes/pull/91469>#91469</a>, <a href=https://github.com/rata>@rata</a>) [SIG Node]</li><li>Fix: license issue in blob disk feature (<a href=https://github.com/kubernetes/kubernetes/pull/92824>#92824</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fixes the flooding warning messages about setting volume ownership for configmap/secret volumes (<a href=https://github.com/kubernetes/kubernetes/pull/92878>#92878</a>, <a href=https://github.com/jvanz>@jvanz</a>) [SIG Instrumentation, Node and Storage]</li><li>Fixes the message about no auth for metrics in scheduler. (<a href=https://github.com/kubernetes/kubernetes/pull/94035>#94035</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG Scheduling]</li><li>Kube-up: defaults to limiting critical pods to the kube-system namespace to match behavior prior to 1.17 (<a href=https://github.com/kubernetes/kubernetes/pull/93121>#93121</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Cloud Provider and Scheduling]</li><li>Kubeadm: Separate argument key/value in log msg (<a href=https://github.com/kubernetes/kubernetes/pull/94016>#94016</a>, <a href=https://github.com/mrueg>@mrueg</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove support for the "ci/k8s-master" version label. This label has been removed in the Kubernetes CI release process and would no longer work in kubeadm. You can use the "ci/latest" version label instead. See kubernetes/test-infra#18517 (<a href=https://github.com/kubernetes/kubernetes/pull/93626>#93626</a>, <a href=https://github.com/vikkyomkar>@vikkyomkar</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the CoreDNS check for known image digests when applying the addon (<a href=https://github.com/kubernetes/kubernetes/pull/94506>#94506</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubernetes is now built with go1.15.0 (<a href=https://github.com/kubernetes/kubernetes/pull/93939>#93939</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release and Testing]</li><li>Kubernetes is now built with go1.15.0-rc.2 (<a href=https://github.com/kubernetes/kubernetes/pull/93827>#93827</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Node, Release and Testing]</li><li>Lock ExternalPolicyForExternalIP to default, this feature gate will be removed in 1.22. (<a href=https://github.com/kubernetes/kubernetes/pull/94581>#94581</a>, <a href=https://github.com/knabben>@knabben</a>) [SIG Network]</li><li>Service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset is removed. All Standard load balancers will always enable tcp resets. (<a href=https://github.com/kubernetes/kubernetes/pull/94297>#94297</a>, <a href=https://github.com/MarcPow>@MarcPow</a>) [SIG Cloud Provider]</li><li>Stop propagating SelfLink (deprecated in 1.16) in kube-apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/94397>#94397</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery and Testing]</li><li>Strip unnecessary security contexts on Windows (<a href=https://github.com/kubernetes/kubernetes/pull/93475>#93475</a>, <a href=https://github.com/ravisantoshgudimetla>@ravisantoshgudimetla</a>) [SIG Node, Testing and Windows]</li><li>To ensure the code be strong, add unit test for GetAddressAndDialer (<a href=https://github.com/kubernetes/kubernetes/pull/93180>#93180</a>, <a href=https://github.com/FreeZhang61>@FreeZhang61</a>) [SIG Node]</li><li>Update CNI plugins to v0.8.7 (<a href=https://github.com/kubernetes/kubernetes/pull/94367>#94367</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Cloud Provider, Network, Node, Release and Testing]</li><li>Update Golang to v1.14.5<ul><li>Update repo-infra to 0.0.7 (to support go1.14.5 and go1.13.13)<ul><li>Includes:<ul><li><a href=mailto:bazelbuild/bazel-toolchains@3.3.2>bazelbuild/bazel-toolchains@3.3.2</a></li><li><a href=mailto:bazelbuild/rules_go@v0.22.7>bazelbuild/rules_go@v0.22.7</a> (<a href=https://github.com/kubernetes/kubernetes/pull/93088>#93088</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release and Testing]</li></ul></li></ul></li></ul></li><li>Update Golang to v1.14.6<ul><li>Update repo-infra to 0.0.8 (to support go1.14.6 and go1.13.14)<ul><li>Includes:<ul><li><a href=mailto:bazelbuild/bazel-toolchains@3.4.0>bazelbuild/bazel-toolchains@3.4.0</a></li><li><a href=mailto:bazelbuild/rules_go@v0.22.8>bazelbuild/rules_go@v0.22.8</a> (<a href=https://github.com/kubernetes/kubernetes/pull/93198>#93198</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release and Testing]</li></ul></li></ul></li></ul></li><li>Update cri-tools to <a href=https://github.com/kubernetes-sigs/cri-tools/releases/tag/v1.19.0>v1.19.0</a> (<a href=https://github.com/kubernetes/kubernetes/pull/94307>#94307</a>, <a href=https://github.com/xmudrii>@xmudrii</a>) [SIG Cloud Provider]</li><li>Update default etcd server version to 3.4.9 (<a href=https://github.com/kubernetes/kubernetes/pull/92349>#92349</a>, <a href=https://github.com/jingyih>@jingyih</a>) [SIG API Machinery, Cloud Provider, Cluster Lifecycle and Testing]</li><li>Update etcd client side to v3.4.13 (<a href=https://github.com/kubernetes/kubernetes/pull/94259>#94259</a>, <a href=https://github.com/jingyih>@jingyih</a>) [SIG API Machinery and Cloud Provider]</li><li><code>kubectl get ingress</code> now prefers the <code>networking.k8s.io/v1</code> over <code>extensions/v1beta1</code> (deprecated since v1.14). To explicitly request the deprecated version, use <code>kubectl get ingress.v1beta1.extensions</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/94309>#94309</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and CLI]</li></ul><h2 id=dependencies-8>Dependencies</h2><h3 id=added-8>Added</h3><ul><li>github.com/Azure/go-autorest: <a href=https://github.com/Azure/go-autorest/tree/v14.2.0>v14.2.0+incompatible</a></li><li>github.com/fvbommel/sortorder: <a href=https://github.com/fvbommel/sortorder/tree/v1.0.1>v1.0.1</a></li><li>github.com/yuin/goldmark: <a href=https://github.com/yuin/goldmark/tree/v1.1.27>v1.1.27</a></li><li>sigs.k8s.io/structured-merge-diff/v4: v4.0.1</li></ul><h3 id=changed-8>Changed</h3><ul><li>github.com/Azure/go-autorest/autorest/adal: <a href=https://github.com/Azure/go-autorest/autorest/adal/compare/v0.8.2...v0.9.0>v0.8.2 → v0.9.0</a></li><li>github.com/Azure/go-autorest/autorest/date: <a href=https://github.com/Azure/go-autorest/autorest/date/compare/v0.2.0...v0.3.0>v0.2.0 → v0.3.0</a></li><li>github.com/Azure/go-autorest/autorest/mocks: <a href=https://github.com/Azure/go-autorest/autorest/mocks/compare/v0.3.0...v0.4.0>v0.3.0 → v0.4.0</a></li><li>github.com/Azure/go-autorest/autorest: <a href=https://github.com/Azure/go-autorest/autorest/compare/v0.9.6...v0.11.1>v0.9.6 → v0.11.1</a></li><li>github.com/Azure/go-autorest/logger: <a href=https://github.com/Azure/go-autorest/logger/compare/v0.1.0...v0.2.0>v0.1.0 → v0.2.0</a></li><li>github.com/Azure/go-autorest/tracing: <a href=https://github.com/Azure/go-autorest/tracing/compare/v0.5.0...v0.6.0>v0.5.0 → v0.6.0</a></li><li>github.com/Microsoft/hcsshim: <a href=https://github.com/Microsoft/hcsshim/compare/v0.8.9...5eafd15>v0.8.9 → 5eafd15</a></li><li>github.com/cilium/ebpf: <a href=https://github.com/cilium/ebpf/compare/9f1617e...1c8d4c9>9f1617e → 1c8d4c9</a></li><li>github.com/containerd/cgroups: <a href=https://github.com/containerd/cgroups/compare/bf292b2...0dbf7f0>bf292b2 → 0dbf7f0</a></li><li>github.com/coredns/corefile-migration: <a href=https://github.com/coredns/corefile-migration/compare/v1.0.8...v1.0.10>v1.0.8 → v1.0.10</a></li><li>github.com/evanphx/json-patch: <a href=https://github.com/evanphx/json-patch/compare/e83c0a1...v4.9.0>e83c0a1 → v4.9.0+incompatible</a></li><li>github.com/google/cadvisor: <a href=https://github.com/google/cadvisor/compare/8450c56...v0.37.0>8450c56 → v0.37.0</a></li><li>github.com/json-iterator/go: <a href=https://github.com/json-iterator/go/compare/v1.1.9...v1.1.10>v1.1.9 → v1.1.10</a></li><li>github.com/opencontainers/go-digest: <a href=https://github.com/opencontainers/go-digest/compare/v1.0.0-rc1...v1.0.0>v1.0.0-rc1 → v1.0.0</a></li><li>github.com/opencontainers/runc: <a href=https://github.com/opencontainers/runc/compare/1b94395...819fcc6>1b94395 → 819fcc6</a></li><li>github.com/prometheus/client_golang: <a href=https://github.com/prometheus/client_golang/compare/v1.6.0...v1.7.1>v1.6.0 → v1.7.1</a></li><li>github.com/prometheus/common: <a href=https://github.com/prometheus/common/compare/v0.9.1...v0.10.0>v0.9.1 → v0.10.0</a></li><li>github.com/prometheus/procfs: <a href=https://github.com/prometheus/procfs/compare/v0.0.11...v0.1.3>v0.0.11 → v0.1.3</a></li><li>github.com/rubiojr/go-vhd: <a href=https://github.com/rubiojr/go-vhd/compare/0bfd3b3...02e2102>0bfd3b3 → 02e2102</a></li><li>github.com/storageos/go-api: <a href=https://github.com/storageos/go-api/compare/343b3ef...v2.2.0>343b3ef → v2.2.0+incompatible</a></li><li>github.com/urfave/cli: <a href=https://github.com/urfave/cli/compare/v1.22.1...v1.22.2>v1.22.1 → v1.22.2</a></li><li>go.etcd.io/etcd: 54ba958 → dd1b699</li><li>golang.org/x/crypto: bac4c82 → 75b2880</li><li>golang.org/x/mod: v0.1.0 → v0.3.0</li><li>golang.org/x/net: d3edc99 → ab34263</li><li>golang.org/x/tools: c00d67e → c1934b7</li><li>k8s.io/kube-openapi: 656914f → 6aeccd4</li><li>k8s.io/system-validators: v1.1.2 → v1.2.0</li><li>k8s.io/utils: 6e3d28b → d5654de</li></ul><h3 id=removed-8>Removed</h3><ul><li>github.com/godbus/dbus: <a href=https://github.com/godbus/dbus/tree/ade71ed>ade71ed</a></li><li>github.com/xlab/handysort: <a href=https://github.com/xlab/handysort/tree/fb3537e>fb3537e</a></li><li>sigs.k8s.io/structured-merge-diff/v3: v3.0.0</li><li>vbom.ml/util: db5cfe1</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-85b7e96ac42e5e28ec570ad43f0ef5cd>1.2 - Kubernetes version and version skew support policy</h1><p>This document describes the maximum version skew supported between various Kubernetes components.
Specific cluster deployment tools may place additional restrictions on version skew.</p><h2 id=supported-versions>Supported versions</h2><p>Kubernetes versions are expressed as <strong>x.y.z</strong>,
where <strong>x</strong> is the major version, <strong>y</strong> is the minor version, and <strong>z</strong> is the patch version, following <a href=https://semver.org/>Semantic Versioning</a> terminology.
For more information, see <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#kubernetes-release-versioning>Kubernetes Release Versioning</a>.</p><p>The Kubernetes project maintains release branches for the most recent three minor releases (1.24, 1.23, 1.22). Kubernetes 1.19 and newer receive approximately 1 year of patch support. Kubernetes 1.18 and older received approximately 9 months of patch support.</p><p>Applicable fixes, including security fixes, may be backported to those three release branches, depending on severity and feasibility.
Patch releases are cut from those branches at a <a href=https://git.k8s.io/sig-release/releases/patch-releases.md#cadence>regular cadence</a>, plus additional urgent releases, when required.</p><p>The <a href=https://git.k8s.io/sig-release/release-managers.md>Release Managers</a> group owns this decision.</p><p>For more information, see the Kubernetes <a href=https://git.k8s.io/sig-release/releases/patch-releases.md>patch releases</a> page.</p><h2 id=supported-version-skew>Supported version skew</h2><h3 id=kube-apiserver>kube-apiserver</h3><p>In <a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>highly-available (HA) clusters</a>, the newest and oldest <code>kube-apiserver</code> instances must be within one minor version.</p><p>Example:</p><ul><li>newest <code>kube-apiserver</code> is at <strong>1.20</strong></li><li>other <code>kube-apiserver</code> instances are supported at <strong>1.20</strong> and <strong>1.19</strong></li></ul><h3 id=kubelet>kubelet</h3><p><code>kubelet</code> must not be newer than <code>kube-apiserver</code>, and may be up to two minor versions older.</p><p>Example:</p><ul><li><code>kube-apiserver</code> is at <strong>1.20</strong></li><li><code>kubelet</code> is supported at <strong>1.20</strong>, <strong>1.19</strong>, and <strong>1.18</strong></li></ul><blockquote class="note callout"><div><strong>Note:</strong> If version skew exists between <code>kube-apiserver</code> instances in an HA cluster, this narrows the allowed <code>kubelet</code> versions.</div></blockquote><p>Example:</p><ul><li><code>kube-apiserver</code> instances are at <strong>1.20</strong> and <strong>1.19</strong></li><li><code>kubelet</code> is supported at <strong>1.19</strong>, and <strong>1.18</strong> (<strong>1.20</strong> is not supported because that would be newer than the <code>kube-apiserver</code> instance at version <strong>1.19</strong>)</li></ul><h3 id=kube-controller-manager-kube-scheduler-and-cloud-controller-manager>kube-controller-manager, kube-scheduler, and cloud-controller-manager</h3><p><code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> must not be newer than the <code>kube-apiserver</code> instances they communicate with. They are expected to match the <code>kube-apiserver</code> minor version, but may be up to one minor version older (to allow live upgrades).</p><p>Example:</p><ul><li><code>kube-apiserver</code> is at <strong>1.20</strong></li><li><code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> are supported at <strong>1.20</strong> and <strong>1.19</strong></li></ul><blockquote class="note callout"><div><strong>Note:</strong> If version skew exists between <code>kube-apiserver</code> instances in an HA cluster, and these components can communicate with any <code>kube-apiserver</code> instance in the cluster (for example, via a load balancer), this narrows the allowed versions of these components.</div></blockquote><p>Example:</p><ul><li><code>kube-apiserver</code> instances are at <strong>1.20</strong> and <strong>1.19</strong></li><li><code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> communicate with a load balancer that can route to any <code>kube-apiserver</code> instance</li><li><code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> are supported at <strong>1.19</strong> (<strong>1.20</strong> is not supported because that would be newer than the <code>kube-apiserver</code> instance at version <strong>1.19</strong>)</li></ul><h3 id=kubectl>kubectl</h3><p><code>kubectl</code> is supported within one minor version (older or newer) of <code>kube-apiserver</code>.</p><p>Example:</p><ul><li><code>kube-apiserver</code> is at <strong>1.20</strong></li><li><code>kubectl</code> is supported at <strong>1.21</strong>, <strong>1.20</strong>, and <strong>1.19</strong></li></ul><blockquote class="note callout"><div><strong>Note:</strong> If version skew exists between <code>kube-apiserver</code> instances in an HA cluster, this narrows the supported <code>kubectl</code> versions.</div></blockquote><p>Example:</p><ul><li><code>kube-apiserver</code> instances are at <strong>1.20</strong> and <strong>1.19</strong></li><li><code>kubectl</code> is supported at <strong>1.20</strong> and <strong>1.19</strong> (other versions would be more than one minor version skewed from one of the <code>kube-apiserver</code> components)</li></ul><h2 id=supported-component-upgrade-order>Supported component upgrade order</h2><p>The supported version skew between components has implications on the order in which components must be upgraded.
This section describes the order in which components must be upgraded to transition an existing cluster from version <strong>1.19</strong> to version <strong>1.20</strong>.</p><h3 id=kube-apiserver-1>kube-apiserver</h3><p>Pre-requisites:</p><ul><li>In a single-instance cluster, the existing <code>kube-apiserver</code> instance is <strong>1.19</strong></li><li>In an HA cluster, all <code>kube-apiserver</code> instances are at <strong>1.19</strong> or <strong>1.20</strong> (this ensures maximum skew of 1 minor version between the oldest and newest <code>kube-apiserver</code> instance)</li><li>The <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> instances that communicate with this server are at version <strong>1.19</strong> (this ensures they are not newer than the existing API server version, and are within 1 minor version of the new API server version)</li><li><code>kubelet</code> instances on all nodes are at version <strong>1.19</strong> or <strong>1.18</strong> (this ensures they are not newer than the existing API server version, and are within 2 minor versions of the new API server version)</li><li>Registered admission webhooks are able to handle the data the new <code>kube-apiserver</code> instance will send them:<ul><li><code>ValidatingWebhookConfiguration</code> and <code>MutatingWebhookConfiguration</code> objects are updated to include any new versions of REST resources added in <strong>1.20</strong> (or use the <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchpolicy><code>matchPolicy: Equivalent</code> option</a> available in v1.15+)</li><li>The webhooks are able to handle any new versions of REST resources that will be sent to them, and any new fields added to existing versions in <strong>1.20</strong></li></ul></li></ul><p>Upgrade <code>kube-apiserver</code> to <strong>1.20</strong></p><blockquote class="note callout"><div><strong>Note:</strong> Project policies for <a href=/docs/reference/using-api/deprecation-policy/>API deprecation</a> and
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md>API change guidelines</a>
require <code>kube-apiserver</code> to not skip minor versions when upgrading, even in single-instance clusters.</div></blockquote><h3 id=kube-controller-manager-kube-scheduler-and-cloud-controller-manager-1>kube-controller-manager, kube-scheduler, and cloud-controller-manager</h3><p>Pre-requisites:</p><ul><li>The <code>kube-apiserver</code> instances these components communicate with are at <strong>1.20</strong> (in HA clusters in which these control plane components can communicate with any <code>kube-apiserver</code> instance in the cluster, all <code>kube-apiserver</code> instances must be upgraded before upgrading these components)</li></ul><p>Upgrade <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>cloud-controller-manager</code> to <strong>1.20</strong></p><h3 id=kubelet-1>kubelet</h3><p>Pre-requisites:</p><ul><li>The <code>kube-apiserver</code> instances the <code>kubelet</code> communicates with are at <strong>1.20</strong></li></ul><p>Optionally upgrade <code>kubelet</code> instances to <strong>1.20</strong> (or they can be left at <strong>1.19</strong> or <strong>1.18</strong>)</p><blockquote class="note callout"><div><strong>Note:</strong> Before performing a minor version <code>kubelet</code> upgrade, <a href=/docs/tasks/administer-cluster/safely-drain-node/>drain</a> pods from that node.
In-place minor version <code>kubelet</code> upgrades are not supported.</div></blockquote><blockquote class="warning callout"><div><strong>Warning:</strong><p>Running a cluster with <code>kubelet</code> instances that are persistently two minor versions behind <code>kube-apiserver</code> is not recommended:</p><ul><li>they must be upgraded within one minor version of <code>kube-apiserver</code> before the control plane can be upgraded</li><li>it increases the likelihood of running <code>kubelet</code> versions older than the three maintained minor releases</li></ul></div></blockquote><h3 id=kube-proxy>kube-proxy</h3><ul><li><code>kube-proxy</code> must be the same minor version as <code>kubelet</code> on the node.</li><li><code>kube-proxy</code> must not be newer than <code>kube-apiserver</code>.</li><li><code>kube-proxy</code> must be at most two minor versions older than <code>kube-apiserver.</code></li></ul><p>Example:</p><p>If <code>kube-proxy</code> version is <strong>1.18</strong>:</p><ul><li><code>kubelet</code> version must be at the same minor version as <strong>1.18</strong>.</li><li><code>kube-apiserver</code> version must be between <strong>1.18</strong> and <strong>1.20</strong>, inclusive.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0b597086a9d1382f86abadcfeab657d6>2 - Learning environment</h1><h2 id=kind>kind</h2><p><a href=https://kind.sigs.k8s.io/docs/><code>kind</code></a> lets you run Kubernetes on
your local computer. This tool requires that you have
<a href=https://docs.docker.com/get-docker/>Docker</a> installed and configured.</p><p>The kind <a href=https://kind.sigs.k8s.io/docs/user/quick-start/>Quick Start</a> page
shows you what you need to do to get up and running with kind.</p><h2 id=minikube>minikube</h2><p>Like <code>kind</code>, <a href=https://minikube.sigs.k8s.io/><code>minikube</code></a> is a tool that lets you run Kubernetes
locally. <code>minikube</code> runs a single-node Kubernetes cluster on your personal
computer (including Windows, macOS and Linux PCs) so that you can try out
Kubernetes, or for daily development work.</p><p>You can follow the official
<a href=https://minikube.sigs.k8s.io/docs/start/>Get Started!</a> guide if your focus is
on getting the tool installed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4e14853fdaa3bd273f31a60112b9b5ac>3 - Production environment</h1></div><div class=td-content><h1 id=pg-a77d3feb6e6d9978f32fa14622642e9a>3.1 - Container runtimes</h1><p>You need to install a
<a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtime">container runtime</a>
into each node in the cluster so that Pods can run there. This page outlines
what is involved and describes related tasks for setting up nodes.</p><p>This page lists details for using several common container runtimes with
Kubernetes, on Linux:</p><ul><li><a href=#containerd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li><li><a href=#docker>Docker</a></li></ul><blockquote class="note callout"><div><strong>Note:</strong> For other operating systems, look for documentation specific to your platform.</div></blockquote><h2 id=cgroup-drivers>Cgroup drivers</h2><p>Control groups are used to constrain resources that are allocated to processes.</p><p>When <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> is chosen as the init
system for a Linux distribution, the init process generates and consumes a root control group
(<code>cgroup</code>) and acts as a cgroup manager.
Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. It's possible
to configure your container runtime and the kubelet to use <code>cgroupfs</code>. Using <code>cgroupfs</code> alongside
systemd means that there will be two different cgroup managers.</p><p>A single cgroup manager simplifies the view of what resources are being allocated
and will by default have a more consistent view of the available and in-use resources.
When there are two cgroup managers on a system, you end up with two views of those resources.
In the field, people have reported cases where nodes that are configured to use <code>cgroupfs</code>
for the kubelet and Docker, but <code>systemd</code> for the rest of the processes, become unstable under
resource pressure.</p><p>Changing the settings such that your container runtime and kubelet use <code>systemd</code> as the cgroup driver
stabilized the system. To configure this for Docker, set <code>native.cgroupdriver=systemd</code>.</p><blockquote class="caution callout"><div><strong>Caution:</strong><p>Changing the cgroup driver of a Node that has joined a cluster is strongly <em>not</em> recommended.<br>If the kubelet has created Pods using the semantics of one cgroup driver, changing the container
runtime to another cgroup driver can cause errors when trying to re-create the Pod sandbox
for such existing Pods. Restarting the kubelet may not solve such errors.</p><p>If you have automation that makes it feasible, replace the node with another using the updated
configuration, or reinstall it using automation.</p></div></blockquote><h2 id=container-runtimes>Container runtimes</h2><blockquote class="callout caution" role=alert><strong>Caution:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects. This page follows <a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>CNCF website guidelines</a> by listing projects alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change.</blockquote><h3 id=containerd>containerd</h3><p>This section contains the necessary steps to use containerd as CRI runtime.</p><p>Use the following commands to install Containerd on your system:</p><p>Install and configure prerequisites:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style=color:#b44>overlay
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style=color:#080;font-style:italic># Setup required sysctl params, these persist across reboots.</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># Apply sysctl params without reboot</span>
sudo sysctl --system
</code></pre></div><p>Install containerd:</p><ul class="nav nav-tabs" id=tab-cri-containerd-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-containerd-installation-0 role=tab aria-controls=tab-cri-containerd-installation-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-containerd-installation-1 role=tab aria-controls=tab-cri-containerd-installation-1>Windows (PowerShell)</a></li></ul><div class=tab-content id=tab-cri-containerd-installation><div id=tab-cri-containerd-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-containerd-installation-0><p><ol><li><p>Install the <code>containerd.io</code> package from the official Docker repositories. Instructions for setting up the Docker repository for your respective Linux distribution and installing the <code>containerd.io</code> package can be found at <a href=https://docs.docker.com/engine/install/#server>Install Docker Engine</a>.</p></li><li><p>Configure containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre></div></li><li><p>Restart containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart containerd
</code></pre></div></li></ol></div><div id=tab-cri-containerd-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-containerd-installation-1><p><p>Start a Powershell session, set <code>$Version</code> to the desired version (ex: <code>$Version=1.4.3</code>), and then run the following commands:</p><ol><li><p>Download containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>curl.exe -L https<span>:</span>//github.com/containerd/containerd/releases/download/v<span style=color:#b8860b>$Version</span>/containerd-<span style=color:#b8860b>$Version</span>-windows-amd64.tar.gz -o containerd-windows-amd64.tar.gz
tar.exe xvf .\containerd-windows-amd64.tar.gz
</code></pre></div></li><li><p>Extract and configure:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Copy-Item</span> -Path <span style=color:#b44>&#34;.\bin\&#34;</span> -Destination <span style=color:#b44>&#34;$Env:ProgramFiles\containerd&#34;</span> -Recurse -Force
<span style=color:#a2f>cd </span><span style=color:#b8860b>$Env:ProgramFiles</span>\containerd\
.\containerd.exe config <span style=color:#a2f;font-weight:700>default</span> | <span style=color:#a2f>Out-File</span> config.toml -Encoding ascii

<span style=color:#080;font-style:italic># Review the configuration. Depending on setup you may want to adjust:</span>
<span style=color:#080;font-style:italic># - the sandbox_image (Kubernetes pause image)</span>
<span style=color:#080;font-style:italic># - cni bin_dir and conf_dir locations</span>
<span style=color:#a2f>Get-Content</span> config.toml

<span style=color:#080;font-style:italic># (Optional - but highly recommended) Exclude containerd from Windows Defender Scans</span>
<span style=color:#a2f>Add-MpPreference</span> -ExclusionProcess <span style=color:#b44>&#34;$Env:ProgramFiles\containerd\containerd.exe&#34;</span>
</code></pre></div></li><li><p>Start containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>.\containerd.exe --register-service
<span style=color:#a2f>Start-Service</span> containerd
</code></pre></div></li></ol></div></div><h4 id=containerd-systemd>Using the <code>systemd</code> cgroup driver</h4><p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p><pre><code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
  ...
  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre><p>If you apply this change make sure to restart containerd again:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart containerd
</code></pre></div><p>When using kubeadm, manually configure the
<a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node>cgroup driver for kubelet</a>.</p><h3 id=cri-o>CRI-O</h3><p>This section contains the necessary steps to install CRI-O as a container runtime.</p><p>Use the following commands to install CRI-O on your system:</p><blockquote class="note callout"><div><strong>Note:</strong> The CRI-O major and minor versions must match the Kubernetes major and minor versions.
For more information, see the <a href=https://github.com/cri-o/cri-o#compatibility-matrix-cri-o--kubernetes>CRI-O compatibility matrix</a>.</div></blockquote><p>Install and configure prerequisites:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Create the .conf file to load the modules at bootup</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf
</span><span style=color:#b44>overlay
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style=color:#080;font-style:italic># Set up required sysctl params, these persist across reboots.</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>EOF</span>

sudo sysctl --system
</code></pre></div><ul class="nav nav-tabs" id=tab-cri-cri-o-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-cri-o-installation-0 role=tab aria-controls=tab-cri-cri-o-installation-0 aria-selected=true>Debian</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-1 role=tab aria-controls=tab-cri-cri-o-installation-1>Ubuntu</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-2 role=tab aria-controls=tab-cri-cri-o-installation-2>CentOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-3 role=tab aria-controls=tab-cri-cri-o-installation-3>openSUSE Tumbleweed</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-4 role=tab aria-controls=tab-cri-cri-o-installation-4>Fedora</a></li></ul><div class=tab-content id=tab-cri-cri-o-installation><div id=tab-cri-cri-o-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-cri-o-installation-0><p><p>To install CRI-O on the following operating systems, set the environment variable <code>OS</code>
to the appropriate value from the following table:</p><table><thead><tr><th>Operating system</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Debian Unstable</td><td><code>Debian_Unstable</code></td></tr><tr><td>Debian Testing</td><td><code>Debian_Testing</code></td></tr></tbody></table><p><br>Then, set <code>$VERSION</code> to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set <code>VERSION=1.20</code>.
You can pin your installation to a specific release.
To install version 1.20.0, set <code>VERSION=1.20:1.20.0</code>.<br></p><p>Then run</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style=color:#b44>deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style=color:#b44>EOF</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style=color:#b44>deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style=color:#b44>EOF</span>

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div><div id=tab-cri-cri-o-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-1><p><p>To install on the following operating systems, set the environment variable <code>OS</code> to the appropriate field in the following table:</p><table><thead><tr><th>Operating system</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Ubuntu 20.04</td><td><code>xUbuntu_20.04</code></td></tr><tr><td>Ubuntu 19.10</td><td><code>xUbuntu_19.10</code></td></tr><tr><td>Ubuntu 19.04</td><td><code>xUbuntu_19.04</code></td></tr><tr><td>Ubuntu 18.04</td><td><code>xUbuntu_18.04</code></td></tr></tbody></table><p><br>Then, set <code>$VERSION</code> to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set <code>VERSION=1.20</code>.
You can pin your installation to a specific release.
To install version 1.20.0, set <code>VERSION=1.20:1.20.0</code>.<br></p><p>Then run</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style=color:#b44>deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style=color:#b44>EOF</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style=color:#b44>deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style=color:#b44>EOF</span>

curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers-cri-o.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div><div id=tab-cri-cri-o-installation-2 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-2><p><p>To install on the following operating systems, set the environment variable <code>OS</code> to the appropriate field in the following table:</p><table><thead><tr><th>Operating system</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Centos 8</td><td><code>CentOS_8</code></td></tr><tr><td>Centos 8 Stream</td><td><code>CentOS_8_Stream</code></td></tr><tr><td>Centos 7</td><td><code>CentOS_7</code></td></tr></tbody></table><p><br>Then, set <code>$VERSION</code> to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set <code>VERSION=1.20</code>.
You can pin your installation to a specific release.
To install version 1.20.0, set <code>VERSION=1.20:1.20.0</code>.<br></p><p>Then run</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo
sudo yum install cri-o
</code></pre></div></div><div id=tab-cri-cri-o-installation-3 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-3><p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo zypper install cri-o
</code></pre></div></div><div id=tab-cri-cri-o-installation-4 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-4><p><p>Set <code>$VERSION</code> to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, <code>VERSION=1.20</code>.</p><p>You can find available versions with:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo dnf module list cri-o
</code></pre></div><p>CRI-O does not support pinning to specific releases on Fedora.</p><p>Then run</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo dnf module <span style=color:#a2f>enable</span> cri-o:<span style=color:#b8860b>$VERSION</span>
sudo dnf install cri-o
</code></pre></div></div></div><p>Start CRI-O:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl daemon-reload
sudo systemctl <span style=color:#a2f>enable</span> crio --now
</code></pre></div><p>Refer to the <a href=https://github.com/cri-o/cri-o/blob/master/install.md>CRI-O installation guide</a>
for more information.</p><h4 id=cgroup-driver>cgroup driver</h4><p>CRI-O uses the systemd cgroup driver per default. To switch to the <code>cgroupfs</code>
cgroup driver, either edit <code>/etc/crio/crio.conf</code> or place a drop-in
configuration in <code>/etc/crio/crio.conf.d/02-cgroup-manager.conf</code>, for example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml>[crio.runtime]
conmon_cgroup = <span style=color:#b44>&#34;pod&#34;</span>
cgroup_manager = <span style=color:#b44>&#34;cgroupfs&#34;</span>
</code></pre></div><p>Please also note the changed <code>conmon_cgroup</code>, which has to be set to the value
<code>pod</code> when using CRI-O with <code>cgroupfs</code>. It is generally necessary to keep the
cgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O
in sync.</p><h3 id=docker>Docker</h3><ol><li><p>On each of your nodes, install the Docker for your Linux distribution as per <a href=https://docs.docker.com/engine/install/#server>Install Docker Engine</a>. You can find the latest validated version of Docker in this <a href=https://git.k8s.io/kubernetes/build/dependencies.yaml>dependencies</a> file.</p></li><li><p>Configure the Docker daemon, in particular to use systemd for the management of the container’s cgroups.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir /etc/docker
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/docker/daemon.json
</span><span style=color:#b44>{
</span><span style=color:#b44>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span style=color:#b44>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span style=color:#b44>  &#34;log-opts&#34;: {
</span><span style=color:#b44>    &#34;max-size&#34;: &#34;100m&#34;
</span><span style=color:#b44>  },
</span><span style=color:#b44>  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span><span style=color:#b44>}
</span><span style=color:#b44>EOF</span>
</code></pre></div><blockquote class="note callout"><div><strong>Note:</strong> <code>overlay2</code> is the preferred storage driver for systems running Linux kernel version 4.0 or higher, or RHEL or CentOS using version 3.10.0-514 and above.</div></blockquote></li><li><p>Restart Docker and enable on boot:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl <span style=color:#a2f>enable</span> docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></div></li></ol><blockquote class="note callout"><div><strong>Note:</strong><p>For more information refer to</p><ul><li><a href=https://docs.docker.com/config/daemon/>Configure the Docker daemon</a></li><li><a href=https://docs.docker.com/config/daemon/systemd/>Control Docker with systemd</a></li></ul></div></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-00e1646f68aeb89f9722cf6f6cfcad94>3.2 - Installing Kubernetes with deployment tools</h1></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>3.2.1 - Bootstrapping clusters with kubeadm</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>3.2.1.1 - Installing kubeadm</h1><p><img src=https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png align=right width=150px>This page shows how to install the <code>kubeadm</code> toolbox.
For information how to create a cluster with kubeadm once you have performed this installation process, see the <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Using kubeadm to Create a Cluster</a> page.</p><h2 id=before-you-begin>Before you begin</h2><ul><li>A compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions based on Debian and Red Hat, and those distributions without a package manager.</li><li>2 GB or more of RAM per machine (any less will leave little room for your apps).</li><li>2 CPUs or more.</li><li>Full network connectivity between all machines in the cluster (public or private network is fine).</li><li>Unique hostname, MAC address, and product_uuid for every node. See <a href=#verify-mac-address>here</a> for more details.</li><li>Certain ports are open on your machines. See <a href=#check-required-ports>here</a> for more details.</li><li>Swap disabled. You <strong>MUST</strong> disable swap in order for the kubelet to work properly.</li></ul><h2 id=verify-mac-address>Verify the MAC address and product_uuid are unique for every node</h2><ul><li>You can get the MAC address of the network interfaces using the command <code>ip link</code> or <code>ifconfig -a</code></li><li>The product_uuid can be checked by using the command <code>sudo cat /sys/class/dmi/id/product_uuid</code></li></ul><p>It is very likely that hardware devices will have unique addresses, although some virtual machines may have
identical values. Kubernetes uses these values to uniquely identify the nodes in the cluster.
If these values are not unique to each node, the installation process
may <a href=https://github.com/kubernetes/kubeadm/issues/31>fail</a>.</p><h2 id=check-network-adapters>Check network adapters</h2><p>If you have more than one network adapter, and your Kubernetes components are not reachable on the default
route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.</p><h2 id=letting-iptables-see-bridged-traffic>Letting iptables see bridged traffic</h2><p>Make sure that the <code>br_netfilter</code> module is loaded. This can be done by running <code>lsmod | grep br_netfilter</code>. To load it explicitly call <code>sudo modprobe br_netfilter</code>.</p><p>As a requirement for your Linux Node's iptables to correctly see bridged traffic, you should ensure <code>net.bridge.bridge-nf-call-iptables</code> is set to 1 in your <code>sysctl</code> config, e.g.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span><span style=color:#b44>EOF</span>
sudo sysctl --system
</code></pre></div><p>For more details please see the <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>Network Plugin Requirements</a> page.</p><h2 id=check-required-ports>Check required ports</h2><h3 id=control-plane-node-s>Control-plane node(s)</h3><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443*</td><td>Kubernetes API server</td><td>All</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd server client API</td><td>kube-apiserver, etcd</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td><td>Self</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td><td>Self</td></tr></tbody></table><h3 id=worker-node-s>Worker node(s)</h3><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>NodePort Services†</td><td>All</td></tr></tbody></table><p>† Default port range for <a href=/docs/concepts/services-networking/service/>NodePort Services</a>.</p><p>Any port numbers marked with * are overridable, so you will need to ensure any
custom ports you provide are also open.</p><p>Although etcd ports are included in control-plane nodes, you can also host your own
etcd cluster externally or on custom ports.</p><p>The pod network plugin you use (see below) may also require certain ports to be
open. Since this differs with each pod network plugin, please see the
documentation for the plugins about what port(s) those need.</p><h2 id=installing-runtime>Installing runtime</h2><p>To run containers in Pods, Kubernetes uses a
<a class=glossary-tooltip title="The container runtime is the software that is responsible for running containers." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label="container runtime">container runtime</a>.</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linux nodes</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>other operating systems</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><p>By default, Kubernetes uses the
<a class=glossary-tooltip title="An API for container runtimes to integrate with kubelet" data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label="Container Runtime Interface">Container Runtime Interface</a> (CRI)
to interface with your chosen container runtime.</p><p>If you don't specify a runtime, kubeadm automatically tries to detect an installed
container runtime by scanning through a list of well known Unix domain sockets.
The following table lists container runtimes and their associated socket paths:</p><table><caption style=display:none>Container runtimes and their socket paths</caption><thead><tr><th>Runtime</th><th>Path to Unix domain socket</th></tr></thead><tbody><tr><td>Docker</td><td><code>/var/run/dockershim.sock</code></td></tr><tr><td>containerd</td><td><code>/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>/var/run/crio/crio.sock</code></td></tr></tbody></table><p><br>If both Docker and containerd are detected, Docker takes precedence. This is
needed because Docker 18.09 ships with containerd and both are detectable even if you only
installed Docker.
If any other two or more runtimes are detected, kubeadm exits with an error.</p><p>The kubelet integrates with Docker through the built-in <code>dockershim</code> CRI implementation.</p><p>See <a href=/docs/setup/production-environment/container-runtimes/>container runtimes</a>
for more information.</p></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><p>By default, kubeadm uses <a class=glossary-tooltip title="Docker is a software technology providing operating-system-level virtualization also known as containers." data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a> as the container runtime.
The kubelet integrates with Docker through the built-in <code>dockershim</code> CRI implementation.</p><p>See <a href=/docs/setup/production-environment/container-runtimes/>container runtimes</a>
for more information.</p></div></div><h2 id=installing-kubeadm-kubelet-and-kubectl>Installing kubeadm, kubelet and kubectl</h2><p>You will install these packages on all of your machines:</p><ul><li><p><code>kubeadm</code>: the command to bootstrap the cluster.</p></li><li><p><code>kubelet</code>: the component that runs on all of the machines in your cluster
and does things like starting pods and containers.</p></li><li><p><code>kubectl</code>: the command line util to talk to your cluster.</p></li></ul><p>kubeadm <strong>will not</strong> install or manage <code>kubelet</code> or <code>kubectl</code> for you, so you will
need to ensure they match the version of the Kubernetes control plane you want
kubeadm to install for you. If you do not, there is a risk of a version skew occurring that
can lead to unexpected, buggy behaviour. However, <em>one</em> minor version skew between the
kubelet and the control plane is supported, but the kubelet version may never exceed the API
server version. For example, the kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server,
but not vice versa.</p><p>For information about installing <code>kubectl</code>, see <a href=/docs/tasks/tools/>Install and set up kubectl</a>.</p><blockquote class="warning callout"><div><strong>Warning:</strong> These instructions exclude all Kubernetes packages from any system upgrades.
This is because kubeadm and Kubernetes require
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>special attention to upgrade</a>.</div></blockquote><p>For more information on version skews, see:</p><ul><li>Kubernetes <a href=/docs/setup/release/version-skew-policy/>version and version-skew policy</a></li><li>Kubeadm-specific <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>version skew policy</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>Debian-based distributions</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>Red Hat-based distributions</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>Without a package manager</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><ol><li><p>Update the <code>apt</code> package index and install packages needed to use the Kubernetes <code>apt</code> repository:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre></div></li><li><p>Download the Google Cloud public signing key:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</code></pre></div></li><li><p>Add the Kubernetes <code>apt</code> repository:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre></div></li><li><p>Update <code>apt</code> package index, install kubelet, kubeadm and kubectl, and pin their version:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div></li></ol></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span><span style=color:#b44>[kubernetes]
</span><span style=color:#b44>name=Kubernetes
</span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span><span style=color:#b44>enabled=1
</span><span style=color:#b44>gpgcheck=1
</span><span style=color:#b44>repo_gpgcheck=1
</span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># Set SELinux in permissive mode (effectively disabling it)</span>
sudo setenforce <span style=color:#666>0</span>
sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes

sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div><p><strong>Notes:</strong></p><ul><li><p>Setting SELinux in permissive mode by running <code>setenforce 0</code> and <code>sed ...</code> effectively disables it.
This is required to allow containers to access the host filesystem, which is needed by pod networks for example.
You have to do this until SELinux support is improved in the kubelet.</p></li><li><p>You can leave SELinux enabled if you know how to configure it but it may require settings that are not supported by kubeadm.</p></li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>Install CNI plugins (required for most pod network):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
sudo mkdir -p /opt/cni/bin
curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-amd64-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C /opt/cni/bin -xz
</code></pre></div><p>Define the directory to download command files</p><blockquote class="note callout"><div><strong>Note:</strong> The <code>DOWNLOAD_DIR</code> variable must be set to a writable directory.
If you are running Flatcar Container Linux, set <code>DOWNLOAD_DIR=/opt/bin</code>.</div></blockquote><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span>/usr/local/bin
sudo mkdir -p <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</code></pre></div><p>Install crictl (required for kubeadm / Kubelet Container Runtime Interface (CRI))</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.17.0&#34;</span>
curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-amd64.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</code></pre></div><p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
<span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/amd64/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>

<span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.4.0&#34;</span>
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre></div><p>Enable and start <code>kubelet</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div><blockquote class="note callout"><div><strong>Note:</strong> The Flatcar Container Linux distribution mounts the <code>/usr</code> directory as a read-only filesystem.
Before bootstrapping your cluster, you need to take additional steps to configure a writable directory.
See the <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>Kubeadm Troubleshooting guide</a> to learn how to set up a writable directory.</div></blockquote></div></div><p>The kubelet is now restarting every few seconds, as it waits in a crashloop for
kubeadm to tell it what to do.</p><h2 id=configure-cgroup-driver-used-by-kubelet-on-control-plane-node>Configure cgroup driver used by kubelet on control-plane node</h2><p>When using Docker, kubeadm will automatically detect the cgroup driver for the kubelet
and set it in the <code>/var/lib/kubelet/config.yaml</code> file during runtime.</p><p>If you are using a different CRI, you must pass your <code>cgroupDriver</code> value to <code>kubeadm init</code>, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>&lt;value&gt;<span style=color:#bbb>
</span></code></pre></div><p>For further details, please read <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>Using kubeadm init with a configuration file</a>
and the <a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code> reference</a></p><p>Please mind, that you <strong>only</strong> have to do that if the cgroup driver of your CRI
is not <code>cgroupfs</code>, because that is the default value in the kubelet already.</p><blockquote class="note callout"><div><strong>Note:</strong> Since <code>--cgroup-driver</code> flag has been deprecated by the kubelet, if you have that in <code>/var/lib/kubelet/kubeadm-flags.env</code>
or <code>/etc/default/kubelet</code>(<code>/etc/sysconfig/kubelet</code> for RPMs), please remove it and use the KubeletConfiguration instead
(stored in <code>/var/lib/kubelet/config.yaml</code> by default).</div></blockquote><p>The automatic detection of cgroup driver for other container runtimes
like CRI-O and containerd is work in progress.</p><h2 id=troubleshooting>Troubleshooting</h2><p>If you are running into difficulties with kubeadm, please consult our <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting docs</a>.</p><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Using kubeadm to Create a Cluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>3.2.1.2 - Troubleshooting kubeadm</h1><p>As with any program, you might run into an error installing or running kubeadm.
This page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.</p><p>If your problem is not listed below, please follow the following steps:</p><ul><li><p>If you think your problem is a bug with kubeadm:</p><ul><li>Go to <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> and search for existing issues.</li><li>If no issue exists, please <a href=https://github.com/kubernetes/kubeadm/issues/new>open one</a> and follow the issue template.</li></ul></li><li><p>If you are unsure about how kubeadm works, you can ask on <a href=https://slack.k8s.io/>Slack</a> in <code>#kubeadm</code>,
or open a question on <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>. Please include
relevant tags like <code>#kubernetes</code> and <code>#kubeadm</code> so folks can help you.</p></li></ul><h2 id=not-possible-to-join-a-v1-18-node-to-a-v1-17-cluster-due-to-missing-rbac>Not possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC</h2><p>In v1.18 kubeadm added prevention for joining a Node in the cluster if a Node with the same name already exists.
This required adding RBAC for the bootstrap-token user to be able to GET a Node object.</p><p>However this causes an issue where <code>kubeadm join</code> from v1.18 cannot join a cluster created by kubeadm v1.17.</p><p>To workaround the issue you have two options:</p><p>Execute <code>kubeadm init phase bootstrap-token</code> on a control-plane node using kubeadm v1.18.
Note that this enables the rest of the bootstrap-token permissions as well.</p><p>or</p><p>Apply the following RBAC manually using <code>kubectl apply -f ...</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:bootstrappers:kubeadm:default-node-token<span style=color:#bbb>
</span></code></pre></div><h2 id=ebtables-or-some-similar-executable-not-found-during-installation><code>ebtables</code> or some similar executable not found during installation</h2><p>If you see the following warnings while running <code>kubeadm init</code></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
<span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</code></pre></div><p>Then you may be missing <code>ebtables</code>, <code>ethtool</code> or a similar executable on your node. You can install them with the following commands:</p><ul><li>For Ubuntu/Debian users, run <code>apt install ebtables ethtool</code>.</li><li>For CentOS/Fedora users, run <code>yum install ebtables ethtool</code>.</li></ul><h2 id=kubeadm-blocks-waiting-for-control-plane-during-installation>kubeadm blocks waiting for control plane during installation</h2><p>If you notice that <code>kubeadm init</code> hangs after printing out the following line:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</code></pre></div><p>This may be caused by a number of problems. The most common are:</p><ul><li><p>network connection problems. Check that your machine has full network connectivity before continuing.</p></li><li><p>the default cgroup driver configuration for the kubelet differs from that used by Docker.
Check the system log file (e.g. <code>/var/log/message</code>) or examine the output from <code>journalctl -u kubelet</code>. If you see something like the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>error: failed to run Kubelet: failed to create kubelet:
misconfiguration: kubelet cgroup driver: <span style=color:#b44>&#34;systemd&#34;</span> is different from docker cgroup driver: <span style=color:#b44>&#34;cgroupfs&#34;</span>
</code></pre></div><p>There are two common ways to fix the cgroup driver problem:</p><ol><li><p>Install Docker again following instructions
<a href=/docs/setup/production-environment/container-runtimes/#docker>here</a>.</p></li><li><p>Change the kubelet config to match the Docker cgroup driver manually, you can refer to
<a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node>Configure cgroup driver used by kubelet on control-plane node</a></p></li></ol></li><li><p>control plane Docker containers are crashlooping or hanging. You can check this by running <code>docker ps</code> and investigating each container by running <code>docker logs</code>.</p></li></ul><h2 id=kubeadm-blocks-when-removing-managed-containers>kubeadm blocks when removing managed containers</h2><p>The following could happen if Docker halts and does not remove any Kubernetes-managed containers:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo kubeadm reset
</code></pre></div><pre><code class=language-console data-lang=console>[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;
[reset] Removing kubernetes-managed containers
(block)
</code></pre><p>A possible solution is to restart the Docker service and then re-run <code>kubeadm reset</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart docker.service
sudo kubeadm reset
</code></pre></div><p>Inspecting the logs for docker may also be useful:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>journalctl -u docker
</code></pre></div><h2 id=pods-in-runcontainererror-crashloopbackoff-or-error-state>Pods in <code>RunContainerError</code>, <code>CrashLoopBackOff</code> or <code>Error</code> state</h2><p>Right after <code>kubeadm init</code> there should not be any pods in these states.</p><ul><li>If there are pods in one of these states <em>right after</em> <code>kubeadm init</code>, please open an
issue in the kubeadm repo. <code>coredns</code> (or <code>kube-dns</code>) should be in the <code>Pending</code> state
until you have deployed the network add-on.</li><li>If you see Pods in the <code>RunContainerError</code>, <code>CrashLoopBackOff</code> or <code>Error</code> state
after deploying the network add-on and nothing happens to <code>coredns</code> (or <code>kube-dns</code>),
it's very likely that the Pod Network add-on that you installed is somehow broken.
You might have to grant it more RBAC privileges or use a newer version. Please file
an issue in the Pod Network providers' issue tracker and get the issue triaged there.</li><li>If you install a version of Docker older than 1.12.1, remove the <code>MountFlags=slave</code> option
when booting <code>dockerd</code> with <code>systemd</code> and restart <code>docker</code>. You can see the MountFlags in <code>/usr/lib/systemd/system/docker.service</code>.
MountFlags can interfere with volumes mounted by Kubernetes, and put the Pods in <code>CrashLoopBackOff</code> state.
The error happens when Kubernetes does not find <code>var/run/secrets/kubernetes.io/serviceaccount</code> files.</li></ul><h2 id=coredns-or-kube-dns-is-stuck-in-the-pending-state><code>coredns</code> (or <code>kube-dns</code>) is stuck in the <code>Pending</code> state</h2><p>This is <strong>expected</strong> and part of the design. kubeadm is network provider-agnostic, so the admin
should <a href=/docs/concepts/cluster-administration/addons/>install the pod network add-on</a>
of choice. You have to install a Pod Network
before CoreDNS may be deployed fully. Hence the <code>Pending</code> state before the network is set up.</p><h2 id=hostport-services-do-not-work><code>HostPort</code> services do not work</h2><p>The <code>HostPort</code> and <code>HostIP</code> functionality is available depending on your Pod Network
provider. Please contact the author of the Pod Network add-on to find out whether
<code>HostPort</code> and <code>HostIP</code> functionality are available.</p><p>Calico, Canal, and Flannel CNI providers are verified to support HostPort.</p><p>For more information, see the <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap documentation</a>.</p><p>If your network provider does not support the portmap CNI plugin, you may need to use the <a href=/docs/concepts/services-networking/service/#nodeport>NodePort feature of
services</a> or use <code>HostNetwork=true</code>.</p><h2 id=pods-are-not-accessible-via-their-service-ip>Pods are not accessible via their Service IP</h2><ul><li><p>Many network add-ons do not yet enable <a href=/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip>hairpin mode</a>
which allows pods to access themselves via their Service IP. This is an issue related to
<a href=https://github.com/containernetworking/cni/issues/476>CNI</a>. Please contact the network
add-on provider to get the latest status of their support for hairpin mode.</p></li><li><p>If you are using VirtualBox (directly or via Vagrant), you will need to
ensure that <code>hostname -i</code> returns a routable IP address. By default the first
interface is connected to a non-routable host-only network. A work around
is to modify <code>/etc/hosts</code>, see this <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>
for an example.</p></li></ul><h2 id=tls-certificate-errors>TLS certificate errors</h2><p>The following error indicates a possible certificate mismatch.</p><pre><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;)
</code></pre><ul><li><p>Verify that the <code>$HOME/.kube/config</code> file contains a valid certificate, and
regenerate a certificate if necessary. The certificates in a kubeconfig file
are base64 encoded. The <code>base64 --decode</code> command can be used to decode the certificate
and <code>openssl x509 -text -noout</code> can be used for viewing the certificate information.</p></li><li><p>Unset the <code>KUBECONFIG</code> environment variable using:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#a2f>unset</span> KUBECONFIG
</code></pre></div><p>Or set it to the default <code>KUBECONFIG</code> location:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div></li><li><p>Another workaround is to overwrite the existing <code>kubeconfig</code> for the "admin" user:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
mkdir <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div></li></ul><h2 id=default-nic-when-using-flannel-as-the-pod-network-in-vagrant>Default NIC When using flannel as the pod network in Vagrant</h2><p>The following error might indicate that something was wrong in the pod network:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</code></pre></div><ul><li><p>If you're using flannel as the pod network inside Vagrant, then you will have to specify the default interface name for flannel.</p><p>Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address <code>10.0.2.15</code>, is for external traffic that gets NATed.</p><p>This may lead to problems with flannel, which defaults to the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this, pass the <code>--iface eth1</code> flag to flannel so that the second interface is chosen.</p></li></ul><h2 id=non-public-ip-used-for-containers>Non-public IP used for containers</h2><p>In some situations <code>kubectl logs</code> and <code>kubectl run</code> commands may return with the following errors in an otherwise functional cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</code></pre></div><ul><li><p>This may be due to Kubernetes using an IP that can not communicate with other IPs on the seemingly same subnet, possibly by policy of the machine provider.</p></li><li><p>DigitalOcean assigns a public IP to <code>eth0</code> as well as a private one to be used internally as anchor for their floating IP feature, yet <code>kubelet</code> will pick the latter as the node's <code>InternalIP</code> instead of the public one.</p><p>Use <code>ip addr show</code> to check for this scenario instead of <code>ifconfig</code> because <code>ifconfig</code> will not display the offending alias IP address. Alternatively an API endpoint specific to DigitalOcean allows to query for the anchor IP from the droplet:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</code></pre></div><p>The workaround is to tell <code>kubelet</code> which IP to use using <code>--node-ip</code>. When using DigitalOcean, it can be the public one (assigned to <code>eth0</code>) or the private one (assigned to <code>eth1</code>) should you want to use the optional private network. The <a href=https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go><code>KubeletExtraArgs</code> section of the kubeadm <code>NodeRegistrationOptions</code> structure</a> can be used for this.</p><p>Then restart <code>kubelet</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>systemctl daemon-reload
systemctl restart kubelet
</code></pre></div></li></ul><h2 id=coredns-pods-have-crashloopbackoff-or-error-state><code>coredns</code> pods have <code>CrashLoopBackOff</code> or <code>Error</code> state</h2><p>If you have nodes that are running SELinux with an older version of Docker you might experience a scenario
where the <code>coredns</code> pods are not starting. To solve that you can try one of the following options:</p><ul><li><p>Upgrade to a <a href=/docs/setup/production-environment/container-runtimes/#docker>newer version of Docker</a>.</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>Disable SELinux</a>.</p></li><li><p>Modify the <code>coredns</code> deployment to set <code>allowPrivilegeEscalation</code> to <code>true</code>:</p></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</code></pre></div><p>Another cause for CoreDNS to have <code>CrashLoopBackOff</code> is when a CoreDNS Pod deployed in Kubernetes detects a loop. <a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>A number of workarounds</a>
are available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.</p><blockquote class="warning callout"><div><strong>Warning:</strong> Disabling SELinux or setting <code>allowPrivilegeEscalation</code> to <code>true</code> can compromise
the security of your cluster.</div></blockquote><h2 id=etcd-pods-restart-continually>etcd pods restart continually</h2><p>If you encounter the following error:</p><pre><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &quot;process_linux.go:110: decoding init error from pipe caused \&quot;read parent: connection reset by peer\&quot;&quot;
</code></pre><p>this issue appears if you run CentOS 7 with Docker 1.13.1.84.
This version of Docker can prevent the kubelet from executing into the etcd container.</p><p>To work around the issue, choose one of these options:</p><ul><li>Roll back to an earlier version of Docker, such as 1.13.1-75</li></ul><pre><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre><ul><li>Install one of the more recent recommended versions, such as 18.06:</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
</code></pre></div><h2 id=not-possible-to-pass-a-comma-separated-list-of-values-to-arguments-inside-a-component-extra-args-flag>Not possible to pass a comma separated list of values to arguments inside a <code>--component-extra-args</code> flag</h2><p><code>kubeadm init</code> flags such as <code>--component-extra-args</code> allow you to pass custom arguments to a control-plane
component like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing
the values (<code>mapStringString</code>).</p><p>If you decide to pass an argument that supports multiple, comma-separated values such as
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"</code> this flag will fail with
<code>flag: malformed pair, expect string=string</code>. This happens because the list of arguments for
<code>--apiserver-extra-args</code> expects <code>key=value</code> pairs and in this case <code>NamespacesExists</code> is considered
as a key that is missing a value.</p><p>Alternatively, you can try separating the <code>key=value</code> pairs like so:
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>
but this will result in the key <code>enable-admission-plugins</code> only having the value of <code>NamespaceExists</code>.</p><p>A known workaround is to use the kubeadm <a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags>configuration file</a>.</p><h2 id=kube-proxy-scheduled-before-node-is-initialized-by-cloud-controller-manager>kube-proxy scheduled before node is initialized by cloud-controller-manager</h2><p>In cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before
the cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail
to pick up the node's IP address properly and has knock-on effects to the proxy function managing
load balancers.</p><p>The following error can be seen in kube-proxy Pods:</p><pre><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>A known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane
nodes regardless of their conditions, keeping it off of other nodes until their initial guarding
conditions abate:</p><pre><code>kubectl -n kube-system patch ds kube-proxy -p='{ &quot;spec&quot;: { &quot;template&quot;: { &quot;spec&quot;: { &quot;tolerations&quot;: [ { &quot;key&quot;: &quot;CriticalAddonsOnly&quot;, &quot;operator&quot;: &quot;Exists&quot; }, { &quot;effect&quot;: &quot;NoSchedule&quot;, &quot;key&quot;: &quot;node-role.kubernetes.io/master&quot; } ] } } } }'
</code></pre><p>The tracking issue for this problem is <a href=https://github.com/kubernetes/kubeadm/issues/1027>here</a>.</p><h2 id=the-noderegistration-taints-field-is-omitted-when-marshalling-kubeadm-configuration>The NodeRegistration.Taints field is omitted when marshalling kubeadm configuration</h2><p><em>Note: This <a href=https://github.com/kubernetes/kubeadm/issues/1358>issue</a> only applies to tools that marshal kubeadm types (e.g. to a YAML configuration file). It will be fixed in kubeadm API v1beta2.</em></p><p>By default, kubeadm applies the <code>node-role.kubernetes.io/master:NoSchedule</code> taint to control-plane nodes.
If you prefer kubeadm to not taint the control-plane node, and set <code>InitConfiguration.NodeRegistration.Taints</code> to an empty slice,
the field will be omitted when marshalling. When the field is omitted, kubeadm applies the default taint.</p><p>There are at least two workarounds:</p><ol><li><p>Use the <code>node-role.kubernetes.io/master:PreferNoSchedule</code> taint instead of an empty slice. <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Pods will get scheduled on masters</a>, unless other nodes have capacity.</p></li><li><p>Remove the taint after kubeadm init exits:</p></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h2 id=usr-mounted-read-only><code>/usr</code> is mounted read-only on nodes</h2><p>On Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory <code>/usr</code> is mounted as a read-only filesystem.
For <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>flex-volume support</a>,
Kubernetes components like the kubelet and kube-controller-manager use the default path of
<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>, yet the flex-volume directory <em>must be writeable</em>
for the feature to work.</p><p>To workaround this issue you can configure the flex-volume directory using the kubeadm
<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>configuration file</a>.</p><p>On the primary control-plane Node (created using <code>kubeadm init</code>) pass the following
file using <code>--config</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>On joining Nodes:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Alternatively, you can modify <code>/etc/fstab</code> to make the <code>/usr</code> mount writeable, but please
be advised that this is modifying a design principle of the Linux distribution.</p><h2 id=kubeadm-upgrade-plan-prints-out-context-deadline-exceeded-error-message><code>kubeadm upgrade plan</code> prints out <code>context deadline exceeded</code> error message</h2><p>This error message is shown when upgrading a Kubernetes cluster with <code>kubeadm</code> in the case of running an external etcd. This is not a critical bug and happens because older versions of kubeadm perform a version check on the external etcd cluster. You can proceed with <code>kubeadm upgrade apply ...</code>.</p><p>This issue is fixed as of version 1.19.</p><h2 id=kubeadm-reset-unmounts-var-lib-kubelet><code>kubeadm reset</code> unmounts <code>/var/lib/kubelet</code></h2><p>If <code>/var/lib/kubelet</code> is being mounted, performing a <code>kubeadm reset</code> will effectively unmount it.</p><p>To workaround the issue, re-mount the <code>/var/lib/kubelet</code> directory after performing the <code>kubeadm reset</code> operation.</p><p>This is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>3.2.1.3 - Creating a cluster with kubeadm</h1><p><img src=https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png align=right width=150px>Using <code>kubeadm</code>, you can create a minimum viable Kubernetes cluster that conforms to best practices. In fact, you can use <code>kubeadm</code> to set up a cluster that will pass the <a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>Kubernetes Conformance tests</a>.
<code>kubeadm</code> also supports other cluster
lifecycle functions, such as <a href=/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap tokens</a> and cluster upgrades.</p><p>The <code>kubeadm</code> tool is good if you need:</p><ul><li>A simple way for you to try out Kubernetes, possibly for the first time.</li><li>A way for existing users to automate setting up a cluster and test their application.</li><li>A building block in other ecosystem and/or installer tools with a larger
scope.</li></ul><p>You can install and use <code>kubeadm</code> on various machines: your laptop, a set
of cloud servers, a Raspberry Pi, and more. Whether you're deploying into the
cloud or on-premises, you can integrate <code>kubeadm</code> into provisioning systems such
as Ansible or Terraform.</p><h2 id=before-you-begin>Before you begin</h2><p>To follow this guide, you need:</p><ul><li>One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.</li><li>2 GiB or more of RAM per machine--any less leaves little room for your
apps.</li><li>At least 2 CPUs on the machine that you use as a control-plane node.</li><li>Full network connectivity among all machines in the cluster. You can use either a
public or a private network.</li></ul><p>You also need to use a version of <code>kubeadm</code> that can deploy the version
of Kubernetes that you want to use in your new cluster.</p><p><a href=/docs/setup/release/version-skew-policy/#supported-versions>Kubernetes' version and version skew support policy</a> applies to <code>kubeadm</code> as well as to Kubernetes overall.
Check that policy to learn about what versions of Kubernetes and <code>kubeadm</code>
are supported. This page is written for Kubernetes v1.20.</p><p>The <code>kubeadm</code> tool's overall feature state is General Availability (GA). Some sub-features are
still under active development. The implementation of creating the cluster may change
slightly as the tool evolves, but the overall implementation should be pretty stable.</p><blockquote class="note callout"><div><strong>Note:</strong> Any commands under <code>kubeadm alpha</code> are, by definition, supported on an alpha level.</div></blockquote><h2 id=objectives>Objectives</h2><ul><li>Install a single control-plane Kubernetes cluster</li><li>Install a Pod network on the cluster so that your Pods can
talk to each other</li></ul><h2 id=instructions>Instructions</h2><h3 id=installing-kubeadm-on-your-hosts>Installing kubeadm on your hosts</h3><p>See <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>"Installing kubeadm"</a>.</p><blockquote class="note callout"><div><strong>Note:</strong><p>If you have already installed kubeadm, run <code>apt-get update && apt-get upgrade</code> or <code>yum update</code> to get the latest version of kubeadm.</p><p>When you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for
kubeadm to tell it what to do. This crashloop is expected and normal.
After you initialize your control-plane, the kubelet runs normally.</p></div></blockquote><h3 id=initializing-your-control-plane-node>Initializing your control-plane node</h3><p>The control-plane node is the machine where the control plane components run, including
<a class=glossary-tooltip title="Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data." data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> (the cluster database) and the
<a class=glossary-tooltip title="Control plane component that serves the Kubernetes API." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API Server">API Server</a>
(which the <a class=glossary-tooltip title="A command line tool for communicating with a Kubernetes API server." data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a> command line tool
communicates with).</p><ol><li>(Recommended) If you have plans to upgrade this single control-plane <code>kubeadm</code> cluster
to high availability you should specify the <code>--control-plane-endpoint</code> to set the shared endpoint
for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.</li><li>Choose a Pod network add-on, and verify whether it requires any arguments to
be passed to <code>kubeadm init</code>. Depending on which
third-party provider you choose, you might need to set the <code>--pod-network-cidr</code> to
a provider-specific value. See <a href=#pod-network>Installing a Pod network add-on</a>.</li><li>(Optional) Since version 1.14, <code>kubeadm</code> tries to detect the container runtime on Linux
by using a list of well known domain socket paths. To use different container runtime or
if there are more than one installed on the provisioned node, specify the <code>--cri-socket</code>
argument to <code>kubeadm init</code>. See <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>Installing runtime</a>.</li><li>(Optional) Unless otherwise specified, <code>kubeadm</code> uses the network interface associated
with the default gateway to set the advertise address for this particular control-plane node's API server.
To use a different network interface, specify the <code>--apiserver-advertise-address=&lt;ip-address></code> argument
to <code>kubeadm init</code>. To deploy an IPv6 Kubernetes cluster using IPv6 addressing, you
must specify an IPv6 address, for example <code>--apiserver-advertise-address=fd00::101</code></li><li>(Optional) Run <code>kubeadm config images pull</code> prior to <code>kubeadm init</code> to verify
connectivity to the gcr.io container image registry.</li></ol><p>To initialize the control-plane node run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init &lt;args&gt;
</code></pre></div><h3 id=considerations-about-apiserver-advertise-address-and-controlplaneendpoint>Considerations about apiserver-advertise-address and ControlPlaneEndpoint</h3><p>While <code>--apiserver-advertise-address</code> can be used to set the advertise address for this particular
control-plane node's API server, <code>--control-plane-endpoint</code> can be used to set the shared endpoint
for all control-plane nodes.</p><p><code>--control-plane-endpoint</code> allows both IP addresses and DNS names that can map to IP addresses.
Please contact your network administrator to evaluate possible solutions with respect to such mapping.</p><p>Here is an example mapping:</p><pre><code>192.168.0.102 cluster-endpoint
</code></pre><p>Where <code>192.168.0.102</code> is the IP address of this node and <code>cluster-endpoint</code> is a custom DNS name that maps to this IP.
This will allow you to pass <code>--control-plane-endpoint=cluster-endpoint</code> to <code>kubeadm init</code> and pass the same DNS name to
<code>kubeadm join</code>. Later you can modify <code>cluster-endpoint</code> to point to the address of your load-balancer in an
high availability scenario.</p><p>Turning a single control plane cluster created without <code>--control-plane-endpoint</code> into a highly available cluster
is not supported by kubeadm.</p><h3 id=more-information>More information</h3><p>For more information about <code>kubeadm init</code> arguments, see the <a href=/docs/reference/setup-tools/kubeadm/>kubeadm reference guide</a>.</p><p>To configure <code>kubeadm init</code> with a configuration file see <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>Using kubeadm init with a configuration file</a>.</p><p>To customize control plane components, including optional IPv6 assignment to liveness probe for control plane components and etcd server, provide extra arguments to each component as documented in <a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>custom arguments</a>.</p><p>To run <code>kubeadm init</code> again, you must first <a href=#tear-down>tear down the cluster</a>.</p><p>If you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets
have container image support for this architecture.</p><p><code>kubeadm init</code> first runs a series of prechecks to ensure that the machine
is ready to run Kubernetes. These prechecks expose warnings and exit on errors. <code>kubeadm init</code>
then downloads and installs the cluster control plane components. This may take several minutes.
After it finishes you should see:</p><pre><code class=language-none data-lang=none>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>To make kubectl work for your non-root user, run these commands, which are
also part of the <code>kubeadm init</code> output:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div><p>Alternatively, if you are the <code>root</code> user, you can run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div><p>Make a record of the <code>kubeadm join</code> command that <code>kubeadm init</code> outputs. You
need this command to <a href=#join-nodes>join nodes to your cluster</a>.</p><p>The token is used for mutual authentication between the control-plane node and the joining
nodes. The token included here is secret. Keep it safe, because anyone with this
token can add authenticated nodes to your cluster. These tokens can be listed,
created, and deleted with the <code>kubeadm token</code> command. See the
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadm reference guide</a>.</p><h3 id=pod-network>Installing a Pod network add-on</h3><blockquote class="caution callout"><div><strong>Caution:</strong><p>This section contains important information about networking setup and
deployment order.
Read all of this advice carefully before proceeding.</p><p><strong>You must deploy a
<a class=glossary-tooltip title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni target=_blank aria-label="Container Network Interface">Container Network Interface</a>
(CNI) based Pod network add-on so that your Pods can communicate with each other.
Cluster DNS (CoreDNS) will not start up before a network is installed.</strong></p><ul><li><p>Take care that your Pod network must not overlap with any of the host
networks: you are likely to see problems if there is any overlap.
(If you find a collision between your network plugin's preferred Pod
network and some of your host networks, you should think of a suitable
CIDR block to use instead, then use that during <code>kubeadm init</code> with
<code>--pod-network-cidr</code> and as a replacement in your network plugin's YAML).</p></li><li><p>By default, <code>kubeadm</code> sets up your cluster to use and enforce use of
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a> (role based access
control).
Make sure that your Pod network plugin supports RBAC, and so do any manifests
that you use to deploy it.</p></li><li><p>If you want to use IPv6--either dual-stack, or single-stack IPv6 only
networking--for your cluster, make sure that your Pod network plugin
supports IPv6.
IPv6 support was added to CNI in <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a>.</p></li></ul></div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> Kubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.
If you find an issue related to a CNI plugin you should log a ticket in its respective issue
tracker instead of the kubeadm or kubernetes issue trackers.</div></blockquote><p>Several external projects provide Kubernetes Pod networks using CNI, some of which also
support <a href=/docs/concepts/services-networking/network-policies/>Network Policy</a>.</p><p>See a list of add-ons that implement the
<a href=/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes networking model</a>.</p><p>You can install a Pod network add-on with the following command on the
control-plane node or a node that has the kubeconfig credentials:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f &lt;add-on.yaml&gt;
</code></pre></div><p>You can install only one Pod network per cluster.</p><p>Once a Pod network has been installed, you can confirm that it is working by
checking that the CoreDNS Pod is <code>Running</code> in the output of <code>kubectl get pods --all-namespaces</code>.
And once the CoreDNS Pod is up and running, you can continue by joining your nodes.</p><p>If your network is not working or CoreDNS is not in the <code>Running</code> state, check out the
<a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting guide</a>
for <code>kubeadm</code>.</p><h3 id=control-plane-node-isolation>Control plane node isolation</h3><p>By default, your cluster will not schedule Pods on the control-plane node for security
reasons. If you want to be able to schedule Pods on the control-plane node, for example for a
single-machine Kubernetes cluster for development, run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre></div><p>With output looking something like:</p><pre><code>node &quot;test-01&quot; untainted
taint &quot;node-role.kubernetes.io/master:&quot; not found
taint &quot;node-role.kubernetes.io/master:&quot; not found
</code></pre><p>This will remove the <code>node-role.kubernetes.io/master</code> taint from any nodes that
have it, including the control-plane node, meaning that the scheduler will then be able
to schedule Pods everywhere.</p><h3 id=join-nodes>Joining your nodes</h3><p>The nodes are where your workloads (containers and Pods, etc) run. To add new nodes to your cluster do the following for each machine:</p><ul><li>SSH to the machine</li><li>Become root (e.g. <code>sudo su -</code>)</li><li>Run the command that was output by <code>kubeadm init</code>. For example:</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre></div><p>If you do not have the token, you can get it by running the following command on the control-plane node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token list
</code></pre></div><p>The output is similar to this:</p><pre><code class=language-console data-lang=console>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
                                                   signing          token generated by     bootstrappers:
                                                                    'kubeadm init'.        kubeadm:
                                                                                           default-node-token
</code></pre><p>By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired,
you can create a new token by running the following command on the control-plane node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token create
</code></pre></div><p>The output is similar to this:</p><pre><code class=language-console data-lang=console>5didvk.d09sbcov8ph2amjw
</code></pre><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the following command chain on the control-plane node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</code></pre></div><p>The output is similar to:</p><pre><code class=language-console data-lang=console>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</code></pre><blockquote class="note callout"><div><strong>Note:</strong> To specify an IPv6 tuple for <code>&lt;control-plane-host>:&lt;control-plane-port></code>, IPv6 address must be enclosed in square brackets, for example: <code>[fd00::101]:2073</code>.</div></blockquote><p>The output should look something like:</p><pre><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code> when run on the control-plane node.</p><h3 id=optional-controlling-your-cluster-from-machines-other-than-the-control-plane-node>(Optional) Controlling your cluster from machines other than the control-plane node</h3><p>In order to get a kubectl on some other computer (e.g. laptop) to talk to your
cluster, you need to copy the administrator kubeconfig file from your control-plane node
to your workstation like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
</code></pre></div><blockquote class="note callout"><div><strong>Note:</strong><p>The example above assumes SSH access is enabled for root. If that is not the
case, you can copy the <code>admin.conf</code> file to be accessible by some other user
and <code>scp</code> using that other user instead.</p><p>The <code>admin.conf</code> file gives the user <em>superuser</em> privileges over the cluster.
This file should be used sparingly. For normal users, it's recommended to
generate an unique credential to which you grant privileges. You can do
this with the <code>kubeadm alpha kubeconfig user --client-name &lt;CN></code>
command. That command will print out a KubeConfig file to STDOUT which you
should save to a file and distribute to your user. After that, grant
privileges by using <code>kubectl create (cluster)rolebinding</code>.</p></div></blockquote><h3 id=optional-proxying-api-server-to-localhost>(Optional) Proxying API Server to localhost</h3><p>If you want to connect to the API Server from outside the cluster you can use
<code>kubectl proxy</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
</code></pre></div><p>You can now access the API Server locally at <code>http://localhost:8001/api/v1</code></p><h2 id=tear-down>Clean up</h2><p>If you used disposable servers for your cluster, for testing, you can
switch those off and do no further clean up. You can use
<code>kubectl config delete-cluster</code> to delete your local references to the
cluster.</p><p>However, if you want to deprovision your cluster more cleanly, you should
first <a href=/docs/reference/generated/kubectl/kubectl-commands#drain>drain the node</a>
and make sure that the node is empty, then deconfigure the node.</p><h3 id=remove-the-node>Remove the node</h3><p>Talking to the control-plane node with the appropriate credentials, run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
</code></pre></div><p>Before removing the node, reset the state installed by <code>kubeadm</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm reset
</code></pre></div><p>The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</code></pre></div><p>If you want to reset the IPVS tables, you must run the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ipvsadm -C
</code></pre></div><p>Now remove the node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl delete node &lt;node name&gt;
</code></pre></div><p>If you wish to start over, run <code>kubeadm init</code> or <code>kubeadm join</code> with the
appropriate arguments.</p><h3 id=clean-up-the-control-plane>Clean up the control plane</h3><p>You can use <code>kubeadm reset</code> on the control plane host to trigger a best-effort
clean up.</p><p>See the <a href=/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>
reference documentation for more information about this subcommand and its
options.</p><h2 id=whats-next>What's next</h2><ul><li>Verify that your cluster is running properly with <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a></li><li><a id=lifecycle>See <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a>
for details about upgrading your cluster using <code>kubeadm</code>.</li><li>Learn about advanced <code>kubeadm</code> usage in the <a href=/docs/reference/setup-tools/kubeadm/kubeadm>kubeadm reference documentation</a></li><li>Learn more about Kubernetes <a href=/docs/concepts/>concepts</a> and <a href=/docs/reference/kubectl/overview/><code>kubectl</code></a>.</li><li>See the <a href=/docs/concepts/cluster-administration/networking/>Cluster Networking</a> page for a bigger list
of Pod network add-ons.</li><li><a id=other-addons>See the <a href=/docs/concepts/cluster-administration/addons/>list of add-ons</a> to
explore other add-ons, including tools for logging, monitoring, network policy, visualization &
control of your Kubernetes cluster.</li><li>Configure how your cluster handles logs for cluster events and from
applications running in Pods.
See <a href=/docs/concepts/cluster-administration/logging/>Logging Architecture</a> for
an overview of what is involved.</li></ul><h3 id=feedback>Feedback</h3><ul><li>For bugs, visit the <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li><li>For support, visit the
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a> Slack channel</li><li>General SIG Cluster Lifecycle development Slack channel:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG Cluster Lifecycle <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG Cluster Lifecycle mailing list:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>Version skew policy</h2><p>The <code>kubeadm</code> tool of version v1.20 may deploy clusters with a control plane of version v1.20 or v1.19.
<code>kubeadm</code> v1.20 can also upgrade an existing kubeadm-created cluster of version v1.19.</p><p>Due to that we can't see into the future, kubeadm CLI v1.20 may or may not be able to deploy v1.21 clusters.</p><p>These resources provide more information on supported version skew between kubelets and the control plane, and other Kubernetes components:</p><ul><li>Kubernetes <a href=/docs/setup/release/version-skew-policy/>version and version-skew policy</a></li><li>Kubeadm-specific <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>installation guide</a></li></ul><h2 id=limitations>Limitations</h2><h3 id=resilience>Cluster resilience</h3><p>The cluster created here has a single control-plane node, with a single etcd database
running on it. This means that if the control-plane node fails, your cluster may lose
data and may need to be recreated from scratch.</p><p>Workarounds:</p><ul><li><p>Regularly <a href=https://coreos.com/etcd/docs/latest/admin_guide.html>back up etcd</a>. The
etcd data directory configured by kubeadm is at <code>/var/lib/etcd</code> on the control-plane node.</p></li><li><p>Use multiple control-plane nodes. You can read
<a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>Options for Highly Available topology</a> to pick a cluster
topology that provides <a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>high-availability</a>.</p></li></ul><h3 id=multi-platform>Platform compatibility</h3><p>kubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x
following the <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>multi-platform
proposal</a>.</p><p>Multiplatform container images for the control plane and addons are also supported since v1.12.</p><p>Only some of the network providers offer solutions for all platforms. Please consult the list of
network providers above or the documentation from each provider to figure out whether the provider
supports your chosen platform.</p><h2 id=troubleshooting>Troubleshooting</h2><p>If you are running into difficulties with kubeadm, please consult our <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting docs</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>3.2.1.4 - Customizing control plane configuration with kubeadm</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [stable]</code></div><p>The kubeadm <code>ClusterConfiguration</code> object exposes the field <code>extraArgs</code> that can override the default flags passed to control plane
components such as the APIServer, ControllerManager and Scheduler. The components are defined using the following fields:</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li></ul><p>The <code>extraArgs</code> field consist of <code>key: value</code> pairs. To override a flag for a control plane component:</p><ol><li>Add the appropriate fields to your configuration.</li><li>Add the flags to override to the field.</li><li>Run <code>kubeadm init</code> with <code>--config &lt;YOUR CONFIG YAML></code>.</li></ol><p>For more details on each field in the configuration you can navigate to our
<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2#ClusterConfiguration>API reference pages</a>.</p><blockquote class="note callout"><div><strong>Note:</strong> You can generate a <code>ClusterConfiguration</code> object with default values by running <code>kubeadm config print init-defaults</code> and saving the output to a file of your choice.</div></blockquote><h2 id=apiserver-flags>APIServer flags</h2><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>reference documentation for kube-apiserver</a>.</p><p>Example usage:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertise-address</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.0.103</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></code></pre></div><h2 id=controllermanager-flags>ControllerManager flags</h2><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>reference documentation for kube-controller-manager</a>.</p><p>Example usage:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=scheduler-flags>Scheduler flags</h2><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>reference documentation for kube-scheduler</a>.</p><p>Example usage:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubeconfig</span>:<span style=color:#bbb> </span>/home/johndoe/kubeconfig.yaml<span style=color:#bbb>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>3.2.1.5 - Options for Highly Available topology</h1><p>This page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.</p><p>You can set up an HA cluster:</p><ul><li>With stacked control plane nodes, where etcd nodes are colocated with control plane nodes</li><li>With external etcd nodes, where etcd runs on separate nodes from the control plane</li></ul><p>You should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.</p><blockquote class="note callout"><div><strong>Note:</strong> kubeadm bootstraps the etcd cluster statically. Read the etcd <a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>Clustering Guide</a>
for more details.</div></blockquote><h2 id=stacked-etcd-topology>Stacked etcd topology</h2><p>A stacked HA cluster is a <a href=https://en.wikipedia.org/wiki/Network_topology>topology</a> where the distributed
data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by
kubeadm that run control plane components.</p><p>Each control plane node runs an instance of the <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kube-controller-manager</code>.
The <code>kube-apiserver</code> is exposed to worker nodes using a load balancer.</p><p>Each control plane node creates a local etcd member and this etcd member communicates only with
the <code>kube-apiserver</code> of this node. The same applies to the local <code>kube-controller-manager</code>
and <code>kube-scheduler</code> instances.</p><p>This topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster
with external etcd nodes, and simpler to manage for replication.</p><p>However, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control
plane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.</p><p>You should therefore run a minimum of three stacked control plane nodes for an HA cluster.</p><p>This is the default topology in kubeadm. A local etcd member is created automatically
on control plane nodes when using <code>kubeadm init</code> and <code>kubeadm join --control-plane</code>.</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt="Stacked etcd topology"></p><h2 id=external-etcd-topology>External etcd topology</h2><p>An HA cluster with external etcd is a <a href=https://en.wikipedia.org/wiki/Network_topology>topology</a> where the distributed data storage cluster provided by etcd is external to the cluster formed by the nodes that run control plane components.</p><p>Like the stacked etcd topology, each control plane node in an external etcd topology runs an instance of the <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kube-controller-manager</code>. And the <code>kube-apiserver</code> is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the <code>kube-apiserver</code> of each control plane node.</p><p>This topology decouples the control plane and etcd member. It therefore provides an HA setup where
losing a control plane instance or an etcd member has less impact and does not affect
the cluster redundancy as much as the stacked HA topology.</p><p>However, this topology requires twice the number of hosts as the stacked HA topology.
A minimum of three hosts for control plane nodes and three hosts for etcd nodes are required for an HA cluster with this topology.</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt="External etcd topology"></p><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>Set up a highly available cluster with kubeadm</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>3.2.1.6 - Creating Highly Available clusters with kubeadm</h1><p>This page explains two different approaches to setting up a highly available Kubernetes
cluster using kubeadm:</p><ul><li>With stacked control plane nodes. This approach requires less infrastructure. The etcd members
and control plane nodes are co-located.</li><li>With an external etcd cluster. This approach requires more infrastructure. The
control plane nodes and etcd members are separated.</li></ul><p>Before proceeding, you should carefully consider which approach best meets the needs of your applications
and environment. <a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>This comparison topic</a> outlines the advantages and disadvantages of each.</p><p>If you encounter issues with setting up the HA cluster, please provide us with feedback
in the kubeadm <a href=https://github.com/kubernetes/kubeadm/issues/new>issue tracker</a>.</p><p>See also <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>The upgrade documentation</a>.</p><blockquote class="caution callout"><div><strong>Caution:</strong> This page does not address running your cluster on a cloud provider. In a cloud
environment, neither approach documented here works with Service objects of type
LoadBalancer, or with dynamic PersistentVolumes.</div></blockquote><h2 id=before-you-begin>Before you begin</h2><p>For both methods you need this infrastructure:</p><ul><li>Three machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum requirements</a> for
the control-plane nodes</li><li>Three machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum
requirements</a> for the workers</li><li>Full network connectivity between all machines in the cluster (public or
private network)</li><li>sudo privileges on all machines</li><li>SSH access from one device to all nodes in the system</li><li><code>kubeadm</code> and <code>kubelet</code> installed on all machines. <code>kubectl</code> is optional.</li></ul><p>For the external etcd cluster only, you also need:</p><ul><li>Three additional machines for etcd members</li></ul><h2 id=first-steps-for-both-methods>First steps for both methods</h2><h3 id=create-load-balancer-for-kube-apiserver>Create load balancer for kube-apiserver</h3><blockquote class="note callout"><div><strong>Note:</strong> There are many configurations for load balancers. The following example is only one
option. Your cluster requirements may need a different configuration.</div></blockquote><ol><li><p>Create a kube-apiserver load balancer with a name that resolves to DNS.</p><ul><li><p>In a cloud environment you should place your control plane nodes behind a TCP
forwarding load balancer. This load balancer distributes traffic to all
healthy control plane nodes in its target list. The health check for
an apiserver is a TCP check on the port the kube-apiserver listens on
(default value <code>:6443</code>).</p></li><li><p>It is not recommended to use an IP address directly in a cloud environment.</p></li><li><p>The load balancer must be able to communicate with all control plane nodes
on the apiserver port. It must also allow incoming traffic on its
listening port.</p></li><li><p>Make sure the address of the load balancer always matches
the address of kubeadm's <code>ControlPlaneEndpoint</code>.</p></li><li><p>Read the <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing>Options for Software Load Balancing</a>
guide for more details.</p></li></ul></li><li><p>Add the first control plane nodes to the load balancer and test the
connection:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>nc -v LOAD_BALANCER_IP PORT
</code></pre></div><ul><li>A connection refused error is expected because the apiserver is not yet
running. A timeout, however, means the load balancer cannot communicate
with the control plane node. If a timeout occurs, reconfigure the load
balancer to communicate with the control plane node.</li></ul></li><li><p>Add the remaining control plane nodes to the load balancer target group.</p></li></ol><h2 id=stacked-control-plane-and-etcd-nodes>Stacked control plane and etcd nodes</h2><h3 id=steps-for-the-first-control-plane-node>Steps for the first control plane node</h3><ol><li><p>Initialize the control plane:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</code></pre></div><ul><li><p>You can use the <code>--kubernetes-version</code> flag to set the Kubernetes version to use.
It is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.</p></li><li><p>The <code>--control-plane-endpoint</code> flag should be set to the address or DNS and port of the load balancer.</p></li><li><p>The <code>--upload-certs</code> flag is used to upload the certificates that should be shared
across all the control-plane instances to the cluster. If instead, you prefer to copy certs across
control-plane nodes manually or using automation tools, please remove this flag and refer to <a href=#manual-certs>Manual
certificate distribution</a> section below.</p></li></ul><blockquote class="note callout"><div><strong>Note:</strong> The <code>kubeadm init</code> flags <code>--config</code> and <code>--certificate-key</code> cannot be mixed, therefore if you want
to use the <a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>kubeadm configuration</a>
you must add the <code>certificateKey</code> field in the appropriate config locations
(under <code>InitConfiguration</code> and <code>JoinConfiguration: controlPlane</code>).</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> Some CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not.
See the <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNI network documentation</a>.
To add a pod CIDR pass the flag <code>--pod-network-cidr</code>, or if you are using a kubeadm configuration file
set the <code>podSubnet</code> field under the <code>networking</code> object of <code>ClusterConfiguration</code>.</div></blockquote><ul><li><p>The output looks similar to:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>...
You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:
    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div></li><li><p>Copy this output to a text file. You will need it later to join control plane and worker nodes to the cluster.</p></li><li><p>When <code>--upload-certs</code> is used with <code>kubeadm init</code>, the certificates of the primary control plane
are encrypted and uploaded in the <code>kubeadm-certs</code> Secret.</p></li><li><p>To re-upload the certificates and generate a new decryption key, use the following command on a control plane
node that is already joined to the cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm init phase upload-certs --upload-certs
</code></pre></div></li><li><p>You can also specify a custom <code>--certificate-key</code> during <code>init</code> that can later be used by <code>join</code>.
To generate such a key you can use the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubeadm certs certificate-key
</code></pre></div></li></ul><blockquote class="note callout"><div><strong>Note:</strong> The <code>kubeadm-certs</code> Secret and decryption key expire after two hours.</div></blockquote><blockquote class="caution callout"><div><strong>Caution:</strong> As stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!</div></blockquote></li><li><p>Apply the CNI plugin of your choice:
<a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>Follow these instructions</a>
to install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable.</p><p>In this example we are using Weave Net:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div></li><li><p>Type the following and watch the pods of the control plane components get started:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl get pod -n kube-system -w
</code></pre></div></li></ol><h3 id=steps-for-the-rest-of-the-control-plane-nodes>Steps for the rest of the control plane nodes</h3><blockquote class="note callout"><div><strong>Note:</strong> Since kubeadm version 1.15 you can join multiple control-plane nodes in parallel.
Prior to this version, you must join new control plane nodes sequentially, only after
the first node has finished initializing.</div></blockquote><p>For each additional control plane node you should:</p><ol><li><p>Execute the join command that was previously given to you by the <code>kubeadm init</code> output on the first node.
It should look something like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</code></pre></div><ul><li>The <code>--control-plane</code> flag tells <code>kubeadm join</code> to create a new control plane.</li><li>The <code>--certificate-key ...</code> will cause the control plane certificates to be downloaded
from the <code>kubeadm-certs</code> Secret in the cluster and be decrypted using the given key.</li></ul></li></ol><h2 id=external-etcd-nodes>External etcd nodes</h2><p>Setting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd
with the exception that you should setup etcd first, and you should pass the etcd information
in the kubeadm config file.</p><h3 id=set-up-the-etcd-cluster>Set up the etcd cluster</h3><ol><li><p>Follow <a href=/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>these instructions</a> to set up the etcd cluster.</p></li><li><p>Setup SSH as described <a href=#manual-certs>here</a>.</p></li><li><p>Copy the following files from any etcd node in the cluster to the first control plane node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</code></pre></div><ul><li>Replace the value of <code>CONTROL_PLANE</code> with the <code>user@host</code> of the first control-plane node.</li></ul></li></ol><h3 id=set-up-the-first-control-plane-node>Set up the first control plane node</h3><ol><li><p>Create a file called <code>kubeadm-config.yaml</code> with the following contents:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
</code></pre></li></ol><blockquote class="note callout"><div><strong>Note:</strong> The difference between stacked etcd and external etcd here is that the external etcd setup requires
a configuration file with the etcd endpoints under the <code>external</code> object for <code>etcd</code>.
In the case of the stacked etcd topology this is managed automatically.</div></blockquote><ul><li>Replace the following variables in the config template with the appropriate values for your cluster:</li></ul><pre><code>- `LOAD_BALANCER_DNS`
- `LOAD_BALANCER_PORT`
- `ETCD_0_IP`
- `ETCD_1_IP`
- `ETCD_2_IP`
</code></pre><p>The following steps are similar to the stacked etcd setup:</p><ol><li><p>Run <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> on this node.</p></li><li><p>Write the output join commands that are returned to a text file for later use.</p></li><li><p>Apply the CNI plugin of your choice. The given example is for Weave Net:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div></li></ol><h3 id=steps-for-the-rest-of-the-control-plane-nodes-1>Steps for the rest of the control plane nodes</h3><p>The steps are the same as for the stacked etcd setup:</p><ul><li>Make sure the first control plane node is fully initialized.</li><li>Join each control plane node with the join command you saved to a text file. It's recommended
to join the control plane nodes one at a time.</li><li>Don't forget that the decryption key from <code>--certificate-key</code> expires after two hours, by default.</li></ul><h2 id=common-tasks-after-bootstrapping-control-plane>Common tasks after bootstrapping control plane</h2><h3 id=install-workers>Install workers</h3><p>Worker nodes can be joined to the cluster with the command you stored previously
as the output from the <code>kubeadm init</code> command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div><h2 id=manual-certs>Manual certificate distribution</h2><p>If you choose to not use <code>kubeadm init</code> with the <code>--upload-certs</code> flag this means that
you are going to have to manually copy the certificates from the primary control plane node to the
joining control plane nodes.</p><p>There are many ways to do this. In the following example we are using <code>ssh</code> and <code>scp</code>:</p><p>SSH is required if you want to control all nodes from a single machine.</p><ol><li><p>Enable ssh-agent on your main device that has access to all other nodes in
the system:</p><pre><code>eval $(ssh-agent)
</code></pre></li><li><p>Add your SSH identity to the session:</p><pre><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>SSH between nodes to check that the connection is working correctly.</p><ul><li><p>When you SSH to any node, make sure to add the <code>-A</code> flag:</p><pre><code>ssh -A 10.0.0.7
</code></pre></li><li><p>When using sudo on any node, make sure to preserve the environment so SSH
forwarding works:</p><pre><code>sudo -E -s
</code></pre></li></ul></li><li><p>After configuring SSH on all the nodes you should run the following script on the first control plane node after
running <code>kubeadm init</code>. This script will copy the certificates from the first control plane node to the other
control plane nodes:</p><p>In the following example, replace <code>CONTROL_PLANE_IPS</code> with the IP addresses of the
other control plane nodes.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
<span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
<span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
    <span style=color:#080;font-style:italic># Quote this line if you are using external etcd</span>
    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div><blockquote class="caution callout"><div><strong>Caution:</strong> Copy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates
with the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,
the creation of additional nodes could fail due to a lack of required SANs.</div></blockquote></li><li><p>Then on each joining control plane node you have to run the following script before running <code>kubeadm join</code>.
This script will move the previously copied certificates from the home directory to <code>/etc/kubernetes/pki</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
<span style=color:#080;font-style:italic># Quote this line if you are using external etcd</span>
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>3.2.1.7 - Set up a High Availability etcd cluster with kubeadm</h1><blockquote class="note callout"><div><strong>Note:</strong> While kubeadm is being used as the management tool for external etcd nodes
in this guide, please note that kubeadm does not plan to support certificate rotation
or upgrades for such nodes. The long term plan is to empower the tool
<a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> to manage these
aspects.</div></blockquote><p>Kubeadm defaults to running a single member etcd cluster in a static pod managed
by the kubelet on the control plane node. This is not a high availability setup
as the etcd cluster contains only one member and cannot sustain any members
becoming unavailable. This task walks through the process of creating a high
availability etcd cluster of three members that can be used as an external etcd
when using kubeadm to set up a kubernetes cluster.</p><h2 id=before-you-begin>Before you begin</h2><ul><li>Three hosts that can talk to each other over ports 2379 and 2380. This
document assumes these default ports. However, they are configurable through
the kubeadm config file.</li><li>Each host must <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>have docker, kubelet, and kubeadm installed</a>.</li><li>Each host should have access to the Kubernetes container image registry (<code>k8s.gcr.io</code>) or list/pull the required etcd image using
<code>kubeadm config images list/pull</code>. This guide will setup etcd instances as
<a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a> managed by a kubelet.</li><li>Some infrastructure to copy files between hosts. For example <code>ssh</code> and <code>scp</code>
can satisfy this requirement.</li></ul><h2 id=setting-up-the-cluster>Setting up the cluster</h2><p>The general approach is to generate all certs on one node and only distribute
the <em>necessary</em> files to the other nodes.</p><blockquote class="note callout"><div><strong>Note:</strong> kubeadm contains all the necessary crytographic machinery to generate
the certificates described below; no other cryptographic tooling is required for
this example.</div></blockquote><ol><li><p>Configure the kubelet to be a service manager for etcd.</p><p><blockquote class="note callout"><div><strong>Note:</strong> You must do this on every host where etcd should be running.</div></blockquote>Since etcd was created first, you must override the service priority by creating a new unit file
that has higher precedence than the kubeadm-provided kubelet unit file.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span><span style=color:#b44>[Service]
</span><span style=color:#b44>ExecStart=
</span><span style=color:#b44>#  Replace &#34;systemd&#34; with the cgroup driver of your container runtime. The default value in the kubelet is &#34;cgroupfs&#34;.
</span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span><span style=color:#b44>Restart=always
</span><span style=color:#b44>EOF</span>

systemctl daemon-reload
systemctl restart kubelet
</code></pre></div><p>Check the kubelet status to ensure it is running.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>systemctl status kubelet
</code></pre></div></li><li><p>Create configuration files for kubeadm.</p><p>Generate one kubeadm configuration file for each host that will have an etcd
member running on it using the following script.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#080;font-style:italic># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8

<span style=color:#080;font-style:italic># Create temp directories to store files that will end up on other hosts.</span>
mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/

<span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
<span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>

<span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
<span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta2&#34;
</span><span style=color:#b44>kind: ClusterConfiguration
</span><span style=color:#b44>etcd:
</span><span style=color:#b44>    local:
</span><span style=color:#b44>        serverCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        peerCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        extraArgs:
</span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
</span><span style=color:#b44>            initial-cluster-state: new
</span><span style=color:#b44>            name: ${NAME}
</span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>EOF</span>
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div></li><li><p>Generate the certificate authority</p><p>If you already have a CA then the only action that is copying the CA's <code>crt</code> and
<code>key</code> file to <code>/etc/kubernetes/pki/etcd/ca.crt</code> and
<code>/etc/kubernetes/pki/etcd/ca.key</code>. After those files have been copied,
proceed to the next step, "Create certificates for each member".</p><p>If you do not already have a CA then run this command on <code>$HOST0</code> (where you
generated the configuration files for kubeadm).</p><pre><code>kubeadm init phase certs etcd-ca
</code></pre><p>This creates two files</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>Create certificates for each member</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
<span style=color:#080;font-style:italic># cleanup non-reusable certificates</span>
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
<span style=color:#080;font-style:italic># No need to move the certs because they are for HOST0</span>

<span style=color:#080;font-style:italic># clean up certs that should not be copied off this host</span>
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</code></pre></div></li><li><p>Copy certificates and kubeadm configs</p><p>The certificates have been generated and now they must be moved to their
respective hosts.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/
</code></pre></div></li><li><p>Ensure all expected files exist</p><p>The complete list of required files on <code>$HOST0</code> is:</p><pre><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST1</code>:</p><pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST2</code></p><pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>Create the static pod manifests</p><p>Now that the certificates and configs are in place it's time to create the
manifests. On each host run the <code>kubeadm</code> command to generate a static manifest
for etcd.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/home/ubuntu/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/home/ubuntu/kubeadmcfg.yaml
</code></pre></div></li><li><p>Optional: Check the cluster health</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
...
https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</code></pre></div><ul><li>Set <code>${ETCD_TAG}</code> to the version tag of your etcd image. For example <code>3.4.3-0</code>. To see the etcd image and tag that kubeadm uses execute <code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>, where <code>${K8S_VERSION}</code> is for example <code>v1.17.0</code></li><li>Set <code>${HOST0}</code>to the IP address of the host you are testing.</li></ul></li></ol><h2 id=what-s-next>What's next</h2><p>Once you have a working 3 member etcd cluster, you can continue setting up a
highly available control plane using the <a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>external etcd method with
kubeadm</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>3.2.1.8 - Configuring each kubelet in your cluster using kubeadm</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [stable]</code></div><p>The lifecycle of the kubeadm CLI tool is decoupled from the
<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet</a>, which is a daemon that runs
on each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is
initialized or upgraded, whereas the kubelet is always running in the background.</p><p>Since the kubelet is a daemon, it needs to be maintained by some kind of an init
system or service manager. When the kubelet is installed using DEBs or RPMs,
systemd is configured to manage the kubelet. You can use a different service
manager instead, but you need to configure it manually.</p><p>Some kubelet configuration details need to be the same across all kubelets involved in the cluster, while
other configuration aspects need to be set on a per-kubelet basis to accommodate the different
characteristics of a given machine (such as OS, storage, and networking). You can manage the configuration
of your kubelets manually, but kubeadm now provides a <code>KubeletConfiguration</code> API type for
<a href=#configure-kubelets-using-kubeadm>managing your kubelet configurations centrally</a>.</p><h2 id=kubelet-configuration-patterns>Kubelet configuration patterns</h2><p>The following sections describe patterns to kubelet configuration that are simplified by
using kubeadm, rather than managing the kubelet configuration for each Node manually.</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>Propagating cluster-level configuration to each kubelet</h3><p>You can provide the kubelet with default values to be used by <code>kubeadm init</code> and <code>kubeadm join</code>
commands. Interesting examples include using a different CRI runtime or setting the default subnet
used by services.</p><p>If you want your services to use the subnet <code>10.96.0.0/12</code> as the default for services, you can pass
the <code>--service-cidr</code> parameter to kubeadm:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init --service-cidr 10.96.0.0/12
</code></pre></div><p>Virtual IPs for services are now allocated from this subnet. You also need to set the DNS address used
by the kubelet, using the <code>--cluster-dns</code> flag. This setting needs to be the same for every kubelet
on every manager and Node in the cluster. The kubelet provides a versioned, structured API object
that can configure most parameters in the kubelet and push out this configuration to each running
kubelet in the cluster. This object is called
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>.
The <code>KubeletConfiguration</code> allows the user to specify flags such as the cluster DNS IP addresses expressed as
a list of values to a camelCased key, illustrated by the following example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></code></pre></div><p>For more details on the <code>KubeletConfiguration</code> have a look at <a href=#configure-kubelets-using-kubeadm>this section</a>.</p><h3 id=providing-instance-specific-configuration-details>Providing instance-specific configuration details</h3><p>Some hosts require specific kubelet configurations due to differences in hardware, operating system,
networking, or other host-specific parameters. The following list provides a few examples.</p><ul><li><p>The path to the DNS resolution file, as specified by the <code>--resolv-conf</code> kubelet
configuration flag, may differ among operating systems, or depending on whether you are using
<code>systemd-resolved</code>. If this path is wrong, DNS resolution will fail on the Node whose kubelet
is configured incorrectly.</p></li><li><p>The Node API object <code>.metadata.name</code> is set to the machine's hostname by default,
unless you are using a cloud provider. You can use the <code>--hostname-override</code> flag to override the
default behavior if you need to specify a Node name different from the machine's hostname.</p></li><li><p>Currently, the kubelet cannot automatically detect the cgroup driver used by the CRI runtime,
but the value of <code>--cgroup-driver</code> must match the cgroup driver used by the CRI runtime to ensure
the health of the kubelet.</p></li><li><p>Depending on the CRI runtime your cluster uses, you may need to specify different flags to the kubelet.
For instance, when using Docker, you need to specify flags such as <code>--network-plugin=cni</code>, but if you
are using an external runtime, you need to specify <code>--container-runtime=remote</code> and specify the CRI
endpoint using the <code>--container-runtime-endpoint=&lt;path></code>.</p></li></ul><p>You can specify these flags by configuring an individual kubelet's configuration in your service manager,
such as systemd.</p><h2 id=configure-kubelets-using-kubeadm>Configure kubelets using kubeadm</h2><p>It is possible to configure the kubelet that kubeadm will start if a custom <code>KubeletConfiguration</code>
API object is passed with a configuration file like so <code>kubeadm ... --config some-config-file.yaml</code>.</p><p>By calling <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code> you can
see all the default values for this structure.</p><p>Also have a look at the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/>reference for the KubeletConfiguration</a>
for more information on the individual fields.</p><h3 id=workflow-when-using-kubeadm-init>Workflow when using <code>kubeadm init</code></h3><p>When you call <code>kubeadm init</code>, the kubelet configuration is marshalled to disk
at <code>/var/lib/kubelet/config.yaml</code>, and also uploaded to a ConfigMap in the cluster. The ConfigMap
is named <code>kubelet-config-1.X</code>, where <code>X</code> is the minor version of the Kubernetes version you are
initializing. A kubelet configuration file is also written to <code>/etc/kubernetes/kubelet.conf</code> with the
baseline cluster-wide configuration for all kubelets in the cluster. This configuration file
points to the client certificates that allow the kubelet to communicate with the API server. This
addresses the need to
<a href=#propagating-cluster-level-configuration-to-each-kubelet>propagate cluster-level configuration to each kubelet</a>.</p><p>To address the second pattern of
<a href=#providing-instance-specific-configuration-details>providing instance-specific configuration details</a>,
kubeadm writes an environment file to <code>/var/lib/kubelet/kubeadm-flags.env</code>, which contains a list of
flags to pass to the kubelet when it starts. The flags are presented in the file like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</code></pre></div><p>In addition to the flags used when starting the kubelet, the file also contains dynamic
parameters such as the cgroup driver and whether to use a different CRI runtime socket
(<code>--cri-socket</code>).</p><p>After marshalling these two files to disk, kubeadm attempts to run the following two
commands, if you are using systemd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>If the reload and restart are successful, the normal <code>kubeadm init</code> workflow continues.</p><h3 id=workflow-when-using-kubeadm-join>Workflow when using <code>kubeadm join</code></h3><p>When you run <code>kubeadm join</code>, kubeadm uses the Bootstrap Token credential to perform
a TLS bootstrap, which fetches the credential needed to download the
<code>kubelet-config-1.X</code> ConfigMap and writes it to <code>/var/lib/kubelet/config.yaml</code>. The dynamic
environment file is generated in exactly the same way as <code>kubeadm init</code>.</p><p>Next, <code>kubeadm</code> runs the following two commands to load the new configuration into the kubelet:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>After the kubelet loads the new configuration, kubeadm writes the
<code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig file, which contains a CA certificate and Bootstrap
Token. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique
credential, which is stored in <code>/etc/kubernetes/kubelet.conf</code>. When this file is written, the kubelet
has finished performing the TLS Bootstrap.</p><h2 id=the-kubelet-drop-in-file-for-systemd>The kubelet drop-in file for systemd</h2><p><code>kubeadm</code> ships with configuration for how systemd should run the kubelet.
Note that the kubeadm CLI command never touches this drop-in file.</p><p>This configuration file installed by the <code>kubeadm</code>
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEB</a> or
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPM package</a> is written to
<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> and is used by systemd.
It augments the basic
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service><code>kubelet.service</code> for RPM</a> or
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service><code>kubelet.service</code> for DEB</a>:</p><pre><code class=language-none data-lang=none>[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;
# This is a file that &quot;kubeadm init&quot; and &quot;kubeadm join&quot; generate at runtime, populating
the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,
# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>This file specifies the default locations for all of the files managed by kubeadm for the kubelet.</p><ul><li>The KubeConfig file to use for the TLS Bootstrap is <code>/etc/kubernetes/bootstrap-kubelet.conf</code>,
but it is only used if <code>/etc/kubernetes/kubelet.conf</code> does not exist.</li><li>The KubeConfig file with the unique kubelet identity is <code>/etc/kubernetes/kubelet.conf</code>.</li><li>The file containing the kubelet's ComponentConfig is <code>/var/lib/kubelet/config.yaml</code>.</li><li>The dynamic environment file that contains <code>KUBELET_KUBEADM_ARGS</code> is sourced from <code>/var/lib/kubelet/kubeadm-flags.env</code>.</li><li>The file that can contain user-specified flag overrides with <code>KUBELET_EXTRA_ARGS</code> is sourced from
<code>/etc/default/kubelet</code> (for DEBs), or <code>/etc/sysconfig/kubelet</code> (for RPMs). <code>KUBELET_EXTRA_ARGS</code>
is last in the flag chain and has the highest priority in the event of conflicting settings.</li></ul><h2 id=kubernetes-binaries-and-package-contents>Kubernetes binaries and package contents</h2><p>The DEB and RPM packages shipped with the Kubernetes releases are:</p><table><thead><tr><th>Package name</th><th>Description</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td>Installs the <code>/usr/bin/kubeadm</code> CLI tool and the <a href=#the-kubelet-drop-in-file-for-systemd>kubelet drop-in file</a> for the kubelet.</td></tr><tr><td><code>kubelet</code></td><td>Installs the kubelet binary in <code>/usr/bin</code> and CNI binaries in <code>/opt/cni/bin</code>.</td></tr><tr><td><code>kubectl</code></td><td>Installs the <code>/usr/bin/kubectl</code> binary.</td></tr><tr><td><code>cri-tools</code></td><td>Installs the <code>/usr/bin/crictl</code> binary from the <a href=https://github.com/kubernetes-sigs/cri-tools>cri-tools git repository</a>.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ed857e09999827b013ee9062dc9c59bb>3.2.1.9 - Configuring your kubernetes cluster to self-host the control plane</h1><h3 id=self-hosting>Self-hosting the Kubernetes control plane</h3><p>kubeadm allows you to experimentally create a <em>self-hosted</em> Kubernetes control
plane. This means that key components such as the API server, controller
manager, and scheduler run as <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet pods</a>
configured via the Kubernetes API instead of <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>
configured in the kubelet via static files.</p><p>To create a self-hosted cluster see the
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-alpha/#cmd-selfhosting>kubeadm alpha selfhosting pivot</a> command.</p><h4 id=caveats>Caveats</h4><blockquote class="caution callout"><div><strong>Caution:</strong> This feature pivots your cluster into an unsupported state, rendering kubeadm unable
to manage you cluster any longer. This includes <code>kubeadm upgrade</code>.</div></blockquote><ol><li><p>Self-hosting in 1.8 and later has some important limitations. In particular, a
self-hosted cluster <em>cannot recover from a reboot of the control-plane node</em>
without manual intervention.</p></li><li><p>By default, self-hosted control plane Pods rely on credentials loaded from
<a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a>
volumes. Except for initial creation, these credentials are not managed by
kubeadm.</p></li><li><p>The self-hosted portion of the control plane does not include etcd,
which still runs as a static Pod.</p></li></ol><h4 id=process>Process</h4><p>The self-hosting bootstrap process is documented in the <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-self-hosting>kubeadm design
document</a>.</p><p>In summary, <code>kubeadm alpha selfhosting</code> works as follows:</p><ol><li><p>Waits for this bootstrap static control plane to be running and
healthy. This is identical to the <code>kubeadm init</code> process without self-hosting.</p></li><li><p>Uses the static control plane Pod manifests to construct a set of
DaemonSet manifests that will run the self-hosted control plane.
It also modifies these manifests where necessary, for example adding new volumes
for secrets.</p></li><li><p>Creates DaemonSets in the <code>kube-system</code> namespace and waits for the
resulting Pods to be running.</p></li><li><p>Once self-hosted Pods are operational, their associated static Pods are deleted
and kubeadm moves on to install the next component. This triggers kubelet to
stop those static Pods.</p></li><li><p>When the original static control plane stops, the new self-hosted control
plane is able to bind to listening ports and become active.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>3.2.2 - Installing Kubernetes with kops</h1><p>This quickstart shows you how to easily install a Kubernetes cluster on AWS.
It uses a tool called <a href=https://github.com/kubernetes/kops><code>kops</code></a>.</p><p>kops is an automated provisioning system:</p><ul><li>Fully automated installation</li><li>Uses DNS to identify clusters</li><li>Self-healing: everything runs in Auto-Scaling Groups</li><li>Multiple OS support (Debian, Ubuntu 16.04 supported, CentOS & RHEL, Amazon Linux and CoreOS) - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a></li><li>High-Availability support - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/high_availability.md>high_availability.md</a></li><li>Can directly provision, or generate terraform manifests - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a></li></ul><h2 id=before-you-begin>Before you begin</h2><ul><li><p>You must have <a href=/docs/tasks/tools/>kubectl</a> installed.</p></li><li><p>You must <a href=https://github.com/kubernetes/kops#installing>install</a> <code>kops</code> on a 64-bit (AMD64 and Intel 64) device architecture.</p></li><li><p>You must have an <a href=https://docs.aws.amazon.com/polly/latest/dg/setting-up.html>AWS account</a>, generate <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>IAM keys</a> and <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration>configure</a> them. The IAM user will need <a href=https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user>adequate permissions</a>.</p></li></ul><h2 id=creating-a-cluster>Creating a cluster</h2><h3 id=1-5-install-kops>(1/5) Install kops</h3><h4 id=installation>Installation</h4><p>Download kops from the <a href=https://github.com/kubernetes/kops/releases>releases page</a> (it is also convenient to build from source):</p><ul class="nav nav-tabs" id=kops-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#kops-installation-0 role=tab aria-controls=kops-installation-0 aria-selected=true>macOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#kops-installation-1 role=tab aria-controls=kops-installation-1>Linux</a></li></ul><div class=tab-content id=kops-installation><div id=kops-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=kops-installation-0><p><p>Download the latest release with the command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-darwin-amd64
</code></pre></div><p>To download a specific version, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div><p>For example, to download kops version v1.15.0 type:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO  https://github.com/kubernetes/kops/releases/download/1.15.0/kops-darwin-amd64
</code></pre></div><p>Make the kops binary executable.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>chmod +x kops-darwin-amd64
</code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mv kops-darwin-amd64 /usr/local/bin/kops
</code></pre></div><p>You can also install kops using <a href=https://brew.sh/>Homebrew</a>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</code></pre></div></div><div id=kops-installation-1 class=tab-pane role=tabpanel aria-labelledby=kops-installation-1><p><p>Download the latest release with the command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-linux-amd64
</code></pre></div><p>To download a specific version of kops, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</code></pre></div><p>For example, to download kops version v1.15.0 type:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO  https://github.com/kubernetes/kops/releases/download/1.15.0/kops-linux-amd64
</code></pre></div><p>Make the kops binary executable</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>chmod +x kops-linux-amd64
</code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mv kops-linux-amd64 /usr/local/bin/kops
</code></pre></div><p>You can also install kops using <a href=https://docs.brew.sh/Homebrew-on-Linux>Homebrew</a>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</code></pre></div></div></div><h3 id=2-5-create-a-route53-domain-for-your-cluster>(2/5) Create a route53 domain for your cluster</h3><p>kops uses DNS for discovery, both inside the cluster and outside, so that you can reach the kubernetes API server
from clients.</p><p>kops has a strong opinion on the cluster name: it should be a valid DNS name. By doing so you will
no longer get your clusters confused, you can share clusters with your colleagues unambiguously,
and you can reach them without relying on remembering an IP address.</p><p>You can, and probably should, use subdomains to divide your clusters. As our example we will use
<code>useast1.dev.example.com</code>. The API server endpoint will then be <code>api.useast1.dev.example.com</code>.</p><p>A Route53 hosted zone can serve subdomains. Your hosted zone could be <code>useast1.dev.example.com</code>,
but also <code>dev.example.com</code> or even <code>example.com</code>. kops works with any of these, so typically
you choose for organization reasons (e.g. you are allowed to create records under <code>dev.example.com</code>,
but not under <code>example.com</code>).</p><p>Let's assume you're using <code>dev.example.com</code> as your hosted zone. You create that hosted zone using
the <a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>normal process</a>, or
with a command such as <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>.</p><p>You must then set up your NS records in the parent domain, so that records in the domain will resolve. Here,
you would create NS records in <code>example.com</code> for <code>dev</code>. If it is a root domain name you would configure the NS
records at your domain registrar (e.g. <code>example.com</code> would need to be configured where you bought <code>example.com</code>).</p><p>Verify your route53 domain setup (it is the #1 cause of problems!). You can double-check that
your cluster is configured correctly if you have the dig tool by running:</p><p><code>dig NS dev.example.com</code></p><p>You should see the 4 NS records that Route53 assigned your hosted zone.</p><h3 id=3-5-create-an-s3-bucket-to-store-your-clusters-state>(3/5) Create an S3 bucket to store your clusters state</h3><p>kops lets you manage your clusters even after installation. To do this, it must keep track of the clusters
that you have created, along with their configuration, the keys they are using etc. This information is stored
in an S3 bucket. S3 permissions are used to control access to the bucket.</p><p>Multiple clusters can use the same S3 bucket, and you can share an S3 bucket between your colleagues that
administer the same clusters - this is much easier than passing around kubecfg files. But anyone with access
to the S3 bucket will have administrative access to all your clusters, so you don't want to share it beyond
the operations team.</p><p>So typically you have one S3 bucket for each ops team (and often the name will correspond
to the name of the hosted zone above!)</p><p>In our example, we chose <code>dev.example.com</code> as our hosted zone, so let's pick <code>clusters.dev.example.com</code> as
the S3 bucket name.</p><ul><li><p>Export <code>AWS_PROFILE</code> (if you need to select a profile for the AWS CLI to work)</p></li><li><p>Create the S3 bucket using <code>aws s3 mb s3://clusters.dev.example.com</code></p></li><li><p>You can <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> and then kops will use this location by default.
We suggest putting this in your bash profile or similar.</p></li></ul><h3 id=4-5-build-your-cluster-configuration>(4/5) Build your cluster configuration</h3><p>Run <code>kops create cluster</code> to create your cluster configuration:</p><p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p><p>kops will create the configuration for your cluster. Note that it <em>only</em> creates the configuration, it does
not actually create the cloud resources - you'll do that in the next step with a <code>kops update cluster</code>. This
give you an opportunity to review the configuration or change it.</p><p>It prints commands you can use to explore further:</p><ul><li>List your clusters with: <code>kops get cluster</code></li><li>Edit this cluster with: <code>kops edit cluster useast1.dev.example.com</code></li><li>Edit your node instance group: <code>kops edit ig --name=useast1.dev.example.com nodes</code></li><li>Edit your master instance group: <code>kops edit ig --name=useast1.dev.example.com master-us-east-1c</code></li></ul><p>If this is your first time using kops, do spend a few minutes to try those out! An instance group is a
set of instances, which will be registered as kubernetes nodes. On AWS this is implemented via auto-scaling-groups.
You can have several instance groups, for example if you wanted nodes that are a mix of spot and on-demand instances, or
GPU and non-GPU instances.</p><h3 id=5-5-create-the-cluster-in-aws>(5/5) Create the cluster in AWS</h3><p>Run "kops update cluster" to create your cluster in AWS:</p><p><code>kops update cluster useast1.dev.example.com --yes</code></p><p>That takes a few seconds to run, but then your cluster will likely take a few minutes to actually be ready.
<code>kops update cluster</code> will be the tool you'll use whenever you change the configuration of your cluster; it
applies the changes you have made to the configuration to your cluster - reconfiguring AWS or kubernetes as needed.</p><p>For example, after you <code>kops edit ig nodes</code>, then <code>kops update cluster --yes</code> to apply your configuration, and
sometimes you will also have to <code>kops rolling-update cluster</code> to roll out the configuration immediately.</p><p>Without <code>--yes</code>, <code>kops update cluster</code> will show you a preview of what it is going to do. This is handy
for production clusters!</p><h3 id=explore-other-add-ons>Explore other add-ons</h3><p>See the <a href=/docs/concepts/cluster-administration/addons/>list of add-ons</a> to explore other add-ons, including tools for logging, monitoring, network policy, visualization, and control of your Kubernetes cluster.</p><h2 id=cleanup>Cleanup</h2><ul><li>To delete your cluster: <code>kops delete cluster useast1.dev.example.com --yes</code></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn more about Kubernetes <a href=/docs/concepts/>concepts</a> and <a href=/docs/reference/kubectl/overview/><code>kubectl</code></a>.</li><li>Learn more about <code>kops</code> <a href=https://kops.sigs.k8s.io/>advanced usage</a> for tutorials, best practices and advanced configuration options.</li><li>Follow <code>kops</code> community discussions on Slack: <a href=https://github.com/kubernetes/kops#other-ways-to-communicate-with-the-contributors>community discussions</a></li><li>Contribute to <code>kops</code> by addressing or raising an issue <a href=https://github.com/kubernetes/kops/issues>GitHub Issues</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b4964187fe973644e06ee629eff1de>3.2.3 - Installing Kubernetes with Kubespray</h1><p>This quickstart helps to install a Kubernetes cluster hosted on GCE, Azure, OpenStack, AWS, vSphere, Packet (bare metal), Oracle Cloud Infrastructure (Experimental) or Baremetal with <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>.</p><p>Kubespray is a composition of <a href=https://docs.ansible.com/>Ansible</a> playbooks, <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md>inventory</a>, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Kubespray provides:</p><ul><li>a highly available cluster</li><li>composable attributes</li><li>support for most popular Linux distributions<ul><li>Ubuntu 16.04, 18.04, 20.04</li><li>CentOS/RHEL/Oracle Linux 7, 8</li><li>Debian Buster, Jessie, Stretch, Wheezy</li><li>Fedora 31, 32</li><li>Fedora CoreOS</li><li>openSUSE Leap 15</li><li>Flatcar Container Linux by Kinvolk</li></ul></li><li>continuous integration tests</li></ul><p>To choose a tool which best fits your use case, read <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>this comparison</a> to
<a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a> and <a href=/docs/setup/production-environment/tools/kops/>kops</a>.</p><h2 id=creating-a-cluster>Creating a cluster</h2><h3 id=1-5-meet-the-underlay-requirements>(1/5) Meet the underlay requirements</h3><p>Provision servers with the following <a href=https://github.com/kubernetes-sigs/kubespray#requirements>requirements</a>:</p><ul><li><strong>Ansible v2.9 and python-netaddr is installed on the machine that will run Ansible commands</strong></li><li><strong>Jinja 2.11 (or newer) is required to run the Ansible Playbooks</strong></li><li>The target servers must have access to the Internet in order to pull docker images. Otherwise, additional configuration is required (<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>See Offline Environment</a>)</li><li>The target servers are configured to allow <strong>IPv4 forwarding</strong></li><li><strong>Your ssh key must be copied</strong> to all the servers part of your inventory</li><li>The <strong>firewalls are not managed</strong>, you'll need to implement your own rules the way you used to. in order to avoid any issue during deployment you should disable your firewall</li><li>If kubespray is ran from non-root user account, correct privilege escalation method should be configured in the target servers. Then the <code>ansible_become</code> flag or command parameters <code>--become</code> or <code>-b</code> should be specified</li></ul><p>Kubespray provides the following utilities to help provision your environment:</p><ul><li><a href=https://www.terraform.io/>Terraform</a> scripts for the following cloud providers:<ul><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack>OpenStack</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet>Packet</a></li></ul></li></ul><h3 id=2-5-compose-an-inventory-file>(2/5) Compose an inventory file</h3><p>After you provision your servers, create an <a href=https://docs.ansible.com/ansible/intro_inventory.html>inventory file for Ansible</a>. You can do this manually or via a dynamic inventory script. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>Building your own inventory</a>".</p><h3 id=3-5-plan-your-cluster-deployment>(3/5) Plan your cluster deployment</h3><p>Kubespray provides the ability to customize many aspects of the deployment:</p><ul><li>Choice deployment mode: kubeadm or non-kubeadm</li><li>CNI (networking) plugins</li><li>DNS configuration</li><li>Choice of control plane: native/binary or containerized</li><li>Component versions</li><li>Calico route reflectors</li><li>Component runtime options<ul><li><a class=glossary-tooltip title="Docker is a software technology providing operating-system-level virtualization also known as containers." data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a></li><li><a class=glossary-tooltip title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li><li><a class=glossary-tooltip title="A lightweight container runtime specifically for Kubernetes" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li></ul></li><li>Certificate generation methods</li></ul><p>Kubespray customizations can be made to a <a href=https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html>variable file</a>. If you are getting started with Kubespray, consider using the Kubespray defaults to deploy your cluster and explore Kubernetes.</p><h3 id=4-5-deploy-a-cluster>(4/5) Deploy a Cluster</h3><p>Next, deploy your cluster:</p><p>Cluster deployment using <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</code></pre></div><p>Large deployments (100+ nodes) may require <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>specific adjustments</a> for best results.</p><h3 id=5-5-verify-the-deployment>(5/5) Verify the deployment</h3><p>Kubespray provides a way to verify inter-pod connectivity and DNS resolve with <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>. Netchecker ensures the netchecker-agents pods can resolve DNS requests and ping each over within the default namespace. Those pods mimic similar behavior of the rest of the workloads and serve as cluster health indicators.</p><h2 id=cluster-operations>Cluster operations</h2><p>Kubespray provides additional playbooks to manage your cluster: <em>scale</em> and <em>upgrade</em>.</p><h3 id=scale-your-cluster>Scale your cluster</h3><p>You can add worker nodes from your cluster by running the scale playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>Adding nodes</a>".
You can remove worker nodes from your cluster by running the remove-node playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>Remove nodes</a>".</p><h3 id=upgrade-your-cluster>Upgrade your cluster</h3><p>You can upgrade your cluster by running the upgrade-cluster playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>Upgrades</a>".</p><h2 id=cleanup>Cleanup</h2><p>You can reset your nodes and wipe out all components installed with Kubespray via the <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset playbook</a>.</p><blockquote class="caution callout"><div><strong>Caution:</strong> When running the reset playbook, be sure not to accidentally target your production cluster!</div></blockquote><h2 id=feedback>Feedback</h2><ul><li>Slack Channel: <a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a> (You can get your invite <a href=https://slack.k8s.io/>here</a>)</li><li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub Issues</a></li></ul><h2 id=what-s-next>What's next</h2><p>Check out planned work on Kubespray's <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>roadmap</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d2f55eefe7222b7c637875af9c3ec199>3.3 - Turnkey Cloud Solutions</h1><p>This page provides a list of Kubernetes certified solution providers. From each
provider page, you can learn how to install and setup production
ready clusters.</p><script>function updateLandscapeSource(a,b){console.log({button:a,shouldUpdateFragment:b});try{if(b)window.location.hash="#"+a.id;else{var c=document.querySelectorAll("#landscape");let b=a.dataset.landscapeTypes,d="https://landscape.cncf.io/card-mode?category="+encodeURIComponent(b)+"&grouping=category&embed=yes";c[0].src=d}}catch(a){console.log({message:"error handling Landscape switch",error:a})}}document.addEventListener("DOMContentLoaded",function(){var c,a;let b=()=>{if(window.location.hash){let a=document.querySelectorAll(".landscape-trigger"+window.location.hash);a.length==1&&(landscapeSource=a[0],console.log("Updating Landscape source based on fragment:",window.location.hash.substring(1)),updateLandscapeSource(landscapeSource,!1))}};if(c=document.querySelectorAll(".landscape-trigger"),c.forEach(a=>{a.onclick=function(){updateLandscapeSource(a,!0)}}),a=document.querySelectorAll(".landscape-trigger.landscape-default"),a.length==1){let b=a[0];updateLandscapeSource(b,!1)}window.addEventListener("hashchange",b,!1),b()})</script><div id=frameHolder><iframe frameborder=0 id=landscape scrolling=no src="https://landscape.cncf.io/card-mode?category=certified-kubernetes-hosted&grouping=category&embed=yes" style=width:1px;min-width:100%></iframe>
<script src=https://landscape.cncf.io/iframeResizer.js></script></div></div><div class=td-content style=page-break-before:always><h1 id=pg-acce7e24090fea04715a7a516ba3e69b>3.4 - Windows in Kubernetes</h1></div><div class=td-content><h1 id=pg-a307d413f1f7430fced233023087e2a1>3.4.1 - Intro to Windows support in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that run in many organizations. <a href=https://aka.ms/windowscontainers>Windows containers</a> provide a modern way to encapsulate processes and package dependencies, making it easier to use DevOps practices and follow cloud native patterns for Windows applications. Kubernetes has become the defacto standard container orchestrator, and the release of Kubernetes 1.14 includes production support for scheduling Windows containers on Windows nodes in a Kubernetes cluster, enabling a vast ecosystem of Windows applications to leverage the power of Kubernetes. Organizations with investments in Windows-based applications and Linux-based applications don't have to look for separate orchestrators to manage their workloads, leading to increased operational efficiencies across their deployments, regardless of operating system.</p><h2 id=windows-containers-in-kubernetes>Windows containers in Kubernetes</h2><p>To enable the orchestration of Windows containers in Kubernetes, include Windows nodes in your existing Linux cluster. Scheduling Windows containers in <a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> on Kubernetes is similar to scheduling Linux-based containers.</p><p>In order to run Windows containers, your Kubernetes cluster must include multiple operating systems, with control plane nodes running Linux and workers running either Windows or Linux depending on your workload needs. Windows Server 2019 is the only Windows operating system supported, enabling <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Kubernetes Node</a> on Windows (including kubelet, <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd>container runtime</a>, and kube-proxy). For a detailed explanation of Windows distribution channels see the <a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Microsoft documentation</a>.</p><blockquote class="note callout"><div><strong>Note:</strong> The Kubernetes control plane, including the <a href=/docs/concepts/overview/components/>master components</a>, continues to run on Linux. There are no plans to have a Windows-only Kubernetes cluster.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> In this document, when we talk about Windows containers we mean Windows containers with process isolation. Windows containers with <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container>Hyper-V isolation</a> is planned for a future release.</div></blockquote><h2 id=supported-functionality-and-limitations>Supported Functionality and Limitations</h2><h3 id=supported-functionality>Supported Functionality</h3><h4 id=windows-os-version-support>Windows OS Version Support</h4><p>Refer to the following table for Windows operating system support in Kubernetes. A single heterogeneous Kubernetes cluster can have both Windows and Linux worker nodes. Windows containers have to be scheduled on Windows nodes and Linux containers on Linux nodes.</p><table><thead><tr><th>Kubernetes version</th><th>Windows Server LTSC releases</th><th>Windows Server SAC releases</th><th></th></tr></thead><tbody><tr><td><em>Kubernetes v1.17</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809</td><td></td></tr><tr><td><em>Kubernetes v1.18</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809, Windows Server ver 1903, Windows Server ver 1909</td><td></td></tr><tr><td><em>Kubernetes v1.19</em></td><td>Windows Server 2019</td><td>Windows Server ver 1909, Windows Server ver 2004</td><td></td></tr><tr><td><em>Kubernetes v1.20</em></td><td>Windows Server 2019</td><td>Windows Server ver 1909, Windows Server ver 2004</td><td></td></tr></tbody></table><p><blockquote class="note callout"><div><strong>Note:</strong> Information on the different Windows Server servicing channels including their support models can be found at <a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Windows Server servicing channels</a>.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> We don't expect all Windows customers to update the operating system for their apps frequently. Upgrading your applications is what dictates and necessitates upgrading or introducing new nodes to the cluster. For the customers that chose to upgrade their operating system for containers running on Kubernetes, we will offer guidance and step-by-step instructions when we add support for a new operating system version. This guidance will include recommended upgrade procedures for upgrading user applications together with cluster nodes. Windows nodes adhere to Kubernetes <a href=/docs/setup/release/version-skew-policy/>version-skew policy</a> (node to control plane versioning) the same way as Linux nodes do today.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> The Windows Server Host Operating System is subject to the <a href=https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing>Windows Server</a> licensing. The Windows Container images are subject to the <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula>Supplemental License Terms for Windows containers</a>.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> Windows containers with process isolation have strict compatibility rules, <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility>where the host OS version must match the container base image OS version</a>. Once we support Windows containers with Hyper-V isolation in Kubernetes, the limitation and compatibility rules will change.</div></blockquote></p><h4 id=pause-image>Pause Image</h4><p>Microsoft maintains a Windows pause infrastructure container at <code>mcr.microsoft.com/oss/kubernetes/pause:1.4.1</code>.</p><h4 id=compute>Compute</h4><p>From an API and kubectl perspective, Windows containers behave in much the same way as Linux-based containers. However, there are some notable differences in key functionality which are outlined in the <a href=#limitations>limitation section</a>.</p><p>Key Kubernetes elements work the same way in Windows as they do in Linux. In this section, we talk about some of the key workload enablers and how they map to Windows.</p><ul><li><p><a href=/docs/concepts/workloads/pods/>Pods</a></p><p>A Pod is the basic building block of Kubernetes–the smallest and simplest unit in the Kubernetes object model that you create or deploy. You may not deploy Windows and Linux containers in the same Pod. All containers in a Pod are scheduled onto a single Node where each Node represents a specific platform and architecture. The following Pod capabilities, properties and events are supported with Windows containers:</p><ul><li>Single or multiple containers per Pod with process isolation and volume sharing</li><li>Pod status fields</li><li>Readiness and Liveness probes</li><li>postStart & preStop container lifecycle events</li><li>ConfigMap, Secrets: as environment variables or volumes</li><li>EmptyDir</li><li>Named pipe host mounts</li><li>Resource limits</li></ul></li><li><p><a href=/docs/concepts/workloads/controllers/>Controllers</a></p><p>Kubernetes controllers handle the desired state of Pods. The following workload controllers are supported with Windows containers:</p><ul><li>ReplicaSet</li><li>ReplicationController</li><li>Deployments</li><li>StatefulSets</li><li>DaemonSet</li><li>Job</li><li>CronJob</li></ul></li><li><p><a href=/docs/concepts/services-networking/service/>Services</a></p><p>A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. You can use services for cross-operating system connectivity. In Windows, services can utilize the following types, properties and capabilities:</p><ul><li>Service Environment variables</li><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li><li>Headless services</li></ul></li></ul><p>Pods, Controllers and Services are critical elements to managing Windows workloads on Kubernetes. However, on their own they are not enough to enable the proper lifecycle management of Windows workloads in a dynamic cloud native environment. We added support for the following features:</p><ul><li>Pod and container metrics</li><li>Horizontal Pod Autoscaler support</li><li>kubectl Exec</li><li>Resource Quotas</li><li>Scheduler preemption</li></ul><h4 id=container-runtime>Container Runtime</h4><h5 id=docker-ee>Docker EE</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Docker EE-basic 19.03+ is the recommended container runtime for all Windows Server versions. This works with the dockershim code included in the kubelet.</p><h5 id=cri-containerd>CRI-ContainerD</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p><a class=glossary-tooltip title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=ContainerD>ContainerD</a> 1.4.0+ can also be used as the container runtime for Windows Kubernetes nodes.</p><p>Learn how to <a href=/docs/setup/production-environment/container-runtimes/#install-containerd>install ContainerD on a Windows</a>.</p><blockquote class="caution callout"><div><strong>Caution:</strong> There is a <a href=/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations>known limitation</a> when using GMSA with ContainerD to access Windows network shares which requires a kernel patch. Updates to address this limitation are currently available for Windows Server, Version 2004 and will be available for Windows Server 2019 in early 2021. Check for updates on the <a href=https://github.com/microsoft/Windows-Containers/issues/44>Microsoft Windows Containers issue tracker</a>.</div></blockquote><h4 id=persistent-storage>Persistent Storage</h4><p>Kubernetes <a href=/docs/concepts/storage/volumes/>volumes</a> enable complex applications, with data persistence and Pod volume sharing requirements, to be deployed on Kubernetes. Management of persistent volumes associated with a specific storage back-end or protocol includes actions such as: provisioning/de-provisioning/resizing of volumes, attaching/detaching a volume to/from a Kubernetes node and mounting/dismounting a volume to/from individual containers in a pod that needs to persist data. The code implementing these volume management actions for a specific storage back-end or protocol is shipped in the form of a Kubernetes volume <a href=/docs/concepts/storage/volumes/#types-of-volumes>plugin</a>. The following broad classes of Kubernetes volume plugins are supported on Windows:</p><h5 id=in-tree-volume-plugins>In-tree Volume Plugins</h5><p>Code associated with in-tree volume plugins ship as part of the core Kubernetes code base. Deployment of in-tree volume plugins do not require installation of additional scripts or deployment of separate containerized plugin components. These plugins can handle: provisioning/de-provisioning and resizing of volumes in the storage backend, attaching/detaching of volumes to/from a Kubernetes node and mounting/dismounting a volume to/from individual containers in a pod. The following in-tree plugins support Windows nodes:</p><ul><li><a href=/docs/concepts/storage/volumes/#awselasticblockstore>awsElasticBlockStore</a></li><li><a href=/docs/concepts/storage/volumes/#azuredisk>azureDisk</a></li><li><a href=/docs/concepts/storage/volumes/#azurefile>azureFile</a></li><li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk>gcePersistentDisk</a></li><li><a href=/docs/concepts/storage/volumes/#vspherevolume>vsphereVolume</a></li></ul><h5 id=flexvolume-plugins>FlexVolume Plugins</h5><p>Code associated with <a href=/docs/concepts/storage/volumes/#flexVolume>FlexVolume</a> plugins ship as out-of-tree scripts or binaries that need to be deployed directly on the host. FlexVolume plugins handle attaching/detaching of volumes to/from a Kubernetes node and mounting/dismounting a volume to/from individual containers in a pod. Provisioning/De-provisioning of persistent volumes associated with FlexVolume plugins may be handled through an external provisioner that is typically separate from the FlexVolume plugins. The following FlexVolume <a href=https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows>plugins</a>, deployed as powershell scripts on the host, support Windows nodes:</p><ul><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd>SMB</a></li><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd>iSCSI</a></li></ul><h5 id=csi-plugins>CSI Plugins</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><p>Code associated with <a class=glossary-tooltip title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> plugins ship as out-of-tree scripts and binaries that are typically distributed as container images and deployed using standard Kubernetes constructs like DaemonSets and StatefulSets. CSI plugins handle a wide range of volume management actions in Kubernetes: provisioning/de-provisioning/resizing of volumes, attaching/detaching of volumes to/from a Kubernetes node and mounting/dismounting a volume to/from individual containers in a pod, backup/restore of persistent data using snapshots and cloning. CSI plugins typically consist of node plugins (that run on each node as a DaemonSet) and controller plugins.</p><p>CSI node plugins (especially those associated with persistent volumes exposed as either block devices or over a shared file-system) need to perform various privileged operations like scanning of disk devices, mounting of file systems, etc. These operations differ for each host operating system. For Linux worker nodes, containerized CSI node plugins are typically deployed as privileged containers. For Windows worker nodes, privileged operations for containerized CSI node plugins is supported using <a href=https://github.com/kubernetes-csi/csi-proxy>csi-proxy</a>, a community-managed, stand-alone binary that needs to be pre-installed on each Windows node. Please refer to the deployment guide of the CSI plugin you wish to deploy for further details.</p><h4 id=networking>Networking</h4><p>Networking for Windows containers is exposed through <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNI plugins</a>. Windows containers function similarly to virtual machines in regards to networking. Each container has a virtual network adapter (vNIC) which is connected to a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the Host Compute Service (HCS) work together to create containers and attach container vNICs to networks. HCS is responsible for the management of containers whereas HNS is responsible for the management of networking resources such as:</p><ul><li>Virtual networks (including creation of vSwitches)</li><li>Endpoints / vNICs</li><li>Namespaces</li><li>Policies (Packet encapsulations, Load-balancing rules, ACLs, NAT'ing rules, etc.)</li></ul><p>The following service spec types are supported:</p><ul><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li></ul><h5 id=network-modes>Network modes</h5><p>Windows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay, Transparent, and NAT. In a heterogeneous cluster with Windows and Linux worker nodes, you need to select a networking solution that is compatible on both Windows and Linux. The following out-of-tree plugins are supported on Windows, with recommendations on when to use each CNI:</p><table><thead><tr><th>Network Driver</th><th>Description</th><th>Container Packet Modifications</th><th>Network Plugins</th><th>Network Plugin Characteristics</th></tr></thead><tbody><tr><td>L2bridge</td><td>Containers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.</td><td>MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge>win-bridge</a>, <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a>, Flannel host-gateway uses win-bridge</td><td>win-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.</td></tr><tr><td>L2Tunnel</td><td>This is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.</td><td>MAC rewritten, IP visible on the underlay network</td><td><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a></td><td>Azure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that <a href=https://azure.microsoft.com/en-us/services/virtual-network/>Azure Virtual Network provides</a>. For example, securely connect to Azure services or use Azure NSGs. See <a href=https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking>azure-cni for some examples</a></td></tr><tr><td>Overlay (Overlay networking for Windows in Kubernetes is in <em>alpha</em> stage)</td><td>Containers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.</td><td>Encapsulated with an outer header.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay>Win-overlay</a>, Flannel VXLAN (uses win-overlay)</td><td>win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires <a href=https://support.microsoft.com/help/4489899>KB4489899</a> on Windows Server 2019.</td></tr><tr><td>Transparent (special use case for <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>)</td><td>Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).</td><td>Packet is encapsulated either via <a href=https://datatracker.ietf.org/doc/draft-gross-geneve/>GENEVE</a> or <a href=https://datatracker.ietf.org/doc/draft-davie-stt/>STT</a> tunneling to reach pods which are not on the same host.<br>Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.<br>NAT is done for north-south communication.</td><td><a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a></td><td><a href=https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib>Deploy via ansible</a>. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.</td></tr><tr><td>NAT (<em>not used in Kubernetes</em>)</td><td>Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called <a href=https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/>WinNAT</a></td><td>MAC and IP is rewritten to host MAC/IP.</td><td><a href=https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat>nat</a></td><td>Included here for completeness</td></tr></tbody></table><p>As outlined above, the <a href=https://github.com/coreos/flannel>Flannel</a> CNI <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>meta plugin</a> is also supported on <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental>Windows</a> via the <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLAN network backend</a> (<strong>alpha support</strong> ; delegates to win-overlay) and <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw>host-gateway network backend</a> (stable support; delegates to win-bridge). This plugin supports delegating to one of the reference CNI plugins (win-overlay, win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for automatic node subnet lease assignment and HNS network creation. This plugin reads in its own configuration file (cni.conf), and aggregates it with the environment variables from the FlannelD generated subnet.env file. It then delegates to one of the reference CNI plugins for network plumbing, and sends the correct configuration containing the node-assigned subnet to the IPAM plugin (e.g. host-local).</p><p>For the node, pod, and service objects, the following network flows are supported for TCP/UDP traffic:</p><ul><li>Pod -> Pod (IP)</li><li>Pod -> Pod (Name)</li><li>Pod -> Service (Cluster IP)</li><li>Pod -> Service (PQDN, but only if there are no ".")</li><li>Pod -> Service (FQDN)</li><li>Pod -> External (IP)</li><li>Pod -> External (DNS)</li><li>Node -> Pod</li><li>Pod -> Node</li></ul><h5 id=ipam>IP address management (IPAM)</h5><p>The following IPAM options are supported on Windows:</p><ul><li><a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local>Host-local</a></li><li>HNS IPAM (Inbox platform IPAM, this is a fallback when no IPAM is set)</li><li><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md>Azure-vnet-ipam</a> (for azure-cni only)</li></ul><h5 id=load-balancing-and-services>Load balancing and Services</h5><p>On Windows, you can use the following settings to configure Services and load balancing behavior:</p><table><caption style=display:none>Windows Service Settings</caption><thead><tr><th>Feature</th><th>Description</th><th>Supported Kubernetes version</th><th>Supported Windows OS build</th><th>How to enable</th></tr></thead><tbody><tr><td>Session affinity</td><td>Ensures that connections from a particular client are passed to the same Pod each time.</td><td>v1.19+</td><td><a href=https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/>Windows Server vNext Insider Preview Build 19551</a> (or higher)</td><td>Set <code>service.spec.sessionAffinity</code> to "ClientIP"</td></tr><tr><td>Direct Server Return</td><td>Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives with the source IP set as the originating pod IP. Promises lower latency and scalability.</td><td>v1.15+</td><td>Windows Server, version 2004</td><td>Set the following flags in kube-proxy: <code>--feature-gates="WinDSR=true" --enable-dsr=true</code></td></tr><tr><td>Preserve-Destination</td><td>Skips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. This setting will also ensure that the client IP of incoming packets get preserved.</td><td>v1.15+</td><td>Windows Server, version 1903 (or higher)</td><td>Set <code>"preserve-destination": "true"</code> in service annotations and enable DSR flags in kube-proxy.</td></tr><tr><td>IPv4/IPv6 dual-stack networking</td><td>Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a cluster</td><td>v1.19+</td><td>Windows Server vNext Insider Preview Build 19603 (or higher)</td><td>See <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack</a></td></tr></tbody></table><h4 id=ipv4-ipv6-dual-stack>IPv4/IPv6 dual-stack</h4><p>You can enable IPv4/IPv6 dual-stack networking for <code>l2bridge</code> networks using the <code>IPv6DualStack</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>. See <a href=/docs/concepts/services-networking/dual-stack#enable-ipv4ipv6-dual-stack>enable IPv4/IPv6 dual stack</a> for more details.</p><blockquote class="note callout"><div><strong>Note:</strong> On Windows, using IPv6 with Kubernetes require Windows Server, version 2004 (kernel version 10.0.19041.610) or later.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong> Overlay (VXLAN) networks on Windows do not support dual-stack networking today.</div></blockquote><h3 id=limitations>Limitations</h3><p>Windows is only supported as a worker node in the Kubernetes architecture and component matrix. This means that a Kubernetes cluster must always include Linux master nodes, zero or more Linux worker nodes, and zero or more Windows worker nodes.</p><h4 id=resource-handling>Resource Handling</h4><p>Linux cgroups are used as a pod boundary for resource controls in Linux. Containers are created within that boundary for network, process and file system isolation. The cgroups APIs can be used to gather cpu/io/memory stats. In contrast, Windows uses a Job object per container with a system namespace filter to contain all processes in a container and provide logical isolation from the host. There is no way to run a Windows container without the namespace filtering in place. This means that system privileges cannot be asserted in the context of the host, and thus privileged containers are not available on Windows. Containers cannot assume an identity from the host because the Security Account Manager (SAM) is separate.</p><h4 id=resource-reservations>Resource Reservations</h4><h5 id=memory-reservations>Memory Reservations</h5><p>Windows does not have an out-of-memory process killer as Linux does. Windows always treats all user-mode memory allocations as virtual, and pagefiles are mandatory. The net effect is that Windows won't reach out of memory conditions the same way Linux does, and processes page to disk instead of being subject to out of memory (OOM) termination. If memory is over-provisioned and all physical memory is exhausted, then paging can slow down performance.</p><p>Keeping memory usage within reasonable bounds is possible using the kubelet parameters <code>--kubelet-reserve</code> and/or <code>--system-reserve</code> to account for memory usage on the node (outside of containers). This reduces <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>.</p><blockquote class="note callout"><div><strong>Note:</strong> As you deploy workloads, use resource limits (must set only limits or limits must equal requests) on containers. This also subtracts from NodeAllocatable and prevents the scheduler from adding more pods once a node is full.</div></blockquote><p>A best practice to avoid over-provisioning is to configure the kubelet with a system reserved memory of at least 2GB to account for Windows, Docker, and Kubernetes processes.</p><h5 id=cpu-reservations>CPU Reservations</h5><p>To account for Windows, Docker and other Kubernetes host processes it is recommended to reserve a percentage of CPU so they are able to respond to events. This value needs to be scaled based on the number of CPU cores available on the Windows node.To determine this percentage a user should identify the maximum pod density for each of their nodes and monitor the CPU usage of the system services choosing a value that meets their workload needs.</p><p>Keeping CPU usage within reasonable bounds is possible using the kubelet parameters <code>--kubelet-reserve</code> and/or <code>--system-reserve</code> to account for CPU usage on the node (outside of containers). This reduces <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>.</p><h4 id=feature-restrictions>Feature Restrictions</h4><ul><li>TerminationGracePeriod: not implemented</li><li>Single file mapping: to be implemented with CRI-ContainerD</li><li>Termination message: to be implemented with CRI-ContainerD</li><li>Privileged Containers: not currently supported in Windows containers</li><li>HugePages: not currently supported in Windows containers</li><li>The existing node problem detector is Linux-only and requires privileged containers. In general, we don't expect this to be used on Windows because privileged containers are not supported</li><li>Not all features of shared namespaces are supported (see API section for more details)</li></ul><h4 id=difference-in-behavior-of-flags-when-compared-to-linux>Difference in behavior of flags when compared to Linux</h4><p>The behavior of the following kubelet flags is different on Windows nodes as described below:</p><ul><li><code>--kubelet-reserve</code>, <code>--system-reserve</code> , and <code>--eviction-hard</code> flags update Node Allocatable</li><li>Eviction by using <code>--enforce-node-allocable</code> is not implemented</li><li>Eviction by using <code>--eviction-hard</code> and <code>--eviction-soft</code> are not implemented</li><li>MemoryPressure Condition is not implemented</li><li>There are no OOM eviction actions taken by the kubelet</li><li>Kubelet running on the windows node does not have memory restrictions. <code>--kubelet-reserve</code> and <code>--system-reserve</code> do not set limits on kubelet or processes running on the host. This means kubelet or a process on the host could cause memory resource starvation outside the node-allocatable and scheduler</li><li>An additional flag to set the priority of the kubelet process is available on the Windows nodes called <code>--windows-priorityclass</code>. This flag allows kubelet process to get more CPU time slices when compared to other processes running on the Windows host. More information on the allowable values and their meaning is available at <a href=https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class>Windows Priority Classes</a>. In order for kubelet to always have enough CPU cycles it is recommended to set this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> and above</li></ul><h4 id=storage>Storage</h4><p>Windows has a layered filesystem driver to mount container layers and create a copy filesystem based on NTFS. All file paths in the container are resolved only within the context of that container.</p><ul><li>With Docker Volume mounts can only target a directory in the container, and not an individual file. This limitation does not exist with CRI-containerD.</li><li>Volume mounts cannot project files or directories back to the host filesystem</li><li>Read-only filesystems are not supported because write access is always required for the Windows registry and SAM database. However, read-only volumes are supported</li><li>Volume user-masks and permissions are not available. Because the SAM is not shared between the host & container, there's no mapping between them. All permissions are resolved within the context of the container</li></ul><p>As a result, the following storage functionality is not supported on Windows nodes</p><ul><li>Volume subpath mounts. Only the entire volume can be mounted in a Windows container.</li><li>Subpath volume mounting for Secrets</li><li>Host mount projection</li><li>DefaultMode (due to UID/GID dependency)</li><li>Read-only root filesystem. Mapped volumes still support readOnly</li><li>Block device mapping</li><li>Memory as the storage medium</li><li>File system features like uui/guid, per-user Linux filesystem permissions</li><li>NFS based storage/volume support</li><li>Expanding the mounted volume (resizefs)</li></ul><h4 id=networking-limitations>Networking</h4><p>Windows Container Networking differs in some important ways from Linux networking. The <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture>Microsoft documentation for Windows Container Networking</a> contains additional details and background.</p><p>The Windows host networking service and virtual switch implement namespacing and can create virtual NICs as needed for a pod or container. However, many configurations such as DNS, routes, and metrics are stored in the Windows registry database rather than /etc/... files as they are on Linux. The Windows registry for the container is separate from that of the host, so concepts like mapping /etc/resolv.conf from the host into a container don't have the same effect they would on Linux. These must be configured using Windows APIs run in the context of that container. Therefore CNI implementations need to call the HNS instead of relying on file mappings to pass network details into the pod or container.</p><p>The following networking functionality is not supported on Windows nodes</p><ul><li>Host networking mode is not available for Windows pods</li><li>Local NodePort access from the node itself fails (works for other nodes or external clients)</li><li>Accessing service VIPs from nodes will be available with a future release of Windows Server</li><li>A single service can only support up to 64 backend pods / unique destination IPs</li><li>Overlay networking support in kube-proxy is an alpha release. In addition, it requires <a href=https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887>KB4482887</a> to be installed on Windows Server 2019</li><li>Local Traffic Policy and DSR mode</li><li>Windows containers connected to l2bridge, l2tunnel, or overlay networks do not support communicating over the IPv6 stack. There is outstanding Windows platform work required to enable these network drivers to consume IPv6 addresses and subsequent Kubernetes work in kubelet, kube-proxy, and CNI plugins.</li><li>Outbound communication using the ICMP protocol via the win-overlay, win-bridge, and Azure-CNI plugin. Specifically, the Windows data plane (<a href=https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/>VFP</a>) doesn't support ICMP packet transpositions. This means:<ul><li>ICMP packets directed to destinations within the same network (e.g. pod to pod communication via ping) work as expected and without any limitations</li><li>TCP/UDP packets work as expected and without any limitations</li><li>ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping) cannot be transposed and thus will not be routed back to their source</li><li>Since TCP/UDP packets can still be transposed, one can substitute <code>ping &lt;destination></code> with <code>curl &lt;destination></code> to be able to debug connectivity to the outside world.</li></ul></li></ul><p>These features were added in Kubernetes v1.15:</p><ul><li><code>kubectl port-forward</code></li></ul><h5 id=cni-plugins>CNI Plugins</h5><ul><li>Windows reference network plugins win-bridge and win-overlay do not currently implement <a href=https://github.com/containernetworking/cni/blob/master/SPEC.md>CNI spec</a> v0.4.0 due to missing "CHECK" implementation.</li><li>The Flannel VXLAN CNI has the following limitations on Windows:</li></ul><ol><li>Node-pod connectivity isn't possible by design. It's only possible for local pods with Flannel v0.12.0 (or higher).</li><li>We are restricted to using VNI 4096 and UDP port 4789. The VNI limitation is being worked on and will be overcome in a future release (open-source flannel changes). See the official <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>Flannel VXLAN</a> backend docs for more details on these parameters.</li></ol><h5 id=dns-limitations>DNS</h5><ul><li>ClusterFirstWithHostNet is not supported for DNS. Windows treats all names with a '.' as a FQDN and skips PQDN resolution</li><li>On Linux, you have a DNS suffix list, which is used when trying to resolve PQDNs. On Windows, we only have 1 DNS suffix, which is the DNS suffix associated with that pod's namespace (mydns.svc.cluster.local for example). Windows can resolve FQDNs and services or names resolvable with only that suffix. For example, a pod spawned in the default namespace, will have the DNS suffix <strong>default.svc.cluster.local</strong>. On a Windows pod, you can resolve both <strong>kubernetes.default.svc.cluster.local</strong> and <strong>kubernetes</strong>, but not the in-betweens, like <strong>kubernetes.default</strong> or <strong>kubernetes.default.svc</strong>.</li><li>On Windows, there are multiple DNS resolvers that can be used. As these come with slightly different behaviors, using the <code>Resolve-DNSName</code> utility for name query resolutions is recommended.</li></ul><h5 id=ipv6>IPv6</h5><p>Kubernetes on Windows does not support single-stack "IPv6-only" networking. However,dual-stack IPv4/IPv6 networking for pods and nodes with single-family services is supported. See <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack networking</a> for more details.</p><h5 id=session-affinity>Session affinity</h5><p>Setting the maximum session sticky time for Windows services using <code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> is not supported.</p><h5 id=security>Security</h5><p>Secrets are written in clear text on the node's volume (as compared to tmpfs/in-memory on linux). This means customers have to do two things</p><ol><li>Use file ACLs to secure the secrets file location</li><li>Use volume-level encryption using <a href=https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a></li></ol><p><a href=/docs/tasks/configure-pod-container/configure-runasusername>RunAsUsername</a> can be specified for Windows Pod's or Container's to execute the Container processes as a node-default user. This is roughly equivalent to <a href=/docs/concepts/policy/pod-security-policy/#users-and-groups>RunAsUser</a>.</p><p>Linux specific pod security context privileges such as SELinux, AppArmor, Seccomp, Capabilities (POSIX Capabilities), and others are not supported.</p><p>In addition, as mentioned already, privileged containers are not supported on Windows.</p><h4 id=api>API</h4><p>There are no differences in how most of the Kubernetes APIs work for Windows. The subtleties around what's different come down to differences in the OS and container runtime. In certain situations, some properties on workload APIs such as Pod or Container were designed with an assumption that they are implemented on Linux, failing to run on Windows.</p><p>At a high level, these OS concepts are different:</p><ul><li>Identity - Linux uses userID (UID) and groupID (GID) which are represented as integer types. User and group names are not canonical - they are an alias in <code>/etc/groups</code> or <code>/etc/passwd</code> back to UID+GID. Windows uses a larger binary security identifier (SID) which is stored in the Windows Security Access Manager (SAM) database. This database is not shared between the host and containers, or between containers.</li><li>File permissions - Windows uses an access control list based on SIDs, rather than a bitmask of permissions and UID+GID</li><li>File paths - convention on Windows is to use <code>\</code> instead of <code>/</code>. The Go IO libraries accept both types of file path separators. However, when you're setting a path or command line that's interpreted inside a container, <code>\</code> may be needed.</li><li>Signals - Windows interactive apps handle termination differently, and can implement one or more of these:<ul><li>A UI thread handles well-defined messages including WM_CLOSE</li><li>Console apps handle ctrl-c or ctrl-break using a Control Handler</li><li>Services register a Service Control Handler function that can accept SERVICE_CONTROL_STOP control codes</li></ul></li></ul><p>Exit Codes follow the same convention where 0 is success, nonzero is failure. The specific error codes may differ across Windows and Linux. However, exit codes passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.</p><h5 id=v1-container>V1.Container</h5><ul><li>V1.Container.ResourceRequirements.limits.cpu and V1.Container.ResourceRequirements.limits.memory - Windows doesn't use hard limits for CPU allocations. Instead, a share system is used. The existing fields based on millicores are scaled into relative shares that are followed by the Windows scheduler. <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go>see: kuberuntime/helpers_windows.go</a>, <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls>see: resource controls in Microsoft docs</a><ul><li>Huge pages are not implemented in the Windows container runtime, and are not available. They require <a href=https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support>asserting a user privilege</a> that's not configurable for containers.</li></ul></li><li>V1.Container.ResourceRequirements.requests.cpu and V1.Container.ResourceRequirements.requests.memory - Requests are subtracted from node available resources, so they can be used to avoid overprovisioning a node. However, they cannot be used to guarantee resources in an overprovisioned node. They should be applied to all containers as a best practice if the operator wants to avoid overprovisioning entirely.</li><li>V1.Container.SecurityContext.allowPrivilegeEscalation - not possible on Windows, none of the capabilities are hooked up</li><li>V1.Container.SecurityContext.Capabilities - POSIX capabilities are not implemented on Windows</li><li>V1.Container.SecurityContext.privileged - Windows doesn't support privileged containers</li><li>V1.Container.SecurityContext.procMount - Windows doesn't have a /proc filesystem</li><li>V1.Container.SecurityContext.readOnlyRootFilesystem - not possible on Windows, write access is required for registry & system processes to run inside the container</li><li>V1.Container.SecurityContext.runAsGroup - not possible on Windows, no GID support</li><li>V1.Container.SecurityContext.runAsNonRoot - Windows does not have a root user. The closest equivalent is ContainerAdministrator which is an identity that doesn't exist on the node.</li><li>V1.Container.SecurityContext.runAsUser - not possible on Windows, no UID support as int.</li><li>V1.Container.SecurityContext.seLinuxOptions - not possible on Windows, no SELinux</li><li>V1.Container.terminationMessagePath - this has some limitations in that Windows doesn't support mapping single files. The default value is /dev/termination-log, which does work because it does not exist on Windows by default.</li></ul><h5 id=v1-pod>V1.Pod</h5><ul><li>V1.Pod.hostIPC, v1.pod.hostpid - host namespace sharing is not possible on Windows</li><li>V1.Pod.hostNetwork - There is no Windows OS support to share the host network</li><li>V1.Pod.dnsPolicy - ClusterFirstWithHostNet - is not supported because Host Networking is not supported on Windows.</li><li>V1.Pod.podSecurityContext - see V1.PodSecurityContext below</li><li>V1.Pod.shareProcessNamespace - this is a beta feature, and depends on Linux namespaces which are not implemented on Windows. Windows cannot share process namespaces or the container's root filesystem. Only the network can be shared.</li><li>V1.Pod.terminationGracePeriodSeconds - this is not fully implemented in Docker on Windows, see: <a href=https://github.com/moby/moby/issues/25982>reference</a>. The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT, then Windows waits 5 seconds by default, and finally shuts down all processes using the normal Windows shutdown behavior. The 5 second default is actually in the Windows registry <a href=https://github.com/moby/moby/issues/25982#issuecomment-426441183>inside the container</a>, so it can be overridden when the container is built.</li><li>V1.Pod.volumeDevices - this is a beta feature, and is not implemented on Windows. Windows cannot attach raw block devices to pods.</li><li>V1.Pod.volumes - EmptyDir, Secret, ConfigMap, HostPath - all work and have tests in TestGrid<ul><li>V1.emptyDirVolumeSource - the Node default medium is disk on Windows. Memory is not supported, as Windows does not have a built-in RAM disk.</li></ul></li><li>V1.VolumeMount.mountPropagation - mount propagation is not supported on Windows.</li></ul><h5 id=v1-podsecuritycontext>V1.PodSecurityContext</h5><p>None of the PodSecurityContext fields work on Windows. They're listed here for reference.</p><ul><li>V1.PodSecurityContext.SELinuxOptions - SELinux is not available on Windows</li><li>V1.PodSecurityContext.RunAsUser - provides a UID, not available on Windows</li><li>V1.PodSecurityContext.RunAsGroup - provides a GID, not available on Windows</li><li>V1.PodSecurityContext.RunAsNonRoot - Windows does not have a root user. The closest equivalent is ContainerAdministrator which is an identity that doesn't exist on the node.</li><li>V1.PodSecurityContext.SupplementalGroups - provides GID, not available on Windows</li><li>V1.PodSecurityContext.Sysctls - these are part of the Linux sysctl interface. There's no equivalent on Windows.</li></ul><h4 id=operating-system-version-restrictions>Operating System Version Restrictions</h4><p>Windows has strict compatibility rules, where the host OS version must match the container base image OS version. Only Windows containers with a container operating system of Windows Server 2019 are supported. Hyper-V isolation of containers, enabling some backward compatibility of Windows container image versions, is planned for a future release.</p><h2 id=troubleshooting>Getting Help and Troubleshooting</h2><p>Your main source of help for troubleshooting your Kubernetes cluster should start with this <a href=/docs/tasks/debug-application-cluster/troubleshooting/>section</a>. Some additional, Windows-specific troubleshooting help is included in this section. Logs are an important element of troubleshooting issues in Kubernetes. Make sure to include them any time you seek troubleshooting assistance from other contributors. Follow the instructions in the SIG-Windows <a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>contributing guide on gathering logs</a>.</p><ol><li><p>How do I know start.ps1 completed successfully?</p><p>You should see kubelet, kube-proxy, and (if you chose Flannel as your networking solution) flanneld host-agent processes running on your node, with running logs being displayed in separate PowerShell windows. In addition to this, your Windows node should be listed as "Ready" in your Kubernetes cluster.</p></li><li><p>Can I configure the Kubernetes node processes to run in the background as services?</p><p>Kubelet and kube-proxy are already configured to run as native Windows Services, offering resiliency by re-starting the services automatically in the event of failure (for example a process crash). You have two options for configuring these node components as services.</p><ol><li><p>As native Windows Services</p><p>Kubelet & kube-proxy can be run as native Windows Services using <code>sc.exe</code>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Create the services for kubelet and kube-proxy in two separate commands</span>
sc.exe create &lt;component_name&gt; binPath= <span style=color:#b44>&#34;&lt;path_to_binary&gt; --service &lt;other_args&gt;&#34;</span>

<span style=color:#080;font-style:italic># Please note that if the arguments contain spaces, they must be escaped.</span>
sc.exe create kubelet binPath= <span style=color:#b44>&#34;C:\kubelet.exe --service --hostname-override &#39;minion&#39; &lt;other_args&gt;&#34;</span>

<span style=color:#080;font-style:italic># Start the services</span>
<span style=color:#a2f>Start-Service</span> kubelet
<span style=color:#a2f>Start-Service</span> kube-proxy

<span style=color:#080;font-style:italic># Stop the service</span>
<span style=color:#a2f>Stop-Service</span> kubelet (-Force)
<span style=color:#a2f>Stop-Service</span> kube-proxy (-Force)

<span style=color:#080;font-style:italic># Query the service status</span>
<span style=color:#a2f>Get-Service</span> kubelet
<span style=color:#a2f>Get-Service</span> kube-proxy
</code></pre></div></li><li><p>Using nssm.exe</p><p>You can also always use alternative service managers like <a href=https://nssm.cc/>nssm.exe</a> to run these processes (flanneld, kubelet & kube-proxy) in the background for you. You can use this <a href=https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1>sample script</a>, leveraging nssm.exe to register kubelet, kube-proxy, and flanneld.exe to run as Windows services in the background.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>register-svc</span>.ps1 -NetworkMode &lt;Network mode&gt; -ManagementIP &lt;Windows Node IP&gt; -ClusterCIDR &lt;Cluster subnet&gt; -KubeDnsServiceIP &lt;Kube-dns Service IP&gt; -LogDir &lt;Directory to place logs&gt;

<span style=color:#080;font-style:italic># NetworkMode      = The network mode l2bridge (flannel host-gw, also the default value) or overlay (flannel vxlan) chosen as a network solution</span>
<span style=color:#080;font-style:italic># ManagementIP     = The IP address assigned to the Windows node. You can use ipconfig to find this</span>
<span style=color:#080;font-style:italic># ClusterCIDR      = The cluster subnet range. (Default value 10.244.0.0/16)</span>
<span style=color:#080;font-style:italic># KubeDnsServiceIP = The Kubernetes DNS service IP (Default value 10.96.0.10)</span>
<span style=color:#080;font-style:italic># LogDir           = The directory where kubelet and kube-proxy logs are redirected into their respective output files (Default value C:\k)</span>
</code></pre></div><p>If the above referenced script is not suitable, you can manually configure nssm.exe using the following examples.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Register flanneld.exe</span>
nssm install flanneld C:\flannel\flanneld.exe
nssm <span style=color:#a2f>set </span>flanneld AppParameters --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;ManagementIP&gt; --ip-masq=1 --kube-subnet-mgr=1
nssm <span style=color:#a2f>set </span>flanneld AppEnvironmentExtra NODE_NAME=&lt;hostname&gt;
nssm <span style=color:#a2f>set </span>flanneld AppDirectory C:\flannel
nssm <span style=color:#a2f>start </span>flanneld

<span style=color:#080;font-style:italic># Register kubelet.exe</span>
<span style=color:#080;font-style:italic># Microsoft releases the pause infrastructure container at mcr.microsoft.com/oss/kubernetes/pause:1.4.1</span>
nssm install kubelet C:\k\kubelet.exe
nssm <span style=color:#a2f>set </span>kubelet AppParameters --hostname-override=&lt;hostname&gt; --v=6 --pod-infra-container-image=mcr.microsoft.com/oss/kubernetes/pause<span>:</span>1.4.1 --resolv-conf=<span style=color:#b44>&#34;&#34;</span> --allow-privileged=true --enable-debugging-handlers --cluster-dns=&lt;DNS-service-IP&gt; --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=promiscuous-bridge --image-pull-progress-deadline=20m --cgroups-per-qos=false  --log-dir=&lt;log directory&gt; --logtostderr=false --enforce-node-allocatable=<span style=color:#b44>&#34;&#34;</span> --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config
nssm <span style=color:#a2f>set </span>kubelet AppDirectory C:\k
nssm <span style=color:#a2f>start </span>kubelet

<span style=color:#080;font-style:italic># Register kube-proxy.exe (l2bridge / host-gw)</span>
nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --hostname-override=&lt;hostname&gt;--kubeconfig=c:\k\config --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm.exe <span style=color:#a2f>set </span>kube-proxy AppEnvironmentExtra KUBE_NETWORK=cbr0
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy

<span style=color:#080;font-style:italic># Register kube-proxy.exe (overlay / vxlan)</span>
nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --feature-gates=<span style=color:#b44>&#34;WinOverlay=true&#34;</span> --hostname-override=&lt;hostname&gt; --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=&lt;source-vip&gt; --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy
</code></pre></div><p>For initial troubleshooting, you can use the following flags in <a href=https://nssm.cc/>nssm.exe</a> to redirect stdout and stderr to a output file:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStdout C:\k\mysvc.log
nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStderr C:\k\mysvc.log
</code></pre></div><p>For additional details, see official <a href=https://nssm.cc/usage>nssm usage</a> docs.</p></li></ol></li><li><p>My Windows Pods do not have network connectivity</p><p>If you are using virtual machines, ensure that MAC spoofing is enabled on all the VM network adapter(s).</p></li><li><p>My Windows Pods cannot ping external resources</p><p>Windows Pods do not have outbound rules programmed for the ICMP protocol today. However, TCP/UDP is supported. When trying to demonstrate connectivity to resources outside of the cluster, please substitute <code>ping &lt;IP></code> with corresponding <code>curl &lt;IP></code> commands.</p><p>If you are still facing problems, most likely your network configuration in <a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf>cni.conf</a> deserves some extra attention. You can always edit this static file. The configuration update will apply to any newly created Kubernetes resources.</p><p>One of the Kubernetes networking requirements (see <a href=/docs/concepts/cluster-administration/networking/>Kubernetes model</a>) is for cluster communication to occur without NAT internally. To honor this requirement, there is an <a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20>ExceptionList</a> for all the communication where we do not want outbound NAT to occur. However, this also means that you need to exclude the external IP you are trying to query from the ExceptionList. Only then will the traffic originating from your Windows pods be SNAT'ed correctly to receive a response from the outside world. In this regard, your ExceptionList in <code>cni.conf</code> should look as follows:</p><pre><code class=language-conf data-lang=conf>&quot;ExceptionList&quot;: [
                &quot;10.244.0.0/16&quot;,  # Cluster subnet
                &quot;10.96.0.0/12&quot;,   # Service subnet
                &quot;10.127.130.0/24&quot; # Management (host) subnet
            ]
</code></pre></li><li><p>My Windows node cannot access NodePort service</p><p>Local NodePort access from the node itself fails. This is a known limitation. NodePort access works from other nodes or external clients.</p></li><li><p>vNICs and HNS endpoints of containers are being deleted</p><p>This issue can be caused when the <code>hostname-override</code> parameter is not passed to <a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a>. To resolve it, users need to pass the hostname to kube-proxy as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>C:\k\kube-proxy.exe --hostname-override=$(hostname)
</code></pre></div></li><li><p>With flannel my nodes are having issues after rejoining a cluster</p><p>Whenever a previously deleted node is being re-joined to the cluster, flannelD tries to assign a new pod subnet to the node. Users should remove the old pod subnet configuration files in the following paths:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Remove-Item</span> C:\k\SourceVip.json
<span style=color:#a2f>Remove-Item</span> C:\k\SourceVipRequest.json
</code></pre></div></li><li><p>After launching <code>start.ps1</code>, flanneld is stuck in "Waiting for the Network to be created"</p><p>There are numerous reports of this <a href=https://github.com/coreos/flannel/issues/1066>issue</a>; most likely it is a timing issue for when the management IP of the flannel network is set. A workaround is to relaunch start.ps1 or relaunch it manually as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>PS </span>C:&gt; <span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;NODE_NAME&#34;</span>, <span style=color:#b44>&#34;&lt;Windows_Worker_Hostname&gt;&#34;</span>)
<span style=color:#a2f>PS </span>C:&gt; C:\flannel\flanneld.exe --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;Windows_Worker_Node_IP&gt; --ip-masq=1 --kube-subnet-mgr=1
</code></pre></div></li><li><p>My Windows Pods cannot launch because of missing <code>/run/flannel/subnet.env</code></p><p>This indicates that Flannel didn't launch correctly. You can either try to restart flanneld.exe or you can copy the files over manually from <code>/run/flannel/subnet.env</code> on the Kubernetes master to <code>C:\run\flannel\subnet.env</code> on the Windows worker node and modify the <code>FLANNEL_SUBNET</code> row to a different number. For example, if node subnet 10.244.4.1/24 is desired:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-env data-lang=env><span style=color:#b8860b>FLANNEL_NETWORK</span><span style=color:#666>=</span>10.244.0.0/16
<span style=color:#b8860b>FLANNEL_SUBNET</span><span style=color:#666>=</span>10.244.4.1/24
<span style=color:#b8860b>FLANNEL_MTU</span><span style=color:#666>=</span><span style=color:#666>1500</span>
<span style=color:#b8860b>FLANNEL_IPMASQ</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
</code></pre></div></li><li><p>My Windows node cannot access my services using the service IP</p><p>This is a known limitation of the current networking stack on Windows. Windows Pods are able to access the service IP however.</p></li><li><p>No network adapter is found when starting kubelet</p><p>The Windows networking stack needs a virtual adapter for Kubernetes networking to work. If the following commands return no results (in an admin shell), virtual network creation — a necessary prerequisite for Kubelet to work — has failed:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Get-HnsNetwork</span> | ? Name <span style=color:#666>-ieq</span> <span style=color:#b44>&#34;cbr0&#34;</span>
<span style=color:#a2f>Get-NetAdapter</span> | ? Name <span style=color:#666>-Like</span> <span style=color:#b44>&#34;vEthernet (Ethernet*&#34;</span>
</code></pre></div><p>Often it is worthwhile to modify the <a href=https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7>InterfaceName</a> parameter of the start.ps1 script, in cases where the host's network adapter isn't "Ethernet". Otherwise, consult the output of the <code>start-kubelet.ps1</code> script to see if there are errors during virtual network creation.</p></li><li><p>My Pods are stuck at "Container Creating" or restarting over and over</p><p>Check that your pause image is compatible with your OS version. The <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources>instructions</a> assume that both the OS and the containers are version 1803. If you have a later version of Windows, such as an Insider build, you need to adjust the images accordingly. Please refer to the Microsoft's <a href=https://hub.docker.com/u/microsoft/>Docker repository</a> for images. Regardless, both the pause image Dockerfile and the sample service expect the image to be tagged as :latest.</p></li><li><p>DNS resolution is not properly working</p><p>Check the DNS limitations for Windows in this <a href=#dns-limitations>section</a>.</p></li><li><p><code>kubectl port-forward</code> fails with "unable to do port forwarding: wincat not found"</p><p>This was implemented in Kubernetes 1.15 by including wincat.exe in the pause infrastructure container <code>mcr.microsoft.com/oss/kubernetes/pause:1.4.1</code>. Be sure to use these versions or newer ones.
If you would like to build your own pause infrastructure container be sure to include <a href=https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat>wincat</a>.</p></li><li><p>My Kubernetes installation is failing because my Windows Server node is behind a proxy</p><p>If you are behind a proxy, the following PowerShell environment variables must be defined:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell><span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTP_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:80/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
<span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTPS_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:443/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
</code></pre></div></li><li><p>What is a <code>pause</code> container?</p><p>In a Kubernetes Pod, an infrastructure or "pause" container is first created to host the container endpoint. Containers that belong to the same pod, including infrastructure and worker containers, share a common network namespace and endpoint (same IP and port space). Pause containers are needed to accommodate worker containers crashing or restarting without losing any of the networking configuration.</p><p>The "pause" (infrastructure) image is hosted on Microsoft Container Registry (MCR). You can access it using <code>mcr.microsoft.com/oss/kubernetes/pause:1.4.1</code>. For more details, see the <a href=https://github.com/kubernetes-sigs/windows-testing/blob/master/images/pause/Dockerfile>DOCKERFILE</a>.</p></li></ol><h3 id=further-investigation>Further investigation</h3><p>If these steps don't resolve your problem, you can get help running Windows containers on Windows nodes in Kubernetes through:</p><ul><li>StackOverflow <a href=https://stackoverflow.com/questions/tagged/windows-server-container>Windows Server Container</a> topic</li><li>Kubernetes Official Forum <a href=https://discuss.kubernetes.io/>discuss.kubernetes.io</a></li><li>Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-windows>#SIG-Windows Channel</a></li></ul><h2 id=reporting-issues-and-feature-requests>Reporting Issues and Feature Requests</h2><p>If you have what looks like a bug, or you would like to make a feature request, please use the <a href=https://github.com/kubernetes/kubernetes/issues>GitHub issue tracking system</a>. You can open issues on <a href=https://github.com/kubernetes/kubernetes/issues/new/choose>GitHub</a> and assign them to SIG-Windows. You should first search the list of issues in case it was reported previously and comment with your experience on the issue and add additional logs. SIG-Windows Slack is also a great avenue to get some initial support and troubleshooting ideas prior to creating a ticket.</p><p>If filing a bug, please include detailed information about how to reproduce the problem, such as:</p><ul><li>Kubernetes version: kubectl version</li><li>Environment details: Cloud provider, OS distro, networking choice and configuration, and Docker version</li><li>Detailed steps to reproduce the problem</li><li><a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>Relevant logs</a></li><li>Tag the issue sig/windows by commenting on the issue with <code>/sig windows</code> to bring it to a SIG-Windows member's attention</li></ul><h2 id=what-s-next>What's next</h2><p>We have a lot of features in our roadmap. An abbreviated high level list is included below, but we encourage you to view our <a href=https://github.com/orgs/kubernetes/projects/8>roadmap project</a> and help us make Windows support better by <a href=https://github.com/kubernetes/community/blob/master/sig-windows/>contributing</a>.</p><h3 id=hyper-v-isolation>Hyper-V isolation</h3><p>Hyper-V isolation is required to enable the following use cases for Windows containers in Kubernetes:</p><ul><li>Hypervisor-based isolation between pods for additional security</li><li>Backwards compatibility allowing a node to run a newer Windows Server version without requiring containers to be rebuilt</li><li>Specific CPU/NUMA settings for a pod</li><li>Memory isolation and reservations</li></ul><p>Hyper-V isolation support will be added in a later release and will require CRI-Containerd.</p><h3 id=deployment-with-kubeadm-and-cluster-api>Deployment with kubeadm and cluster API</h3><p>Kubeadm is becoming the de facto standard for users to deploy a Kubernetes
cluster. Windows node support in kubeadm is currently a work-in-progress but a
guide is available <a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>here</a>.
We are also making investments in cluster API to ensure Windows nodes are
properly provisioned.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a51e66c5de55f9093a8dc55742006d3>3.4.2 - Guide for scheduling Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that run in many organizations. This guide walks you through the steps to configure and deploy a Windows container in Kubernetes.</p><h2 id=objectives>Objectives</h2><ul><li>Configure an example deployment to run Windows containers on the Windows node</li><li>(Optional) Configure an Active Directory Identity for your Pod using Group Managed Service Accounts (GMSA)</li></ul><h2 id=before-you-begin>Before you begin</h2><ul><li>Create a Kubernetes cluster that includes a <a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes>master and a worker node running Windows Server</a></li><li>It is important to note that creating and deploying services and workloads on Kubernetes behaves in much the same way for Linux and Windows containers. <a href=/docs/reference/kubectl/overview/>Kubectl commands</a> to interface with the cluster are identical. The example in the section below is provided to jumpstart your experience with Windows containers.</li></ul><h2 id=getting-started-deploying-a-windows-container>Getting Started: Deploying a Windows container</h2><p>To deploy a Windows container on Kubernetes, you must first create an example application. The example YAML file below creates a simple webserver application. Create a service spec named <code>win-webserver.yaml</code> with the contents below:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># the port that this service should serve on</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windowswebserver<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- powershell.exe<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- -command<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:#b44>&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString+=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></code></pre></div><blockquote class="note callout"><div><strong>Note:</strong> Port mapping is also supported, but for simplicity in this example the container port 80 is exposed directly to the service.</div></blockquote><ol><li><p>Check that all nodes are healthy:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get nodes
</code></pre></div></li><li><p>Deploy the service and watch for pod updates:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f win-webserver.yaml
kubectl get pods -o wide -w
</code></pre></div><p>When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.</p></li><li><p>Check that the deployment succeeded. To verify:</p><ul><li>Two containers per pod on the Windows node, use <code>docker ps</code></li><li>Two pods listed from the Linux master, use <code>kubectl get pods</code></li><li>Node-to-pod communication across the network, <code>curl</code> port 80 of your pod IPs from the Linux master to check for a web server response</li><li>Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node) using docker exec or kubectl exec</li><li>Service-to-pod communication, <code>curl</code> the virtual service IP (seen under <code>kubectl get services</code>) from the Linux master and from individual pods</li><li>Service discovery, <code>curl</code> the service name with the Kubernetes <a href=/docs/concepts/services-networking/dns-pod-service/#services>default DNS suffix</a></li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux master or machines outside of the cluster</li><li>Outbound connectivity, <code>curl</code> external IPs from inside the pod using kubectl exec</li></ul></li></ol><blockquote class="note callout"><div><strong>Note:</strong> Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack. Only Windows pods are able to access service IPs.</div></blockquote><h2 id=observability>Observability</h2><h3 id=capturing-logs-from-workloads>Capturing logs from workloads</h3><p>Logs are an important element of observability; they enable users to gain insights into the operational aspect of workloads and are a key ingredient to troubleshooting issues. Because Windows containers and workloads inside Windows containers behave differently from Linux containers, users had a hard time collecting logs, limiting operational visibility. Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows) or push entries to the application event log. <a href=https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor>LogMonitor</a>, an open source tool by Microsoft, is the recommended way to monitor configured log sources inside a Windows container. LogMonitor supports monitoring event logs, ETW providers, and custom application logs, piping them to STDOUT for consumption by <code>kubectl logs &lt;pod></code>.</p><p>Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.</p><h2 id=using-configurable-container-usernames>Using configurable Container usernames</h2><p>Starting with Kubernetes v1.16, Windows containers can be configured to run their entrypoints and processes with different usernames than the image defaults. The way this is achieved is a bit different from the way it is done for Linux containers. Learn more about it <a href=/docs/tasks/configure-pod-container/configure-runasusername/>here</a>.</p><h2 id=managing-workload-identity-with-group-managed-service-accounts>Managing Workload Identity with Group Managed Service Accounts</h2><p>Starting with Kubernetes v1.14, Windows container workloads can be configured to use Group Managed Service Accounts (GMSA). Group Managed Service Accounts are a specific type of Active Directory account that provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers. Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA. Learn more about configuring and using GMSA for Windows containers <a href=/docs/tasks/configure-pod-container/configure-gmsa/>here</a>.</p><h2 id=taints-and-tolerations>Taints and Tolerations</h2><p>Users today need to use some combination of taints and node selectors in order to keep Linux and Windows workloads on their respective OS-specific nodes. This likely imposes a burden only on Windows users. The recommended approach is outlined below, with one of its main goals being that this approach should not break compatibility for existing Linux workloads.</p><h3 id=ensuring-os-specific-workloads-land-on-the-appropriate-container-host>Ensuring OS-specific workloads land on the appropriate container host</h3><p>Users can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations. All Kubernetes nodes today have the following default labels:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>If a Pod specification does not specify a nodeSelector like <code>"kubernetes.io/os": windows</code>, it is possible the Pod can be scheduled on any host, Windows or Linux. This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux. The best practice is to use a nodeSelector.</p><p>However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers, as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators. In those situations, you may be hesitant to make the configuration change to add nodeSelectors. The alternative is to use Taints. Because the kubelet can set Taints during registration, it could easily be modified to automatically add a taint when running on Windows only.</p><p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p><p>By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods). In order for a Windows Pod to be scheduled on a Windows node, it would need both the nodeSelector to choose Windows, and the appropriate matching toleration.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;os&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><h3 id=handling-multiple-windows-versions-in-the-same-cluster>Handling multiple Windows versions in the same cluster</h3><p>The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and nodeSelectors.</p><p>Kubernetes 1.17 automatically adds a new label <code>node.kubernetes.io/windows-build</code> to simplify this. If you're running an older version, then it's recommended to add this label manually to Windows nodes.</p><p>This label reflects the Windows major, minor, and build number that need to match for compatibility. Here are values used today for each Windows Server version.</p><table><thead><tr><th>Product Name</th><th>Build Number(s)</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1809</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1903</td><td>10.0.18362</td></tr></tbody></table><h3 id=simplifying-with-runtimeclass>Simplifying with RuntimeClass</h3><p><a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a> can be used to simplify the process of using taints and tolerations. A cluster administrator can create a <code>RuntimeClass</code> object which is used to encapsulate these taints and tolerations.</p><ol><li>Save this file to <code>runtimeClasses.yml</code>. It includes the appropriate <code>nodeSelector</code> for the Windows OS, architecture, and version.</li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;docker&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduling</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;windows&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;amd64&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>os<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Equal<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></code></pre></div><ol><li>Run <code>kubectl create -f runtimeClasses.yml</code> using as a cluster administrator</li><li>Add <code>runtimeClassName: windows-2019</code> as appropriate to Pod specs</li></ol><p>For example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>.1<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>300Mi<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-84b6491601d6a2b3da4cd5a105c866ba>4 - Best practices</h1></div><div class=td-content><h1 id=pg-c797ee17120176c685455db89ae091a9>4.1 - Considerations for large clusters</h1><p>A cluster is a set of <a class=glossary-tooltip title="A node is a worker machine in Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a> (physical
or virtual machines) running Kubernetes agents, managed by the
<a class=glossary-tooltip title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." data-toggle=tooltip data-placement=top href="/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label="control plane">control plane</a>.
Kubernetes v1.20 supports clusters with up to 5000 nodes. More specifically,
Kubernetes is designed to accommodate configurations that meet <em>all</em> of the following criteria:</p><ul><li>No more than 100 pods per node</li><li>No more than 5000 nodes</li><li>No more than 150000 total pods</li><li>No more than 300000 total containers</li></ul><p>You can scale your cluster by adding or removing nodes. The way you do this depends
on how your cluster is deployed.</p><h2 id=quota-issues>Cloud provider resource quotas</h2><p>To avoid running into cloud provider quota issues, when creating a cluster with many nodes,
consider:</p><ul><li>Request a quota increase for cloud resources such as:<ul><li>Computer instances</li><li>CPUs</li><li>Storage volumes</li><li>In-use IP addresses</li><li>Packet filtering rule sets</li><li>Number of load balancers</li><li>Network subnets</li><li>Log streams</li></ul></li><li>Gate the cluster scaling actions to brings up new nodes in batches, with a pause
between batches, because some cloud providers rate limit the creation of new instances.</li></ul><h2 id=control-plane-components>Control plane components</h2><p>For a large cluster, you need a control plane with sufficient compute and other
resources.</p><p>Typically you would run one or two control plane instances per failure zone,
scaling those instances vertically first and then scaling horizontally after reaching
the point of falling returns to (vertical) scale.</p><p>You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes
nodes do not automatically steer traffic towards control-plane endpoints that are in the
same failure zone; however, your cloud provider might have its own mechanisms to do this.</p><p>For example, using a managed load balancer, you configure the load balancer to send traffic
that originates from the kubelet and Pods in failure zone <em>A</em>, and direct that traffic only
to the control plane hosts that are also in zone <em>A</em>. If a single control-plane host or
endpoint failure zone <em>A</em> goes offline, that means that all the control-plane traffic for
nodes in zone <em>A</em> is now being sent between zones. Running multiple control plane hosts in
each zone makes that outcome less likely.</p><h3 id=etcd-storage>etcd storage</h3><p>To improve performance of large clusters, you can store Event objects in a separate
dedicated etcd instance.</p><p>When creating a cluster, you can (using custom tooling):</p><ul><li>start and configure additional etcd instance</li><li>configure the <a class=glossary-tooltip title="Control plane component that serves the Kubernetes API." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label="API server">API server</a> to use it for storing events</li></ul><h2 id=addon-resources>Addon resources</h2><p>Kubernetes <a href=/docs/concepts/configuration/manage-resources-containers/>resource limits</a>
help to minimize the impact of memory leaks and other ways that pods and containers can
impact on other components. These resource limits apply to
<a class=glossary-tooltip title="Resources that extend the functionality of Kubernetes." data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addon>addon</a> resources just as they apply to application workloads.</p><p>For example, you can set CPU and memory limits for a logging component:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fluent/fluentd-kubernetes-daemonset:v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></code></pre></div><p>Addons' default limits are typically based on data collected from experience running
each addon on small or medium Kubernetes clusters. When running on large
clusters, addons often consume more of some resources than their default limits.
If a large cluster is deployed without adjusting these values, the addon(s)
may continuously get killed because they keep hitting the memory limit.
Alternatively, the addon may run but with poor performance due to CPU time
slice restrictions.</p><p>To avoid running into cluster addon resource issues, when creating a cluster with
many nodes, consider the following:</p><ul><li>Some addons scale vertically - there is one replica of the addon for the cluster
or serving a whole failure zone. For these addons, increase requests and limits
as you scale out your cluster.</li><li>Many addons scale horizontally - you add capacity by running more pods - but with
a very large cluster you may also need to raise CPU or memory limits slightly.
The VerticalPodAutoscaler can run in <em>recommender</em> mode to provide suggested
figures for requests and limits.</li><li>Some addons run as one copy per node, controlled by a <a class=glossary-tooltip title="Ensures a copy of a Pod is running across a set of nodes in a cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>: for example, a node-level log aggregator. Similar to
the case with horizontally-scaled addons, you may also need to raise CPU or memory
limits slightly.</li></ul><h2 id=what-s-next>What's next</h2><p><code>VerticalPodAutoscaler</code> is a custom resource that you can deploy into your cluster
to help you manage resource requests and limits for pods.<br>Visit <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaler</a>
to learn more about <code>VerticalPodAutoscaler</code> and how you can use it to scale cluster
components, including cluster-critical addons.</p><p>The <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>cluster autoscaler</a>
integrates with a number of cloud providers to help you run the right number of
nodes for the level of resource demand in your cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-970615c97499e3651fd3a98e0387cefc>4.2 - Running in multiple zones</h1><p>This page describes running Kubernetes across multiple zones.</p><h2 id=background>Background</h2><p>Kubernetes is designed so that a single Kubernetes cluster can run
across multiple failure zones, typically where these zones fit within
a logical grouping called a <em>region</em>. Major cloud providers define a region
as a set of failure zones (also called <em>availability zones</em>) that provide
a consistent set of features: within a region, each zone offers the same
APIs and services.</p><p>Typical cloud architectures aim to minimize the chance that a failure in
one zone also impairs services in another zone.</p><h2 id=control-plane-behavior>Control plane behavior</h2><p>All <a href=/docs/concepts/overview/components/#control-plane-components>control plane components</a>
support running as a pool of interchangeable resources, replicated per
component.</p><p>When you deploy a cluster control plane, place replicas of
control plane components across multiple failure zones. If availability is
an important concern, select at least three failure zones and replicate
each individual control plane component (API server, scheduler, etcd,
cluster controller manager) across at least three failure zones.
If you are running a cloud controller manager then you should
also replicate this across all the failure zones you selected.</p><blockquote class="note callout"><div><strong>Note:</strong> Kubernetes does not provide cross-zone resilience for the API server
endpoints. You can use various techniques to improve availability for
the cluster API server, including DNS round-robin, SRV records, or
a third-party load balancing solution with health checking.</div></blockquote><h2 id=node-behavior>Node behavior</h2><p>Kubernetes automatically spreads the Pods for
workload resources (such as <a class=glossary-tooltip title="Manages a replicated application on your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>
or <a class=glossary-tooltip title="Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>)
across different nodes in a cluster. This spreading helps
reduce the impact of failures.</p><p>When nodes start up, the kubelet on each node automatically adds
<a class=glossary-tooltip title="Tags objects with identifying attributes that are meaningful and relevant to users." data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> to the Node object
that represents that specific kubelet in the Kubernetes API.
These labels can include
<a href=/docs/reference/labels-annotations-taints/#topologykubernetesiozone>zone information</a>.</p><p>If your cluster spans multiple zones or regions, you can use node labels
in conjunction with
<a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>Pod topology spread constraints</a>
to control how Pods are spread across your cluster among fault domains:
regions, zones, and even specific nodes.
These hints enable the
<a class=glossary-tooltip title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> to place
Pods for better expected availability, reducing the risk that a correlated
failure affects your whole workload.</p><p>For example, you can set a constraint to make sure that the
3 replicas of a StatefulSet are all running in different zones to each
other, whenever that is feasible. You can define this declaratively
without explicitly defining which availability zones are in use for
each workload.</p><h3 id=distributing-nodes-across-zones>Distributing nodes across zones</h3><p>Kubernetes' core does not create nodes for you; you need to do that yourself,
or use a tool such as the <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a> to
manage nodes on your behalf.</p><p>Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.</p><h2 id=manual-zone-assignment-for-pods>Manual zone assignment for Pods</h2><p>You can apply <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector>node selector constraints</a>
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.</p><h2 id=storage-access-for-zones>Storage access for zones</h2><p>When persistent volumes are created, the <code>PersistentVolumeLabel</code>
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>
automatically adds zone labels to any PersistentVolumes that are linked to a specific
zone. The <a class=glossary-tooltip title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> then ensures,
through its <code>NoVolumeZoneConflict</code> predicate, that pods which claim a given PersistentVolume
are only placed into the same zone as that volume.</p><p>You can specify a <a class=glossary-tooltip title="A StorageClass provides a way for administrators to describe different available storage types." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/storage-classes target=_blank aria-label=StorageClass>StorageClass</a>
for PersistentVolumeClaims that specifies the failure domains (zones) that the
storage in that class may use.
To learn about configuring a StorageClass that is aware of failure domains or zones,
see <a href=/docs/concepts/storage/storage-classes/#allowed-topologies>Allowed topologies</a>.</p><h2 id=networking>Networking</h2><p>By itself, Kubernetes does not include zone-aware networking. You can use a
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a>
to configure cluster networking, and that network solution might have zone-specific
elements. For example, if your cloud provider supports Services with
<code>type=LoadBalancer</code>, the load balancer might only send traffic to Pods running in the
same zone as the load balancer element processing a given connection.
Check your cloud provider's documentation for details.</p><p>For custom or on-premises deployments, similar considerations apply.
<a class=glossary-tooltip title="A way to expose an application running on a set of Pods as a network service." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> and
<a class=glossary-tooltip title="An API object that manages external access to the services in a cluster, typically HTTP." data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/ingress/ target=_blank aria-label=Ingress>Ingress</a> behavior, including handling
of different failure zones, does vary depending on exactly how your cluster is set up.</p><h2 id=fault-recovery>Fault recovery</h2><p>When you set up your cluster, you might also need to consider whether and how
your setup can restore service if all the failure zones in a region go
off-line at the same time. For example, do you rely on there being at least
one node able to run Pods in a zone?<br>Make sure that any cluster-critical repair work does not rely
on there being at least one healthy node in your cluster. For example: if all nodes
are unhealthy, you might need to run a repair Job with a special
<a class=glossary-tooltip title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=toleration>toleration</a> so that the repair
can complete enough to bring at least one node into service.</p><p>Kubernetes doesn't come with an answer for this challenge; however, it's
something to consider.</p><h2 id=what-s-next>What's next</h2><p>To learn how the scheduler places Pods in a cluster, honoring the configured constraints,
visit <a href=/docs/concepts/scheduling-eviction/>Scheduling and Eviction</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>4.3 - Validate node setup</h1><h2 id=node-conformance-test>Node Conformance Test</h2><p><em>Node conformance test</em> is a containerized test framework that provides a system
verification and functionality test for a node. The test validates whether the
node meets the minimum requirements for Kubernetes; a node that passes the test
is qualified to join a Kubernetes cluster.</p><h2 id=node-prerequisite>Node Prerequisite</h2><p>To run node conformance test, a node must satisfy the same prerequisites as a
standard Kubernetes node. At a minimum, the node should have the following
daemons installed:</p><ul><li>Container Runtime (Docker)</li><li>Kubelet</li></ul><h2 id=running-node-conformance-test>Running Node Conformance Test</h2><p>To run the node conformance test, perform the following steps:</p><ol><li>Work out the value of the <code>--kubeconfig</code> option for the kubelet; for example:
<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
Because the test framework starts a local control plane to test the kubelet,
use <code>http://localhost:8080</code> as the URL of the API server.
There are some other kubelet command line parameters you may want to use:</li></ol><ul><li><code>--pod-cidr</code>: If you are using <code>kubenet</code>, you should specify an arbitrary CIDR
to Kubelet, for example <code>--pod-cidr=10.180.0.0/24</code>.</li><li><code>--cloud-provider</code>: If you are using <code>--cloud-provider=gce</code>, you should
remove the flag to run the test.</li></ul><ol start=2><li>Run the node conformance test with command:</li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># $CONFIG_DIR is the pod manifest path of your Kubelet.</span>
<span style=color:#080;font-style:italic># $LOG_DIR is the test output path.</span>
sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  k8s.gcr.io/node-test:0.2
</code></pre></div><h2 id=running-node-conformance-test-for-other-architectures>Running Node Conformance Test for Other Architectures</h2><p>Kubernetes also provides node conformance test docker images for other
architectures:</p><table><thead><tr><th>Arch</th><th style=text-align:center>Image</th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td></tr></tbody></table><h2 id=running-selected-test>Running Selected Test</h2><p>To run specific tests, overwrite the environment variable <code>FOCUS</code> with the
regular expression of tests you want to run.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
  k8s.gcr.io/node-test:0.2
</code></pre></div><p>To skip specific tests, overwrite the environment variable <code>SKIP</code> with the
regular expression of tests you want to skip.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Run all conformance tests but skip MirrorPod test</span>
  k8s.gcr.io/node-test:0.2
</code></pre></div><p>Node conformance test is a containerized version of <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md>node e2e test</a>.
By default, it runs all conformance tests.</p><p>Theoretically, you can run any node e2e test if you configure the container and
mount required volumes properly. But <strong>it is strongly recommended to only run conformance
test</strong>, because it requires much more complex configuration to run non-conformance test.</p><h2 id=caveats>Caveats</h2><ul><li>The test leaves some docker images on the node, including the node conformance
test image and images of containers used in the functionality
test.</li><li>The test leaves dead containers on the node. These containers are created
during the functionality test.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>4.4 - PKI certificates and requirements</h1><p>Kubernetes requires PKI certificates for authentication over TLS.
If you install Kubernetes with <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>, the certificates that your cluster requires are automatically generated.
You can also generate your own certificates -- for example, to keep your private keys more secure by not storing them on the API server.
This page explains the certificates that your cluster requires.</p><h2 id=how-certificates-are-used-by-your-cluster>How certificates are used by your cluster</h2><p>Kubernetes requires PKI for the following operations:</p><ul><li>Client certificates for the kubelet to authenticate to the API server</li><li>Server certificate for the API server endpoint</li><li>Client certificates for administrators of the cluster to authenticate to the API server</li><li>Client certificates for the API server to talk to the kubelets</li><li>Client certificate for the API server to talk to etcd</li><li>Client certificate/kubeconfig for the controller manager to talk to the API server</li><li>Client certificate/kubeconfig for the scheduler to talk to the API server.</li><li>Client and server certificates for the <a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-proxy</a></li></ul><blockquote class="note callout"><div><strong>Note:</strong> <code>front-proxy</code> certificates are required only if you run kube-proxy to support <a href=/docs/tasks/extend-kubernetes/setup-extension-api-server/>an extension API server</a>.</div></blockquote><p>etcd also implements mutual TLS to authenticate clients and peers.</p><h2 id=where-certificates-are-stored>Where certificates are stored</h2><p>If you install Kubernetes with kubeadm, certificates are stored in <code>/etc/kubernetes/pki</code>. All paths in this documentation are relative to that directory.</p><h2 id=configure-certificates-manually>Configure certificates manually</h2><p>If you don't want kubeadm to generate the required certificates, you can create them in either of the following ways.</p><h3 id=single-root-ca>Single root CA</h3><p>You can create a single root CA, controlled by an administrator. This root CA can then create multiple intermediate CAs, and delegate all further creation to Kubernetes itself.</p><p>Required CAs:</p><table><thead><tr><th>path</th><th>Default CN</th><th>description</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes general CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>For all etcd-related functions</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>For the <a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-end proxy</a></td></tr></tbody></table><p>On top of the above CAs, it is also necessary to get a public/private key pair for service account management, <code>sa.key</code> and <code>sa.pub</code>.</p><h3 id=all-certificates>All certificates</h3><p>If you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.</p><p>Required certificates:</p><table><thead><tr><th>Default CN</th><th>Parent CA</th><th>O (in Subject)</th><th>kind</th><th>hosts (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: any other IP or DNS name you contact your cluster on (as used by <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>
the load balancer stable IP and/or DNS name, <code>kubernetes</code>, <code>kubernetes.default</code>, <code>kubernetes.default.svc</code>,
<code>kubernetes.default.svc.cluster</code>, <code>kubernetes.default.svc.cluster.local</code>)</p><p>where <code>kind</code> maps to one or more of the <a href=https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage>x509 key usage</a> types:</p><table><thead><tr><th>kind</th><th>Key usage</th></tr></thead><tbody><tr><td>server</td><td>digital signature, key encipherment, server auth</td></tr><tr><td>client</td><td>digital signature, key encipherment, client auth</td></tr></tbody></table><blockquote class="note callout"><div><strong>Note:</strong> Hosts/SAN listed above are the recommended ones for getting a working cluster; if required by a specific setup, it is possible to add additional SANs on all the server certificates.</div></blockquote><blockquote class="note callout"><div><strong>Note:</strong><p>For kubeadm users only:</p><ul><li>The scenario where you are copying to your cluster CA certificates without private keys is referred as external CA in the kubeadm documentation.</li><li>If you are comparing the above list with a kubeadm generated PKI, please be aware that <code>kube-etcd</code>, <code>kube-etcd-peer</code> and <code>kube-etcd-healthcheck-client</code> certificates
are not generated in case of external etcd.</li></ul></div></blockquote><h3 id=certificate-paths>Certificate paths</h3><p>Certificates should be placed in a recommended path (as used by <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>).
Paths should be specified using the given argument regardless of location.</p><table><thead><tr><th>Default CN</th><th>recommended key path</th><th>recommended cert path</th><th>command</th><th>key argument</th><th>cert argument</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>Same considerations apply for the service account key pair:</p><table><thead><tr><th>private key path</th><th>public key path</th><th>command</th><th>argument</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><h2 id=configure-certificates-for-user-accounts>Configure certificates for user accounts</h2><p>You must manually configure these administrator account and service accounts:</p><table><thead><tr><th>filename</th><th>credential name</th><th>Default CN</th><th>O (in Subject)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> (see note)</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><blockquote class="note callout"><div><strong>Note:</strong> The value of <code>&lt;nodeName></code> for <code>kubelet.conf</code> <strong>must</strong> match precisely the value of the node name provided by the kubelet as it registers with the apiserver. For further details, read the <a href=/docs/reference/access-authn-authz/node/>Node Authorization</a>.</div></blockquote><ol><li><p>For each config, generate an x509 cert/key pair with the given CN and O.</p></li><li><p>Run <code>kubectl</code> as follows for each config:</p></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</code></pre></div><p>These files are used as follows:</p><table><thead><tr><th>filename</th><th>command</th><th>comment</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>Configures administrator user for the cluster</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>One required for each node in the cluster.</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>Must be added to manifest in <code>manifests/kube-controller-manager.yaml</code></td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>Must be added to manifest in <code>manifests/kube-scheduler.yaml</code></td></tr></tbody></table></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>