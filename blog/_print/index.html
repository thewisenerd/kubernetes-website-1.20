<!doctype html><html lang=en class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=zh href=https://kubernetes.io/zh/blog/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/blog/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/blog/><link rel=alternate hreflang=it href=https://kubernetes.io/it/blog/><link rel=alternate hreflang=de href=https://kubernetes.io/de/blog/><link rel=alternate hreflang=es href=https://kubernetes.io/es/blog/><link rel=alternate hreflang=pt href=https://kubernetes.io/pt/blog/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/blog/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/blog/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Kubernetes Blog | Kubernetes</title><meta property="og:title" content="Kubernetes Blog"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/blog/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Kubernetes Blog"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes Blog"><meta name=twitter:description content="Production-Grade Container Orchestration"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/blog/"><meta property="og:title" content="Kubernetes Blog"><meta name=twitter:title content="Kubernetes Blog"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class="td-section td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/docs/>Documentation</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/blog/>Kubernetes Blog</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/blog/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/blog/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/blog/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/blog/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/blog/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh/blog/>中文 Chinese</a>
<a class=dropdown-item href=/ko/blog/>한국어 Korean</a>
<a class=dropdown-item href=/fr/blog/>Français</a>
<a class=dropdown-item href=/it/blog/>Italiano</a>
<a class=dropdown-item href=/de/blog/>Deutsch</a>
<a class=dropdown-item href=/es/blog/>Español</a>
<a class=dropdown-item href=/pt/blog/>Português</a>
<a class=dropdown-item href=/vi/blog/>Tiếng Việt</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/>Return to the regular view of this page</a>.</p></div><h1 class=title>Kubernetes Blog</h1><ul><li><a href=#pg-ea1371ce5ca79a0c65620b5bfa13e8b0>PodSecurityPolicy Deprecation: Past, Present, and Future</a></li><li><a href=#pg-69e11eee0766a8c8b934b7a30143dc2b>The Evolution of Kubernetes Dashboard</a></li><li><a href=#pg-89bf34261431a73d36cd93a22c4b1e31>A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</a></li><li><a href=#pg-c06e007d763ae4af520000567d5724c6>Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</a></li><li><a href=#pg-affda69f3ae1111f8ade984c2fca19bf>Third Party Device Metrics Reaches GA</a></li><li><a href=#pg-34d9423bdc9e6ba8808d2cc317f13351>Kubernetes 1.20: Granular Control of Volume Permission Changes</a></li><li><a href=#pg-75cd63817127b5552cbec02c54f84994>Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</a></li><li><a href=#pg-ced3d9254f631ed8dfb689bd1dd7c259>Kubernetes 1.20: The Raddest Release</a></li><li><a href=#pg-bd9668ad6296597dee13c6ff3324b319>GSoD 2020: Improving the API Reference Experience</a></li><li><a href=#pg-4f7882145418a14c275e957b0e8979f4>Dockershim Deprecation FAQ</a></li><li><a href=#pg-d0001c21fb6f2f9b21c0faa11a149fae>Don't Panic: Kubernetes and Docker</a></li><li><a href=#pg-93f76fe9b1d669349b8d9228bd9661b4>Cloud native security for your clusters</a></li><li><a href=#pg-beabb22510a72a7565a7e034c4a88d15>Remembering Dan Kohn</a></li><li><a href=#pg-614959f25c84da82dbfcf7287dd203aa>Announcing the 2020 Steering Committee Election Results</a></li><li><a href=#pg-fcbc001800469c30982bca14879a3acb>Contributing to the Development Guide</a></li><li><a href=#pg-b29c63b1bca54209fe57b9ee517b9576>GSoC 2020 - Building operators for cluster addons</a></li><li><a href=#pg-d9e35cec65c1d4e7fff2cc66f8423d52>Introducing Structured Logs</a></li><li><a href=#pg-e99acccbdb5593353cf25826879389e4>Warning: Helpful Warnings Ahead</a></li><li><a href=#pg-c911fec01c1b20f202397599afbea2d2>Scaling Kubernetes Networking With EndpointSlices</a></li><li><a href=#pg-7d8bbd8a56ada09e69d73bcf3909a2cb>Ephemeral volumes with storage capacity tracking: EmptyDir on steroids</a></li><li><a href=#pg-1518ae59824ac977039974dd95569a66>Increasing the Kubernetes Support Window to One Year</a></li><li><a href=#pg-217df5d2adc08627a1318be8af2d2186>Kubernetes 1.19: Accentuate the Paw-sitive</a></li><li><a href=#pg-dc8f1ff78aa5c93bd94abaefc83b6e8d>Moving Forward From Beta</a></li><li><a href=#pg-4f3d630442b7114ebb79e901c3987452>Introducing Hierarchical Namespaces</a></li><li><a href=#pg-9a1d876453f4173bc6e8e98926903aa9>Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</a></li><li><a href=#pg-462bb8169b38d42b97315b740fae2e19>Music and math: the Kubernetes 1.17 release interview</a></li><li><a href=#pg-c4eb578511a6303fc1731bbf4979a273>SIG-Windows Spotlight</a></li><li><a href=#pg-dc026ba2ad8a7e732e1a2228a0cdb179>Working with Terraform and Kubernetes</a></li><li><a href=#pg-3b8326c4fb648224fb9cbdd4289cf6e8>A Better Docs UX With Docsy</a></li><li><a href=#pg-43dc00bef57d6128b69dd2afe2143f47>Supporting the Evolving Ingress Specification in Kubernetes 1.18</a></li><li><a href=#pg-3c57eea59e90455a9ab22078c7fc1c3f>K8s KPIs with Kuberhealthy</a></li><li><a href=#pg-fde3199bf6bbc61752a7fbe6c3468c89>My exciting journey into Kubernetes’ history</a></li><li><a href=#pg-d33d7145d9c3587598bfedee38e4dc48>An Introduction to the K8s-Infrastructure Working Group</a></li><li><a href=#pg-fe388f92f881375f927332275d292981>WSL+Docker: Kubernetes on the Windows Desktop</a></li><li><a href=#pg-9cfa9082e191b21fc55a408bdb127095>How Docs Handle Third Party and Dual Sourced Content</a></li><li><a href=#pg-966d134b4e43eb259c24922ba4635311>Introducing PodTopologySpread</a></li><li><a href=#pg-bba5d56256c14a660449365ac3803ece>Two-phased Canary Rollout with Open Source Gloo</a></li><li><a href=#pg-e62fc55b46cc494898cd6d42cd437646>Cluster API v1alpha3 Delivers New Features and an Improved User Experience</a></li><li><a href=#pg-cf21d879935f0508e0410a1deb1e1339>How Kubernetes contributors are building a better communication process</a></li><li><a href=#pg-b0f1df51dcc400e9f59126cd3120fb45>API Priority and Fairness Alpha</a></li><li><a href=#pg-e4bdb6be4946e6f9bbea5e9b7a7aeb45>Introducing Windows CSI support alpha for Kubernetes</a></li><li><a href=#pg-c1a4f74c03f8c8a0605e152bb006a5e1>Improvements to the Ingress API in Kubernetes 1.18</a></li><li><a href=#pg-4771ce69b566c519a6f2d40cece40049>Kubernetes 1.18 Feature Server-side Apply Beta 2</a></li><li><a href=#pg-d1357e1de94f06acc5d502535391b75b>Kubernetes Topology Manager Moves to Beta - Align Up!</a></li><li><a href=#pg-b510a26fb64945b86595c4992b051dbd>Kubernetes 1.18: Fit & Finish</a></li><li><a href=#pg-d2b274af3e065e40387815705125ab62>Join SIG Scalability and Learn Kubernetes the Hard Way</a></li><li><a href=#pg-e71e6d28cf7cadef7b3c7f414de2496c>Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</a></li><li><a href=#pg-12001c222627cf4a9b495bdceed5ba8a>Contributor Summit Amsterdam Postponed</a></li><li><a href=#pg-44e9c6545509d2a79b8cb50de92907dc>Bring your ideas to the world with kubectl plugins</a></li><li><a href=#pg-a9ad2d376559402c732199cd9182446c>Contributor Summit Amsterdam Schedule Announced</a></li><li><a href=#pg-398cfdd54f0eb2fc9ac98357a2ae424c>Deploying External OpenStack Cloud Provider with Kubeadm</a></li><li><a href=#pg-e332ba17f4aeb04e399bda303f280be7>KubeInvaders - Gamified Chaos Engineering Tool for Kubernetes</a></li><li><a href=#pg-199e33ba6883a21dfa31ee59c9b686e2>CSI Ephemeral Inline Volumes</a></li><li><a href=#pg-0152ead4467ad6155722ba3071c4c16a>Reviewing 2019 in Docs</a></li><li><a href=#pg-951c3eb34e5599cb9806fb380774663c>Kubernetes on MIPS</a></li><li><a href=#pg-20eba094421fb566b3bc2a466a227395>Announcing the Kubernetes bug bounty program</a></li><li><a href=#pg-0e2b6512216f3a839a62bc7e32719ef3>Remembering Brad Childs</a></li><li><a href=#pg-376a8b22b6e032784b022da0e4b4e12a>Testing of CSI drivers</a></li><li><a href=#pg-9e22e84688a3e1e7622241facd484d8a>Kubernetes 1.17: Stability</a></li><li><a href=#pg-fffcd16b4766965ab6bdcab60b22457d>Kubernetes 1.17 Feature: Kubernetes Volume Snapshot Moves to Beta</a></li><li><a href=#pg-ee4de1e99955b909ad9a400db28557fb>Kubernetes 1.17 Feature: Kubernetes In-Tree to CSI Volume Migration Moves to Beta</a></li><li><a href=#pg-eb9814896ba0d24124ab45f0ea3c5c02>When you're in the release team, you're family: the Kubernetes 1.16 release interview</a></li><li><a href=#pg-545b4843398d50cb878ea56279c98d76>Gardener Project Update</a></li><li><a href=#pg-601488fd56d4096c09b2a9959d0e7a43>Develop a Kubernetes controller in Java</a></li><li><a href=#pg-b16b2be554bf6272235c07443e287255>Running Kubernetes locally on Linux with Microk8s</a></li><li><a href=#pg-199a2412eb47773a4ae2e740cb9d0ded>Grokkin' the Docs</a></li><li><a href=#pg-464e22d2595c073a96a61069f2fd202f>Kubernetes Documentation Survey</a></li><li><a href=#pg-fb4706cbf52d410fa926ccb78d30b84f>Contributor Summit San Diego Schedule Announced!</a></li><li><a href=#pg-fc6fc922d4286e1fe8e3289d1094bdf8>2019 Steering Committee Election Results</a></li><li><a href=#pg-aed805df2d0756cee0216566e206c2e0>Contributor Summit San Diego Registration Open!</a></li><li><a href=#pg-848cbe6754810f86c360ea5906b4e16d>Kubernetes 1.16: Custom Resources, Overhauled Metrics, and Volume Extensions</a></li><li><a href=#pg-dcfa29bde9d7191277f1eb6654189b99>Announcing etcd 3.4</a></li><li><a href=#pg-87d6b9119a0096eaf905a06844242333>OPA Gatekeeper: Policy and Governance for Kubernetes</a></li><li><a href=#pg-0ead202f9f4c8cc2966564a0649a52f6>Get started with Kubernetes (using Python)</a></li><li><a href=#pg-21fa818ae63e4f4893fae6a94d6d008e>Deprecated APIs Removed In 1.16: Here’s What You Need To Know</a></li><li><a href=#pg-5b056cc37326bb16c7334dca184370f4>Recap of Kubernetes Contributor Summit Barcelona 2019</a></li><li><a href=#pg-0f38c170d1a63c8632eb3ec160e4f619>Automated High Availability in kubeadm v1.15: Batteries Included But Swappable</a></li><li><a href=#pg-b240f0214be444bad86ee5626abfc100>Introducing Volume Cloning Alpha for Kubernetes</a></li><li><a href=#pg-a292df47eabb3613fa5e9bf7722de93b>Future of CRDs: Structural Schemas</a></li><li><a href=#pg-73c739c23ae254717a9e5f21cb90dc1e>Kubernetes 1.15: Extensibility and Continuous Improvement</a></li><li><a href=#pg-037f0eed4d73549cc2f339290039e718>Join us at the Contributor Summit in Shanghai</a></li><li><a href=#pg-e8bd664629095c8948e08f12a40e814c>Kyma - extend and build on Kubernetes with ease</a></li><li><a href=#pg-e13ad74063938b0e8e09f8920001a9e7>Kubernetes, Cloud Native, and the Future of Software</a></li><li><a href=#pg-61a7d410a8742331c4512c365210431d>Expanding our Contributor Workshops</a></li><li><a href=#pg-087af74107d4d9256924532b505ac3b0>Cat shirts and Groundhog Day: the Kubernetes 1.14 release interview</a></li><li><a href=#pg-d6c48cd505e152b46e8e288eb622abf3>Join us for the 2019 KubeCon Diversity Lunch & Hack</a></li><li><a href=#pg-d21acaf645c18e3c642d1cbc4482e575>How You Can Help Localize Kubernetes Docs</a></li><li><a href=#pg-5a761bb35081bc64e1f7247cf0de7cf0>Hardware Accelerated SSL/TLS Termination in Ingress Controllers using Kubernetes Device Plugins and RuntimeClass</a></li><li><a href=#pg-9a73688c3a1583ce5dc650516cef2c37>Introducing kube-iptables-tailer: Better Networking Visibility in Kubernetes Clusters</a></li><li><a href=#pg-29a85b82a82ea398f1862b692cc0983e>The Future of Cloud Providers in Kubernetes</a></li><li><a href=#pg-edde2cfeb109220abd1d26cb390060b0>Pod Priority and Preemption in Kubernetes</a></li><li><a href=#pg-d59cccde9a0772812b0e105b30f5b7c7>Process ID Limiting for Stability Improvements in Kubernetes 1.14</a></li><li><a href=#pg-710bd32d821afac28e34b616fa45b092>Kubernetes 1.14: Local Persistent Volumes GA</a></li><li><a href=#pg-c1183bd13c423e576d983d923912b48f>Kubernetes v1.14 delivers production-level support for Windows nodes and Windows containers</a></li><li><a href=#pg-13d6a556a0a6fc7a3c2f8be730f926db>kube-proxy Subtleties: Debugging an Intermittent Connection Reset</a></li><li><a href=#pg-850bc36dd2ebd986000338492953967f>Running Kubernetes locally on Linux with Minikube - now with Kubernetes 1.14 support</a></li><li><a href=#pg-e78429ce029dc0853cf8b0f87aab2d66>Kubernetes 1.14: Production-level support for Windows Nodes, Kubectl Updates, Persistent Local Volumes GA</a></li><li><a href=#pg-0ec42c7422a7e045f9bdf5079f72cb85>Kubernetes End-to-end Testing for Everyone</a></li><li><a href=#pg-09e07ea24cc309e3d8a5fcd4c22bafe4>A Guide to Kubernetes Admission Controllers</a></li><li><a href=#pg-34045ffb28ad186738e3c43508ded599>A Look Back and What's in Store for Kubernetes Contributor Summits</a></li><li><a href=#pg-fdffb224c52f5dc23b58f61b3bacc2a7>KubeEdge, a Kubernetes Native Edge Computing Framework</a></li><li><a href=#pg-adc41261346fc01af487b093b7dac8c7>Kubernetes Setup Using Ansible and Vagrant</a></li><li><a href=#pg-19cd0c4556be5dfd65e562e60778ef83>Raw Block Volume support to Beta</a></li><li><a href=#pg-01e11a71b0c44482439c19248d7559c5>Automate Operations on your Cluster with OperatorHub.io</a></li><li><a href=#pg-4b1a370b544d5c6b8d88292e9f3b691b>Building a Kubernetes Edge (Ingress) Control Plane for Envoy v2</a></li><li><a href=#pg-83e0e3e4554a758f903a88d1bfb65118>Runc and CVE-2019-5736</a></li><li><a href=#pg-e7d9b9c3a596900b88b644a72f10d2d3>Poseidon-Firmament Scheduler – Flow Network Graph Based Scheduler</a></li><li><a href=#pg-cbbb564a6c5a16bbf59a355b79dfee03>Update on Volume Snapshot Alpha for Kubernetes</a></li><li><a href=#pg-a9a9fec669d1f677c64e58cb5feb3b43>Container Storage Interface (CSI) for Kubernetes GA</a></li><li><a href=#pg-bab3cd81db1bb9b4d144a957121355a9>APIServer dry-run and kubectl diff</a></li><li><a href=#pg-c6d29bd07e6e325a3eea49e1cf2cfefd>Kubernetes Federation Evolution</a></li><li><a href=#pg-f9a43c5c77029d8b5d1f024a2c34e056>etcd: Current status and future roadmap</a></li><li><a href=#pg-015d4916d80a01d1f89156ab0583fd6c>New Contributor Workshop Shanghai</a></li><li><a href=#pg-5277ea0d4854d7761ef876f9ed4a151b>Production-Ready Kubernetes Cluster Creation with kubeadm</a></li><li><a href=#pg-02d3dfa7ff4d2d5a8f59e09d88b9217e>Kubernetes 1.13: Simplified Cluster Management with Kubeadm, Container Storage Interface (CSI), and CoreDNS as Default DNS are Now Generally Available</a></li><li><a href=#pg-5193292ae0d55535d8029ff9a7566fca>Kubernetes Docs Updates, International Edition</a></li><li><a href=#pg-15ef0489e073bddfa8d3cd35e3b189df>gRPC Load Balancing on Kubernetes without Tears</a></li><li><a href=#pg-e07d63b1f3e1dff24501d92717c4ce52>Tips for Your First Kubecon Presentation - Part 2</a></li><li><a href=#pg-5defc40e42ae29676f37e15b364e2948>Tips for Your First Kubecon Presentation - Part 1</a></li><li><a href=#pg-5acf5044e610fc3bc7a65c6616cdf99d>Kubernetes 2018 North American Contributor Summit</a></li><li><a href=#pg-241e2cb43c3584096fd288edfdbbdebb>2018 Steering Committee Election Results</a></li><li><a href=#pg-f460d202f8a5ec9dfaefd832c33cfa7a>Topology-Aware Volume Provisioning in Kubernetes</a></li><li><a href=#pg-744fea7e150e5cd9616530e5a9339c47>Kubernetes v1.12: Introducing RuntimeClass</a></li><li><a href=#pg-893d8df6ae10a8e8631db99cae979e87>Introducing Volume Snapshot Alpha for Kubernetes</a></li><li><a href=#pg-af9abd5ba8bd4dd15dc8503f6dc2b1ff>Support for Azure VMSS, Cluster-Autoscaler and User Assigned Identity</a></li><li><a href=#pg-2323347af026c34a08d65e44879345ed>Introducing the Non-Code Contributor’s Guide</a></li><li><a href=#pg-cfe43151cb19b506570e75c8f0a61259>KubeDirector: The easy way to run complex stateful applications on Kubernetes</a></li><li><a href=#pg-0fc9e8fed3020770724c866512e3bc08>Building a Network Bootable Server Farm for Kubernetes with LTSP</a></li><li><a href=#pg-e38db7f640df9065a21c341cfbe4705c>Health checking gRPC servers on Kubernetes</a></li><li><a href=#pg-16e83787afdcc9efee9bb0c798bd9134>Kubernetes 1.12: Kubelet TLS Bootstrap and Azure Virtual Machine Scale Sets (VMSS) Move to General Availability</a></li><li><a href=#pg-a027c0eb9a8cd0ade81a98a50bed51b1>Hands On With Linkerd 2.0</a></li><li><a href=#pg-7855b5e12fb40ef21a91c33a9b5b9b59>2018 Steering Committee Election Cycle Kicks Off</a></li><li><a href=#pg-976a1c2afde41e47eb984e28c88c7085>The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</a></li><li><a href=#pg-0e02457e323dc3bf8fea133fc9657a2c>Introducing Kubebuilder: an SDK for building Kubernetes APIs using CRDs</a></li><li><a href=#pg-ee824f5efff26f6e8a06bcfecb11a9ce>Out of the Clouds onto the Ground: How to Make Kubernetes Production Grade Anywhere</a></li><li><a href=#pg-5cd188187bd5cac97e56eebd5891dd04>Dynamically Expand Volume with CSI and Kubernetes</a></li><li><a href=#pg-952c1a872bccffe28c4f0737dd9a3548>KubeVirt: Extending Kubernetes with CRDs for Virtualized Workloads</a></li><li><a href=#pg-04b7e5d7125964da493f2ae401baca35>Feature Highlight: CPU Manager</a></li><li><a href=#pg-82df37be5b315b12749701fd3a3426c2>The History of Kubernetes & the Community Behind It</a></li><li><a href=#pg-c61517aae0c272388dc0a2b9867a2b0d>Kubernetes Wins the 2018 OSCON Most Impact Award</a></li><li><a href=#pg-b5c84fcebf9b418559579d3f0ae1ba90>11 Ways (Not) to Get Hacked</a></li><li><a href=#pg-c7a6473357fd6808fc9ac06453b45883>How the sausage is made: the Kubernetes 1.11 release interview, from the Kubernetes Podcast</a></li><li><a href=#pg-4ce1d602e0512565b4d1b6e9a57367e1>Resizing Persistent Volumes using Kubernetes</a></li><li><a href=#pg-37f9b93223232a11f579d220e199b9d1>Dynamic Kubelet Configuration</a></li><li><a href=#pg-4d1b8243b08fd75f7c6f7a1ee1b31e1c>CoreDNS GA for Kubernetes Cluster DNS</a></li><li><a href=#pg-2e5e79eb95ef7d07df43b968c4f8c326>Meet Our Contributors - Monthly Streaming YouTube Mentoring Series</a></li><li><a href=#pg-ff1a9a29a39941763cf19d669d7fbbcc>IPVS-Based In-Cluster Load Balancing Deep Dive</a></li><li><a href=#pg-56b1754609f34aa3b199a3e9af607d92>Airflow on Kubernetes (Part 1): A Different Kind of Operator</a></li><li><a href=#pg-9398fd1cfd158f73710f9e089de0fbac>Kubernetes 1.11: In-Cluster Load Balancing and CoreDNS Plugin Graduate to General Availability</a></li><li><a href=#pg-416d1fed0ff270b6e7f8838c494f3117>Dynamic Ingress in Kubernetes</a></li><li><a href=#pg-bfad75a93816a9020a268af7543370fe>4 Years of K8s</a></li><li><a href=#pg-819e0ad707ce8b6e6e6d031bb89f354d>Say Hello to Discuss Kubernetes</a></li><li><a href=#pg-eb3baac87f981215a7ff26debd0db5d3>Introducing kustomize; Template-free Configuration Customization for Kubernetes</a></li><li><a href=#pg-567d5b08adf4dd0e7e63748193b37c61>Kubernetes Containerd Integration Goes GA</a></li><li><a href=#pg-c66e92b0acdfea516c8983cc62d032fd>Getting to Know Kubevirt</a></li><li><a href=#pg-86c8f3b0d16c35a20cb363fb5f3f686d>Gardener - The Kubernetes Botanist</a></li><li><a href=#pg-097264f98e024a5d7313268988d5ac8f>Docs are Migrating from Jekyll to Hugo</a></li><li><a href=#pg-ff99d5de7d522c022ef82151d5e698fa>Announcing Kubeflow 0.1</a></li><li><a href=#pg-9b38d1f6d5473fb1c25057fda021e39c>Current State of Policy in Kubernetes</a></li><li><a href=#pg-b1db29e1fa629592ea431f588f2145fc>Developing on Kubernetes</a></li><li><a href=#pg-c43d9f2ccc16c77f1acf7bacc29a1d6a>Zero-downtime Deployment in Kubernetes with Jenkins</a></li><li><a href=#pg-0b6913088687941c6716b37e47562f16>Kubernetes Community - Top of the Open Source Charts in 2017</a></li><li><a href=#pg-7873247c56ddad517959742a96b86aa0>Kubernetes Application Survey 2018 Results</a></li><li><a href=#pg-64a93ffbe0608e658b1e538bcc0cfba4>Local Persistent Volumes for Kubernetes Goes Beta</a></li><li><a href=#pg-53c6acca1722e82e1dde5f0e07f5d0b7>Migrating the Kubernetes Blog</a></li><li><a href=#pg-71feebf36b2dd94ef96cf22800c61a81>Container Storage Interface (CSI) for Kubernetes Goes Beta</a></li><li><a href=#pg-2fcbe5a762cb6818b72059b6ba792071>Fixing the Subpath Volume Vulnerability in Kubernetes</a></li><li><a href=#pg-f004182d259941e1fe7af510b3ab62f1>Kubernetes 1.10: Stabilizing Storage, Security, and Networking</a></li><li><a href=#pg-58315accba126f61921f8cdcf34ab405>Principles of Container-based Application Design</a></li><li><a href=#pg-3fa9fa5e19bf21d00ebfb18ca31a9f61>Expanding User Support with Office Hours</a></li><li><a href=#pg-949c70280976aa8bb648ff21796fa901>How to Integrate RollingUpdate Strategy for TPR in Kubernetes</a></li><li><a href=#pg-7de8566e09f1f9563eebe44fedd66cc5>Apache Spark 2.3 with Native Kubernetes Support</a></li><li><a href=#pg-46ba7c4e2f63c0643b347ea9a32b2bed>Kubernetes: First Beta Version of Kubernetes 1.10 is Here</a></li><li><a href=#pg-b417e9a3d65d70ec0a327cce96177503>Reporting Errors from Control Plane to Applications Using Kubernetes Events</a></li><li><a href=#pg-7e3c4ed0cfb0bab3da0672d198f0d0fd>Core Workloads API GA</a></li><li><a href=#pg-e75b26684359faadd1d61c5d0c6cbe40>Introducing client-go version 6</a></li><li><a href=#pg-e69ea2d567ef1a27cb6eae6d90305e92>Extensible Admission is Beta</a></li><li><a href=#pg-9566f2f64212ff7827bef718c0f48163>Introducing Container Storage Interface (CSI) Alpha for Kubernetes</a></li><li><a href=#pg-f018685bed3dd1eaad90e61436e60e10>Kubernetes v1.9 releases beta support for Windows Server Containers</a></li><li><a href=#pg-50e09dc12a57b620dfc2e36758a63903>Five Days of Kubernetes 1.9</a></li><li><a href=#pg-2f6487a6fb6a258f8dd34903ea98ed36>Introducing Kubeflow - A Composable, Portable, Scalable ML Stack Built for Kubernetes</a></li><li><a href=#pg-bb3da9db31fdb4f015c6930857e2df92>Kubernetes 1.9: Apps Workloads GA and Expanded Ecosystem</a></li><li><a href=#pg-a5aff75686a87220001cbe27bcda2cad>Using eBPF in Kubernetes</a></li><li><a href=#pg-e8fa47a93b8e7ae8336cb369f1710f7d>PaddlePaddle Fluid: Elastic Deep Learning on Kubernetes</a></li><li><a href=#pg-bb3ab16fad15d53ca096effeae4cc253>Autoscaling in Kubernetes</a></li><li><a href=#pg-b848899277d49228b42ed8a172d8bd98>Certified Kubernetes Conformance Program: Launch Celebration Round Up</a></li><li><a href=#pg-6e2f778b3b741a46a450bca4c9d0aee8>Kubernetes is Still Hard (for Developers)</a></li><li><a href=#pg-2605319ddc2ce7e9f0fcc32f893d4b82>Securing Software Supply Chain with Grafeas</a></li><li><a href=#pg-ce02ca3769c2c57ae67a61b96ef89b47>Containerd Brings More Container Runtime Options for Kubernetes</a></li><li><a href=#pg-a6359bde9cacd401a01c05bfedf54a5a>Kubernetes the Easy Way</a></li><li><a href=#pg-ef6eae1d4faf2d7f58a8043ea4f40f28>Enforcing Network Policies in Kubernetes</a></li><li><a href=#pg-c701c89bced6c8429688cf438865f315>Using RBAC, Generally Available in Kubernetes v1.8</a></li><li><a href=#pg-bec925973774692910e48225303890bc>It Takes a Village to Raise a Kubernetes</a></li><li><a href=#pg-8526f677b2724ff689dd611cabc2ca93>kubeadm v1.8 Released: Introducing Easy Upgrades for Kubernetes Clusters</a></li><li><a href=#pg-bccec10413bd4f89b97d497368323e1b>Five Days of Kubernetes 1.8</a></li><li><a href=#pg-98f2cfc12ebe8b53a6367d81dee740f0>Introducing Software Certification for Kubernetes</a></li><li><a href=#pg-4c76b2587819b72e51c461cf5726f85e>Request Routing and Policy Management with the Istio Service Mesh</a></li><li><a href=#pg-b7983d3e161ebbda5791cc9abf7d749d>Kubernetes Community Steering Committee Election Results</a></li><li><a href=#pg-5239569ee3ce5580a4696122578567c9>Kubernetes 1.8: Security, Workloads and Feature Depth</a></li><li><a href=#pg-daccff325ab87127da1590be1ccfbf9d>Kubernetes StatefulSets & DaemonSets Updates</a></li><li><a href=#pg-7265277509164b163724b0537be1457b>Introducing the Resource Management Working Group</a></li><li><a href=#pg-d6b5dd1ab80a70cf585e5b6868705c22>Windows Networking at Parity with Linux for Kubernetes</a></li><li><a href=#pg-0694164705b83bd74e3c69392dc49e48>Kubernetes Meets High-Performance Computing</a></li><li><a href=#pg-6eeb81d6317e8723709eeca85651ebea>High Performance Networking with EC2 Virtual Private Clouds</a></li><li><a href=#pg-dc37213cd51941b1ae482892c4d1abef>Kompose Helps Developers Move Docker Compose Files to Kubernetes</a></li><li><a href=#pg-6e5fb5a8969c05484c12254b6d9f8832>Happy Second Birthday: A Kubernetes Retrospective</a></li><li><a href=#pg-8e70c4f34c6fee1bd77b95674c6aab11>How Watson Health Cloud Deploys Applications with Kubernetes</a></li><li><a href=#pg-c7f4a9fd3148d9d2aacf9908e495a5c4>Kubernetes 1.7: Security Hardening, Stateful Application Updates and Extensibility</a></li><li><a href=#pg-3673321caf498b340318f7e226fd10bf>Managing microservices with the Istio service mesh</a></li><li><a href=#pg-d2e8ec9a30d9d2f99a45ec9a89161236>Draft: Kubernetes container development made easy</a></li><li><a href=#pg-aabe890b5b99e32986fd3915a6a4e63c>Kubernetes: a monitoring guide</a></li><li><a href=#pg-bd6b46c25ade710315e209edb61a4474>Kubespray Ansible Playbooks foster Collaborative Kubernetes Ops</a></li><li><a href=#pg-a13190552c5d0d7417c1f06a29226b51>Dancing at the Lip of a Volcano: The Kubernetes Security Process - Explained</a></li><li><a href=#pg-f0c782c272f0f9e71f7dee809a947166>How Bitmovin is Doing Multi-Stage Canary Deployments with Kubernetes in the Cloud and On-Prem</a></li><li><a href=#pg-f1c179178bbf223f64c5892c367783ca>RBAC Support in Kubernetes</a></li><li><a href=#pg-60d2a1aa4dc581b6ce80903823f5829a>Configuring Private DNS Zones and Upstream Nameservers in Kubernetes</a></li><li><a href=#pg-8864feaccfcc300b9c4bc3b8d07ed0c0>Advanced Scheduling in Kubernetes</a></li><li><a href=#pg-2c544fae5405bf09bdedea192e9755bb>Scalability updates in Kubernetes 1.6: 5,000 node and 150,000 pod clusters</a></li><li><a href=#pg-048341ca6d1d0496267de51f0f5cb1a3>Dynamic Provisioning and Storage Classes in Kubernetes</a></li><li><a href=#pg-f282dcdc94740ae673746535ed7c2f69>Five Days of Kubernetes 1.6</a></li><li><a href=#pg-becd4dd38c8bc7a17aff746a61683974>Kubernetes 1.6: Multi-user, Multi-workloads at Scale</a></li><li><a href=#pg-51bc9cd5c5388c387daf2e05502b4226>The K8sPort: Engaging Kubernetes Community One Activity at a Time</a></li><li><a href=#pg-4fd7ad490e1c089f40a637090b05933e>Deploying PostgreSQL Clusters using StatefulSets</a></li><li><a href=#pg-1a75a28dfa116a64ae5bb5f2f7999206>Containers as a Service, the foundation for next generation PaaS</a></li><li><a href=#pg-28efa8d9359621bd8692cb07ad010bd6>Inside JD.com's Shift to Kubernetes from OpenStack</a></li><li><a href=#pg-f517e7895817589b238b5c59ced90adc>Run Deep Learning with PaddlePaddle on Kubernetes</a></li><li><a href=#pg-44820bd994caefe115ad67f27663cb15>Highly Available Kubernetes Clusters</a></li><li><a href=#pg-3199113628dca07f5421996fe436e09f>Fission: Serverless Functions as a Service for Kubernetes</a></li><li><a href=#pg-4b822a3b7c095f8374ab1a6d2eada556>Running MongoDB on Kubernetes with StatefulSets</a></li><li><a href=#pg-829a2195d95f150a53deb09e9bd1bc4f>How we run Kubernetes in Kubernetes aka Kubeception</a></li><li><a href=#pg-2d5204101b1ade13068e270bbda62e97>Scaling Kubernetes deployments with Policy-Based Networking</a></li><li><a href=#pg-372ad7968dfb46477d8c96369e9219d9>A Stronger Foundation for Creating and Managing Kubernetes Clusters</a></li><li><a href=#pg-24b3af7cfba57bc76db8e0c014a09c5b>Kubernetes UX Survey Infographic</a></li><li><a href=#pg-2df10d56069a485f6d0ee66040e4ede2>Kubernetes supports OpenAPI</a></li><li><a href=#pg-9e45a8ed610420252c4ab6b425d46fba>Cluster Federation in Kubernetes 1.5</a></li><li><a href=#pg-f405bcd4c695718170de3c183e0a7d25>Windows Server Support Comes to Kubernetes</a></li><li><a href=#pg-0ddb1478a7369bfd58673119e4301139>StatefulSet: Run and Scale Stateful Applications Easily in Kubernetes</a></li><li><a href=#pg-ba63ed484cd3b615d9113d8d667125ed>Five Days of Kubernetes 1.5</a></li><li><a href=#pg-494578f429515204637242c396d1a982>Introducing Container Runtime Interface (CRI) in Kubernetes</a></li><li><a href=#pg-a92c18fbb36d093063ff234fa1be4c8a>Kubernetes 1.5: Supporting Production Workloads</a></li><li><a href=#pg-7b694c77d4fe4176f6ec0f57581364dc>From Network Policies to Security Policies</a></li><li><a href=#pg-293519915b8d1da34a529c467aa7befc>Kompose: a tool to go from Docker-compose to Kubernetes</a></li><li><a href=#pg-2458771745f80457c8fc3f1e6452617b>Kubernetes Containers Logging and Monitoring with Sematext</a></li><li><a href=#pg-f54b9570f4be9ede34f6357dcca19f53>Visualize Kubelet Performance with Node Dashboard</a></li><li><a href=#pg-0b41d8cdfadf3d0d3eb7dcd9678b71b2>CNCF Partners With The Linux Foundation To Launch New Kubernetes Certification, Training and Managed Service Provider Program</a></li><li><a href=#pg-84fe92b048c2f52dc4fc501556e035ad>Bringing Kubernetes Support to Azure Container Service</a></li><li><a href=#pg-6b8f188e91bc6dd64e07580f41f6625e>Modernizing the Skytap Cloud Micro-Service Architecture with Kubernetes</a></li><li><a href=#pg-f93ca998ac0832d18ab5e9f99c6068c8>Introducing Kubernetes Service Partners program and a redesigned Partners page</a></li><li><a href=#pg-baa70553d58aac3ab52bfa4847e42a1c>Tail Kubernetes with Stern</a></li><li><a href=#pg-1506704143da62d21a148e789d8291a9>How We Architected and Run Kubernetes on OpenStack at Scale at Yahoo! JAPAN</a></li><li><a href=#pg-2d950d4e9814e2f8994cbf690cc47031>Building Globally Distributed Services using Kubernetes Cluster Federation</a></li><li><a href=#pg-16fb1c5dead23666e30b1418a94197dd>Helm Charts: making it simple to package and deploy common applications on Kubernetes</a></li><li><a href=#pg-92cc300cd30f566a0af0bcdc0a7dacf3>Dynamic Provisioning and Storage Classes in Kubernetes</a></li><li><a href=#pg-21b445dae15d024753bd2d6582499298>How we improved Kubernetes Dashboard UI in 1.4 for your production needs​</a></li><li><a href=#pg-443d8a6c83042d3ba3f3b04ea096de55>How we made Kubernetes insanely easy to install</a></li><li><a href=#pg-bd677efd34376db88d210c2ea44e4f7e>How Qbox Saved 50% per Month on AWS Bills Using Kubernetes and Supergiant</a></li><li><a href=#pg-e90162db2220a568bae8d0960d896400>Kubernetes 1.4: Making it easy to run on Kubernetes anywhere</a></li><li><a href=#pg-2ef1d895f447dec7f1aabe5f71823479>High performance network policies in Kubernetes clusters</a></li><li><a href=#pg-781919e7905e0e2d92c56ebdc7f69aa8>Creating a PostgreSQL Cluster using Helm</a></li><li><a href=#pg-0397a899d8e711d2c9cd190f224eb8e0>Deploying to Multiple Kubernetes Clusters with kit</a></li><li><a href=#pg-1f2dbd97148947aa9b519760ad190b8b>Cloud Native Application Interfaces</a></li><li><a href=#pg-d6be24e48058aca09b2667629db622d7>Security Best Practices for Kubernetes Deployment</a></li><li><a href=#pg-0bdd9be1eea1fe9ccb870ef33266eb15>Scaling Stateful Applications using Kubernetes Pet Sets and FlexVolumes with Datera Elastic Data Fabric</a></li><li><a href=#pg-ae83c9a79ab68c86b7fe479bf9b90a6e>Kubernetes Namespaces: use cases and insights</a></li><li><a href=#pg-28828ac2d323ef978d2063693d4b7de1>SIG Apps: build apps for and operate them in Kubernetes</a></li><li><a href=#pg-e61f98be9ec40572e2f7acaff60a9177>Create a Couchbase cluster using Kubernetes</a></li><li><a href=#pg-2c86abe782cde8635d5620f1e6bc5e7e>Challenges of a Remotely Managed, On-Premises, Bare-Metal Kubernetes Cluster</a></li><li><a href=#pg-4491736ff7cd2b1bb3cd806b10bfa89a>Why OpenStack's embrace of Kubernetes is great for both communities</a></li><li><a href=#pg-802922a147c9feff6990b4a886b0f000>A Very Happy Birthday Kubernetes</a></li><li><a href=#pg-e74a934a1afe2c3d5f1bb068f967c67f>Happy Birthday Kubernetes. Oh, the places you’ll go!</a></li><li><a href=#pg-1822219f56b54758063f2e9a87e9b2b1>The Bet on Kubernetes, a Red Hat Perspective</a></li><li><a href=#pg-d0c04eabe8ccc353a49856a0440f8725>Bringing End-to-End Kubernetes Testing to Azure (Part 2)</a></li><li><a href=#pg-1496574418064c58fdb070c88d97a679>Dashboard - Full Featured Web Interface for Kubernetes</a></li><li><a href=#pg-60f9ed7fa5018fcbffae1e1455adcd36>Steering an Automation Platform at Wercker with Kubernetes</a></li><li><a href=#pg-7bef62438e14fb0960709d89c1672c30>Citrix + Kubernetes = A Home Run</a></li><li><a href=#pg-a60daf951d17ab6e4d59dd70eade6968>Cross Cluster Services - Achieving Higher Availability for your Kubernetes Applications</a></li><li><a href=#pg-9e91022359423a7a5fdefbcd200af963>Stateful Applications in Containers!? Kubernetes 1.3 Says “Yes!”</a></li><li><a href=#pg-b79d59018d2be0d5c94232073f6200f5>Thousand Instances of Cassandra using Kubernetes Pet Set</a></li><li><a href=#pg-a9cee2fd740428ef2eeefa7c16810a78>Autoscaling in Kubernetes</a></li><li><a href=#pg-a5dbe52c882fae6ff5183978c49068da>Kubernetes in Rancher: the further evolution</a></li><li><a href=#pg-5229e3092ff989c2c2c3d40703f3ebc8>Five Days of Kubernetes 1.3</a></li><li><a href=#pg-69c04566fe91de95a935091eb52cd6fc>Minikube: easily run Kubernetes locally</a></li><li><a href=#pg-d95ea49b72ae0d4ffcb02bbff05ffc22>rktnetes brings rkt container engine to Kubernetes</a></li><li><a href=#pg-595bc7391d774dd446856ad8f4f770c2>Updates to Performance and Scalability in Kubernetes 1.3 -- 2,000 node 60,000 pod clusters</a></li><li><a href=#pg-220eff9e67339951de66ffbac1eff1cd>Kubernetes 1.3: Bridging Cloud Native and Enterprise Workloads</a></li><li><a href=#pg-139f28e64f23fe38bf154ae843d00991>Container Design Patterns</a></li><li><a href=#pg-e713facfb8218a33c5a87cc80eaa761a>The Illustrated Children's Guide to Kubernetes</a></li><li><a href=#pg-34e25bc84f07afaa2906f5f35dd9f997>Bringing End-to-End Kubernetes Testing to Azure (Part 1)</a></li><li><a href=#pg-66d9eafbf6937a1c1c0f4bbaf1d6edd4>Hypernetes: Bringing Security and Multi-tenancy to Kubernetes</a></li><li><a href=#pg-dfd7c2731ea9e6f52443db7ecb32aa82>CoreOS Fest 2016: CoreOS and Kubernetes Community meet in Berlin (& San Francisco)</a></li><li><a href=#pg-a3e6efc3369171db872d37129309ee87>Introducing the Kubernetes OpenStack Special Interest Group</a></li><li><a href=#pg-07647e8992c66dab6b47a8c680bb0ecd>SIG-UI: the place for building awesome user interfaces for Kubernetes</a></li><li><a href=#pg-f5bc74c369fabdc9aabeb7ba9c4f3d42>SIG-ClusterOps: Promote operability and interoperability of Kubernetes clusters</a></li><li><a href=#pg-d3f23f912c594dbbccd2c55d5b380e84>SIG-Networking: Kubernetes Network Policy APIs Coming in 1.3</a></li><li><a href=#pg-4a815a0bb19673c92b920bef9ec0c84c>How to deploy secure, auditable, and reproducible Kubernetes clusters on AWS</a></li><li><a href=#pg-584fa2494155bf80b285f6d9e0dd5284>Adding Support for Kubernetes in Rancher</a></li><li><a href=#pg-f5d7064a37fa7662dc317c17c1ef7ed2>Container survey results - March 2016</a></li><li><a href=#pg-58e22d29b43afda73c42bce51426e19d>Configuration management with Containers</a></li><li><a href=#pg-aed7a51bb7920b1d7de6348f6e0c9e42>Using Deployment objects with Kubernetes 1.2</a></li><li><a href=#pg-7418a8c542b8eb8b0648387e10d74720>Kubernetes 1.2 and simplifying advanced networking with Ingress</a></li><li><a href=#pg-4f5d79599c93a734cf95c3da38dd3518>Using Spark and Zeppelin to process big data on Kubernetes 1.2</a></li><li><a href=#pg-6e843f3c8bfc0bfa0651915953bf9a9e>AppFormix: Helping Enterprises Operationalize Kubernetes</a></li><li><a href=#pg-c304efe1c6fd9abd01050e643397611e>Building highly available applications using Kubernetes new multi-zone clusters (a.k.a. 'Ubernetes Lite')</a></li><li><a href=#pg-d38901f59009438f4e70b9e6bf1222db>1000 nodes and beyond: updates to Kubernetes performance and scalability in 1.2</a></li><li><a href=#pg-e4ab3e898fd10f3c8c0558d745f82a44>Five Days of Kubernetes 1.2</a></li><li><a href=#pg-f8b659de13c7fd55ca0915af25e1a707>How container metadata changes your point of view</a></li><li><a href=#pg-0c81a9189333c197f7d01a8f12182057>Scaling neural network image classification using Kubernetes with TensorFlow Serving</a></li><li><a href=#pg-59cd1c44d61e1d516de74b705e94160b>Kubernetes 1.2: Even more performance upgrades, plus easier application deployment and management</a></li><li><a href=#pg-d6b1b3e51eb1319f61cd392193402025>ElasticBox introduces ElasticKube to help manage Kubernetes within the enterprise</a></li><li><a href=#pg-6f7e2e76af47eb65983039de87289b56>Kubernetes in the Enterprise with Fujitsu’s Cloud Load Control</a></li><li><a href=#pg-8d7eb4d2c3ea7e0c8a7600c4633741fb>Kubernetes Community Meeting Notes - 20160225</a></li><li><a href=#pg-65a3c01be5e43cd74d91ae7b341ba25d>State of the Container World, February 2016</a></li><li><a href=#pg-a59c3151a2598aa5539dd1e5f8b30666>KubeCon EU 2016: Kubernetes Community in London</a></li><li><a href=#pg-5c389f15f0500871a7419690797bb894>Kubernetes Community Meeting Notes - 20160218</a></li><li><a href=#pg-cff3eab8d736e2f13bf6269240002c94>Kubernetes Community Meeting Notes - 20160211</a></li><li><a href=#pg-9e9c02244708058c8ef00d746c53046e>ShareThis: Kubernetes In Production</a></li><li><a href=#pg-ca2bb4aa037f8b34a465c33394c21026>Kubernetes Community Meeting Notes - 20160204</a></li><li><a href=#pg-9393cc8cec34b63465f7932dca6f62eb>Kubernetes Community Meeting Notes - 20160128</a></li><li><a href=#pg-7b6c140fad77ba1f495571539d32b7aa>State of the Container World, January 2016</a></li><li><a href=#pg-eace1ed65ee850592eaf33f599130b33>Kubernetes Community Meeting Notes - 20160114</a></li><li><a href=#pg-a053c79699463dc190af082ab57521dd>Kubernetes Community Meeting Notes - 20160121</a></li><li><a href=#pg-bf667c51e2011c6904d9968c4e0e4850>Why Kubernetes doesn’t use libnetwork</a></li><li><a href=#pg-f2fbd7b648c34ea2b5bc519080891738>Simple leader election with Kubernetes and Docker</a></li><li><a href=#pg-1718a0c9c9389c18359c3cfc4c12df57>Creating a Raspberry Pi cluster running Kubernetes, the installation (Part 2)</a></li><li><a href=#pg-9ead5345aff00f9fdaf83a1f428f0559>Managing Kubernetes Pods, Services and Replication Controllers with Puppet</a></li><li><a href=#pg-83aeab877e6fb05786088a780849b599>How Weave built a multi-deployment solution for Scope using Kubernetes</a></li><li><a href=#pg-cf6e9f299e66c20fe0263f3d4484de3c>Creating a Raspberry Pi cluster running Kubernetes, the shopping list (Part 1)</a></li><li><a href=#pg-0949771c0825e8ab602bc8ccff39f4ab>Monitoring Kubernetes with Sysdig</a></li><li><a href=#pg-b84d35cb4a57e7bd8d88b045b7b67c9f>One million requests per second: Dependable and dynamic distributed systems at scale</a></li><li><a href=#pg-992cbdcc8b44baeab01f6bc0ef8b4eae>Kubernetes 1.1 Performance upgrades, improved tooling and a growing community</a></li><li><a href=#pg-9838f9decb115e0a07e334aa453f42f0>Kubernetes as Foundation for Cloud Native PaaS</a></li><li><a href=#pg-70135ddec926f5b40edc548eb31767e5>Some things you didn’t know about kubectl</a></li><li><a href=#pg-644461d4337b4f197866d72505c5fcf5>Kubernetes Performance Measurements and Roadmap</a></li><li><a href=#pg-1540b060dbf14e693016d6b73970786e>Using Kubernetes Namespaces to Manage Environments</a></li><li><a href=#pg-a5d839dbca2e85bc033ad97da511f701>Weekly Kubernetes Community Hangout Notes - July 31 2015</a></li><li><a href=#pg-cc822ac3940885eb5a69dbb66cf6311b>The Growing Kubernetes Ecosystem</a></li><li><a href=#pg-155f4585980e0a3cc7cc913d89e65e77>Weekly Kubernetes Community Hangout Notes - July 17 2015</a></li><li><a href=#pg-b1952d21d9f607dfdf6c5276208b640e>Strong, Simple SSL for Kubernetes Services</a></li><li><a href=#pg-f2ff4be51ddb944d673294af1833fbf6>Weekly Kubernetes Community Hangout Notes - July 10 2015</a></li><li><a href=#pg-c8d31fe28a6a2f2c0b717dd1b1f21835>Announcing the First Kubernetes Enterprise Training Course</a></li><li><a href=#pg-1ca4bcef94d711edf520d75208ea5e0a>How did the Quake demo from DockerCon Work?</a></li><li><a href=#pg-6aa3a3141fa72e55eef75e87fa6e9c6b>Kubernetes 1.0 Launch Event at OSCON</a></li><li><a href=#pg-9a4ed4202cfa82b6304415acbd82e5b7>The Distributed System ToolKit: Patterns for Composite Containers</a></li><li><a href=#pg-d3c0039c2d00bc08b5dbc16a4f7f29e2>Slides: Cluster Management with Kubernetes, talk given at the University of Edinburgh</a></li><li><a href=#pg-b87887ddd92e224cf8adab9f25724a65>Cluster Level Logging with Kubernetes</a></li><li><a href=#pg-0bd7fb297b618aacb101744a911d5f19>Weekly Kubernetes Community Hangout Notes - May 22 2015</a></li><li><a href=#pg-3b8bd1fa11dba03c4234640bf5bebd0d>Kubernetes on OpenStack</a></li><li><a href=#pg-5e05e7f4d2c034dc92074e1f775a8b28>Docker and Kubernetes and AppC</a></li><li><a href=#pg-c1a4261c381e56c7a5f0cb522da45fda>Weekly Kubernetes Community Hangout Notes - May 15 2015</a></li><li><a href=#pg-5ae4b125dfdf018d47858d52d3823ab4>Kubernetes Release: 0.17.0</a></li><li><a href=#pg-52ce46eb666293177a169315be15b141>Resource Usage Monitoring in Kubernetes</a></li><li><a href=#pg-47a6b0051c9c158177447b53745a04dd>Kubernetes Release: 0.16.0</a></li><li><a href=#pg-801e598fff1bb0a31a4322ac49bf5bf1>Weekly Kubernetes Community Hangout Notes - May 1 2015</a></li><li><a href=#pg-05680064b0bb19d48e280af9d6152529>AppC Support for Kubernetes through RKT</a></li><li><a href=#pg-c21afabad8ee96c94af485cfbcc582a5>Weekly Kubernetes Community Hangout Notes - April 24 2015</a></li><li><a href=#pg-cb033081886f937e8367cbdc7c9aa2f6>Borg: The Predecessor to Kubernetes</a></li><li><a href=#pg-cd6be1fea08ace15bbf056a7d703bd90>Kubernetes and the Mesosphere DCOS</a></li><li><a href=#pg-5a383598a2a3cf35ecf5f846ce0d2772>Weekly Kubernetes Community Hangout Notes - April 17 2015</a></li><li><a href=#pg-5f615a3f6d44366138310ebc9b83a28e>Introducing Kubernetes API Version v1beta3</a></li><li><a href=#pg-79141a7a1c693c72f0adcbe84b41d683>Kubernetes Release: 0.15.0</a></li><li><a href=#pg-87053e12af520116753ef6507a31f8f7>Weekly Kubernetes Community Hangout Notes - April 10 2015</a></li><li><a href=#pg-d1dfdc30e1999f3dfec7c332480b37e4>Faster than a speeding Latte</a></li><li><a href=#pg-1f81358fabb3f1915bb19a2c37312d39>Weekly Kubernetes Community Hangout Notes - April 3 2015</a></li><li><a href=#pg-dbae61099ed11bff986f221ad7cf6c18>Participate in a Kubernetes User Experience Study</a></li><li><a href=#pg-6fac4a6542db8c1435843173a812efe8>Weekly Kubernetes Community Hangout Notes - March 27 2015</a></li><li><a href=#pg-64e10f0095f0c1f67ad7b47541c7860c>Kubernetes Gathering Videos</a></li><li><a href=#pg-38fe5117270c19dea659675ee7fa1a8a>Welcome to the Kubernetes Blog!</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-ea1371ce5ca79a0c65620b5bfa13e8b0>PodSecurityPolicy Deprecation: Past, Present, and Future</h1><div class="td-byline mb-4"><time datetime=2021-04-06 class=text-muted>Tuesday, April 06, 2021</time></div><p><strong>Author:</strong> Tabitha Sable (Kubernetes SIG Security)</p><p>PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week. This starts the countdown to its removal, but doesn’t change anything else. PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely. In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.</p><p>What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next? How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP, so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.</p><h2 id=what-does-deprecation-mean-in-kubernetes>What does deprecation mean in Kubernetes?</h2><p>Whenever a Kubernetes feature is set to go away, our <a href=/docs/reference/using-api/deprecation-policy/>deprecation policy</a> is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.</p><p>Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations, PodSecurityPolicy will continue to be fully functional for several more releases. The current plan is to remove PSP from Kubernetes in the 1.25 release.</p><p>Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.</p><h2 id=what-is-podsecuritypolicy>What is PodSecurityPolicy?</h2><p><a href=/docs/concepts/policy/pod-security-policy/>PodSecurityPolicy</a> is a built-in <a href=/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/>admission controller</a> that allows a cluster administrator to control security-sensitive aspects of the Pod specification.</p><p>First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet. Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and cannot run.</p><p>One more important thing to know about PodSecurityPolicy: it’s not the same as <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context>PodSecurityContext</a>.</p><p>A part of the Pod specification, PodSecurityContext (and its per-container counterpart <code>SecurityContext</code>) is the collection of fields that specify many of the security-relevant settings for a Pod. The security context dictates to the kubelet and container runtime how the Pod should actually be run. In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.</p><p>The deprecation of PSP does not affect PodSecurityContext in any way.</p><h2 id=why-did-we-need-podsecuritypolicy>Why did we need PodSecurityPolicy?</h2><p>In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that represent the building blocks of software applications. The various controllers inside a Kubernetes cluster react to these resources, creating further Kubernetes resources or configuring some software or hardware to accomplish our goals.</p><p>In most Kubernetes clusters, RBAC (Role-Based Access Control) <a href=/docs/reference/access-authn-authz/rbac/#role-and-clusterrole>rules</a> control access to these resources. <code>list</code>, <code>get</code>, <code>create</code>, <code>edit</code>, and <code>delete</code> are the sorts of API operations that RBAC cares about, but <em>RBAC does not consider what settings are being put into the resources it controls</em>. For example, a Pod can be almost anything from a simple webserver to a privileged command prompt offering full access to the underlying server node and all the data. It’s all the same to RBAC: a Pod is a Pod is a Pod.</p><p>To control what sorts of settings are allowed in the resources defined in your cluster, you need Admission Control in addition to RBAC. Since Kubernetes 1.3, PodSecurityPolicy has been the built-in way to do that for security-related Pod fields. Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,” without needing to deploy additional external admission controllers.</p><h2 id=why-is-podsecuritypolicy-going-away>Why is PodSecurityPolicy going away?</h2><p>In the years since PodSecurityPolicy was first introduced, we have realized that PSP has some serious usability problems that can’t be addressed without making breaking changes.</p><p>The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them. It is easy to accidentally grant broader permissions than intended, and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy, but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod. Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely, and it’s impossible for PSP to ever be enabled by default.</p><p>For more information about these and other PSP difficulties, check out SIG Auth’s KubeCon NA 2019 Maintainer Track session video:<div class=youtube-quote-sm><iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video"></iframe></div></p><p>Today, you’re not limited only to deploying PSP or writing your own custom admission controller. Several external admission controllers are available that incorporate lessons learned from PSP to provide a better user experience. <a href=https://github.com/cruise-automation/k-rail>K-Rail</a>, <a href=https://github.com/kyverno/kyverno/>Kyverno</a>, and <a href=https://github.com/open-policy-agent/gatekeeper/>OPA/Gatekeeper</a> are all well-known, and each has its fans.</p><p>Although there are other good options available now, we believe there is still value in having a built-in admission controller available as a choice for users. With this in mind, we turn toward building what’s next, inspired by the lessons learned from PSP.</p><h2 id=what-s-next>What’s next?</h2><p>Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members have been working together for months to ensure that what’s coming next is going to be awesome. We have developed a Kubernetes Enhancement Proposal (<a href=https://github.com/kubernetes/enhancements/issues/2579>KEP 2579</a>) and a prototype for a new feature, currently being called by the temporary name "PSP Replacement Policy." We are targeting an Alpha release in Kubernetes 1.22.</p><p>PSP Replacement Policy starts with the realization that since there is a robust ecosystem of external admission controllers already available, PSP’s replacement doesn’t need to be all things to all people. Simplicity of deployment and adoption is the key advantage a built-in admission controller has compared to an external webhook, so we have focused on how to best utilize that advantage.</p><p>PSP Replacement Policy is designed to be as simple as practically possible while providing enough flexibility to really be useful in production at scale. It has soft rollout features to enable retrofitting it to existing clusters, and is configurable enough that it can eventually be active by default. It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.</p><h2 id=what-does-this-mean-for-you>What does this mean for you?</h2><p>What this all means for you depends on your current PSP situation. If you’re already using PSP, there’s plenty of time to plan your next move. Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.</p><p>If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules, you will likely find the simplicity of PSP Replacement Policy too limiting. Use the next year to evaluate the other admission controller choices in the ecosystem. There are resources available to ease this transition, such as the <a href=https://github.com/open-policy-agent/gatekeeper-library>Gatekeeper Policy Library</a>.</p><p>If your use of PSP is relatively simple, with a few policies and straightforward binding to service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs. Evaluate your PSPs compared to the Kubernetes <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a> to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies. Please follow along with or contribute to the KEP and subsequent development, and try out the Alpha release of PSP Replacement Policy when it becomes available.</p><p>If you’re just beginning your PSP journey, you will save time and effort by keeping it simple. You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs. If you set the cluster default by binding a Baseline or Restricted policy to the <code>system:serviceaccounts</code> group, and then make a more-permissive policy available as needed in certain Namespaces <a href=/docs/concepts/policy/pod-security-policy/#run-another-pod>using ServiceAccount bindings</a>, you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy. If your needs are much more complex than this, your effort is probably better spent adopting one of the more fully-featured external admission controllers mentioned above.</p><p>We’re dedicated to making Kubernetes the best container orchestration tool we can, and sometimes that means we need to remove longstanding features to make space for better things to come. When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move. In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases. Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!</p><p><strong>Acknowledgment:</strong> It takes a wonderful group to make wonderful software. Thanks are due to everyone who has contributed to the PSP replacement effort, especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt. It’s been a joy to work with y’all on this.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-69e11eee0766a8c8b934b7a30143dc2b>The Evolution of Kubernetes Dashboard</h1><div class="td-byline mb-4"><time datetime=2021-03-09 class=text-muted>Tuesday, March 09, 2021</time></div><p>Authors: Marcin Maciaszczyk, Kubermatic & Sebastian Florek, Kubermatic</p><p>In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.</p><h2 id=how-it-all-began>How It All Began</h2><p>The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.</p><p>The very <a href=https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c>first commit</a> to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (<a href=https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd>Sebastian committed on 16 November 2015</a>; <a href=https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55>Marcin committed on 23 November 2015</a>). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.</p><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png alt="The First Version of the User Interface"><figcaption><p>The First Version of the User Interface</p></figcaption></figure><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png alt="Prototype of the New User Interface"><figcaption><p>Prototype of the New User Interface</p></figcaption></figure><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png alt="The Current User Interface"><figcaption><p>The Current User Interface</p></figcaption></figure><p>As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.</p><h2 id=growing-up-the-big-migration>Growing Up - The Big Migration</h2><p>At <a href=https://github.com/kubernetes/dashboard/pull/2727>the beginning of 2018</a>, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.</p><p>The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1>first beta version</a> in July 2019, followed by the <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.0>2.0 release</a> in April 2020 — our baby had grown up.</p><h2 id=where-are-we-standing-in-2021>Where Are We Standing in 2021?</h2><p>Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.2.0>Dashboard v2.2.0</a> provides support for Kubernetes v1.20.</p><p>On top of that, we put in a great deal of effort into <a href=https://github.com/kubernetes/dashboard/issues/5232>improving resource support</a>. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.</p><p>Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)</p><h2 id=what-s-next>What’s Next</h2><p>The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:</p><ul><li>Keep providing support for the new Kubernetes versions</li><li>Keep improving the support for the existing resources</li><li>Keep working on auth system improvements</li><li><a href=https://github.com/kubernetes/dashboard/pull/5449>Rewrite the API to use gRPC and shared informers</a>: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.</li><li>Split the application into two containers, one with the UI and the second with the API running inside.</li></ul><h2 id=the-kubernetes-dashboard-in-numbers>The Kubernetes Dashboard in Numbers</h2><ul><li>Initial commit made on October 16, 2015</li><li>Over 100 million pulls from Dockerhub since the v2 release</li><li>8 supported languages and the next 2 in progress</li><li>Over 3360 closed PRs</li><li>Over 2260 closed issues</li><li>100% coverage of the supported core Kubernetes resources</li><li>Over 9000 stars on GitHub</li><li>Over 237 000 lines of code</li></ul><h2 id=join-us>Join Us</h2><p>As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., <a href="https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">issues with help wanted label</a>. Please feel free to reach out via GitHub or the #sig-ui channel in the <a href=https://slack.k8s.io/>Kubernetes Slack</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-89bf34261431a73d36cd93a22c4b1e31>A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</h1><div class="td-byline mb-4"><time datetime=2020-12-21 class=text-muted>Monday, December 21, 2020</time></div><p><strong>Author</strong>: Chris Seto (Cockroach Labs)</p><p>As long as you're willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will "just work". However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one's own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.</p><p>Scaling any system has two main components:</p><ol><li>Adding or removing infrastructure that the system will run on, and</li><li>Ensuring that the system knows how to handle additional instances of itself being added and removed.</li></ol><p>Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).</p><p>One of Kubernetes' responsibilities is to place "resources" (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: "I must be in availability zone <em>A</em>" (see <a href=/docs/setup/best-practices/multiple-zones/#nodes-are-labeled>Running in multiple zones</a>), or "I can't be placed onto the same node as this other Pod" (see <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>Affinity and anti-affinity</a>).</p><p>As an addition to those constraints, Kubernetes offers <a href=/docs/concepts/workloads/controllers/statefulset/>Statefulsets</a> that provide identity to Pods as well as persistent storage that "follows" these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod's name. It's important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.</p><p>Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see <a href=https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html>Orchestrate CockroachDB in a Single Kubernetes Cluster</a>.
In this article, I'll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.</p><p>A three-node CockroachCloud cluster would look something like this:</p><p><img src=image01.png alt="3-node, multi-zone cockroachdb cluster"></p><p>When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.</p><p><img src=image02.png alt="illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster"></p><p>Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.</p><p>To remove resources from a cluster, we perform these operations in reverse order.</p><p>We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.</p><p><img src=image03.png alt="illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes"></p><p>Now, remember that pods in a StatefulSet of size <em>n</em> must have ids in the range <code>[0,n)</code>. When scaling down a StatefulSet by <em>m</em>, Kubernetes removes <em>m</em> pods, starting from the highest ordinals and moving towards the lowest, <a href=/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees>the reverse in which they were added</a>.
Consider the cluster topology below:</p><p><img src=image04.png alt="illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones"></p><p>As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.</p><p><img src=image05.png alt="illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster"></p><p>However, Kubernetes' scheduler doesn't <em>guarantee</em> the placement above as we expected at first.</p><p>Our combined knowledge of the following is what lead to this misconception.</p><ul><li>Kubernetes' ability to <a href=/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones>automatically spread Pods across zone</a></li><li>The behavior that a StatefulSet with <em>n</em> replicas, when Pods are being deployed, they are created sequentially, in order from <code>{0..n-1}</code>. See <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees>StatefulSet</a> for more details.</li></ul><p>Consider the following topology:</p><p><img src=image06.png alt="illustration: 6-node cockroachdb cluster distributed across 3 availability zones"></p><p>These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!</p><p><img src=image07.png alt="illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ"></p><p>Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.</p><p>To correct the latter issue, we now employ a "hunt and peck" approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.</p><h2 id=a-session-of-brainstorming-left-us-with-3-options>A session of brainstorming left us with 3 options:</h2><h3 id=1-upgrade-to-kubernetes-1-18-and-make-use-of-pod-topology-spread-constraints>1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints</h3><p>While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, <a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>pod topology spread constraints</a> were still a <a href=https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/>beta feature in 1.18</a> which meant that it <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices>wasn't guaranteed to be available in managed clusters</a> even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking <a href=https://caniuse.com/>caniuse.com</a> when Internet Explorer 8 was still around.</p><h3 id=2-deploy-a-statefulset-per-zone>2. Deploy a statefulset <em>per zone</em>.</h3><p>Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.</p><h3 id=3-write-a-custom-kubernetes-scheduler>3. Write a custom Kubernetes scheduler.</h3><p>Thanks to an example from <a href=https://github.com/kelseyhightower/scheduler>Kelsey Hightower</a> and a blog post from <a href=https://banzaicloud.com/blog/k8s-custom-scheduler/>Banzai Cloud</a>, we decided to dive in head first and write our own <a href=/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>custom Kubernetes scheduler</a>.
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of <a href=/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers><code>kubectl get events</code></a> had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the <a href=/docs/concepts/scheduling-eviction/scheduling-framework/>kube-scheduler plugin system</a>. Our next POC was a <code>Filter</code> plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!</p><p>Our <a href=https://github.com/cockroachlabs/crl-scheduler>custom scheduler plugin</a> is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin's implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the <code>schedulerName</code> field in our StatefulSet definitions.</p><hr><p><em><a href=https://twitter.com/_ostriches>Chris Seto</a> is a software engineer at Cockroach Labs and works on their Kubernetes automation for <a href=https://cockroachlabs.cloud>CockroachCloud</a>, CockroachDB.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c06e007d763ae4af520000567d5724c6>Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</h1><div class="td-byline mb-4"><time datetime=2020-12-18 class=text-muted>Friday, December 18, 2020</time></div><p><strong>Author</strong>: Shihang Zhang (Google)</p><p>Typically when a <a href=https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md>CSI</a> driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.</p><p>Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.</p><p>Both of them exhibit the following drawbacks:</p><ul><li>Violating the principle of least privilege</li><li>Every CSI driver needs to re-implement the logic of getting the pod’s service account token</li></ul><p>The second approach is more problematic due to:</p><ul><li>The audience of the token defaults to the kube-apiserver</li><li>The token is not guaranteed to be available (e.g. <code>AutomountServiceAccountToken=false</code>)</li><li>The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See <a href=https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission>file permission section for service account token</a></li><li>The token might be legacy Kubernetes service account token which doesn’t expire if <code>BoundServiceAccountTokenVolume=false</code></li></ul><p>Kubernetes 1.20 introduces an alpha feature, <code>CSIServiceAccountToken</code>, to improve the security posture. The new feature allows CSI drivers to receive pods' <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md>bound service account tokens</a>.</p><p>This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.</p><h2 id=pod-impersonation>Pod Impersonation</h2><h3 id=using-gcp-apis>Using GCP APIs</h3><p>Using <a href=https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity>Workload Identity</a>, a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to <a href=https://cloud.google.com/iam/docs/reference/sts/rest>exchange for GCP tokens</a>. The pod's service account token is plumbed through the volume context in <code>NodePublishVolume</code> RPC calls when the feature <code>CSIServiceAccountToken</code> is enabled. For example: accessing <a href=https://cloud.google.com/secret-manager/>Google Secret Manager</a> via a <a href=https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp>secret store CSI driver</a>.</p><h3 id=using-vault>Using Vault</h3><p>If users configure <a href=https://www.vaultproject.io/docs/auth/kubernetes>Kubernetes as an auth method</a>, Vault uses the <code>TokenReview</code> API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, <a href=https://github.com/hashicorp/secrets-store-csi-driver-provider-vault>secrets store CSI driver</a> and <a href=https://github.com/jetstack/cert-manager-csi>cert manager CSI driver</a>.</p><h2 id=short-lived-volumes>Short-lived Volumes</h2><p>To keep short-lived volumes such as certificates effective, CSI drivers can specify <code>RequiresRepublish=true</code> in their<code>CSIDriver</code> object to have the kubelet periodically call <code>NodePublishVolume</code> on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.</p><h2 id=next-steps>Next steps</h2><p>This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:</p><ul><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md>KEP-1855: Service Account Token for CSI Driver</a></li><li><a href=https://kubernetes-csi.github.io/docs/token-requests.html>Token Requests</a></li></ul><p>Your feedback is always welcome!</p><ul><li>SIG-Auth <a href=https://github.com/kubernetes/community/tree/master/sig-auth#meetings>meets regularly</a> and can be reached via <a href=https://github.com/kubernetes/community/tree/master/sig-auth#contact>Slack and the mailing list</a></li><li>SIG-Storage <a href=https://github.com/kubernetes/community/tree/master/sig-storage#meetings>meets regularly</a> and can be reached via <a href=https://github.com/kubernetes/community/tree/master/sig-storage#contact>Slack and the mailing list</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-affda69f3ae1111f8ade984c2fca19bf>Third Party Device Metrics Reaches GA</h1><div class="td-byline mb-4"><time datetime=2020-12-16 class=text-muted>Wednesday, December 16, 2020</time></div><p><strong>Authors:</strong> Renaud Gaubert (NVIDIA), David Ashpole (Google), and Pramod Ramarao (NVIDIA)</p><p>With Kubernetes 1.20, infrastructure teams who manage large scale Kubernetes clusters, are seeing the graduation of two exciting and long awaited features:</p><ul><li>The Pod Resources API (introduced in 1.13) is finally graduating to GA. This allows Kubernetes plugins to obtain information about the node’s resource usage and assignment; for example: which pod/container consumes which device.</li><li>The <code>DisableAcceleratorMetrics</code> feature (introduced in 1.19) is graduating to beta and will be enabled by default. This removes device metrics reported by the kubelet in favor of the new plugin architecture.</li></ul><p>Many of the features related to fundamental device support (device discovery, plugin, and monitoring) are reaching a strong level of stability.
Kubernetes users should see these features as stepping stones to enable more complex use cases (networking, scheduling, storage, etc.)!</p><p>One such example is Non Uniform Memory Access (NUMA) placement where, when selecting a device, an application typically wants to ensure that data transfer between CPU Memory and Device Memory is as fast as possible. In some cases, incorrect NUMA placement can nullify the benefit of offloading compute to an external device.</p><p>If these are topics of interest to you, consider joining the <a href=https://github.com/kubernetes/community/tree/master/sig-node>Kubernetes Node Special Insterest Group</a> (SIG) for all topics related to the Kubernetes node, the COD (container orchestrated device) workgroup for topics related to runtimes, or the resource management forum for topics related to resource management!</p><h2 id=the-pod-resources-api-why-does-it-need-to-exist>The Pod Resources API - Why does it need to exist?</h2><p>Kubernetes is a vendor neutral platform. If we want it to support device monitoring, adding vendor-specific code in the Kubernetes code base is not an ideal solution. Ultimately, devices are a domain where deep expertise is needed and the best people to add and maintain code in that area are the device vendors themselves.</p><p>The Pod Resources API was built as a solution to this issue. Each vendor can build and maintain their own out-of-tree monitoring plugin. This monitoring plugin, often deployed as a separate pod within a cluster, can then associate the metrics a device emits with the associated pod that's using it.</p><p>For example, use the NVIDIA GPU dcgm-exporter to scrape metrics in Prometheus format:</p><pre><code>$ curl -sL http://127.0.01:8080/metrics


# HELP DCGM_FI_DEV_SM_CLOCK SM clock frequency (in MHz).
# TYPE DCGM_FI_DEV_SM_CLOCK gauge
# HELP DCGM_FI_DEV_MEM_CLOCK Memory clock frequency (in MHz).
# TYPE DCGM_FI_DEV_MEM_CLOCK gauge
# HELP DCGM_FI_DEV_MEMORY_TEMP Memory temperature (in C).
# TYPE DCGM_FI_DEV_MEMORY_TEMP gauge
...
DCGM_FI_DEV_SM_CLOCK{gpu=&quot;0&quot;, UUID=&quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&quot;,container=&quot;foo&quot;,namespace=&quot;bar&quot;,pod=&quot;baz&quot;} 139
DCGM_FI_DEV_MEM_CLOCK{gpu=&quot;0&quot;, UUID=&quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&quot;,container=&quot;foo&quot;,namespace=&quot;bar&quot;,pod=&quot;baz&quot;} 405
DCGM_FI_DEV_MEMORY_TEMP{gpu=&quot;0&quot;, UUID=&quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&quot;,container=&quot;foo&quot;,namespace=&quot;bar&quot;,pod=&quot;baz&quot;} 9223372036854775794
</code></pre><p>Each agent is expected to adhere to the node monitoring guidelines. In other words, plugins are expected to generate metrics in Prometheus format, and new metrics should not have any dependency on the Kubernetes base directly.</p><p>This allows consumers of the metrics to use a compatible monitoring pipeline to collect and analyze metrics from a variety of agents, even if they are maintained by different vendors.</p><p><img src=/images/blog/2020-12-16-third-party-device-metrics-hits-ga/metrics-chart.png alt="Device metrics flowchart"></p><h2 id=nvidia-gpu-metrics-deprecated>Disabling the NVIDIA GPU metrics - Warning</h2><p>With the graduation of the plugin monitoring system, Kubernetes is deprecating the NVIDIA GPU metrics that are being reported by the kubelet.</p><p>With the <a href=/docs/concepts/cluster-administration/system-metrics/#disable-accelerator-metrics>DisableAcceleratorMetrics</a> feature being enabled by default in Kubernetes 1.20, NVIDIA GPUs are no longer special citizens in Kubernetes. This is a good thing in the spirit of being vendor-neutral, and enables the most suited people to maintain their plugin on their own release schedule!</p><p>Users will now need to either install the <a href=https://github.com/NVIDIA/gpu-monitoring-tools>NVIDIA GDGM exporter</a> or use <a href=https://github.com/nvidia/go-nvml>bindings</a> to gather more accurate and complete metrics about NVIDIA GPUs. This deprecation means that you can no longer rely on metrics that were reported by kubelet, such as <code>container_accelerator_duty_cycle</code> or <code>container_accelerator_memory_used_bytes</code> which were used to gather NVIDIA GPU memory utilization.</p><p>This means that users who used to rely on the NVIDIA GPU metrics reported by the kubelet, will need to update their reference and deploy the NVIDIA plugin. Namely the different metrics reported by Kubernetes map to the following metrics:</p><table><thead><tr><th>Kubernetes Metrics</th><th>NVIDIA dcgm-exporter metric</th></tr></thead><tbody><tr><td><code>container_accelerator_duty_cycle</code></td><td><code>DCGM_FI_DEV_GPU_UTIL</code></td></tr><tr><td><code>container_accelerator_memory_used_bytes</code></td><td><code>DCGM_FI_DEV_FB_USED</code></td></tr><tr><td><code>container_accelerator_memory_total_bytes</code></td><td><code>DCGM_FI_DEV_FB_FREE + DCGM_FI_DEV_FB_USED</code></td></tr></tbody></table><p>You might also be interested in other metrics such as <code>DCGM_FI_DEV_GPU_TEMP</code> (the GPU temperature) or DCGM_FI_DEV_POWER_USAGE (the power usage). The <a href=https://github.com/NVIDIA/gpu-monitoring-tools/blob/d5c9bb55b4d1529ca07068b7f81e690921ce2b59/etc/dcgm-exporter/default-counters.csv>default set</a> is available in Nvidia's <a href=https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/group__dcgmFieldIdentifiers.html>Data Center GPU Manager documentation</a>.</p><p>Note that for this release you can still set the <code>DisableAcceleratorMetrics</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> to <em>false</em>, effectively re-enabling the ability for the kubelet to report NVIDIA GPU metrics.</p><p>Paired with the graduation of the Pod Resources API, these tools can be used to generate GPU telemetry <a href=https://grafana.com/grafana/dashboards/12239>that can be used in visualization dashboards</a>, below is an example:</p><p><img src=/images/blog/2020-12-16-third-party-device-metrics-hits-ga/grafana.png alt="Grafana visualization of device metrics"></p><h2 id=the-pod-resources-api-what-can-i-go-on-to-do-with-this>The Pod Resources API - What can I go on to do with this?</h2><p>As soon as this interface was introduced, many vendors started using it for widely different use cases! To list a few examples:</p><p>The <a href=https://github.com/openstack/kuryr-kubernetes>kuryr-kubernetes</a> CNI plugin in tandem with <a href=https://github.com/intel/sriov-network-device-plugin>intel-sriov-device-plugin</a>. This allowed the CNI plugin to know which allocation of SR-IOV Virtual Functions (VFs) the kubelet made and use that information to correctly setup the container network namespace and use a device with the appropriate NUMA node. We also expect this interface to be used to track the allocated and available resources with information about the NUMA topology of the worker node.</p><p>Another use-case is GPU telemetry, where GPU metrics can be associated with the containers and pods that the GPU is assigned to. One such example is the NVIDIA <code>dcgm-exporter</code>, but others can be easily built in the same paradigm.</p><p>The Pod Resources API is a simple gRPC service which informs clients of the pods the kubelet knows. The information concerns the devices assignment the kubelet made and the assignment of CPUs. This information is obtained from the internal state of the kubelet's Device Manager and CPU Manager respectively.</p><p>You can see below a sample example of the API and how a go client could use that information in a few lines:</p><pre><code>service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}

    // Kubernetes 1.21
    rpc Watch(WatchPodResourcesRequest) returns (stream WatchPodResourcesResponse) {}
}
</code></pre><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>main</span>() {
	ctx, cancel <span style=color:#666>:=</span> context.<span style=color:#00a000>WithTimeout</span>(context.<span style=color:#00a000>Background</span>(), connectionTimeout)
	<span style=color:#a2f;font-weight:700>defer</span> <span style=color:#00a000>cancel</span>()

	socket <span style=color:#666>:=</span> <span style=color:#b44>&#34;/var/lib/kubelet/pod-resources/kubelet.sock&#34;</span>
	conn, err <span style=color:#666>:=</span> grpc.<span style=color:#00a000>DialContext</span>(ctx, socket, grpc.<span style=color:#00a000>WithInsecure</span>(), grpc.<span style=color:#00a000>WithBlock</span>(),
		grpc.<span style=color:#00a000>WithDialer</span>(<span style=color:#a2f;font-weight:700>func</span>(addr <span style=color:#0b0;font-weight:700>string</span>, timeout time.Duration) (net.Conn, <span style=color:#0b0;font-weight:700>error</span>) {
			<span style=color:#a2f;font-weight:700>return</span> net.<span style=color:#00a000>DialTimeout</span>(<span style=color:#b44>&#34;unix&#34;</span>, addr, timeout)
		}),
	)

	<span style=color:#a2f;font-weight:700>if</span> err <span style=color:#666>!=</span> <span style=color:#a2f;font-weight:700>nil</span> {
		<span style=color:#a2f>panic</span>(err)
	}

    client <span style=color:#666>:=</span> podresourcesapi.<span style=color:#00a000>NewPodResourcesListerClient</span>(conn)
    resp, err <span style=color:#666>:=</span> client.<span style=color:#00a000>List</span>(ctx, <span style=color:#666>&amp;</span>podresourcesapi.ListPodResourcesRequest{})
	<span style=color:#a2f;font-weight:700>if</span> err <span style=color:#666>!=</span> <span style=color:#a2f;font-weight:700>nil</span> {
		<span style=color:#a2f>panic</span>(err)
	}
	net.<span style=color:#00a000>Printf</span>(<span style=color:#b44>&#34;%+v\n&#34;</span>, resp)
}
</code></pre></div><p>Finally, note that you can watch the number of requests made to the Pod Resources endpoint by watching the new kubelet metric called <code>pod_resources_endpoint_requests_total</code> on the kubelet's <code>/metrics</code> endpoint.</p><h2 id=is-device-monitoring-suitable-for-production-can-i-extend-it-can-i-contribute>Is device monitoring suitable for production? Can I extend it? Can I contribute?</h2><p>Yes! This feature released in 1.13, almost 2 years ago, has seen broad adoption, is already used by different cloud managed services, and with its graduation to G.A in Kubernetes 1.20 is production ready!</p><p>If you are a device vendor, you can start using it today! If you just want to monitor the devices in your cluster, go get the latest version of your monitoring plugin!</p><p>If you feel passionate about that area, join the kubernetes community, help improve the API or contribute the device monitoring plugins!</p><h2 id=acknowledgements>Acknowledgements</h2><p>We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management, SIG-Node and the Resource management forum!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-34d9423bdc9e6ba8808d2cc317f13351>Kubernetes 1.20: Granular Control of Volume Permission Changes</h1><div class="td-byline mb-4"><time datetime=2020-12-14 class=text-muted>Monday, December 14, 2020</time></div><p><strong>Authors</strong>: Hemant Kumar, Red Hat & Christian Huffman, Red Hat</p><p>Kubernetes 1.20 brings two important beta features, allowing Kubernetes admins and users alike to have more adequate control over how volume permissions are applied when a volume is mounted inside a Pod.</p><h3 id=allow-users-to-skip-recursive-permission-changes-on-mount>Allow users to skip recursive permission changes on mount</h3><p>Traditionally if your pod is running as a non-root user (<a href=https://twitter.com/thockin/status/1333892204490735617>which you should</a>), you must specify a <code>fsGroup</code> inside the pod’s security context so that the volume can be readable and writable by the Pod. This requirement is covered in more detail in <a href=https://kubernetes.io/docs/tasks/configure-pod-container/security-context/>here</a>.</p><p>But one side-effect of setting <code>fsGroup</code> is that, each time a volume is mounted, Kubernetes must recursively <code>chown()</code> and <code>chmod()</code> all the files and directories inside the volume - with a few exceptions noted below. This happens even if group ownership of the volume already matches the requested <code>fsGroup</code>, and can be pretty expensive for larger volumes with lots of small files, which causes pod startup to take a long time. This scenario has been a <a href=https://github.com/kubernetes/kubernetes/issues/69699>known problem</a> for a while, and in Kubernetes 1.20 we are providing knobs to opt-out of recursive permission changes if the volume already has the correct permissions.</p><p>When configuring a pod’s security context, set <code>fsGroupChangePolicy</code> to "OnRootMismatch" so if the root of the volume already has the correct permissions, the recursive permission change can be skipped. Kubernetes ensures that permissions of the top-level directory are changed last the first time it applies permissions.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runAsUser</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runAsGroup</span>:<span style=color:#bbb> </span><span style=color:#666>3000</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fsGroup</span>:<span style=color:#bbb> </span><span style=color:#666>2000</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fsGroupChangePolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;OnRootMismatch&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>You can learn more about this in <a href=https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods>Configure volume permission and ownership change policy for Pods</a>.</p><h3 id=allow-csi-drivers-to-declare-support-for-fsgroup-based-permissions>Allow CSI Drivers to declare support for fsGroup based permissions</h3><p>Although the previous section implied that Kubernetes <em>always</em> recursively changes permissions of a volume if a Pod has a <code>fsGroup</code>, this is not strictly true. For certain multi-writer volume types, such as NFS or Gluster, the cluster doesn’t perform recursive permission changes even if the pod has a <code>fsGroup</code>. Other volume types may not even support <code>chown()</code>/<code>chmod()</code>, which rely on Unix-style permission control primitives.</p><p>So how do we know when to apply recursive permission changes and when we shouldn't? For in-tree storage drivers, this was relatively simple. For <a href=https://kubernetes-csi.github.io/docs/introduction.html#introduction>CSI</a> drivers that could span a multitude of platforms and storage types, this problem can be a bigger challenge.</p><p>Previously, whenever a CSI volume was mounted to a Pod, Kubernetes would attempt to automatically determine if the permissions and ownership should be modified. These methods were imprecise and could cause issues as we already mentioned, depending on the storage type.</p><p>The CSIDriver custom resource now has a <code>.spec.fsGroupPolicy</code> field, allowing storage drivers to explicitly opt in or out of these recursive modifications. By having the CSI driver specify a policy for the backing volumes, Kubernetes can avoid needless modification attempts. This optimization helps to reduce volume mount time and also cuts own reporting errors about modifications that would never succeed.</p><h4 id=csidriver-fsgrouppolicy-api>CSIDriver FSGroupPolicy API</h4><p>Three FSGroupPolicy values are available as of Kubernetes 1.20, with more planned for future releases.</p><ul><li><strong>ReadWriteOnceWithFSType</strong> - This is the default policy, applied if no <code>fsGroupPolicy</code> is defined; this preserves the behavior from previous Kubernetes releases. Each volume is examined at mount time to determine if permissions should be recursively applied.</li><li><strong>File</strong> - Always attempt to apply permission modifications, regardless of the filesystem type or PersistentVolumeClaim’s access mode.</li><li><strong>None</strong> - Never apply permission modifications.</li></ul><h4 id=how-do-i-use-it>How do I use it?</h4><p>The only configuration needed is defining <code>fsGroupPolicy</code> inside of the <code>.spec</code> for a CSIDriver. Once that element is defined, any subsequently mounted volumes will automatically use the defined policy. There’s no additional deployment required!</p><h4 id=what-s-next>What’s next?</h4><p>Depending on feedback and adoption, the Kubernetes team plans to push these implementations to GA in either 1.21 or 1.22.</p><h3 id=how-can-i-learn-more>How can I learn more?</h3><p>This feature is explained in more detail in Kubernetes project documentation: <a href=https://kubernetes-csi.github.io/docs/support-fsgroup.html>CSI Driver fsGroup Support</a> and <a href=https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods>Configure volume permission and ownership change policy for Pods </a>.</p><h3 id=how-do-i-get-involved>How do I get involved?</h3><p>The <a href=https://kubernetes.slack.com/messages/csi>Kubernetes Slack channel #csi</a> and any of the <a href=https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact>standard SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and the CSI team.</p><p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group (SIG)</a>. We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-75cd63817127b5552cbec02c54f84994>Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</h1><div class="td-byline mb-4"><time datetime=2020-12-10 class=text-muted>Thursday, December 10, 2020</time></div><p><strong>Authors</strong>: Xing Yang, VMware & Xiangqian Yu, Google</p><p>The Kubernetes Volume Snapshot feature is now GA in Kubernetes v1.20. It was introduced as <a href=https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/>alpha</a> in Kubernetes v1.12, followed by a <a href=https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/>second alpha</a> with breaking changes in Kubernetes v1.13, and promotion to <a href=https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/>beta</a> in Kubernetes 1.17. This blog post summarizes the changes releasing the feature from beta to GA.</p><h2 id=what-is-a-volume-snapshot>What is a volume snapshot?</h2><p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to rehydrate a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).</p><h2 id=why-add-volume-snapshots-to-kubernetes>Why add volume snapshots to Kubernetes?</h2><p>Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster-specific” knowledge.</p><p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database’s volumes before starting a database operation.</p><p>By providing a standard way to trigger volume snapshot operations in Kubernetes, this feature allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of the underlying storage.</p><p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced enterprise-grade storage administration features for Kubernetes, including application or cluster level backup solutions.</p><h2 id=what-s-new-since-beta>What’s new since beta?</h2><p>With the promotion of Volume Snapshot to GA, the feature is enabled by default on standard Kubernetes deployments and cannot be turned off.</p><p>Many enhancements have been made to improve the quality of this feature and to make it production-grade.</p><ul><li><p>The Volume Snapshot APIs and client library were moved to a separate Go module.</p></li><li><p>A snapshot validation webhook has been added to perform necessary validation on volume snapshot objects. More details can be found in the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1900-volume-snapshot-validation-webhook>Volume Snapshot Validation Webhook Kubernetes Enhancement Proposal</a>.</p></li><li><p>Along with the validation webhook, the volume snapshot controller will start labeling invalid snapshot objects that already existed. This allows users to identify, remove any invalid objects, and correct their workflows. Once the API is switched to the v1 type, those invalid objects will not be deletable from the system.</p></li><li><p>To provide better insights into how the snapshot feature is performing, an initial set of operation metrics has been added to the volume snapshot controller.</p></li><li><p>There are more end-to-end tests, running on GCP, that validate the feature in a real Kubernetes cluster. Stress tests (based on Google Persistent Disk and <code>hostPath</code> CSI Drivers) have been introduced to test the robustness of the system.</p></li></ul><p>Other than introducing tightening validation, there is no difference between the v1beta1 and v1 Kubernetes volume snapshot API. In this release (with Kubernetes 1.20), both v1 and v1beta1 are served while the stored API version is still v1beta1. Future releases will switch the stored version to v1 and gradually remove v1beta1 support.</p><h2 id=which-csi-drivers-support-volume-snapshots>Which CSI drivers support volume snapshots?</h2><p>Snapshots are only supported for CSI drivers, not for in-tree or FlexVolume drivers. Ensure the deployed CSI driver on your cluster has implemented the snapshot interfaces. For more information, see <a href=https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/>Container Storage Interface (CSI) for Kubernetes GA</a>.</p><p>Currently more than <a href=https://kubernetes-csi.github.io/docs/drivers.html>50 CSI drivers</a> support the Volume Snapshot feature. The <a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>GCE Persistent Disk CSI Driver</a> has gone through the tests for upgrading from volume snapshots beta to GA. GA level support for other CSI drivers should be available soon.</p><h2 id=who-builds-products-using-volume-snapshots>Who builds products using volume snapshots?</h2><p>As of the publishing of this blog, the following participants from the <a href=https://github.com/kubernetes/community/tree/master/wg-data-protection>Kubernetes Data Protection Working Group</a> are building products or have already built products using Kubernetes volume snapshots.</p><ul><li><a href=https://www.delltechnologies.com/en-us/data-protection/powerprotect-data-manager.htm>Dell-EMC: PowerProtect</a></li><li><a href=https://www.druva.com/>Druva</a></li><li><a href=https://www.kasten.io/>Kasten K10</a></li><li><a href=https://cloud.netapp.com/project-astra>NetApp: Project Astra</a></li><li><a href=https://portworx.com/products/px-backup/>Portworx (PX-Backup)</a></li><li><a href=https://github.com/purestorage/pso-csi>Pure Storage (Pure Service Orchestrator)</a></li><li><a href=https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage>Red Hat OpenShift Container Storage</a></li><li><a href=https://robin.io/storage/>Robin Cloud Native Storage</a></li><li><a href=https://docs.trilio.io/kubernetes/>TrilioVault for Kubernetes</a></li><li><a href=https://github.com/vmware-tanzu/velero-plugin-for-csi>Velero plugin for CSI</a></li></ul><h2 id=how-to-deploy-volume-snapshots>How to deploy volume snapshots?</h2><p>Volume Snapshot feature contains the following components:</p><ul><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd>Kubernetes Volume Snapshot CRDs</a></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller>Volume snapshot controller</a></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/validation-webhook>Snapshot validation webhook</a></li><li>CSI Driver along with <a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/sidecar-controller>CSI Snapshotter sidecar</a></li></ul><p>It is strongly recommended that Kubernetes distributors bundle and deploy the volume snapshot controller, CRDs, and validation webhook as part of their Kubernetes cluster management process (independent of any CSI Driver).</p><blockquote class="warning callout"><div><strong>Warning:</strong> The snapshot validation webhook serves as a critical component to transition smoothly from using v1beta1 to v1 API. Not installing the snapshot validation webhook makes prevention of invalid volume snapshot objects from creation/updating impossible, which in turn will block deletion of invalid volume snapshot objects in coming upgrades.</div></blockquote><p>If your cluster does not come pre-installed with the correct components, you may manually install them. See the <a href=https://github.com/kubernetes-csi/external-snapshotter#readme>CSI Snapshotter</a> README for details.</p><h2 id=how-to-use-volume-snapshots>How to use volume snapshots?</h2><p>Assuming all the required components (including CSI driver) have been already deployed and running on your cluster, you can create volume snapshots using the <code>VolumeSnapshot</code> API object, or use an existing <code>VolumeSnapshot</code> to restore a PVC by specifying the VolumeSnapshot data source on it. For more details, see the <a href=/docs/concepts/storage/volume-snapshots/>volume snapshot documentation</a>.</p><blockquote class="note callout"><div><strong>Note:</strong> The Kubernetes Snapshot API does not provide any application consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using higher level APIs/controllers.</div></blockquote><h3 id=dynamically-provision-a-volume-snapshot>Dynamically provision a volume snapshot</h3><p>To dynamically provision a volume snapshot, create a <code>VolumeSnapshotClass</code> API object first.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapclass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>testdriver.csi.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csi.storage.k8s.io/snapshotter-secret-name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csi.storage.k8s.io/snapshotter-secret-namespace</span>:<span style=color:#bbb> </span>mysecretnamespace<span style=color:#bbb>
</span></code></pre></div><p>Then create a <code>VolumeSnapshot</code> API object from a PVC by specifying the volume snapshot class.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>ns1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>test-snapclass<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentVolumeClaimName</span>:<span style=color:#bbb> </span>test-pvc<span style=color:#bbb>
</span></code></pre></div><h3 id=importing-an-existing-volume-snapshot-with-kubernetes>Importing an existing volume snapshot with Kubernetes</h3><p>To import a pre-existing volume snapshot into Kubernetes, manually create a <code>VolumeSnapshotContent</code> object first.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-content<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>testdriver.csi.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshotHandle</span>:<span style=color:#bbb> </span>7bdd0de3-xxx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></code></pre></div><p>Then create a <code>VolumeSnapshot</code> object pointing to the <code>VolumeSnapshotContent</code> object.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeSnapshotContentName</span>:<span style=color:#bbb> </span>test-content<span style=color:#bbb>
</span></code></pre></div><h3 id=rehydrate-volume-from-snapshot>Rehydrate volume from snapshot</h3><p>A bound and ready <code>VolumeSnapshot</code> object can be used to rehydrate a new volume with data pre-populated from snapshotted data as shown here:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-restore<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>demo-namespace<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>test-storageclass<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></code></pre></div><h2 id=how-to-add-support-for-snapshots-in-a-csi-driver>How to add support for snapshots in a CSI driver?</h2><p>See the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>CSI spec</a> and the <a href=https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html>Kubernetes-CSI Driver Developer Guide</a> for more details on how to implement the snapshot feature in a CSI driver.</p><h2 id=what-are-the-limitations>What are the limitations?</h2><p>The GA implementation of volume snapshots for Kubernetes has the following limitations:</p><ul><li>Does not support reverting an existing PVC to an earlier state represented by a snapshot (only supports provisioning a new volume from a snapshot).</li></ul><h3 id=how-to-learn-more>How to learn more?</h3><p>The code repository for snapshot APIs and controller is here: <a href=https://github.com/kubernetes-csi/external-snapshotter>https://github.com/kubernetes-csi/external-snapshotter</a></p><p>Check out additional documentation on the snapshot feature here: <a href=http://k8s.io/docs/concepts/storage/volume-snapshots>http://k8s.io/docs/concepts/storage/volume-snapshots</a> and <a href=https://kubernetes-csi.github.io/docs/>https://kubernetes-csi.github.io/docs/</a></p><h2 id=how-to-get-involved>How to get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach GA. We want to thank Saad Ali, Michelle Au, Tim Hockin, and Jordan Liggitt for their insightful reviews and thorough consideration with the design, thank Andi Li for his work on adding the support of the snapshot validation webhook, thank Grant Griffiths on implementing metrics support in the snapshot controller and handling password rotation in the validation webhook, thank Chris Henzie, Raunak Shah, and Manohar Reddy for writing critical e2e tests to meet the scalability and stability requirements for graduation, thank Kartik Sharma for moving snapshot APIs and client lib to a separate go module, and thank Raunak Shah and Prafull Ladha for their help with upgrade testing from beta to GA.</p><p>There are many more people who have helped to move the snapshot feature from beta to GA. We want to thank everyone who has contributed to this effort:</p><ul><li><a href=https://github.com/AndiLi99>Andi Li</a></li><li><a href=https://github.com/bswartz>Ben Swartzlander</a></li><li><a href=https://github.com/chrishenzie>Chris Henzie</a></li><li><a href=https://github.com/huffmanca>Christian Huffman</a></li><li><a href=https://github.com/ggriffiths>Grant Griffiths</a></li><li><a href=https://github.com/humblec>Humble Devassy Chirammal</a></li><li><a href=https://github.com/jsafrane>Jan Šafránek</a></li><li><a href=https://github.com/Jiawei0227>Jiawei Wang</a></li><li><a href=https://github.com/jingxu97>Jing Xu</a></li><li><a href=https://github.com/liggitt>Jordan Liggitt</a></li><li><a href=https://github.com/Kartik494>Kartik Sharma</a></li><li><a href=https://github.com/Madhu-1>Madhu Rajanna</a></li><li><a href=https://github.com/boddumanohar>Manohar Reddy</a></li><li><a href=https://github.com/msau42>Michelle Au</a></li><li><a href=https://github.com/pohly>Patrick Ohly</a></li><li><a href=https://github.com/prafull01>Prafull Ladha</a></li><li><a href=https://github.com/prateekpandey14>Prateek Pandey</a></li><li><a href=https://github.com/RaunakShah>Raunak Shah</a></li><li><a href=https://github.com/saad-ali>Saad Ali</a></li><li><a href=https://github.com/saikat-royc>Saikat Roychowdhury</a></li><li><a href=https://github.com/thockin>Tim Hockin</a></li><li><a href=https://github.com/yuxiangqian>Xiangqian Yu</a></li><li><a href=https://github.com/xing-yang>Xing Yang</a></li><li><a href=https://github.com/zhucan>Zhu Can</a></li></ul><p>For those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p>We also hold regular <a href=https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#>Data Protection Working Group meetings</a>. New attendees are welcome to join in discussions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ced3d9254f631ed8dfb689bd1dd7c259>Kubernetes 1.20: The Raddest Release</h1><div class="td-byline mb-4"><time datetime=2020-12-08 class=text-muted>Tuesday, December 08, 2020</time></div><p><strong>Authors:</strong> <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md>Kubernetes 1.20 Release Team</a></p><p>We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha.</p><p>The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem.</p><h2 id=major-themes>Major Themes</h2><h3 id=volume-snapshot-operations-goes-stable>Volume Snapshot Operations Goes Stable</h3><p>This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers.</p><p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions.</p><p>Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster.</p><h3 id=kubectl-debug-graduates-to-beta>Kubectl Debug Graduates to Beta</h3><p>The <code>kubectl alpha debug</code> features graduates to beta in 1.20, becoming <code>kubectl debug</code>. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include:</p><ul><li>Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.</li><li>Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)</li><li>Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.</li></ul><p>Note that as a new built-in command, <code>kubectl debug</code> takes priority over any kubectl plugin named “debug”. You must rename the affected plugin.</p><p>Invocations using <code>kubectl alpha debug</code> are now deprecated and will be removed in a subsequent release. Update your scripts to use <code>kubectl debug</code>. For more information about <code>kubectl debug</code>, see <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/>Debugging Running Pods</a>.</p><h3 id=beta-api-priority-and-fairness>Beta: API Priority and Fairness</h3><p>Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows <code>kube-apiserver</code> to categorize incoming requests by priority levels.</p><h3 id=alpha-with-updates-ipv4-ipv6>Alpha with updates: IPV4/IPV6</h3><p>The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa.</p><h3 id=ga-process-pid-limiting-for-stability>GA: Process PID Limiting for Stability</h3><p>Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine.</p><p>Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node.
After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both <code>SupportNodePidsLimit</code> (node-to-pod PID isolation) and <code>SupportPodPidsLimit</code> (ability to limit PIDs per pod).</p><h3 id=alpha-graceful-node-shutdown>Alpha: Graceful node shutdown</h3><p>Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The <code>GracefulNodeShutdown</code> feature is now in Alpha. <code>GracefulNodeShutdown</code> makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown.</p><h2 id=major-changes>Major Changes</h2><h3 id=dockershim-deprecation>Dockershim Deprecation</h3><p>Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a <a href=https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/>detailed blog post about deprecation</a> with <a href=https://blog.k8s.io/2020/12/02/dockershim-faq/>a dedicated FAQ page for it</a>.</p><h3 id=exec-probe-timeout-handling>Exec Probe Timeout Handling</h3><p>A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field <code>timeoutSeconds</code> was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of <code>1 second</code> will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called <code>ExecProbeTimeout</code>, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to <code>false</code>.</p><p>Please review the updated documentation regarding <a href=docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes>configuring probes</a> for more details.</p><h2 id=other-updates>Other Updates</h2><h3 id=graduated-to-stable>Graduated to Stable</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/585>RuntimeClass</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1929>Built-in API Types Defaults</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/950>Add Pod-Startup Liveness-Probe Holdoff</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1001>Support CRI-ContainerD On Windows</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/614>SCTP Support for Services</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1507>Adding AppProtocol To Services And Endpoints</a></li></ul><h3 id=notable-feature-updates>Notable Feature Updates</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/19>CronJobs</a></li></ul><h1 id=release-notes>Release notes</h1><p>You can check out the full details of the 1.20 release in the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md>release notes</a>.</p><h1 id=availability-of-release>Availability of release</h1><p>Kubernetes 1.20 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0>download on GitHub</a>. There are some great resources out there for getting started with Kubernetes. You can check out some <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with <a href=https://kind.sigs.k8s.io>kind</a>. If you’d like to try building a cluster from scratch, check out the <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kubernetes the Hard Way</a> tutorial by Kelsey Hightower.</p><h1 id=release-team>Release Team</h1><p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community.</p><h1 id=release-logo>Release Logo</h1><p><img src=/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png alt="Kubernetes 1.20 Release Logo"></p><p><a href=https://www.dictionary.com/browse/rad>raddest</a>: <em>adjective</em>, Slang. excellent; wonderful; cool:</p><blockquote><p>The Kubernetes 1.20 Release has been the raddest release yet.</p></blockquote><p>2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to <a href=https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14>Kubernetes 1.14 - Caturnetes</a> with a "rad" cat named Humphrey.</p><p>Humphrey is the release lead's cat and has a permanent <a href=https://www.inverse.com/article/42316-why-do-cats-blep-science-explains><code>blep</code></a>. <em>Rad</em> was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his <em>blep</em> bring you a little joy at the end of 2020!</p><p>The release logo was created by <a href=https://www.instagram.com/robotdancebattle/>Henry Hsu - @robotdancebattle</a>.</p><h1 id=user-highlights>User Highlights</h1><ul><li>Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch <a href=https://youtu.be/Tx8qXC-U3KM>Alena Prokharchyk's KubeCon NA Keynote</a> to learn more about their cloud native journey.</li></ul><h1 id=project-velocity>Project Velocity</h1><p>The <a href=https://k8s.devstats.cncf.io/>CNCF K8s DevStats project</a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p><p>In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&var-period_name=v1.19.0%20-%20now&var-metric=contributions">967 companies</a> and <a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&var-period_name=v1.19.0%20-%20now&var-metric=contributions&var-repogroup_name=Kubernetes&var-country_name=All&var-companies=All">1335 individuals</a> (<a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&from=1601006400000&to=1607576399000&var-repogroup_name=Kubernetes">44 of whom</a> made their first Kubernetes contribution) from <a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&from=1601006400000&to=1607576399000&var-period_name=Quarter&var-countries=All&var-repogroup_name=Kubernetes&var-metric=rcommitters&var-cum=countries">26 countries</a>.</p><h1 id=ecosystem-updates>Ecosystem Updates</h1><ul><li>KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are <a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">now available to all on-demand</a> for anyone still needing to catch up!</li><li>In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given <a href=https://sched.co/eukp>at KubeCon 2020 North America</a>, and the initial impact of this labor <a href=https://github.com/kubernetes/enhancements/issues/2067>can actually be seen in the v1.20 release</a>.</li><li>Previously announced this summer, <a href=https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/>The Certified Kubernetes Security Specialist (CKS) Certification</a> was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?).</li></ul><h1 id=event-updates>Event Updates</h1><p>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/>here</a>. Remember that <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/>the CFP</a> closes on Sunday, December 13, 11:59pm PST!</p><h1 id=upcoming-release-webinar>Upcoming release webinar</h1><p>Stay tuned for the upcoming release webinar happening this January.</p><h1 id=get-involved>Get Involved</h1><p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:</p><ul><li>Find out more about contributing to Kubernetes at the new <a href=https://www.kubernetes.dev/>Kubernetes Contributor website</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li><li>Read more about what’s happening with Kubernetes on the <a href=https://kubernetes.io/blog/>blog</a></li><li>Learn more about the <a href=https://github.com/kubernetes/sig-release/tree/master/release-team>Kubernetes Release Team</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd9668ad6296597dee13c6ff3324b319>GSoD 2020: Improving the API Reference Experience</h1><div class="td-byline mb-4"><time datetime=2020-12-04 class=text-muted>Friday, December 04, 2020</time></div><p><strong>Author</strong>: <a href=https://github.com/feloy>Philippe Martin</a></p><p><em>Editor's note: Better API references have been my goal since I joined Kubernetes docs three and a half years ago. Philippe has succeeded fantastically. More than a better API reference, though, Philippe embodied the best of the Kubernetes community in this project: excellence through collaboration, and a process that made the community itself better. Thanks, Google Season of Docs, for making Philippe's work possible. —Zach Corleissen</em></p><h2 id=introduction>Introduction</h2><p>The <a href=https://developers.google.com/season-of-docs>Google Season of Docs</a> project brings open source organizations and technical writers together to work closely on a specific documentation project.</p><p>I was selected by the CNCF to work on Kubernetes documentation, specifically to make the API Reference documentation more accessible.</p><p>I'm a software developer with a great interest in documentation systems. In the late 90's I started translating Linux-HOWTO documents into French. From one thing to another, I learned about documentation systems. Eventually, I wrote a Linux-HOWTO to help documentarians learn the language used at that time for writing documents, LinuxDoc/SGML.</p><p>Shortly afterward, Linux documentation adopted the DocBook language. I helped some writers rewrite their documents in this format; for example, the Advanced Bash-Scripting Guide. I also worked on the GNU <code>makeinfo</code> program to add DocBook output, making it possible to transform <em>GNU Info</em> documentation into Docbook format.</p><h2 id=background>Background</h2><p>The <a href=https://kubernetes.io/docs/home/>Kubernetes website</a> is built with Hugo from documentation written in Markdown format in the <a href=https://github.com/kubernetes/website>website repository</a>, using the <a href=https://www.docsy.dev/about/>Docsy Hugo theme</a>.</p><p>The existing API reference documentation is a large HTML file generated from the Kubernetes OpenAPI specification.</p><p>On my side, I wanted for some time to make the API Reference more accessible, by:</p><ul><li>building individual and autonomous pages for each Kubernetes resource</li><li>adapting the format to mobile reading</li><li>reusing the website's assets and theme to build, integrate, and display the reference pages</li><li>allowing the search engines to reference the content of the pages</li></ul><p>Around one year ago, I started to work on the generator building the current unique HTML page, to add a DocBook output, so the API Reference could be generated first in DocBook format, and after that in PDF or other formats supported by DocBook processors. The first result has been some <a href=https://github.com/feloy/kubernetes-resources-reference/releases>Ebook files for the API Reference</a> and an auto-edited paper book.</p><p>I decided later to add another output to this generator, to generate Markdown files and create <a href=https://web.archive.org/web/20201022201911/https://www.k8sref.io/docs/workloads/>a website with the API Reference</a>.</p><p>When the CNCF proposed a project for the Google Season of Docs to work on the API Reference, I applied, and the match occurred.</p><h2 id=the-project>The Project</h2><h3 id=swagger-ui>swagger-ui</h3><p>The first idea of the CNCF members that proposed this project was to test the <a href=https://swagger.io/tools/swagger-ui/><code>swagger-ui</code> tool</a>, to try and document the Kubernetes API Reference with this standard tool.</p><p>Because the Kubernetes API is much larger than many other APIs, it has been necessary to write a tool to split the complete API Reference by API Groups, and insert in the Documentation website several <code>swagger-ui</code> components, one for each API Group.</p><p>Generally, APIs are used by developers by calling endpoints with a specific HTTP verb, with specific parameters and waiting for a response. The <code>swagger-ui</code> interface is built for this usage: the interface displays a list of endpoints and their associated verbs, and for each the parameters and responses formats.</p><p>The Kubernetes API is most of the time used differently: users create manifest files containing resources definitions in YAML format, and use the <code>kubectl</code> CLI to <em>apply</em> these manifests to the cluster. In this case, the most important information is the description of the structures used as parameters and responses (the Kubernetes Resources).</p><p>Because of this specificity, we realized that it would be difficult to adapt the <code>swagger-ui</code> interface to satisfy the users of the Kubernetes API and this direction has been abandoned.</p><h3 id=markdown-pages>Markdown pages</h3><p>The second stage of the project has been to adapt the work I had done to create the k8sref.io website, to include it in the official documentation website.</p><p>The main changes have been to:</p><ul><li>use go-templates to represent the output pages, so non-developers can adapt the generated pages without having to edit the generator code</li><li>create a new custom <a href=https://gohugo.io/content-management/shortcodes/>shortcode</a>, to easily create links from inside the website to specific pages of the API reference</li><li>improve the navigation between the sections of the API reference</li><li>add the code of the generator to the Kubernetes GitHub repository containing the different reference generators</li></ul><p>All the discussions and work done can be found in website <a href=https://github.com/kubernetes/website/pull/23294>pull request #23294</a>.</p><p>Adding the generator code to the Kubernetes project happened in <a href=https://github.com/kubernetes-sigs/reference-docs/pull/179>kubernetes-sigs/reference-docs#179</a>.</p><p>Here are the features of the new API Reference to be included in the official documentation website:</p><ul><li>the resources are categorized, in the categories Workloads, Services, Config & Storage, Authentication, Authorization, Policies, Extend, Cluster. This structure is configurable with a simple <a href=https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/toc.yaml><code>toc.yaml</code> file</a></li><li>each page displays associated resources at the first level ; for example: Pod, PodSpec, PodStatus, PodList</li><li>most resource pages inline relevant definitions ; the exceptions are when those definitions are common to several resources, or are too complex to be displayed inline. With the old approach, you had to follow a hyperlink to read each extra detail.</li><li>some widely used definitions, such as <code>ObjectMeta</code>, are documented in a specific page</li><li>required fields are indicated, and placed first</li><li>fields of a resource can be categorized and ordered, with the help of a <a href=https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/fields.yaml><code>fields.yaml</code> file</a></li><li><code>map</code> fields are indicated. For example the <code>.spec.nodeSelector</code> for a <code>Pod</code> is <code>map[string]string</code>, instead of <code>object</code>, using the value of <code>x-kubernetes-list-type</code></li><li>patch strategies are indicated</li><li><code>apiVersion</code> and <code>kind</code> display the value, not the <code>string</code> type</li><li>At the top of a reference page, the page displays the Go import necessary to use these resources from a Go program.</li></ul><p>The work is currently on hold pending the 1.20 release. When the release finishes and the work is integrated, the API reference will be available at <a href=https://kubernetes.io/docs/reference/>https://kubernetes.io/docs/reference/</a>.</p><h3 id=future-work>Future Work</h3><p>There are points to improve, particularly:</p><ul><li>Some Kubernetes resources are deeply nested. Inlining the definition of these resources makes them difficult to understand.</li><li>The created <code>shortcode</code> uses the URL of the page to reference a Resource page. It would be easier for documentarians if they could reference a Resource by its group and name.</li></ul><h2 id=appreciation>Appreciation</h2><p>I would like to thank my mentor <a href=https://github.com/zacharysarah>Zach Corleissen</a> and the lead writers <a href=https://github.com/kbhawkey>Karen Bradshaw</a>, <a href=https://github.com/celestehorgan>Celeste Horgan</a>, <a href=https://github.com/sftim>Tim Bannister</a> and <a href=https://github.com/tengqm>Qiming Teng</a> who supervised me during all the season. They all have been very encouraging and gave me tons of great advice.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4f7882145418a14c275e957b0e8979f4>Dockershim Deprecation FAQ</h1><div class="td-byline mb-4"><time datetime=2020-12-02 class=text-muted>Wednesday, December 02, 2020</time></div><p>This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
<a href=/blog/2020/12/02/dont-panic-kubernetes-and-docker/>Don't Panic: Kubernetes and Docker</a>.</p><h3 id=why-is-dockershim-being-deprecated>Why is dockershim being deprecated?</h3><p>Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.</p><p>Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1985-remove-dockershim>Dockershim Removal Kubernetes Enhancement Proposal</a>.</p><p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.</p><h3 id=can-i-still-use-docker-in-kubernetes-1-20>Can I still use Docker in Kubernetes 1.20?</h3><p>Yes, the only thing changing in 1.20 is a single warning log printed at <a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a>
startup if using Docker as the runtime.</p><h3 id=when-will-dockershim-be-removed>When will dockershim be removed?</h3><p>Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021. We will be working closely with vendors
and other ecosystem groups to ensure a smooth transition and will evaluate things
as the situation evolves.</p><h3 id=will-my-existing-docker-images-still-work>Will my existing Docker images still work?</h3><p>Yes, the images produced from <code>docker build</code> will work with all CRI implementations.
All your existing images will still work exactly the same.</p><h3 id=what-about-private-images>What about private images?</h3><p>Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.</p><h3 id=are-docker-and-containers-the-same-thing>Are Docker and containers the same thing?</h3><p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.</p><h3 id=are-there-examples-of-folks-using-other-runtimes-in-production-today>Are there examples of folks using other runtimes in production today?</h3><p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.</p><p>Additionally, the <a href=https://kind.sigs.k8s.io/>kind</a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the <a href=https://cri-o.io/>CRI-O</a> runtime in production since June 2019.</p><p>For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (<a href=https://cncf.io>CNCF</a>).</p><ul><li><a href=https://github.com/containerd/containerd/blob/master/ADOPTERS.md>containerd</a></li><li><a href=https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md>CRI-O</a></li></ul><h3 id=people-keep-referencing-oci-what-is-that>People keep referencing OCI, what is that?</h3><p>OCI stands for the <a href=https://opencontainers.org/about/overview/>Open Container Initiative</a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of <a href=https://github.com/opencontainers/runc>runc</a>, which is the underlying default runtime for both
<a href=https://containerd.io/>containerd</a> and <a href=https://cri-o.io/>CRI-O</a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.</p><h3 id=which-cri-implementation-should-i-use>Which CRI implementation should I use?</h3><p>That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the <a href="https://landscape.cncf.io/card-mode?category=container-runtime&grouping=category">CNCF landscape</a> in case another would be an
even better fit for your environment.</p><h3 id=what-should-i-look-out-for-when-changing-cri-implementations>What should I look out for when changing CRI implementations?</h3><p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:</p><ul><li>Logging configuration</li><li>Runtime resource limitations</li><li>Node provisioning scripts that call docker or use docker via it's control socket</li><li>Kubectl plugins that require docker CLI or the control socket</li><li>Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)</li><li>Configuration of functionality like <code>registry-mirrors</code> and insecure registries</li><li>Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)</li><li>GPUs or special hardware and how they integrate with your runtime and Kubernetes</li></ul><p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.</p><p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the <a href=https://github.com/kubernetes-sigs/cri-tools><code>crictl</code></a> tool as a drop-in replacement (see <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl>mapping from docker cli to crictl</a>) and for the
latter you can use newer container build options like <a href=https://github.com/genuinetools/img>img</a>, <a href=https://github.com/containers/buildah>buildah</a>,
<a href=https://github.com/GoogleContainerTools/kaniko>kaniko</a>, or <a href=https://github.com/vmware-tanzu/buildkit-cli-for-kubectl>buildkit-cli-for-kubectl</a> that don’t require Docker.</p><p>For containerd, you can start with their <a href=https://github.com/containerd/cri/blob/master/docs/registry.md>documentation</a> to see what configuration
options are available as you migrate things over.</p><p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on <a href=/docs/setup/production-environment/container-runtimes>Container Runtimes</a></p><h3 id=what-if-i-have-more-questions>What if I have more questions?</h3><p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: <a href=https://discuss.kubernetes.io/>https://discuss.kubernetes.io/</a>.</p><p>You can also check out the excellent blog post
<a href=https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m>Wait, Docker is deprecated in Kubernetes now?</a> a more in-depth technical
discussion of the changes.</p><h3 id=can-i-have-a-hug>Can I have a hug?</h3><p>Always and whenever you want! 🤗🤗</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d0001c21fb6f2f9b21c0faa11a149fae>Don't Panic: Kubernetes and Docker</h1><div class="td-byline mb-4"><time datetime=2020-12-02 class=text-muted>Wednesday, December 02, 2020</time></div><p><strong>Authors:</strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas</p><p>Kubernetes is <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation>deprecating
Docker</a>
as a container runtime after v1.20.</p><p><strong>You do not need to panic. It’s not as dramatic as it sounds.</strong></p><p>TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>Container Runtime Interface (CRI)</a>
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.</p><p>If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running <code>docker build</code> can still run in your Kubernetes cluster.</p><p>If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which <a href=https://github.com/Azure/AKS/releases/tag/2020-11-16>defaults to containerd</a>) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.</p><p>If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).</p><h2 id=so-why-the-confusion-and-what-is-everyone-freaking-out-about>So why the confusion and what is everyone freaking out about?</h2><p>We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.</p><p>You see, the thing we call “Docker” isn’t actually one thing—it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.</p><p>As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?</p><p>Docker isn’t compliant with CRI, the <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>Container Runtime Interface</a>.
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic—you just need to change
your container runtime from Docker to another supported container runtime.</p><p>One thing to note: If you are relying on the underlying docker socket
(<code>/var/run/docker.sock</code>) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
<a href=https://github.com/GoogleContainerTools/kaniko>kaniko</a>,
<a href=https://github.com/genuinetools/img>img</a>, and
<a href=https://github.com/containers/buildah>buildah</a>.</p><h2 id=what-does-this-change-mean-for-developers-though-do-we-still-write-dockerfiles-do-we-still-build-things-with-docker>What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?</h2><p>This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image—it’s an OCI (<a href=https://opencontainers.org/>Open Container Initiative</a>) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both <a href=https://containerd.io/>containerd</a> and
<a href=https://cri-o.io/>CRI-O</a> know how to pull those images and run them. This is
why we have a standard for what containers should look like.</p><p>So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay—there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️</p><p>Looking for more answers? Check out our accompanying <a href=/blog/2020/12/02/dockershim-faq/>Dockershim Deprecation FAQ</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-93f76fe9b1d669349b8d9228bd9661b4>Cloud native security for your clusters</h1><div class="td-byline mb-4"><time datetime=2020-11-18 class=text-muted>Wednesday, November 18, 2020</time></div><p><strong>Author</strong>: <a href=https://twitter.com/pudijoglekar>Pushkar Joglekar</a></p><p>Over the last few years a small, security focused community has been working diligently to deepen our understanding of security, given the evolving cloud native infrastructure and corresponding iterative deployment practices. To enable sharing of this knowledge with the rest of the community, members of <a href=https://github.com/cncf/sig-security>CNCF SIG Security</a> (a group which reports into <a href=https://github.com/cncf/toc#sigs>CNCF TOC</a> and who are friends with <a href=https://github.com/kubernetes/community/tree/master/sig-security>Kubernetes SIG Security</a>) led by Emily Fox, collaborated on a whitepaper outlining holistic cloud native security concerns and best practices. After over 1200 comments, changes, and discussions from 35 members across the world, we are proud to share <a href=https://www.cncf.io/blog/2020/11/18/announcing-the-cloud-native-security-white-paper>cloud native security whitepaper v1.0</a> that serves as essential reading for security leadership in enterprises, financial and healthcare industries, academia, government, and non-profit organizations.</p><p>The paper attempts to <em>not</em> focus on any specific <a href=https://www.cncf.io/projects/>cloud native project</a>. Instead, the intent is to model and inject security into four logical phases of cloud native application lifecycle: <em>Develop, Distribute, Deploy, and Runtime</em>.</p><p><img alt="Cloud native application lifecycle phases" src=cloud-native-app-lifecycle-phases.svg style=width:60em;max-width:100%></p><h2 id=kubernetes-native-security-controls>Kubernetes native security controls</h2><p>When using Kubernetes as a workload orchestrator, some of the security controls this version of the whitepaper recommends are:</p><ul><li><a href=/docs/concepts/policy/pod-security-policy/>Pod Security Policies</a>: Implement a single source of truth for “least privilege” workloads across the entire cluster</li><li><a href=/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>Resource requests and limits</a>: Apply requests (soft constraint) and limits (hard constraint) for shared resources such as memory and CPU</li><li><a href=/docs/tasks/debug-application-cluster/audit/>Audit log analysis</a>: Enable Kubernetes API auditing and filtering for security relevant events</li><li><a href=/docs/concepts/architecture/control-plane-node-communication/>Control plane authentication and certificate root of trust</a>: Enable mutual TLS authentication with a trusted CA for communication within the cluster</li><li><a href=/docs/concepts/configuration/secret/>Secrets management</a>: Integrate with a built-in or external secrets store</li></ul><h2 id=cloud-native-complementary-security-controls>Cloud native complementary security controls</h2><p>Kubernetes has direct involvement in the <em>deploy</em> phase and to a lesser extent in the <em>runtime</em> phase. Ensuring the artifacts are securely <em>developed</em> and <em>distributed</em> is necessary for, enabling workloads in Kubernetes to run “secure by default”. Throughout all phases of the Cloud native application life cycle, several complementary security controls exist for Kubernetes orchestrated workloads, which includes but are not limited to:</p><ul><li>Develop:<ul><li>Image signing and verification</li><li>Image vulnerability scanners</li></ul></li><li>Distribute:<ul><li>Pre-deployment checks for detecting excessive privileges</li><li>Enabling observability and logging</li></ul></li><li>Deploy:<ul><li>Using a service mesh for workload authentication and authorization</li><li>Enforcing “default deny” network policies for inter-workload communication via <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugins</a></li></ul></li><li>Runtime:<ul><li>Deploying security monitoring agents for workloads</li><li>Isolating applications that run on the same node using SELinux, AppArmor, etc.</li><li>Scanning configuration against recognized secure baselines for node, workload and orchestrator</li></ul></li></ul><h2 id=understand-first-secure-next>Understand first, secure next</h2><p>The cloud native way, including containers, provides great security benefits for its users: immutability, modularity, faster upgrades and consistent state across the environment. Realizing this fundamental change in “the way things are done”, motivates us to look at security with a cloud native lens. One of the things that was evident for all the authors of the paper was the fact that it’s tough to make smarter decisions on how and what to secure in a cloud native ecosystem if you do not understand the tools, patterns, and frameworks at hand (in addition to knowing your own critical assets). Hence, for all the security practitioners out there who want to be partners rather than a gatekeeper for your friends in Operations, Product Development, and Compliance, let’s make an attempt to <em>learn more so we can secure better</em>.</p><p>We recommend following this <strong>7 step R.U.N.T.I.M.E. path</strong> to get started on cloud native security:</p><ol><li><b>R</b>ead the paper and any linked material in it</li><li><b>U</b>nderstand challenges and constraints for your environment</li><li><b>N</b>ote the content and controls that apply to your environment</li><li><b>T</b>alk about your observations with your peers</li><li><b>I</b>nvolve your leadership and ask for help</li><li><b>M</b>ake a risk profile based on existing and missing security controls</li><li><b>E</b>xpend time, money, and resources that improve security posture and reduce risk where appropriate.</li></ol><h2 id=acknowledgements>Acknowledgements</h2><p>Huge shout out to <em>Emily Fox, Tim Bannister (The Scale Factory), Chase Pettet (Mirantis), and Wayne Haber (GitLab)</em> for contributing with their wonderful suggestions for this blog post.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-beabb22510a72a7565a7e034c4a88d15>Remembering Dan Kohn</h1><div class="td-byline mb-4"><time datetime=2020-11-02 class=text-muted>Monday, November 02, 2020</time></div><p><strong>Author</strong>: The Kubernetes Steering Committee</p><p>Dan Kohn was instrumental in getting Kubernetes and CNCF community to where it is today. He shared our values, motivations, enthusiasm, community spirit, and helped the Kubernetes community to become the best that it could be. Dan loved getting people together to solve problems big and small. He enabled people to grow their individual scope in the community which often helped launch their career in open source software.</p><p>Dan built a coalition around the nascent Kubernetes project and turned that into a cornerstone to build the larger cloud native space. He loved challenges, especially ones where the payoff was great like building worldwide communities, spreading the love of open source, and helping diverse, underprivileged communities and students to get a head start in technology.</p><p>Our heart goes out to his family. Thank you, Dan, for bringing your boys to events in India and elsewhere as we got to know how great you were as a father. Dan, your thoughts and ideas will help us make progress in our journey as a community. Thank you for your life's work!</p><p>If Dan has made an impact on you in some way, please consider adding a memory of him in his <a href=https://github.com/cncf/memorials/blob/master/dan-kohn.md>CNCF memorial</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-614959f25c84da82dbfcf7287dd203aa>Announcing the 2020 Steering Committee Election Results</h1><div class="td-byline mb-4"><time datetime=2020-10-12 class=text-muted>Monday, October 12, 2020</time></div><p><strong>Author</strong>: Kaslin Fields</p><p>The <a href=https://github.com/kubernetes/community/tree/master/events/elections/2020>2020 Steering Committee Election</a> is now complete. In 2019, the committee arrived at its final allocation of 7 seats, 3 of which were up for election in 2020. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.</p><p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their <a href=https://github.com/kubernetes/steering/blob/master/charter.md>charter</a>.</p><h2 id=results>Results</h2><p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):</p><ul><li><strong>Davanum Srinivas (<a href=https://github.com/dims>@dims</a>), VMware</strong></li><li><strong>Jordan Liggitt (<a href=https://github.com/liggitt>@liggitt</a>), Google</strong></li><li><strong>Bob Killen (<a href=https://github.com/mrbobbytables>@mrbobbytables</a>), Google</strong></li></ul><p>They join continuing members Christoph Blecker (<a href=https://github.com/cblecker>@cblecker</a>), Red Hat; Derek Carr (<a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>), Red Hat; Nikhita Raghunath (<a href=https://github.com/nikhita>@nikhita</a>), VMware; and Paris Pittman (<a href=https://github.com/parispittman>@parispittman</a>), Apple. Davanum Srinivas is returning for his second term on the committee.</p><h2 id=big-thanks>Big Thanks!</h2><ul><li>Thank you and congratulations on a successful election to this round’s election officers:<ul><li>Jaice Singer DuMars (<a href=https://github.com/jdumars>@jdumars</a>), Apple</li><li>Ihor Dvoretskyi (<a href=https://github.com/idvoretskyi>@idvoretskyi</a>), CNCF</li><li>Josh Berkus (<a href=https://github.com/jberkus>@jberkus</a>), Red Hat</li></ul></li><li>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:<ul><li>Aaron Crickenberger (<a href=https://github.com/spiffxp>@spiffxp</a>), Google</li><li>and Lachlan Evenson(<a href=https://github.com/lachie8e>@lachie8e)</a>), Microsoft</li></ul></li><li>And thank you to all the candidates who came forward to run for election. As <a href="https://twitter.com/castrojo/status/1315718627639820288?s=20">Jorge Castro put it</a>: we are spoiled with capable, kind, and selfless volunteers who put the needs of the project first.</li></ul><h2 id=get-involved-with-the-steering-committee>Get Involved with the Steering Committee</h2><p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee <a href=https://github.com/kubernetes/steering/projects/1>backlog items</a> and weigh in by filing an issue or creating a PR against their <a href=https://github.com/kubernetes/steering>repo</a>. They have an open meeting on <a href=https://github.com/kubernetes/steering>the first Monday of the month at 6pm UTC</a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list <a href=mailto:steering@kubernetes.io>steering@kubernetes.io</a>.</p><p>You can see what the Steering Committee meetings are all about by watching past meetings on the <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist</a>.</p><hr><p><em>This post was written by the <a href=https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing>Upstream Marketing Working Group</a>. If you want to write stories about the Kubernetes community, learn more about us.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-fcbc001800469c30982bca14879a3acb>Contributing to the Development Guide</h1><div class=lead>A new contributor describes the experience of writing and submitting changes to the Kubernetes Development Guide.</div><div class="td-byline mb-4">By <b>Erik L. Arneson</b> |
<time datetime=2020-10-01 class=text-muted>Thursday, October 01, 2020</time></div><p>When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of "client." I just didn't know what to expect when Google asked my compatriots and me at
<a href=https://lionswaycontent.com/>Lion's Way</a> to make much-needed updates to the Kubernetes Development Guide.</p><p><em>This article originally appeared on the <a href=https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/>Kubernetes Contributor Community blog</a>.</em></p><h2 id=the-delights-of-working-with-a-community>The Delights of Working With a Community</h2><p>As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.</p><p>One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor <a href=https://twitter.com/JoelByronBarker>Joel</a>
handles most of the client contact.</p><p>I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.</p><p>"How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?" These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the <code>#sig-contribex</code> channel on the Kubernetes
Slack and announced that I would be working on the
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/development.md>Development Guide</a>.</p><div class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:810px><img class=card-img-top src=/blog/2020/10/01/contributing-to-the-development-guide/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg width=800 height=450><div class="card-body px-0 pt-2 pb-0"><p class=card-text>"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &mdash; Jorge
Castro, SIG ContribEx co-chair</p></div></div><p>My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the <a href=https://www.kubernetes.dev/resources/code-of-conduct/>Kubernetes Code of Conduct</a> was in
effect, and that we should, like Bill and Ted, be excellent to each other.</p><h2 id=this-doesn-t-mean-it-s-all-easy>This Doesn't Mean It's All Easy</h2><p>The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
<a href=https://github.com/kubernetes/community>Community repo</a>: 267 additions and 88 deletions.</p><p>The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, <a href=https://github.com/kubernetes/community/pull/5003>it was successful</a>.</p><p>Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the <a href=https://github.com/amwat>labyrinthine mind of
a brilliant engineer</a>, and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.</p><p>This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
<a href=https://github.com/kubernetes-sigs/kubetest2><code>kubetest2</code> framework</a> to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
<a href=https://github.com/kubernetes/community/pull/5045>completed pull request</a>.</p><h2 id=nobody-is-the-boss-and-everybody-gives-feedback>Nobody Is the Boss, and Everybody Gives Feedback</h2><p>But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was <em>enjoyable</em>.</p><p>With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.</p><p>In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.</p><p>Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b29c63b1bca54209fe57b9ee517b9576>GSoC 2020 - Building operators for cluster addons</h1><div class="td-byline mb-4"><time datetime=2020-09-16 class=text-muted>Wednesday, September 16, 2020</time></div><p><strong>Author</strong>: Somtochi Onyekwere</p><h1 id=introduction>Introduction</h1><p><a href=https://summerofcode.withgoogle.com/>Google Summer of Code</a> is a global program that is geared towards introducing students to open source. Students are matched with open-source organizations to work with them for three months during the summer.</p><p>My name is Somtochi Onyekwere from the Federal University of Technology, Owerri (Nigeria) and this year, I was given the opportunity to work with Kubernetes (under the CNCF organization) and this led to an amazing summer spent learning, contributing and interacting with the community.</p><p>Specifically, I worked on the <em>Cluster Addons: Package all the things!</em> project. The project focused on building operators for better management of various cluster addons, extending the tooling for building these operators and making the creation of these operators a smooth process.</p><h1 id=background>Background</h1><p>Kubernetes has progressed greatly in the past few years with a flourishing community and a large number of contributors. The codebase is gradually moving away from the monolith structure where all the code resides in the <a href=https://github.com/kubernetes/kubernetes>kubernetes/kubernetes</a> repository to being split into multiple sub-projects. Part of the focus of cluster-addons is to make some of these sub-projects work together in an easy to assemble, self-monitoring, self-healing and Kubernetes-native way. It enables them to work seamlessly without human intervention.</p><p>The community is exploring the use of operators as a mechanism to monitor various resources in the cluster and properly manage these resources. In addition to this, it provides self-healing and it is a kubernetes-native pattern that can encode how best these addons work and manage them properly.</p><p>What are cluster addons? Cluster addons are a collection of resources (like Services and deployment) that are used to give a Kubernetes cluster additional functionalities. They range from things as simple as the Kubernetes dashboards (for visualization) to more complex ones like Calico (for networking). These addons are essential to different applications running in the cluster and the cluster itself. The addon operator provides a nicer way of managing these addons and understanding the health and status of the various resources that comprise the addon. You can get a deeper overview in this <a href=https://kubernetes.io/docs/concepts/overview/components/#addons>article</a>.</p><p>Operators are custom controllers with custom resource definitions that encode application-specific knowledge and are used for managing complex stateful applications. It is a widely accepted pattern. Managing addons via operators, with these operators encoding knowledge of how best the addons work, introduces a lot of advantages while setting standards that will be easy to follow and scale. This <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator>article</a> does a good job of explaining operators.</p><p>The addon operators can solve a lot of problems, but they have their challenges. Those under the <a href=https://github.com/kubernetes-sigs/cluster-addons>cluster-addons project</a> had missing pieces and were still a proof of concept. Generating the RBAC configuration for the operators was a pain and sometimes the operators were given too much privilege. The operators weren’t very extensible as it only pulled manifests from local filesystems or HTTP(s) servers and a lot of simple addons were generating the same code.
I spent the summer working on these issues, looking at them with fresh eyes and coming up with solutions for both the known and unknown issues.</p><h1 id=various-additions-to-kubebuilder-declarative-pattern>Various additions to kubebuilder-declarative-pattern</h1><p>The <a href=https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern>kubebuilder-declarative-pattern</a> (from here on referred to as KDP) repo is an extra layer of addon specific tooling on top of the <a href=https://github.com/kubernetes-sigs/kubebuilder>kubebuilder</a> SDK that is enabled by passing the experimental <code>--pattern=addon</code> flag to <code>kubebuilder create</code> command. Together, they create the base code for the addon operator. During the internship, I worked on a couple of features in KDP and cluster-addons.</p><h2 id=operator-version-checking>Operator version checking</h2><p>Enabling version checks for operators helped in making upgrades/downgrades safer to different versions of the addon, even though the operator had complex logic. It is a way of matching the version of an addon to the version of the operator that knows how to manage it well. Most addons have different versions and these versions might need to be managed differently. This feature checks the custom resource for the <code>addons.k8s.io/min-operator-version</code> annotation which states the minimum operator version that is needed to manage the version against the version of the operator. If the operator version is below the minimum version required, the operator pauses with an error telling the user that the version of the operator is too low. This helps to ensure that the correct operator is being used for the addon.</p><h2 id=git-repository-for-storing-the-manifests>Git repository for storing the manifests</h2><p>Previously, there was support for only local file directories and HTTPS repositories for storing manifests. Giving creators of addon operators the ability to store manifest in GitHub repository enables faster development and version control. When starting the controller, you can pass a flag to specify the location of your channels directory. The channels directory contains the manifests for different versions, the controller pulls the manifest from this directory and applies it to the cluster. During the internship period, I extended it to include Git repositories.</p><h2 id=annotations-to-temporarily-disable-reconciliation>Annotations to temporarily disable reconciliation</h2><p>The reconciliation loop that ensures that the desired state matches the actual state prevents modification of objects in the cluster. This makes it hard to experiment or investigate what might be wrong in the cluster as any changes made are promptly reverted. I resolved this by allowing users to place an <code>addons.k8s.io/ignore</code> annotation on the resource that they don’t want the controller to reconcile. The controller checks for this annotation and doesn’t reconcile that object. To resume reconciliation, the annotation can be removed from the resource.</p><h2 id=unstructured-support-in-kubebuilder-declarative-pattern>Unstructured support in kubebuilder-declarative-pattern</h2><p>One of the operators that I worked on is a generic controller that could manage more than one cluster addon that did not require extra configuration. To do this, the operator couldn’t use a particular type and needed the kubebuilder-declarative-repo to support using the <a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured#Unstructured>unstructured.Unstructured</a> type. There were various functions in the kubebuilder-declarative-pattern that couldn’t handle this type and returned an error if the object passed in was not of type <code>addonsv1alpha1.CommonObject</code>. The functions were modified to handle both <code>unstructured.Unstructured</code> and <code>addonsv1alpha.CommonObject</code>.</p><h1 id=tools-and-cli-programs>Tools and CLI programs</h1><p>There were also some command-line programs I wrote that could be used to make working with addon operators easier. Most of them have uses outside the addon operators as they try to solve a specific problem that could surface anywhere while working with Kubernetes. I encourage you to <a href=https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools>check them out</a> when you have the chance!</p><h2 id=rbac-generator>RBAC Generator</h2><p>One of the biggest concerns with the operator was RBAC. You had to manually look through the manifest and add the RBAC rule for each resource as it needs to have RBAC permissions to create, get, update and delete the resources in the manifest when running in-cluster. Building the <a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen>RBAC generator</a> automated the process of writing the RBAC roles and role bindings. The function of the RBAC generator is simple. It accepts the file name of the manifest as a flag. Then, it parses the manifest and gets the API group and resource name of the resources and adds it to a role. It outputs the role and role binding to stdout or a file if the <code>--out</code> flag is parsed.</p><p>Additionally, the tool enables you to split the RBAC by separating the cluster roles in the manifest. This lessened the security concern of an operator being over-privileged as it needed to have all the permissions that the clusterrole has. If you want to apply the clusterrole yourself and not give the operator these permissions, you can pass in a <code>--supervisory</code> boolean flag so that the generator does not add these permissions to the role. The CLI program resides <a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen>here</a>.</p><h2 id=kubectl-ownerref>Kubectl Ownerref</h2><p>It is hard to find out at a glance which objects were created by an addon custom resource. This kubectl plugin alleviates that pain by displaying all the objects in the cluster that a resource has ownerrefs on. You simply pass the kind and the name of the resource as arguments to the program and it checks the cluster for the objects and gives the kind, name, the namespace of such an object. It could be useful to get a general overview of all the objects that the controller is reconciling by passing in the name and kind of custom resource. The CLI program resides <a href=https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools/kubectl-ownerref>here</a>.</p><h1 id=addon-operators>Addon Operators</h1><p>To fully understand addons operators and make changes to how they are being created, you have to try creating and using them. Part of the summer was spent building operators for some popular addons like the Kubernetes dashboard, flannel, NodeLocalDNS and so on. Please check the <a href=https://github.com/kubernetes-sigs/cluster-addons>cluster-addons</a> repository for the different addon operators. In this section, I will just highlight one that is a little different from the others.</p><h2 id=generic-controller>Generic Controller</h2><p>The generic controller can be shared between addons that don’t require much configuration. This minimizes resource consumption on the cluster as it reduces the number of controllers that need to be run. Also instead of building your own operator, you can just use the generic controller and whenever you feel that your needs have grown and you need a more complex operator, you can always scaffold the code with kubebuilder and continue from where the generic operator stopped. To use the generic controller, you can generate the CustomResourceDefinition(CRD) using this tool (<a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md>generic-addon</a>). You pass in the kind, group, and the location of your channels directory (it could be a Git repository too!). The tool generates the - CRD, RBAC manifest and two custom resources for you.</p><p>The process is as follows:</p><ul><li>Create the Generic CRD</li><li>Generate all the manifests needed with the <a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md><code>generic-addon tool</code></a>.</li></ul><p>This tool creates:</p><ol><li>The CRD for your addon</li><li>The RBAC rules for the CustomResourceDefinitions</li><li>The RBAC rules for applying the manifests</li><li>The custom resource for your addon</li><li>A Generic custom resource</li></ol><p>The Generic custom resource looks like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>addons.x-k8s.io/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Generic<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb> 	</span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>generic-sample<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>objectKind</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NodeLocalDNS<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;v1alpha1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb> </span>addons.x-k8s.io<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>channel</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;../nodelocaldns/channels&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Apply these manifests but ensure to apply the CRD before the CR.
Then, run the Generic controller, either on your machine or in-cluster.</p><p>If you are interested in building an operator, Please check out <a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md>this guide</a>.</p><h1 id=relevant-links>Relevant Links</h1><ul><li><a href=https://github.com/SomtochiAma/gsoc-2020-meta-k8s>Detailed breakdown of work done during the internship</a></li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/addons/0035-20190128-addons-via-operators.md>Addon Operator (KEP)</a></li><li><a href=https://github.com/kubernetes-sigs/cluster-addons/issues/39>Original GSoC Issue</a></li><li><a href=https://github.com/SomtochiAma/gsoc-2020-meta-k8s/blob/master/GSoC%202020%20PROPOSAL%20-%20PACKAGE%20ALL%20THINGS.pdf>Proposal Submitted for GSoC</a></li><li><a href="https://github.com/kubernetes-sigs/cluster-addons/commits?author=SomtochiAma">All commits to kubernetes-sigs/cluster-addons</a></li><li><a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern/commits?author=SomtochiAma">All commits to kubernetes-sigs/kubebuidler-declarative-pattern</a></li></ul><h1 id=further-work>Further Work</h1><p>A lot of work was definitely done on the cluster addons during the GSoC period. But we need more people building operators and using them in the cluster. We need wider adoption in the community. Build operators for your favourite addons and tell us how it went and if you had any issues. Check out this <a href=https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md>README.md</a> to get started.</p><h1 id=appreciation>Appreciation</h1><p>I really want to appreciate my mentors <a href=https://github.com/justinsb>Justin Santa Barbara</a> (Google) and <a href=https://github.com/stealthybox>Leigh Capili</a> (Weaveworks). My internship was awesome because they were awesome. They set a golden standard for what mentorship should be. They were accessible and always available to clear any confusion. I think what I liked best was that they didn’t just dish out tasks, instead, we had open discussions about what was wrong and what could be improved. They are really the best and I hope I get to work with them again!
Also, I want to say a huge thanks to <a href=https://github.com/neolit123>Lubomir I. Ivanov</a> for reviewing this blog post!</p><h1 id=conclusion>Conclusion</h1><p>So far I have learnt a lot about Go, the internals of Kubernetes, and operators. I want to conclude by encouraging people to contribute to open-source (especially Kubernetes :)) regardless of your level of experience. It has been a well-rounded experience for me and I have come to love the community. It is a great initiative and it is a great way to learn and meet awesome people. Special shoutout to Google for organizing this program.</p><p>If you are interested in cluster addons and finding out more on addon operators, you are welcome to join our slack channel on the Kubernetes <a href=https://kubernetes.slack.com/messages/cluster-addons>#cluster-addons</a>.</p><hr><p><em><a href=https://twitter.com/SomtochiAma>Somtochi Onyekwere</a> is a software engineer that loves contributing to open-source and exploring cloud native solutions.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d9e35cec65c1d4e7fff2cc66f8423d52>Introducing Structured Logs</h1><div class="td-byline mb-4"><time datetime=2020-09-04 class=text-muted>Friday, September 04, 2020</time></div><p><strong>Authors:</strong> Marek Siarkowicz (Google), Nathan Beach (Google)</p><p>Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.</p><p>In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.</p><p>To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those "key"="value" pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the <code>--logging-format=json</code> flag.</p><h2 id=using-structured-logs>Using Structured Logs</h2><p>We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang>klog.<span style=color:#00a000>InfoS</span>(<span style=color:#b44>&#34;Pod status updated&#34;</span>, <span style=color:#b44>&#34;pod&#34;</span>, klog.<span style=color:#00a000>KObj</span>(pod), <span style=color:#b44>&#34;status&#34;</span>, status)
</code></pre></div><p>will result in this log:</p><pre><code>I1025 00:15:15.525108       1 controller_utils.go:116] &quot;Pod status updated&quot; pod=&quot;kube-system/kubedns&quot; status=&quot;ready&quot;
</code></pre><p>Or, if the --logging-format=json flag is set, it will result in this output:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
  <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
  <span style=color:green;font-weight:700>&#34;pod&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;coredns&#34;</span>,
    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;kube-system&#34;</span>
  },
  <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
}
</code></pre></div><p>This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.</p><p>With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.</p><p>Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.</p><p>Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.</p><p>Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.</p><h2 id=get-involved>Get Involved</h2><p>While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md>migrate existing log calls to use structured logs</a>. It's a great and easy way to make your first contribution to Kubernetes!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e99acccbdb5593353cf25826879389e4>Warning: Helpful Warnings Ahead</h1><div class="td-byline mb-4"><time datetime=2020-09-03 class=text-muted>Thursday, September 03, 2020</time></div><p><strong>Author</strong>: Jordan Liggitt (Google)</p><p>As Kubernetes maintainers, we're always looking for ways to improve usability while preserving compatibility.
As we develop features, triage bugs, and answer support questions, we accumulate information that would be helpful for Kubernetes users to know.
In the past, sharing that information was limited to out-of-band methods like release notes, announcement emails, documentation, and blog posts.
Unless someone knew to seek out that information and managed to find it, they would not benefit from it.</p><p>In Kubernetes v1.19, we added a feature that allows the Kubernetes API server to
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings>send warnings to API clients</a>.
The warning is sent using a <a href=https://tools.ietf.org/html/rfc7234#section-5.5>standard <code>Warning</code> response header</a>,
so it does not change the status code or response body in any way.
This allows the server to send warnings easily readable by any API client, while remaining compatible with previous client versions.</p><p>Warnings are surfaced by <code>kubectl</code> v1.19+ in <code>stderr</code> output, and by the <code>k8s.io/client-go</code> client library v0.19.0+ in log output.
The <code>k8s.io/client-go</code> behavior can be <a href=#customize-client-handling>overridden per-process or per-client</a>.</p><h2 id=deprecation-warnings>Deprecation Warnings</h2><p>The first way we are using this new capability is to send warnings for use of deprecated APIs.</p><p>Kubernetes is a <a href=https://www.cncf.io/cncf-kubernetes-project-journey/#development-velocity>big, fast-moving project</a>.
Keeping up with the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1180>changes</a>
in each release can be daunting, even for people who work on the project full-time. One important type of change is API deprecations.
As APIs in Kubernetes graduate to GA versions, pre-release API versions are deprecated and eventually removed.</p><p>Even though there is an <a href=/docs/reference/using-api/deprecation-policy/>extended deprecation period</a>,
and deprecations are <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecation>included in release notes</a>,
they can still be hard to track. During the deprecation period, the pre-release API remains functional,
allowing several releases to transition to the stable API version. However, we have found that users often don't even realize
they are depending on a deprecated API version until they upgrade to the release that stops serving it.</p><p>Starting in v1.19, whenever a request is made to a deprecated REST API, a warning is returned along with the API response.
This warning includes details about the release in which the API will no longer be available, and the replacement API version.</p><p>Because the warning originates at the server, and is intercepted at the client level, it works for all kubectl commands,
including high-level commands like <code>kubectl apply</code>, and low-level commands like <code>kubectl get --raw</code>:</p><p><img alt="kubectl applying a manifest file, then displaying a warning message 'networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress'." src=kubectl-warnings.png style=width:637px;max-width:100%></p><p>This helps people affected by the deprecation to know the request they are making is deprecated,
how long they have to address the issue, and what API they should use instead.
This is especially helpful when the user is applying a manifest they didn't create,
so they have time to reach out to the authors to ask for an updated version.</p><p>We also realized that the person <em>using</em> a deprecated API is often not the same person responsible for upgrading the cluster,
so we added two administrator-facing tools to help track use of deprecated APIs and determine when upgrades are safe.</p><h3 id=metrics>Metrics</h3><p>Starting in Kubernetes v1.19, when a request is made to a deprecated REST API endpoint,
an <code>apiserver_requested_deprecated_apis</code> gauge metric is set to <code>1</code> in the kube-apiserver process.
This metric has labels for the API <code>group</code>, <code>version</code>, <code>resource</code>, and <code>subresource</code>,
and a <code>removed_version</code> label that indicates the Kubernetes release in which the API will no longer be served.</p><p>This is an example query using <code>kubectl</code>, <a href=https://github.com/prometheus/prom2json>prom2json</a>,
and <a href=https://stedolan.github.io/jq/>jq</a> to determine which deprecated APIs have been requested
from the current instance of the API server:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl get --raw /metrics | prom2json | jq <span style=color:#b44>&#39;
</span><span style=color:#b44>  .[] | select(.name==&#34;apiserver_requested_deprecated_apis&#34;).metrics[].labels
</span><span style=color:#b44>&#39;</span>
</code></pre></div><p>Output:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;extensions&#34;</span>,
  <span style=color:green;font-weight:700>&#34;removed_release&#34;</span>: <span style=color:#b44>&#34;1.22&#34;</span>,
  <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;ingresses&#34;</span>,
  <span style=color:green;font-weight:700>&#34;subresource&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
  <span style=color:green;font-weight:700>&#34;version&#34;</span>: <span style=color:#b44>&#34;v1beta1&#34;</span>
}
{
  <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;rbac.authorization.k8s.io&#34;</span>,
  <span style=color:green;font-weight:700>&#34;removed_release&#34;</span>: <span style=color:#b44>&#34;1.22&#34;</span>,
  <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;clusterroles&#34;</span>,
  <span style=color:green;font-weight:700>&#34;subresource&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
  <span style=color:green;font-weight:700>&#34;version&#34;</span>: <span style=color:#b44>&#34;v1beta1&#34;</span>
}
</code></pre></div><p>This shows the deprecated <code>extensions/v1beta1</code> Ingress and <code>rbac.authorization.k8s.io/v1beta1</code> ClusterRole APIs
have been requested on this server, and will be removed in v1.22.</p><p>We can join that information with the <code>apiserver_request_total</code> metrics to get more details about the requests being made to these APIs:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl get --raw /metrics | prom2json | jq <span style=color:#b44>&#39;
</span><span style=color:#b44>  # set $deprecated to a list of deprecated APIs
</span><span style=color:#b44>  [
</span><span style=color:#b44>    .[] | 
</span><span style=color:#b44>    select(.name==&#34;apiserver_requested_deprecated_apis&#34;).metrics[].labels |
</span><span style=color:#b44>    {group,version,resource}
</span><span style=color:#b44>  ] as $deprecated 
</span><span style=color:#b44>  
</span><span style=color:#b44>  |
</span><span style=color:#b44>  
</span><span style=color:#b44>  # select apiserver_request_total metrics which are deprecated
</span><span style=color:#b44>  .[] | select(.name==&#34;apiserver_request_total&#34;).metrics[] |
</span><span style=color:#b44>  select(.labels | {group,version,resource} as $key | $deprecated | index($key))
</span><span style=color:#b44>&#39;</span>
</code></pre></div><p>Output:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;code&#34;</span>: <span style=color:#b44>&#34;0&#34;</span>,
    <span style=color:green;font-weight:700>&#34;component&#34;</span>: <span style=color:#b44>&#34;apiserver&#34;</span>,
    <span style=color:green;font-weight:700>&#34;contentType&#34;</span>: <span style=color:#b44>&#34;application/vnd.kubernetes.protobuf;stream=watch&#34;</span>,
    <span style=color:green;font-weight:700>&#34;dry_run&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;extensions&#34;</span>,
    <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;ingresses&#34;</span>,
    <span style=color:green;font-weight:700>&#34;scope&#34;</span>: <span style=color:#b44>&#34;cluster&#34;</span>,
    <span style=color:green;font-weight:700>&#34;subresource&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;WATCH&#34;</span>,
    <span style=color:green;font-weight:700>&#34;version&#34;</span>: <span style=color:#b44>&#34;v1beta1&#34;</span>
  },
  <span style=color:green;font-weight:700>&#34;value&#34;</span>: <span style=color:#b44>&#34;21&#34;</span>
}
{
  <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;code&#34;</span>: <span style=color:#b44>&#34;200&#34;</span>,
    <span style=color:green;font-weight:700>&#34;component&#34;</span>: <span style=color:#b44>&#34;apiserver&#34;</span>,
    <span style=color:green;font-weight:700>&#34;contentType&#34;</span>: <span style=color:#b44>&#34;application/vnd.kubernetes.protobuf&#34;</span>,
    <span style=color:green;font-weight:700>&#34;dry_run&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;extensions&#34;</span>,
    <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;ingresses&#34;</span>,
    <span style=color:green;font-weight:700>&#34;scope&#34;</span>: <span style=color:#b44>&#34;cluster&#34;</span>,
    <span style=color:green;font-weight:700>&#34;subresource&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;LIST&#34;</span>,
    <span style=color:green;font-weight:700>&#34;version&#34;</span>: <span style=color:#b44>&#34;v1beta1&#34;</span>
  },
  <span style=color:green;font-weight:700>&#34;value&#34;</span>: <span style=color:#b44>&#34;1&#34;</span>
}
{
  <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;code&#34;</span>: <span style=color:#b44>&#34;200&#34;</span>,
    <span style=color:green;font-weight:700>&#34;component&#34;</span>: <span style=color:#b44>&#34;apiserver&#34;</span>,
    <span style=color:green;font-weight:700>&#34;contentType&#34;</span>: <span style=color:#b44>&#34;application/json&#34;</span>,
    <span style=color:green;font-weight:700>&#34;dry_run&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;rbac.authorization.k8s.io&#34;</span>,
    <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;clusterroles&#34;</span>,
    <span style=color:green;font-weight:700>&#34;scope&#34;</span>: <span style=color:#b44>&#34;cluster&#34;</span>,
    <span style=color:green;font-weight:700>&#34;subresource&#34;</span>: <span style=color:#b44>&#34;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;LIST&#34;</span>,
    <span style=color:green;font-weight:700>&#34;version&#34;</span>: <span style=color:#b44>&#34;v1beta1&#34;</span>
  },
  <span style=color:green;font-weight:700>&#34;value&#34;</span>: <span style=color:#b44>&#34;1&#34;</span>
}
</code></pre></div><p>The output shows that only read requests are being made to these APIs, and the most requests have been made to watch the deprecated Ingress API.</p><p>You can also find that information through the following Prometheus query,
which returns information about requests made to deprecated APIs which will be removed in v1.22:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-promql data-lang=promql><span style=color:#b8860b>apiserver_requested_deprecated_apis</span>{<span style=color:#a0a000>removed_version</span><span style=color:#666>=</span>&#34;<span style=color:#b44>1.22</span>&#34;}<span style=color:#bbb> </span><span style=color:#666>*</span><span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>on</span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>group</span>,<span style=color:#b8860b>version</span>,<span style=color:#b8860b>resource</span>,<span style=color:#b8860b>subresource</span><span style=color:#666>)</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#a2f;font-weight:700>group_right</span><span style=color:#666>()</span><span style=color:#bbb> </span><span style=color:#b8860b>apiserver_request_total</span><span style=color:#bbb>
</span></code></pre></div><h3 id=audit-annotations>Audit Annotations</h3><p>Metrics are a fast way to check whether deprecated APIs are being used, and at what rate,
but they don't include enough information to identify particular clients or API objects.
Starting in Kubernetes v1.19, <a href=/docs/tasks/debug-application-cluster/audit/>audit events</a>
for requests to deprecated APIs include an audit annotation of <code>"k8s.io/deprecated":"true"</code>.
Administrators can use those audit events to identify specific clients or objects that need to be updated.</p><h2 id=custom-resource-definitions>Custom Resource Definitions</h2><p>Along with the API server ability to warn about deprecated API use, starting in v1.19, a CustomResourceDefinition can indicate a
<a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-deprecation>particular version of the resource it defines is deprecated</a>.
When API requests to a deprecated version of a custom resource are made, a warning message is returned, matching the behavior of built-in APIs.</p><p>The author of the CustomResourceDefinition can also customize the warning for each version if they want to.
This allows them to give a pointer to a migration guide or other information if needed.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiextensions.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>CustomResourceDefinition<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>crontabs.example.com<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>versions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This indicates the v1alpha1 version of the custom resource is deprecated.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># API requests to this version receive a warning in the server response.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deprecated</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This overrides the default warning returned to clients making v1alpha1 API requests.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deprecationWarning</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example.com/v1alpha1 CronTab is deprecated; use example.com/v1 CronTab (see http://example.com/v1alpha1-v1)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>v1beta1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This indicates the v1beta1 version of the custom resource is deprecated.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># API requests to this version receive a warning in the server response.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># A default warning message is returned for this version.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deprecated</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span></code></pre></div><h2 id=admission-webhooks>Admission Webhooks</h2><p><a href=/docs/reference/access-authn-authz/extensible-admission-controllers>Admission webhooks</a>
are the primary way to integrate custom policies or validation with Kubernetes.
Starting in v1.19, admission webhooks can <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#response>return warning messages</a>
that are passed along to the requesting API client. Warnings can be returned with allowed or rejected admission responses.</p><p>As an example, to allow a request but warn about a configuration known not to work well, an admission webhook could send this response:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;admission.k8s.io/v1&#34;</span>,
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;AdmissionReview&#34;</span>,
  <span style=color:green;font-weight:700>&#34;response&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;uid&#34;</span>: <span style=color:#b44>&#34;&lt;value from request.uid&gt;&#34;</span>,
    <span style=color:green;font-weight:700>&#34;allowed&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
    <span style=color:green;font-weight:700>&#34;warnings&#34;</span>: [
      <span style=color:#b44>&#34;.spec.memory: requests &gt;1GB do not work on Fridays&#34;</span>
    ]
  }
}
</code></pre></div><p>If you are implementing a webhook that returns a warning message, here are some tips:</p><ul><li>Don't include a "Warning:" prefix in the message (that is added by clients on output)</li><li>Use warning messages to describe problems the client making the API request should correct or be aware of</li><li>Be brief; limit warnings to 120 characters if possible</li></ul><p>There are many ways admission webhooks could use this new feature, and I'm looking forward to seeing what people come up with.
Here are a couple ideas to get you started:</p><ul><li>webhook implementations adding a "complain" mode, where they return warnings instead of rejections,
to allow trying out a policy to verify it is working as expected before starting to enforce it</li><li>"lint" or "vet"-style webhooks, inspecting objects and surfacing warnings when best practices are not followed</li></ul><h2 id=customize-client-handling>Customize Client Handling</h2><p>Applications that use the <code>k8s.io/client-go</code> library to make API requests can customize
how warnings returned from the server are handled. By default, warnings are logged to
stderr as they are received, but this behavior can be customized
<a href=https://godoc.org/k8s.io/client-go/rest#SetDefaultWarningHandler>per-process</a>
or <a href=https://godoc.org/k8s.io/client-go/rest#Config>per-client</a>.</p><p>This example shows how to make your application behave like <code>kubectl</code>,
overriding message handling process-wide to deduplicate warnings
and highlighting messages using colored output where supported:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>import</span> (
  <span style=color:#b44>&#34;os&#34;</span>
  <span style=color:#b44>&#34;k8s.io/client-go/rest&#34;</span>
  <span style=color:#b44>&#34;k8s.io/kubectl/pkg/util/term&#34;</span>
  <span style=color:#666>...</span>
)

<span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>main</span>() {
  rest.<span style=color:#00a000>SetDefaultWarningHandler</span>(
    rest.<span style=color:#00a000>NewWarningWriter</span>(os.Stderr, rest.WarningWriterOptions{
        <span style=color:#080;font-style:italic>// only print a given warning the first time we receive it
</span><span style=color:#080;font-style:italic></span>        Deduplicate: <span style=color:#a2f;font-weight:700>true</span>,
        <span style=color:#080;font-style:italic>// highlight the output with color when the output supports it
</span><span style=color:#080;font-style:italic></span>        Color: term.<span style=color:#00a000>AllowsColorOutput</span>(os.Stderr),
      },
    ),
  )

  <span style=color:#666>...</span>
</code></pre></div><p>The next example shows how to construct a client that ignores warnings.
This is useful for clients that operate on metadata for all resource types
(found dynamically at runtime using the discovery API)
and do not benefit from warnings about a particular resource being deprecated.
Suppressing deprecation warnings is not recommended for clients that require use of particular APIs.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>import</span> (
  <span style=color:#b44>&#34;k8s.io/client-go/rest&#34;</span>
  <span style=color:#b44>&#34;k8s.io/client-go/kubernetes&#34;</span>
)

<span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>getClientWithoutWarnings</span>(config <span style=color:#666>*</span>rest.Config) (kubernetes.Interface, <span style=color:#0b0;font-weight:700>error</span>) {
  <span style=color:#080;font-style:italic>// copy to avoid mutating the passed-in config
</span><span style=color:#080;font-style:italic></span>  config = rest.<span style=color:#00a000>CopyConfig</span>(config)
  <span style=color:#080;font-style:italic>// set the warning handler for this client to ignore warnings
</span><span style=color:#080;font-style:italic></span>  config.WarningHandler = rest.NoWarnings{}
  <span style=color:#080;font-style:italic>// construct and return the client
</span><span style=color:#080;font-style:italic></span>  <span style=color:#a2f;font-weight:700>return</span> kubernetes.<span style=color:#00a000>NewForConfig</span>(config)
}
</code></pre></div><h2 id=kubectl-strict-mode>Kubectl Strict Mode</h2><p>If you want to be sure you notice deprecations as soon as possible and get a jump start on addressing them,
<code>kubectl</code> added a <code>--warnings-as-errors</code> option in v1.19. When invoked with this option,
<code>kubectl</code> treats any warnings it receives from the server as errors and exits with a non-zero exit code:</p><p><img alt="kubectl applying a manifest file with a --warnings-as-errors flag, displaying a warning message and exiting with a non-zero exit code." src=kubectl-warnings-as-errors.png style=width:637px;max-width:100%></p><p>This could be used in a CI job to apply manifests to a current server,
and required to pass with a zero exit code in order for the CI job to succeed.</p><h2 id=future-possibilities>Future Possibilities</h2><p>Now that we have a way to communicate helpful information to users in context,
we're already considering other ways we can use this to improve people's experience with Kubernetes.
A couple areas we're looking at next are warning about <a href=http://issue.k8s.io/64841#issuecomment-395141013>known problematic values</a>
we cannot reject outright for compatibility reasons, and warning about use of deprecated fields or field values
(like selectors using beta os/arch node labels, <a href=/docs/reference/kubernetes-api/labels-annotations-taints/#beta-kubernetes-io-arch-deprecated>deprecated in v1.14</a>).
I'm excited to see progress in this area, continuing to make it easier to use Kubernetes.</p><hr><p><em><a href=https://twitter.com/liggitt>Jordan Liggitt</a> is a software engineer at Google, and helps lead Kubernetes authentication, authorization, and API efforts.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c911fec01c1b20f202397599afbea2d2>Scaling Kubernetes Networking With EndpointSlices</h1><div class="td-byline mb-4"><time datetime=2020-09-02 class=text-muted>Wednesday, September 02, 2020</time></div><p><strong>Author:</strong> Rob Scott (Google)</p><p>EndpointSlices are an exciting new API that provides a scalable and extensible alternative to the Endpoints API. EndpointSlices track IP addresses, ports, readiness, and topology information for Pods backing a Service.</p><p>In Kubernetes 1.19 this feature is enabled by default with kube-proxy reading from <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a> instead of Endpoints. Although this will mostly be an invisible change, it should result in noticeable scalability improvements in large clusters. It also enables significant new features in future Kubernetes releases like <a href=/docs/concepts/services-networking/service-topology/>Topology Aware Routing</a>.</p><h2 id=scalability-limitations-of-the-endpoints-api>Scalability Limitations of the Endpoints API</h2><p>With the Endpoints API, there was only one Endpoints resource for a Service. That meant that it needed to be able to store IP addresses and ports (network endpoints) for every Pod that was backing the corresponding Service. This resulted in huge API resources. To compound this problem, kube-proxy was running on every node and watching for any updates to Endpoints resources. If even a single network endpoint changed in an Endpoints resource, the whole object would have to be sent to each of those instances of kube-proxy.</p><p>A further limitation of the Endpoints API is that it limits the number of network endpoints that can be tracked for a Service. The default size limit for an object stored in etcd is 1.5MB. In some cases that can limit an Endpoints resource to 5,000 Pod IPs. This is not an issue for most users, but it becomes a significant problem for users with Services approaching this size.</p><p>To show just how significant these issues become at scale it helps to have a simple example. Think about a Service which has 5,000 Pods, it might end up with a 1.5MB Endpoints resource. If even a single network endpoint in that list changes, the full Endpoints resource will need to be distributed to each Node in the cluster. This becomes quite an issue in a large cluster with 3,000 Nodes. Each update would involve sending 4.5GB of data (1.5MB Endpoints * 3,000 Nodes) across the cluster. That's nearly enough to fill up a DVD, and it would happen for each Endpoints change. Imagine a rolling update that results in all 5,000 Pods being replaced - that's more than 22TB (or 5,000 DVDs) worth of data transferred.</p><h2 id=splitting-endpoints-up-with-the-endpointslice-api>Splitting endpoints up with the EndpointSlice API</h2><p>The EndpointSlice API was designed to address this issue with an approach similar to sharding. Instead of tracking all Pod IPs for a Service with a single Endpoints resource, we split them into multiple smaller EndpointSlices.</p><p>Consider an example where a Service is backed by 15 pods. We'd end up with a single Endpoints resource that tracked all of them. If EndpointSlices were configured to store 5 endpoints each, we'd end up with 3 different EndpointSlices:
<img src=/images/blog/2020-09-02-scaling-kubernetes-networking-endpointslices/endpoint-slices.png alt=EndpointSlices></p><p>By default, EndpointSlices store as many as 100 endpoints each, though this can be configured with the <code>--max-endpoints-per-slice</code> flag on kube-controller-manager.</p><h2 id=endpointslices-provide-10x-scalability-improvements>EndpointSlices provide 10x scalability improvements</h2><p>This API dramatically improves networking scalability. Now when a Pod is added or removed, only 1 small EndpointSlice needs to be updated. This difference becomes quite noticeable when hundreds or thousands of Pods are backing a single Service.</p><p>Potentially more significant, now that all Pod IPs for a Service don't need to be stored in a single resource, we don't have to worry about the size limit for objects stored in etcd. EndpointSlices have already been used to scale Services beyond 100,000 network endpoints.</p><p>All of this is brought together with some significant performance improvements that have been made in kube-proxy. When using EndpointSlices at scale, significantly less data will be transferred for endpoints updates and kube-proxy should be faster to update iptables or ipvs rules. Beyond that, Services can now scale to at least 10 times beyond any previous limitations.</p><h2 id=endpointslices-enable-new-functionality>EndpointSlices enable new functionality</h2><p>Introduced as an alpha feature in Kubernetes v1.16, EndpointSlices were built to enable some exciting new functionality in future Kubernetes releases. This could include dual-stack Services, topology aware routing, and endpoint subsetting.</p><p>Dual-Stack Services are an exciting new feature that has been in development alongside EndpointSlices. They will utilize both IPv4 and IPv6 addresses for Services and rely on the addressType field on EndpointSlices to track these addresses by IP family.</p><p>Topology aware routing will update kube-proxy to prefer routing requests within the same zone or region. This makes use of the topology fields stored for each endpoint in an EndpointSlice. As a further refinement of that, we're exploring the potential of endpoint subsetting. This would allow kube-proxy to only watch a subset of EndpointSlices. For example, this might be combined with topology aware routing so that kube-proxy would only need to watch EndpointSlices containing endpoints within the same zone. This would provide another very significant scalability improvement.</p><h2 id=what-does-this-mean-for-the-endpoints-api>What does this mean for the Endpoints API?</h2><p>Although the EndpointSlice API is providing a newer and more scalable alternative to the Endpoints API, the Endpoints API will continue to be considered generally available and stable. The most significant change planned for the Endpoints API will involve beginning to truncate Endpoints that would otherwise run into scalability issues.</p><p>The Endpoints API is not going away, but many new features will rely on the EndpointSlice API. To take advantage of the new scalability and functionality that EndpointSlices provide, applications that currently consume Endpoints will likely want to consider supporting EndpointSlices in the future.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d8bbd8a56ada09e69d73bcf3909a2cb>Ephemeral volumes with storage capacity tracking: EmptyDir on steroids</h1><div class="td-byline mb-4"><time datetime=2020-09-01 class=text-muted>Tuesday, September 01, 2020</time></div><p><strong>Author:</strong> Patrick Ohly (Intel)</p><p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance. Other applications expect some read-only input
data to be present in files, like configuration data or secret keys.</p><p>Kubernetes already supports several kinds of such <a href=/docs/concepts/storage/ephemeral-volumes>ephemeral
volumes</a>, but the
functionality of those is limited to what is implemented inside
Kubernetes.</p><p><a href=https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/>CSI ephemeral volumes</a>
made it possible to extend Kubernetes with CSI
drivers that provide light-weight, local volumes. These <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md#motivation><em>inject
arbitrary states, such as configuration, secrets, identity, variables
or similar
information</em></a>.
CSI drivers must be modified to support this Kubernetes feature,
i.e. normal, standard-compliant CSI drivers will not work, and
by design such volumes are supposed to be usable on whatever node
is chosen for a pod.</p><p>This is problematic for volumes which consume significant resources on
a node or for special storage that is only available on some nodes.
Therefore, Kubernetes 1.19 introduces two new alpha features for
volumes that are conceptually more like the <code>EmptyDir</code> volumes:</p><ul><li><a href=/docs/concepts/storage/ephemeral-volumes#generic-ephemeral-volumes><em>generic</em> ephemeral volumes</a> and</li><li><a href=/docs/concepts/storage/storage-capacity>CSI storage capacity tracking</a>.</li></ul><p>The advantages of the new approach are:</p><ul><li>Storage can be local or network-attached.</li><li>Volumes can have a fixed size that applications are never able to exceed.</li><li>Works with any CSI driver that supports provisioning of persistent
volumes and (for capacity tracking) implements the CSI <code>GetCapacity</code> call.</li><li>Volumes may have some initial data, depending on the driver and
parameters.</li><li>All of the typical volume operations (snapshotting,
resizing, the future storage capacity tracking, etc.)
are supported.</li><li>The volumes are usable with any app controller that accepts
a Pod or volume specification.</li><li>The Kubernetes scheduler itself picks suitable nodes, i.e. there is
no need anymore to implement and configure scheduler extenders and
mutating webhooks.</li></ul><p>This makes generic ephemeral volumes a suitable solution for several
use cases:</p><h1 id=use-cases>Use cases</h1><h2 id=persistent-memory-as-dram-replacement-for-memcached>Persistent Memory as DRAM replacement for memcached</h2><p>Recent releases of memcached added <a href=https://memcached.org/blog/persistent-memory/>support for using Persistent
Memory</a> (PMEM) instead
of standard DRAM. When deploying memcached through one of the app
controllers, generic ephemeral volumes make it possible to request a PMEM volume
of a certain size from a CSI driver like
<a href=https://intel.github.io/pmem-csi/>PMEM-CSI</a>.</p><h2 id=local-lvm-storage-as-scratch-space>Local LVM storage as scratch space</h2><p>Applications working with data sets that exceed the RAM size can
request local storage with performance characteristics or size that is
not met by the normal Kubernetes <code>EmptyDir</code> volumes. For example,
<a href=https://github.com/cybozu-go/topolvm>TopoLVM</a> was written for that
purpose.</p><h2 id=read-only-access-to-volumes-with-data>Read-only access to volumes with data</h2><p>Provisioning a volume might result in a non-empty volume:</p><ul><li><a href=/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support>restore a snapshot</a></li><li><a href=/docs/concepts/storage/volume-pvc-datasource>cloning a volume</a></li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20200120-generic-data-populators.md>generic data populators</a></li></ul><p>Such volumes can be mounted read-only.</p><h1 id=how-it-works>How it works</h1><h2 id=generic-ephemeral-volumes>Generic ephemeral volumes</h2><p>The key idea behind generic ephemeral volumes is that a new volume
source, the so-called
<a href=/docs/reference/generated/kubernetes-api/#ephemeralvolumesource-v1alpha1-core><code>EphemeralVolumeSource</code></a>
contains all fields that are needed to created a volume claim
(historically called persistent volume claim, PVC). A new controller
in the <code>kube-controller-manager</code> waits for Pods which embed such a
volume source and then creates a PVC for that pod. To a CSI driver
deployment, that PVC looks like any other, so no special support is
needed.</p><p>As long as these PVCs exist, they can be used like any other volume claim. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.</p><p>Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (<code>-</code>) in the
middle. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known. The downside is that the name might
be in use already. This is detected by Kubernetes and then blocks Pod
startup.</p><p>To ensure that the volume gets deleted together with the pod, the
controller makes the Pod the owner of the volume claim. When the Pod
gets deleted, the normal garbage-collection mechanism also removes the
claim and thus the volume.</p><p>Claims select the storage driver through the normal storage class
mechanism. Although storage classes with both immediate and late
binding (aka <code>WaitForFirstConsumer</code>) are supported, for ephemeral
volumes it makes more sense to use <code>WaitForFirstConsumer</code>: then Pod
scheduling can take into account both node utilization and
availability of storage when choosing a node. This is where the other
new feature comes in.</p><h2 id=storage-capacity-tracking>Storage capacity tracking</h2><p>Normally, the Kubernetes scheduler has no information about where a
CSI driver might be able to create a volume. It also has no way of
talking directly to a CSI driver to retrieve that information. It
therefore tries different nodes until it finds one where all volumes
can be made available (late binding) or leaves it entirely to the
driver to choose a location (immediate binding).</p><p>The new <a href=/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io><code>CSIStorageCapacity</code> alpha
API</a>
allows storing the necessary information in etcd where it is available to the
scheduler. In contrast to support for generic ephemeral volumes,
storage capacity tracking must be <a href=https://github.com/kubernetes-csi/external-provisioner/blob/master/README.md#capacity-support>enabled when deploying a CSI
driver</a>:
the <code>external-provisioner</code> must be told to publish capacity
information that it then retrieves from the CSI driver through the normal
<code>GetCapacity</code> call.</p><p>When the Kubernetes scheduler needs to choose a node for a Pod with an
unbound volume that uses late binding and the CSI driver deployment
has opted into the feature by setting the <a href=/docs/reference/generated/kubernetes-api/v1.19/#csidriver-v1beta1-storage-k8s-io><code>CSIDriver.storageCapacity</code>
flag</a>
flag, the scheduler automatically filters out nodes that do not have
access to enough storage capacity. This works for generic ephemeral
and persistent volumes but <em>not</em> for CSI ephemeral volumes because the
parameters of those are opaque for Kubernetes.</p><p>As usual, volumes with immediate binding get created before scheduling
pods, with their location chosen by the storage driver. Therefore, the
external-provisioner's default configuration skips storage
classes with immediate binding as the information wouldn't be used anyway.</p><p>Because the Kubernetes scheduler must act on potentially outdated
information, it cannot be ensured that the capacity is still available
when a volume is to be created. Still, the chances that it can be created
without retries should be higher.</p><h1 id=security>Security</h1><h2 id=csistoragecapacity>CSIStorageCapacity</h2><p>CSIStorageCapacity objects are namespaced. When deploying each CSI
drivers in its own namespace and, as recommended, limiting the RBAC
permissions for CSIStorageCapacity to that namespace, it is
always obvious where the data came from. However, Kubernetes does
not check that and typically drivers get installed in the same
namespace anyway, so ultimately drivers are <em>expected to behave</em> and
not publish incorrect data.</p><h2 id=generic-ephemeral-volumes-1>Generic ephemeral volumes</h2><p>If users have permission to create a Pod (directly or indirectly),
then they can also create generic ephemeral volumes even when they do
not have permission to create a volume claim. That's because RBAC
permission checks are applied to the controller which creates the
PVC, not the original user. This is a fundamental change that must be
<a href=/docs/concepts/storage/ephemeral-volumes#security>taken into
account</a> before
enabling the feature in clusters where untrusted users are not
supposed to have permission to create volumes.</p><h1 id=example>Example</h1><p>A <a href=https://github.com/intel/pmem-csi/commits/kubernetes-1-19-blog-post>special branch</a>
in PMEM-CSI contains all the necessary changes to bring up a
Kubernetes 1.19 cluster inside QEMU VMs with both alpha features
enabled. The PMEM-CSI driver code is used unchanged, only the
deployment was updated.</p><p>On a suitable machine (Linux, non-root user can use Docker - see the
<a href=https://intel.github.io/pmem-csi/0.7/docs/autotest.html#qemu-and-kubernetes>QEMU and
Kubernetes</a>
section in the PMEM-CSI documentation), the following commands bring
up a cluster and install the PMEM-CSI driver:</p><pre><code class=language-console data-lang=console>git clone --branch=kubernetes-1-19-blog-post https://github.com/intel/pmem-csi.git
cd pmem-csi
export TEST_KUBERNETES_VERSION=1.19 TEST_FEATURE_GATES=CSIStorageCapacity=true,GenericEphemeralVolume=true TEST_PMEM_REGISTRY=intel
make start &amp;&amp; echo &amp;&amp; test/setup-deployment.sh
</code></pre><p>If all goes well, the output contains the following usage
instructions:</p><pre><code>The test cluster is ready. Log in with [...]/pmem-csi/_work/pmem-govm/ssh.0, run
kubectl once logged in.  Alternatively, use kubectl directly with the
following env variable:
   KUBECONFIG=[...]/pmem-csi/_work/pmem-govm/kube.config

secret/pmem-csi-registry-secrets created
secret/pmem-csi-node-secrets created
serviceaccount/pmem-csi-controller created
...
To try out the pmem-csi driver ephemeral volumes:
   cat deploy/kubernetes-1.19/pmem-app-ephemeral.yaml |
   [...]/pmem-csi/_work/pmem-govm/ssh.0 kubectl create -f -
</code></pre><p>The CSIStorageCapacity objects are not meant to be human-readable, so
some post-processing is needed. The following Golang template filters
all objects by the storage class that the example uses and prints the
name, topology and capacity:</p><pre><code class=language-console data-lang=console>kubectl get \
        -o go-template='{{range .items}}{{if eq .storageClassName &quot;pmem-csi-sc-late-binding&quot;}}{{.metadata.name}} {{.nodeTopology.matchLabels}} {{.capacity}}
{{end}}{{end}}' \
        csistoragecapacities
</code></pre><pre><code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 30716Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
</code></pre><p>One individual object has the following content:</p><pre><code class=language-console data-lang=console>kubectl describe csistoragecapacities/csisc-6cw8j
</code></pre><pre><code>Name:         csisc-sqdnt
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  storage.k8s.io/v1alpha1
Capacity:     30716Mi
Kind:         CSIStorageCapacity
Metadata:
  Creation Timestamp:  2020-08-11T15:41:03Z
  Generate Name:       csisc-
  Managed Fields:
    ...
  Owner References:
    API Version:     apps/v1
    Controller:      true
    Kind:            StatefulSet
    Name:            pmem-csi-controller
    UID:             590237f9-1eb4-4208-b37b-5f7eab4597d1
  Resource Version:  2994
  Self Link:         /apis/storage.k8s.io/v1alpha1/namespaces/default/csistoragecapacities/csisc-sqdnt
  UID:               da36215b-3b9d-404a-a4c7-3f1c3502ab13
Node Topology:
  Match Labels:
    pmem-csi.intel.com/node:  pmem-csi-pmem-govm-worker1
Storage Class Name:           pmem-csi-sc-late-binding
Events:                       &lt;none&gt;
</code></pre><p>Now let's create the example app with one generic ephemeral
volume. The <code>pmem-app-ephemeral.yaml</code> file contains:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># This example Pod definition demonstrates</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># how to use generic ephemeral inline volumes</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#080;font-style:italic># with a PMEM-CSI storage class.</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-app-inline-volume<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-frontend<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>intel/pmem-csi-driver-test:v0.7.14<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;100000&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/data&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-volume<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-volume<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ephemeral</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeClaimTemplate</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>4Gi<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>pmem-csi-sc-late-binding<span style=color:#bbb>
</span></code></pre></div><p>After creating that as shown in the usage instructions above, we have one additional Pod and PVC:</p><pre><code class=language-console data-lang=console>kubectl get pods/my-csi-app-inline-volume -o wide
</code></pre><pre><code>NAME                       READY   STATUS    RESTARTS   AGE     IP          NODE                         NOMINATED NODE   READINESS GATES
my-csi-app-inline-volume   1/1     Running   0          6m58s   10.36.0.2   pmem-csi-pmem-govm-worker1   &lt;none&gt;           &lt;none&gt;
</code></pre><pre><code class=language-console data-lang=console>kubectl get pvc/my-csi-app-inline-volume-my-csi-volume
</code></pre><pre><code>NAME                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE
my-csi-app-inline-volume-my-csi-volume   Bound    pvc-c11eb7ab-a4fa-46fe-b515-b366be908823   4Gi        RWO            pmem-csi-sc-late-binding   9m21s
</code></pre><p>That PVC is owned by the Pod:</p><pre><code class=language-console data-lang=console>kubectl get -o yaml pvc/my-csi-app-inline-volume-my-csi-volume
</code></pre><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: &quot;yes&quot;
    pv.kubernetes.io/bound-by-controller: &quot;yes&quot;
    volume.beta.kubernetes.io/storage-provisioner: pmem-csi.intel.com
    volume.kubernetes.io/selected-node: pmem-csi-pmem-govm-worker1
  creationTimestamp: &quot;2020-08-11T15:44:57Z&quot;
  finalizers:
  - kubernetes.io/pvc-protection
  managedFields:
    ...
  name: my-csi-app-inline-volume-my-csi-volume
  namespace: default
  ownerReferences:
  - apiVersion: v1
    blockOwnerDeletion: true
    controller: true
    kind: Pod
    name: my-csi-app-inline-volume
    uid: 75c925bf-ca8e-441a-ac67-f190b7a2265f
...
</code></pre><p>Eventually, the storage capacity information for <code>pmem-csi-pmem-govm-worker1</code> also gets updated:</p><pre><code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 26620Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
</code></pre><p>If another app needs more than 26620Mi, the Kubernetes
scheduler will not pick <code>pmem-csi-pmem-govm-worker1</code> anymore.</p><h1 id=next-steps>Next steps</h1><p>Both features are under development. Several open questions were
already raised during the alpha review process. The two enhancement
proposals document the work that will be needed for migration to beta and what
alternatives were already considered and rejected:</p><ul><li><a href=https://github.com/kubernetes/enhancements/blob/9d7a75d/keps/sig-storage/1698-generic-ephemeral-volumes/README.md>KEP-1698: generic ephemeral inline
volumes</a></li><li><a href=https://github.com/kubernetes/enhancements/tree/9d7a75d/keps/sig-storage/1472-storage-capacity-tracking>KEP-1472: Storage Capacity
Tracking</a></li></ul><p>Your feedback is crucial for driving that development. SIG-Storage
<a href=https://github.com/kubernetes/community/tree/master/sig-storage#meetings>meets
regularly</a>
and can be reached via <a href=https://github.com/kubernetes/community/tree/master/sig-storage#contact>Slack and a mailing
list</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1518ae59824ac977039974dd95569a66>Increasing the Kubernetes Support Window to One Year</h1><div class="td-byline mb-4"><time datetime=2020-08-31 class=text-muted>Monday, August 31, 2020</time></div><p><strong>Authors:</strong> Tim Pepper (VMware), Nick Young (VMware)</p><p>Starting with Kubernetes 1.19, the support window for Kubernetes versions <a href=https://github.com/kubernetes/enhancements/issues/1498>will increase from 9 months to one year</a>. The longer support window is intended to allow organizations to perform major upgrades at a time of the year that works the best for them.</p><p>This is a big change. For many years, the Kubernetes project has delivered a new minor release (e.g.: 1.13 or 1.14) every 3 months. The project provides bugfix support via patch releases (e.g.: 1.13.Y) for three parallel branches of the codebase. Combined, this led to each minor release (e.g.: 1.13) having a patch release stream of support for approximately 9 months. In the end, a cluster operator had to upgrade at least every 9 months to remain supported.</p><p>A survey conducted in early 2019 by the WG LTS showed that a significant subset of Kubernetes end-users fail to upgrade within the 9-month support period.</p><p><img src=/images/blog/2020-08-31-increase-kubernetes-support-one-year/versions-in-production-text-2.png alt="Versions in Production"></p><p>This, and other responses from the survey, suggest that a considerable portion of our community would better be able to manage their deployments on supported versions if the patch support period were extended to 12-14 months. It appears to be true regardless of whether the users are on DIY builds or commercially vendored distributions. An extension in the patch support length of time would thus lead to a larger percentage of our user base running supported versions compared to what we have now.</p><p>A yearly support period provides the cushion end-users appear to desire, and is more aligned with familiar annual planning cycles.
There are many unknowns about changing the support windows for a project with as many moving parts as Kubernetes. Keeping the change relatively small (relatively being the important word), gives us the chance to find out what those unknowns are in detail and address them.
From Kubernetes version 1.19 on, the support window will be extended to one year. For Kubernetes versions 1.16, 1.17, and 1.18, the story is more complicated.</p><p>All of these versions still fall under the older “three releases support” model, and will drop out of support when 1.19, 1.20 and 1.21 are respectively released. However, because the 1.19 release has been delayed due to the events of 2020, they will end up with close to a year of support (depending on their exact release dates).</p><p>For example, 1.19 was released on the 26th of August 2020, which is 11 months since the release of 1.16. Since 1.16 is still under the old release policy, this means that it is now out of support.</p><p><img src=/images/blog/2020-08-31-increase-kubernetes-support-one-year/support-timeline.png alt="Support Timeline"></p><p>If you’ve got thoughts or feedback, we’d love to hear them. Please contact us on <a href=https://kubernetes.slack.com/messages/wg-lts/>#wg-lts</a> on the Kubernetes Slack, or to the <a href=https://groups.google.com/g/kubernetes-wg-lts>kubernetes-wg-lts mailing list</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-217df5d2adc08627a1318be8af2d2186>Kubernetes 1.19: Accentuate the Paw-sitive</h1><div class="td-byline mb-4"><time datetime=2020-08-26 class=text-muted>Wednesday, August 26, 2020</time></div><p><strong>Authors:</strong> <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md>Kubernetes 1.19 Release Team</a></p><p>Finally, we have arrived with Kubernetes 1.19, the second release for 2020, and by far the longest release cycle lasting 20 weeks in total. It consists of 34 enhancements: 10 enhancements are moving to stable, 15 enhancements in beta, and 9 enhancements in alpha.</p><p>The 1.19 release was quite different from a regular release due to COVID-19, the George Floyd protests, and several other global events that we experienced as a release team. Due to these events, we made the decision to adjust our timeline and allow the SIGs, Working Groups, and contributors more time to get things done. The extra time also allowed for people to take time to focus on their lives outside of the Kubernetes project, and ensure their mental wellbeing was in a good place.</p><p>Contributors are the heart of Kubernetes, not the other way around. The Kubernetes code of conduct asks that people be excellent to one another and despite the unrest in our world, we saw nothing but greatness and humility from the community.</p><h2 id=major-themes>Major Themes</h2><h3 id=increase-kubernetes-support-window-to-one-year>Increase Kubernetes support window to one year</h3><p>A survey conducted in early 2019 by the <a href=https://github.com/kubernetes/community/tree/master/wg-lts#readme>Long Term Support (LTS) working group</a> showed that a significant subset of Kubernetes end-users fail to upgrade within the current 9-month support period.
This, and other responses from the survey, suggest that 30% of users would be able to keep their deployments on supported versions if the patch support period were extended to 12-14 months. This appears to be true regardless of whether the users are on self build or commercially vendored distributions. An extension would thus lead to more than 80% of users being on supported versions, instead of the 50-60% we have now.
A yearly support period provides the cushion end-users appear to desire, and is more in harmony with familiar annual planning cycles.
From Kubernetes version 1.19 on, the support window will be extended to one year.</p><h3 id=storage-capacity-tracking>Storage capacity tracking</h3><p>Traditionally, the Kubernetes scheduler was based on the assumptions that additional persistent storage is available everywhere in the cluster and has infinite capacity. Topology constraints addressed the first point, but up to now pod scheduling was still done without considering that the remaining storage capacity may not be enough to start a new pod. <a href=/docs/concepts/storage/storage-capacity/>Storage capacity tracking</a>, a new alpha feature, addresses that by adding an API for a CSI driver to report storage capacity and uses that information in the Kubernetes scheduler when choosing a node for a pod. This feature serves as a stepping stone for supporting dynamic provisioning for local volumes and other volume types that are more capacity constrained.</p><h4 id=generic-ephemeral-volumes>Generic ephemeral volumes</h4><p>Kubernetes provides volume plugins whose lifecycle is tied to a pod and can be used as scratch space (e.g. the builtin <code>emptydir</code> volume type) or to load some data in to a pod (e.g. the builtin <code>configmap</code> and <code>secret</code> volume types, or “CSI inline volumes”). The new <a href=/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes>generic ephemeral volumes</a> alpha feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume’s lifecycle bound to the Pod.
It can be used to provide scratch storage that is different from the root disk, for example persistent memory, or a separate local disk on that node.
All StorageClass parameters for volume provisioning are supported.
All features supported with PersistentVolumeClaims are supported, such as storage capacity tracking, snapshots and restore, and volume resizing.</p><h4 id=csi-volume-health-monitoring>CSI Volume Health Monitoring</h4><p>The alpha version of CSI health monitoring is being released with Kubernetes 1.19. This feature enables CSI Drivers to share abnormal volume conditions from the underlying storage systems with Kubernetes so that they can be reported as events on PVCs or Pods. This feature serves as a stepping stone towards programmatic detection and resolution of individual volume health issues by Kubernetes.</p><h3 id=ingress-graduates-to-general-availability>Ingress graduates to General Availability</h3><p>In terms of moving the Ingress API towards GA, the API itself has been available in beta for so long that it has attained de facto GA status through usage and adoption (both by users and by load balancer / ingress controller providers). Abandoning it without a full replacement is not a viable approach. It is clearly a useful API and captures a non-trivial set of use cases. At this point, it seems more prudent to declare the current API as something the community will support as a V1, codifying its status, while working on either a V2 Ingress API or an entirely different API with a superset of features.</p><h3 id=structured-logging>Structured logging</h3><p>Before v1.19, logging in the Kubernetes control plane couldn't guarantee any uniform structure for log messages and references to Kubernetes objects in those logs. This makes parsing, processing, storing, querying and analyzing logs hard and forces administrators and developers to rely on ad-hoc solutions in most cases based on some regular expressions. Due to those problems any analytical solution based on those logs is hard to implement and maintain.</p><h4 id=new-klog-methods>New klog methods</h4><p>This Kubernetes release introduces new methods to the <em>klog</em> library that provide a more structured interface for formatting log messages. Each existing formatted log method (<code>Infof</code>, <code>Errorf</code>) is now matched by a structured method (<code>InfoS</code>, <code>ErrorS</code>). The new logging methods accept log messages as a first argument and a list of key-values pairs as a variadic second argument. This approach allows incremental adoption of structured logging without converting <strong>all</strong> of Kubernetes to a new API at one time.</p><h3 id=client-tls-certificate-rotation-for-kubelet>Client TLS certificate rotation for kubelet</h3><p>A kubelet authenticates the kubelet to the kube-apiserver using a private key and certificate. The certificate is supplied to the kubelet when it is first booted, via an out-of-cluster mechanism. Since Kubernetes v1.8, clusters have included a (beta) process for obtaining the initial cert/key pair and rotating it as expiration of the certificate approaches. In Kubernetes v1.19 this graduates to stable.</p><p>During the kubelet start-up sequence, the filesystem is scanned for an existing cert/key pair, which is managed by the certificate manager. In the case that a cert/key is available it will be loaded. If not, the kubelet checks its config file for an encoded certificate value or a file reference in the kubeconfig. If the certificate is a bootstrap certificate, this will be used to generate a key, create a certificate signing request and request a signed certificate from the API server.</p><p>When an expiration approaches the cert manager takes care of providing the correct certificate, generating new private keys and requesting new certificates. With the kubelet requesting certificates be signed as part of its boot sequence, and on an ongoing basis, certificate signing requests from the kubelet need to be auto approved to make cluster administration manageable.</p><h2 id=other-updates>Other Updates</h2><h3 id=graduated-to-stable>Graduated to Stable</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/135>Seccomp</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/266>Kubelet client TLS certificate rotation</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/279>Limit node access to API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/383>Redesign Event API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1453>Graduate Ingress to V1</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1513>CertificateSigningRequest API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1547>Building Kubelet without Docker</a></li></ul><h3 id=major-changes>Major Changes</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/693>Node Topology Manager</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/752>New Endpoint API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1498>Increase Kubernetes support window to one year</a></li></ul><h3 id=other-notable-features>Other Notable Features</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/1451>Run multiple Scheduling Profiles</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1513>CertificateSigningRequest API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1412>Immutable Secrets and ConfigMaps</a></li></ul><h2 id=release-notes>Release Notes</h2><p>Check out the full details of the Kubernetes 1.19 release in our <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md>release notes</a>.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.19 is available for download on <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.19.0>GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a> or run local Kubernetes clusters using Docker container “nodes” with <a href=https://kind.sigs.k8s.io/>KinD</a> (Kubernetes in Docker). You can also easily install 1.19 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h2 id=release-team>Release Team</h2><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md>release team</a> led by Taylor Dolezal, Senior Developer Advocate at HashiCorp. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">49,000 individual contributors</a> to date and an active community of more than 3,000 people.</p><h2 id=release-logo>Release Logo</h2><p>All of you inspired this Kubernetes 1.19 release logo! This release was a bit more of a marathon and a testament to when the world is a wild place, we can come together and do unbelievable things.</p><p><img src=/images/blog/2020-08-26-kubernetes-1.19-release-announcement/accentuate.png alt="Kubernetes 1.19 Release Logo"></p><p>"Accentuate the Paw-sitive" was chosen as the release theme because it captures the positive outlook that the release team had, despite the state of the world. The characters pictured in the 1.19 logo represent everyone's personalities on our release team, from emo to peppy, and beyond!</p><p>About the designer: Hannabeth Lagerlof is a Visual Designer based in Los Angeles, California, and she has an extensive background in Environments and Graphic Design. Hannabeth creates art and user experiences that inspire connection. You can find Hannabeth on Twitter as @emanate_design.</p><h2 id=the-long-run>The Long Run</h2><p>The release was also different from the enhancements side of things. Traditionally, we have had 3-4 weeks between the call for enhancements and Enhancements Freeze, which ends the phase in which contributors can acknowledge whether a particular feature will be part of the cycle. This release cycle, being unique, we had five weeks for the same milestone. The extended duration gave the contributors more time to plan and decide about the graduation of their respective features.</p><p>The milestone until which contributors implement the features was extended from the usual five weeks to 7 weeks. Contributors were provided with 40% more time to work on their features, resulting in reduced fatigue and more to think through about the implementation. We also noticed a considerable reduction in last-minute hustles. There were also a lesser number of exception requests this cycle - 6 compared to 14 the previous release cycle.</p><h2 id=user-highlights>User Highlights</h2><ul><li>The CNCF grants Zalando, Europe’s leading online platform for fashion and lifestyle, the <a href=https://www.cncf.io/announcement/2020/08/20/cloud-native-computing-foundation-grants-zalando-the-top-end-user-award/>Top End User Award</a>. Zalando leverages numerous CNCF projects and open sourced multiple of their own development.</li></ul><h2 id=ecosystem-updates>Ecosystem Updates</h2><ul><li>The CNCF just concluded its very first Virtual KubeCon. All talks are <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/>on-demand</a> for anyone registered, it's not too late!</li><li>The <a href=https://www.cncf.io/blog/2020/07/15/certified-kubernetes-security-specialist-cks-coming-in-november/>Certified Kubernetes Security Specialist</a> (CKS) coming in November! CKS focuses on cluster & system hardening, minimizing microservice vulnerabilities and the security of the supply chain.</li><li>CNCF published the second <a href=https://www.cncf.io/blog/2020/08/14/state-of-cloud-native-development/>State of Cloud Native Development</a>, showing the massively growing number of cloud native developer using container and serverless technology.</li><li><a href=https://www.kubernetes.dev>Kubernetes.dev</a>, a Kubernetes contributor focused website has been launched. It brings the contributor documentation, resources and project event information into one central location.</li></ul><h2 id=project-velocity>Project Velocity</h2><p>The <a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">Kubernetes DevStats dashboard</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. If you want to gather numbers, facts and figures from Kubernetes and the CNCF community it is the best place to start.</p><p>During this release cycle from April till August, 382 different companies and over 2,464 individuals contributed to Kubernetes. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All&from=1585692000000&to=1598392799000">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=upcoming-release-webinar>Upcoming release webinar</h2><p>Join the members of the Kubernetes 1.19 release team on September 25th, 2020 to learn about the major features in this release including storage capacity tracking, structured logging, Ingress V1 GA, and many more. Register here: <a href=https://www.cncf.io/webinars/kubernetes-1-19/>https://www.cncf.io/webinars/kubernetes-1-19/</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our monthly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p><ul><li>Find out more about contributing to Kubernetes at the new <a href=https://www.kubernetes.dev/>Kubernetes Contributor website</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li><li>Read more about what’s happening with Kubernetes on the <a href=https://kubernetes.io/blog/>blog</a></li><li>Learn more about the <a href=https://github.com/kubernetes/sig-release/tree/master/release-team>Kubernetes Release Team</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dc8f1ff78aa5c93bd94abaefc83b6e8d>Moving Forward From Beta</h1><div class="td-byline mb-4"><time datetime=2020-08-21 class=text-muted>Friday, August 21, 2020</time></div><p><strong>Author</strong>: Tim Bannister, The Scale Factory</p><p>In Kubernetes, features follow a defined
<a href=/docs/reference/command-line-tools-reference/feature-gates/#feature-stages>lifecycle</a>.
First, as the twinkle of an eye in an interested developer. Maybe, then,
sketched in online discussions, drawn on the online equivalent of a cafe
napkin. This rough work typically becomes a
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#kubernetes-enhancement-proposal-process>Kubernetes Enhancement Proposal</a> (KEP), and
from there it usually turns into code.</p><p>For Kubernetes v1.20 and onwards, we're focusing on helping that code
graduate into stable features.</p><p>That lifecycle I mentioned runs as follows:</p><p><img src=feature_stages.svg alt="Alpha → Beta → General Availability"></p><p>Usually, alpha features aren't enabled by default. You turn them on by setting a feature
gate; usually, by setting a command line flag on each of the components that use the
feature.</p><p>(If you use Kubernetes through a managed service offering such as AKS, EKS, GKE, etc then
the vendor who runs that service may have decided what feature gates are enabled for you).</p><p>There's a defined process for graduating an existing, alpha feature into the beta phase.
This is important because <strong>beta features are enabled by default</strong>, with the feature flag still
there so cluster operators can opt out if they want.</p><p>A similar but more thorough set of graduation criteria govern the transition to general
availability (GA), also known as "stable". GA features are part of Kubernetes, with a
commitment that they are staying in place throughout the current major version.</p><p>Having beta features on by default lets Kubernetes and its contributors get valuable
real-world feedback. However, there's a mismatch of incentives. Once a feature is enabled
by default, people will use it. Even if there might be a few details to shake out,
the way Kubernetes' REST APIs and conventions work mean that any future stable API is going
to be compatible with the most recent beta API: your API objects won't stop working when
a beta feature graduates to GA.</p><p>For the API and its resources in particular, there's a much less strong incentive to move
features from beta to GA than from alpha to beta. Vendors who want a particular feature
have had good reason to help get code to the point where features are enabled by default,
and beyond that the journey has been less clear.</p><p>KEPs track more than code improvements. Essentially, anything that would need
communicating to the wider community merits a KEP. That said, most KEPs cover
Kubernetes features (and the code to implement them).</p><p>You might know that <a href=/docs/concepts/services-networking/ingress/>Ingress</a>
has been in Kubernetes for a while, but did you realize that it actually went beta in 2015? To help
drive things forward, Kubernetes' Architecture Special Interest Group (SIG) have a new approach in
mind.</p><h2 id=avoiding-permanent-beta>Avoiding permanent beta</h2><p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts a countdown.
The beta-quality API now has <strong>three releases</strong> (about nine calendar months) to either:</p><ul><li>reach GA, and deprecate the beta, or</li><li>have a new beta version (<em>and deprecate the previous beta</em>).</li></ul><p>To be clear, at this point <strong>only REST APIs are affected</strong>. For example, <em>APIListChunking</em> is
a beta feature but isn't itself a REST API. Right now there are no plans to automatically
deprecate <em>APIListChunking</em> nor any other features that aren't REST APIs.</p><p>If a beta API has not graduated to GA after three Kubernetes releases, then the
next Kubernetes release will deprecate that API version. There's no option for
the REST API to stay at the same beta version beyond the first Kubernetes
release to come out after the release window.</p><h3 id=what-this-means-for-you>What this means for you</h3><p>If you're using Kubernetes, there's a good chance that you're using a beta feature. Like
I said, there are lots of them about.
As well as Ingress, you might be using <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJob</a>,
or <a href=/docs/concepts/policy/pod-security-policy/>PodSecurityPolicy</a>, or others.
There's an even bigger chance that you're running on a control plane with at least one beta
feature enabled.</p><p>If you're using or generating Kubernetes manifests that use beta APIs like Ingress, you'll
need to plan to revise those. The current APIs are going to be deprecated following a
schedule (the 9 months I mentioned earlier) and after a further 9 months those deprecated
APIs will be removed. At that point, to stay current with Kubernetes, you should already
have migrated.</p><h3 id=what-this-means-for-kubernetes-contributors>What this means for Kubernetes contributors</h3><p>The motivation here seems pretty clear: get features stable. Guaranteeing that beta
features will be deprecated adds a pretty big incentive so that people who want the
feature continue their effort until the code, documentation and tests are ready for this
feature to graduate to stable, backed by several Kubernetes' releases of evidence in
real-world use.</p><h3 id=what-this-means-for-the-ecosystem>What this means for the ecosystem</h3><p>In my opinion, these harsh-seeming measures make a lot of sense, and are going to be
good for Kubernetes. Deprecating existing APIs, through a rule that applies across all
the different Special Interest Groups (SIGs), helps avoid stagnation and encourages
fixes.</p><p>Let's say that an API goes to beta and then real-world experience shows that it
just isn't right - that, fundamentally, the API has shortcomings. With that 9 month
countdown ticking, the people involved have the means and the justification to revise
and release an API that deals with the problem cases. Anyone who wants to live with
the deprecated API is welcome to - Kubernetes is open source - but their needs do not
have to hold up progress on the feature.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4f3d630442b7114ebb79e901c3987452>Introducing Hierarchical Namespaces</h1><div class="td-byline mb-4"><time datetime=2020-08-14 class=text-muted>Friday, August 14, 2020</time></div><p><strong>Author</strong>: Adrian Ludwin (Google)</p><p>Safely hosting large numbers of users on a single Kubernetes cluster has always
been a troublesome task. One key reason for this is that different organizations
use Kubernetes in different ways, and so no one tenancy model is likely to suit
everyone. Instead, Kubernetes offers you building blocks to create your own
tenancy solution, such as Role Based Access Control (RBAC) and NetworkPolicies;
the better these building blocks, the easier it is to safely build a multitenant
cluster.</p><h1 id=namespaces-for-tenancy>Namespaces for tenancy</h1><p>By far the most important of these building blocks is the namespace, which forms
the backbone of almost all Kubernetes control plane security and sharing
policies. For example, RBAC, NetworkPolicies and ResourceQuotas all respect
namespaces by default, and objects such as Secrets, ServiceAccounts and
Ingresses are freely usable <em>within</em> any one namespace, but fully segregated
from <em>other</em> namespaces.</p><p>Namespaces have two key properties that make them ideal for policy enforcement.
Firstly, they can be used to <strong>represent ownership</strong>. Most Kubernetes objects
<em>must</em> be in a namespace, so if you use namespaces to represent ownership, you
can always count on there being an owner.</p><p>Secondly, namespaces have <strong>authorized creation and use</strong>. Only
highly-privileged users can create namespaces, and other users require explicit
permission to use those namespaces - that is, create, view or modify objects in
those namespaces. This allows them to be carefully created with appropriate
policies, before unprivileged users can create “regular” objects like pods and
services.</p><h1 id=the-limits-of-namespaces>The limits of namespaces</h1><p>However, in practice, namespaces are not flexible enough to meet some common use
cases. For example, let’s say that one team owns several microservices with
different secrets and quotas. Ideally, they should place these services into
different namespaces in order to isolate them from each other, but this presents
two problems.</p><p>Firstly, these namespaces have no common concept of ownership, even though
they’re both owned by the same team. This means that if the team controls
multiple namespaces, not only does Kubernetes not have any record of their
common owner, but namespaced-scoped policies cannot be applied uniformly across
them.</p><p>Secondly, teams generally work best if they can operate autonomously, but since
namespace creation is highly privileged, it’s unlikely that any member of the
dev team is allowed to create namespaces. This means that whenever a team wants
a new namespace, they must raise a ticket to the cluster administrator. While
this is probably acceptable for small organizations, it generates unnecessary
toil as the organization grows.</p><h1 id=introducing-hierarchical-namespaces>Introducing hierarchical namespaces</h1><p><a href=https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic>Hierarchical
namespaces</a>
are a new concept developed by the <a href=https://github.com/kubernetes-sigs/multi-tenancy>Kubernetes Working Group for Multi-Tenancy
(wg-multitenancy)</a> in order to
solve these problems. In its simplest form, a hierarchical namespace is a
regular Kubernetes namespace that contains a small custom resource that
identifies a single, optional, parent namespace. This establishes the concept of
ownership <em>across</em> namespaces, not just <em>within</em> them.</p><p>This concept of ownership enables two additional types of behaviours:</p><ul><li><strong>Policy inheritance:</strong> if one namespace is a child of another, policy objects
such as RBAC RoleBindings are <a href=https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-propagation>copied from the parent to the
child</a>.</li><li><strong>Delegated creation:</strong> you usually need cluster-level privileges to create a
namespace, but hierarchical namespaces adds an alternative:
<a href=https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-subns><em>subnamespaces</em></a>,
which can be manipulated using only limited permissions in the parent
namespace.</li></ul><p>This solves both of the problems for our dev team. The cluster administrator can
create a single “root” namespace for the team, along with all necessary
policies, and then delegate permission to create subnamespaces to members of
that team. Those team members can then create subnamespaces for their own use,
without violating the policies that were imposed by the cluster administrators.</p><h1 id=hands-on-with-hierarchical-namespaces>Hands-on with hierarchical namespaces</h1><p>Hierarchical namespaces are provided by a Kubernetes extension known as the
<a href=https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc><strong>Hierarchical Namespace
Controller</strong></a>,
or <strong>HNC</strong>. The HNC consists of two components:</p><ul><li>The <strong>manager</strong> runs on your cluster, manages subnamespaces, propagates policy
objects, ensures that your hierarchies are legal and manages extension points.</li><li>The <strong>kubectl plugin</strong>, called <code>kubectl-hns</code>, makes it easy for users to
interact with the manager.</li></ul><p>Both can be easily installed from the <a href=https://github.com/kubernetes-sigs/multi-tenancy/releases>releases page of our
repo</a>.</p><p>Let’s see HNC in action. Imagine that I do not have namespace creation
privileges, but I can view the namespace <code>team-a</code> and create subnamespaces
within it<sup><a href=#note-1>1</a></sup>. Using the plugin, I can now say:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl hns create svc1-team-a -n team-a
</code></pre></div><p>This creates a subnamespace called <code>svc1-team-a</code>. Note that since subnamespaces
are just regular Kubernetes namespaces, all subnamespace names must still be
unique.</p><p>I can view the structure of these namespaces by asking for a tree view:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl hns tree team-a
<span style=color:#080;font-style:italic># Output:</span>
team-a
└── svc1-team-a
</code></pre></div><p>And if there were any policies in the parent namespace, these now appear in the
child as well<sup><a href=#note-2>2</a></sup>. For example, let’s say that <code>team-a</code> had
an RBAC RoleBinding called <code>sres</code>. This rolebinding will also be present in the
subnamespace:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl describe rolebinding sres -n svc1-team-a
<span style=color:#080;font-style:italic># Output:</span>
Name:         sres
Labels:       hnc.x-k8s.io/inheritedFrom<span style=color:#666>=</span>team-a  <span style=color:#080;font-style:italic># inserted by HNC</span>
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects: ...
</code></pre></div><p>Finally, HNC adds labels to these namespaces with useful information about the
hierarchy which you can use to apply other policies. For example, you can create
the following NetworkPolicy:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-team-a<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>team-a<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;team-a.tree.hnc.x-k8s.io/depth&#39;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># Label created by HNC</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></code></pre></div><p>This policy will both be propagated to all descendants of <code>team-a</code>, and will
<em>also</em> allow ingress traffic between all of those namespaces. The “tree” label
can only be applied by HNC, and is guaranteed to reflect the latest hierarchy.</p><p>You can learn all about the features of HNC from the <a href=https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc/docs/user-guide>user
guide</a>.</p><h1 id=next-steps-and-getting-involved>Next steps and getting involved</h1><p>If you think that hierarchical namespaces can work for your organization, <a href=https://github.com/kubernetes-sigs/multi-tenancy/releases/tag/hnc-v0.5.1>HNC
v0.5.1 is available on
GitHub</a>.
We’d love to know what you think of it, what problems you’re using it to solve
and what features you’d most like to see added. As with all early software, you
should be cautious about using HNC in production environments, but the more
feedback we get, the sooner we’ll be able to drive to HNC 1.0.</p><p>We’re also open to additional contributors, whether it’s to fix or report bugs,
or help prototype new features such as exceptions, improved monitoring,
hierarchical resource quotas or fine-grained configuration.</p><p>Please get in touch with us via our
<a href=https://github.com/kubernetes-sigs/multi-tenancy>repo</a>, <a href=https://groups.google.com/g/kubernetes-wg-multitenancy>mailing
list</a> or on
<a href=https://kubernetes.slack.com/messages/wg-multitenancy>Slack</a> - we look forward
to hearing from you!</p><hr><p><em><a href=https://twitter.com/aludwin>Adrian Ludwin</a> is a software engineer and the
tech lead for the Hierarchical Namespace Controller.</em></p><a name=note-1><p><em>Note 1: technically, you create a small object called a "subnamespace anchor"
in the parent namespace, and then HNC creates the subnamespace for you.</em></p><a name=note-2><p><em>Note 2: By default, only RBAC Roles and RoleBindings are propagated, but you
can configure HNC to propagate any namespaced Kubernetes object.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9a1d876453f4173bc6e8e98926903aa9>Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</h1><div class="td-byline mb-4"><time datetime=2020-08-03 class=text-muted>Monday, August 03, 2020</time></div><p><strong>Author</strong>: Craig Box (Google)</p><p>The start of the COVID-19 pandemic couldn't delay the release of Kubernetes 1.18, but unfortunately <a href=https://github.com/kubernetes/utils/issues/141>a small bug</a> could — thankfully only by a day. This was the last cat that needed to be herded by 1.18 release lead <a href=https://twitter.com/alejandrox135>Jorge Alarcón</a> before the <a href=https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/>release on March 25</a>.</p><p>One of the best parts about co-hosting the weekly <a href=https://kubernetespodcast.com/>Kubernetes Podcast from Google</a> is the conversations we have with the people who help bring Kubernetes releases together. <a href=https://kubernetespodcast.com/episode/096-kubernetes-1.18/>Jorge was our guest on episode 96</a> back in March, and <a href=https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/>just like last week</a> we are delighted to bring you the transcript of this interview.</p><p>If you'd rather enjoy the "audiobook version", including another interview when 1.19 is released later this month, <a href=https://kubernetespodcast.com/subscribe/>subscribe to the show</a> wherever you get your podcasts.</p><p>In the last few weeks, we've talked to long-time Kubernetes contributors and SIG leads <a href=https://kubernetespodcast.com/episode/114-scheduling/>David Oppenheimer</a>, <a href=https://kubernetespodcast.com/episode/113-instrumentation-and-cadvisor/>David Ashpole</a> and <a href=https://kubernetespodcast.com/episode/111-scalability/>Wojciech Tyczynski</a>. All are worth taking the dog for a longer walk to listen to!</p><hr><p><strong>ADAM GLICK: You're a former physicist. I have to ask, what kind of physics did you work on?</strong></p><p>JORGE ALARCÓN: Back in my days of math and all that, I used to work in <a href=https://en.wikipedia.org/wiki/Computational_biology>computational biology</a> and a little bit of high energy physics. Computational biology was, for the most part, what I spent most of my time on. And it was essentially exploring the big idea of we have the structure of proteins. We know what they're made of. Now, based on that structure, we want to be able to predict <a href=https://en.wikipedia.org/wiki/Protein_folding>how they're going to fold</a> and how they're going to behave, which essentially translates into the whole idea of designing pharmaceuticals, designing vaccines, or anything that you can possibly think of that has any connection whatsoever to a living organism.</p><p><strong>ADAM GLICK: That would seem to ladder itself well into maybe going to something like bioinformatics. Did you take a tour into that, or did you decide to go elsewhere directly?</strong></p><p>JORGE ALARCÓN: It is related, and I worked a little bit with some people that did focus on bioinformatics on the field specifically, but I never took a detour into it. Really, my big idea with computational biology, to be honest, it wasn't even the biology. That's usually what sells it, what people are really interested in, because protein engineering, all the cool and amazing things that you can do.</p><p>Which is definitely good, and I don't want to take away from it. But my big thing is because biology is such a real thing, it is amazingly complicated. And the math— the models that you have to design to study those systems, to be able to predict something that people can actually experiment and measure, it just captivated me. The level of complexity, the beauty, the mechanisms, all the structures that you see once you got through the math and look at things, it just kind of got to me.</p><p><strong>ADAM GLICK: How did you go from that world into the world of Kubernetes?</strong></p><p>JORGE ALARCÓN: That's both a really boring story and an interesting one.</p><p>[LAUGHING]</p><p>I did my thing with physics, and it was good. It was fun. But at some point, I wanted— working in academia— at least my feeling for it is that generally all the people that you're surrounded with are usually academics. Just another bunch of physics, a bunch of mathematicians.</p><p>But very seldom do you actually get the opportunity to take what you're working on and give it to someone else to use. Even with the mathematicians and physicists, the things that we're working on are super specialized, and you can probably find three, four, five people that can actually understand everything that you're saying. A lot of people are going to get the gist of it, but understanding the details, it's somewhat rare.</p><p>One of the things that I absolutely love about tech, about software engineering, coding, all that, is how open and transparent everything is. You can write your library in Python, you can publish it, and suddenly the world is going to actually use it, actually consume it. And because normally, I've seen that it has a large avenue where you can work in something really complicated, you can communicate it, and people can actually go ahead and take it and run with it in their given direction. And that is kind of what happened.</p><p>At some point, by pure accident and chance, I came across this group of people on the internet, and they were in the stages of making up this new group that's called <a href=https://datafordemocracy.org/>Data for Democracy</a>, a non-profit. And the whole idea was the internet, especially Twitter— that's how we congregated— Twitter, the internet. We have a ton of data scientists, people who work as software engineers, and the like. What if we all come together and try to solve some issues that actually affect the daily lives of people. And there were a ton of projects. Helping the ACLU gather data for something interesting that they were doing, gather data and analyze it for local governments— where do you have potholes, how much water is being consumed.</p><p>Try to apply all the science that we knew, combined with all the code that we could write, and offer a good and digestible idea for people to say, OK, this makes sense, let's do something about it— policy, action, whatever. And I started working with this group, Data for Democracy— wonderful set of people. And the person who I believe we can blame for Data for Democracy— the one who got the idea and got it up and running, his name is Jonathan Morgan. And eventually, we got to work together. He started a startup, and I went to work with the startup. And that was essentially the thing that took me away from physics and into the world of software engineering— Data for Democracy, definitely.</p><p><strong>ADAM GLICK: Were you using Kubernetes as part of that work there?</strong></p><p>JORGE ALARCÓN: No, it was simple as it gets. You just try to get some data. You create a couple <a href=https://ipython.org/>IPython notebooks</a>, some setting up of really simple MySQL databases, and that was it.</p><p><strong>ADAM GLICK: Where did you get started using Kubernetes? And was it before you started contributing to it and being a part, or did you decide to jump right in?</strong></p><p>JORGE ALARCÓN: When I first started using Kubernetes, it was also on my first job. So there wasn't a lot of specific training in regards to software engineering or anything of the sort that I did before I actually started working as a software engineer. I just went from physicist to engineer. And in my days of physics, at least on the computer side, I was completely trained in the super old school system administrator, where you have your 10, 20 computers. You know physically where they are, and you have to connect the cables.</p><p><strong>ADAM GLICK: All pets— all pets all the time.</strong></p><p>JORGE ALARCÓN: [LAUGHING] You have to have your huge Python, bash scripts, three, five major versions, all because doing an upgrade will break something really important and you have no idea how to work on it. And that was my training. That was the way that I learned how to do things. Those were the kind of things that I knew how to do.</p><p>And when I got to this company— startup— we were pretty much starting from scratch. We were building a couple applications. We work testing them, we were deploying them on a couple of managed instances. But like everything, there was a lot of toil that we wanted to automate. The whole issue of, OK, after days of work, we finally managed to get this version of the application up and running in these machines.</p><p>It's open to the internet. People can test it out. But it turns out that it is now two weeks behind the latest on all the master branches for this repo, so now we want to update. And we have to go through the process of bringing it back up, creating new machines, do that whole thing. And I had no idea what Kubernetes was, to be honest. My boss at the moment mentioned it to me like, hey, we should use Kubernetes because apparently, Kubernetes is something that might be able to help us here. And we did some— I want to call it research and development.</p><p>It was actually just making— again, startup, small company, small team, so really me just playing around with Kubernetes trying to get it to work, trying to get it to run. I was so lost. I had no idea what I was doing— not enough. I didn't have an idea of how Kubernetes was supposed to help me. And at that point, I did the best Googling that I could manage. Didn't really find a lot of examples. Didn't find a lot of blog posts. It was early.</p><p><strong>ADAM GLICK: What time frame was this?</strong></p><p>JORGE ALARCÓN: Three, four years ago, so definitely not 1.13. That's the best guesstimate that I can give at this point. But I wasn't able to find any good examples, any tutorials. The only book that I was able to get my hands on was the one written by Joe Beda, Kelsey Hightower, and I forget the other author. But what is it? "<a href=%5D(http://shop.oreilly.com/product/0636920223788.do)>Kubernetes— Up and Running</a>"?</p><p>And in general, right now I use it as reference— it's really good. But as a beginner, I still was lost. They give all these amazing examples, they provide the applications, but I had no idea why someone might need a Pod, why someone might need a Deployment. So my last resort was to try and find someone who actually knew Kubernetes.</p><p>By accident, during my eternal Googling, I actually found a link to the <a href=http://slack.kubernetes.io/>Kubernetes Slack</a>. I jumped into the Kubernetes Slack hoping that someone might be able to help me out. And that was my entry point into the Kubernetes community. I just kept on exploring the Slack, tried to see what people were talking about, what they were asking to try to make sense of it, and just kept on iterating. And at some point, I think I got the hang of it.</p><p><strong>ADAM GLICK: What made you decide to be a release lead?</strong></p><p>JORGE ALARCÓN: The answer to this is my answer to why I have been contributing to Kubernetes. I really just want to be able to help out the community. Kubernetes is something that I absolutely adore.</p><p>Comparing Kubernetes to old school system administration, a handful of years ago, it took me like a week to create a node for an application to run. It took me months to get something that vaguely looked like an Ingress resource— just setting up the Nginx, and allowing someone else to actually use my application. And the fact that I could do all of that in five minutes, it really captivated me. Plus I've got to blame it on the physics. The whole idea with physics, I really like the patterns, and I really like the design of Kubernetes.</p><p>Once I actually got the hang of it, I loved the idea of how everything was designed, and I just wanted to learn a lot more about it. And I wanted to help the contributors. I wanted to help the people who actually build it. I wanted to help maintain it, and help provide the information for new contributors or new users. So instead of taking months for them to be up and running, let's just chat about what your issue is, and let's try to get a fix within the next hour or so.</p><p><strong>ADAM GLICK: You work for a stealth startup right now. Is it fair to assume that they're using Kubernetes?</strong></p><p>JORGE ALARCÓN: Yes—</p><p>[LAUGHING]</p><p>—for everything.</p><p><strong>ADAM GLICK: Are you able to say what <a href=https://www.searchable.ai/>Searchable</a> does?</strong></p><p>JORGE ALARCÓN: The thing that we are trying to build is kind of like a search engine for your documents. Usually, if people have a question, they jump on Google. And for the most part, you're going to be able to get a good answer. You can ask something really random, like 'what is the weight of an elephant?'</p><p>Which, if you think about it, it's kind of random, but Google is going to give you an answer. And the thing that we are trying to build is something similar to that, but for files. So essentially, a search engine for your files. And most people, you have your local machine loaded up with— at least mine, I have a couple tens of gigabytes of different files.</p><p>I have Google Drive. I have a lot of documents that live in my email and the like. So the idea is to kind of build a search engine that is going to be able to connect all of those pieces. And besides doing simple word searches— for example, 'Kubernetes interview', and bring me the documents that we're looking at with all the questions— I can also ask things like what issue did I find last week while testing Prometheus. And it's going to be able to read my files, like through natural language processing, understand it, and be able to give me an answer.</p><p><strong>ADAM GLICK: It is a Google for your personal and non-public information, essentially?</strong></p><p>JORGE ALARCÓN: Hopefully.</p><p><strong>ADAM GLICK: Is the work that you do with Kubernetes as the release lead— is that part of your day job, or is that something that you're doing kind of nights and weekends separate from your day job?</strong></p><p>JORGE ALARCÓN: Both. Strictly speaking, my day job is just keep working on the application, build the things that it needs, maintain the infrastructure, and all that. When I started working at the company— which by the way, the person who brought me into the company was also someone that I met from my days in Data for Democracy— we started talking about the work.</p><p>I mentioned that I do a lot of work with the Kubernetes community and if it was OK that I continue doing it. And to my surprise, the answer was not only a yes, but yeah, you can do it during your day work. And at least for the time being, I just balance— I try to keep things organized.</p><p>Some days I just focus on Kubernetes. Some mornings I do Kubernetes. And then afternoon, I do Searchable, vice-versa, or just go back and forth, and try to balance the work as much as possible. But being release lead, definitely, it is a lot, so nights and weekends.</p><p><strong>ADAM GLICK: How much time does it take to be the release lead?</strong></p><p>JORGE ALARCÓN: It varies, but probably, if I had to give an estimate, at the very least you have to be able to dedicate four hours most days.</p><p><strong>ADAM GLICK: Four hours a day?</strong></p><p>JORGE ALARCÓN: Yeah, most days. It varies a lot. For example, at the beginning of the release cycle, you don't need to put in that much work because essentially, you're just waiting and helping people get set up, and people are writing their <a href=https://github.com/kubernetes/enhancements/tree/master/keps>Kubernetes Enhancement Proposals</a>, they are implementing it, and you can answer some questions. It's relatively easy, but for the most part, a lot of the time the four hours go into talking with people, just making sure that, hey, are people actually writing their enhancements, do we have all the enhancements that we want. And most of those fours hours, going around, chatting with people, and making sure that things are being done. And if, for some reason, someone needs help, just directing them to the right place to get their answer.</p><p><strong>ADAM GLICK: What does Searchable get out of you doing this work?</strong></p><p>JORGE ALARCÓN: Physically, nothing. The thing that we're striving for is to give back to the community. My manager/boss/homeslice— I told him I was going to call him my homeslice— both of us have experience working in open source. At some point, he was also working on a project that I'm probably going to mispronounce, but Mahout with Apache.</p><p>And he also has had this experience. And both of us have this general idea and strive to build something for Searchable that's going to be useful for people, but also build knowledge, build guides, build applications that are going to be useful for the community. And at least one of the things that I was able to do right now is be the lead for the Kubernetes team. And this is a way of giving back to the community. We're using Kubernetes to run our things, so let's try to balance how things work.</p><p><strong>ADAM GLICK: Lachlan Evenson was the release lead on 1.16 as well as <a href=https://kubernetespodcast.com/episode/072-kubernetes-1.16/>our guest back in episode 72</a>, and he's returned on this release as the <a href=https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks/emeritus-adviser>emeritus advisor</a>. What did you learn from him?</strong></p><p>JORGE ALARCÓN: Oh, everything. And it actually all started back on 1.16. So like you said, an amazing person— he's an amazing individual. And it's truly an opportunity to be able to work with him. During 1.16, I was the CI Signal lead, and Lachie is very hands on.</p><p>He's not the kind of person to just give you a list of things and say, do them. He actually comes to you, has a conversation, and he works with you more than anything. And when we were working together on 1.16, I got to learn a lot from him in terms of CI Signal. And especially because we talked about everything just to make sure that 1.16 was ready to go, I also got to pick up a couple of things that a release lead has to know, has to be able to do, has to work on to get a release out the door.</p><p>And now, during this release, there is a lot of information that's really useful, and there's a lot of advice and general wisdom that comes in handy. For most of the things that impact a lot of things, we are always in communication. Like, I'm doing this, you're doing that, advice. And essentially, every single thing that we do is pretty much a code review. You do it, and then you wait for someone else to give you comments. And that's been a strong part of our relationship working.</p><p><strong>ADAM GLICK: What would you say the theme for this release is?</strong></p><p>JORGE ALARCÓN: I think one of the themes is "fit and finish". There are a lot of features that we are bumping from alpha to beta, from beta to stable. And we want to make sure that people have a good user experience. Operators and developers alike just want to get rid of as many bugs as possible, improve the flow of things.</p><p>But the other really cool thing is we have about an equal distribution between alpha, beta, and stable. We are also bringing up a lot of new features. So besides making Kubernetes more stable for all the users that are already using it, we are working on bringing up new things that people can try out for the next release and see how it goes in the future.</p><p><strong>ADAM GLICK: Did you have a release team mascot?</strong></p><p>JORGE ALARCÓN: Kind of.</p><p><strong>ADAM GLICK: Who/what was it?</strong></p><p>JORGE ALARCÓN: [LAUGHING] I say kind of because I'm using the mascot in the <a href=https://twitter.com/KubernetesPod/status/1242953121380392963>logo</a>, and the logo is inspired by the Large Hadron Collider.</p><p><strong>ADAM GLICK: Oh, fantastic.</strong></p><p>JORGE ALARCÓN: Being the release lead, I really had to take a chance on this opportunity to use the LHC as the mascot.</p><p><strong>ADAM GLICK: We've had <a href=https://kubernetespodcast.com/episode/062-cern/>some of the folks from the LHC on the show</a>, and I know they listen, and they will be thrilled with that.</strong></p><p>JORGE ALARCÓN: [LAUGHING] Hopefully, they like the logo.</p><p><strong>ADAM GLICK: If you look at this release, what part of this release, what thing that has been added to it are you personally most excited about?</strong></p><p>JORGE ALARCÓN: Like a parent can't choose which child is his or her favorite, you really can't choose a specific thing.</p><p><strong>ADAM GLICK: We have been following online and in the issues an enhancement that's called <a href=https://github.com/kubernetes/enhancements/issues/753>sidecar containers</a>. You'd be able to mark the order of containers starting in a pod. Tim Hockin posted <a href=https://github.com/kubernetes/enhancements/issues/753#issuecomment-597372056>a long comment on behalf of a number of SIG Node contributors</a> citing social, procedural, and technical concerns about what's going on with that— in particular, that it moved out of 1.18 and is now moving to 1.19. Did you have any thoughts on that?</strong></p><p>JORGE ALARCÓN: The sidecar enhancement has definitely been an interesting one. First off, thank you very much to Joseph Irving, the author of the KEP. And thank you very much to Tim Hockin, who voiced out the point of view of the approvers, maintainers of SIG Node. And I guess a little bit of context before we move on is, in the Kubernetes community, we have contributors, we have reviewers, and we have approvers.</p><p>Contributors are people who write PRs, who file issues, who troubleshoot issues. Reviewers are contributors who focus on one or multiple specific areas within the project, and then approvers are maintainers for the specific area, for one or multiple specific areas, of the project. So you can think of approvers as people who have write access in a repo or someplace within a repo.</p><p>The issue with the sidecar enhancement is that it has been deferred for multiple releases now, and that's been because there hasn't been a lot of collaboration between the KEP authors and the approvers for specific parts of the project. Something worthwhile to mention— and this was brought up during the original discussion— is this can obviously be frustrating for both contributors and for approvers. From the contributor's side of things, you are working on something. You are doing your best to make sure that it works.</p><p>And to build something that's going to be used by people, both from the approver side of things and, I think, for the most part, every single person in the Kubernetes community, we are all really excited to see this project grow. We want to help improve it, and we love when new people come in and work on new enhancements, bug fixes, and the like.</p><p>But one of the limitations is the day only has so many hours, and there are only so many things that we can work on at a time. So people prioritize in whatever way works best, and some things just fall behind. And a lot of the time, the things that fall behind are not because people don't want them to continue moving forward, but it's just a limited amount of resources, a limited amount of people.</p><p>And I think this discussion around the sidecar enhancement proposal has been very useful, and it points us to the need for more standardized mentoring programs. This is something that multiple SIGs are working on. For example, SIG Contribex, SIG Cluster Lifecycle, SIG Release. The idea is to standardize some sort of mentoring experience so that we can better prepare new contributors to become reviewers and ultimately approvers.</p><p>Because ultimately at the end of the day, if we have more people who are knowledgeable about Kubernetes, or even some specific area of Kubernetes, we can better distribute the load, and we can better collaborate on whatever new things come up. I think the sidecar enhancement has shown us mentoring is something worthwhile, and we need a lot more of it. Because as much work as we do, more things are going to continue popping in throughout the project. And the more people we have who are comfortable working in these really complicated areas of Kubernetes, the better off that we are going to be.</p><p><strong>ADAM GLICK: Was there any talk of delaying 1.18 due to the current worldwide health situation?</strong></p><p>JORGE ALARCÓN: We thought about it, and the plan was to just wait and see how people felt. Tried make sure that people were comfortable continuing to work and all the people were landing in new enhancements, or fixing tests, or members of the release team who were making sure that things were happening. We wanted to see that people were comfortable, that they could continue doing their job. And for a moment, I actually thought about delaying just outright— we're going to give it more time, and hopefully at some point, things are going to work out.</p><p>But people just continue doing their amazing work. There was no delay. There was no hitch throughout the process. So at some point, I just figured we stay with the current timeline and see how we went. And at this point, things are more or less set.</p><p><strong>ADAM GLICK: Amazing power of a distributed team.</strong></p><p>JORGE ALARCÓN: Yeah, definitely.</p><p>[LAUGHING]</p><p><strong>ADAM GLICK: <a href=https://twitter.com/alejandrox135/status/1239629281766096898>Taylor Dolezal was announced as the 1.19 release lead</a>. Do you know how that choice was made, and by whom?</strong></p><p>JORGE ALARCÓN: I actually got to choose the lead. The practice is the current lead for the release team is going to look at people and see, first off, who's interested and out of the people interested, who can do the job, who's comfortable enough with the release team, with the Kubernetes community at large who can actually commit the amount of hours throughout the next, hopefully, three months.</p><p>And for one, I think Taylor has been part of my team. So there is the release team. Then the release team has multiple subgroups. One of those subgroups is actually just for me and my shadows. So for this release, it was mrbobbytables and Taylor. And Taylor volunteered to take over 1.19, and I'm sure that he will do an amazing job.</p><p><strong>ADAM GLICK: I am as well. What advice will you give Taylor?</strong></p><p>JORGE ALARCÓN: Over-communicate as much as possible. Normally, if you made it to the point that you are the lead for a release, or even the shadow for a release, you more or less are familiar with a lot of the work— CI Signal, enhancements, documentation, and the like. And a lot of people, if they know how to do their job, they might tell themselves, yeah, I could do it— no need to worry about it. I'm just going to go ahead and sign this PR, debug this test, whatever.</p><p>But one of the interesting aspects is whenever we are actually working in a release, 50% of the work has to go into actually making the release happen. The other 50% of the work has to go into mentoring people, and making sure the newcomers, new members are able to learn everything that they need to learn to do your job, you being in the lead for a subgroup or the entire team. And whenever you actually see that things need to happen, just over-communicate.</p><p>Try to provide the opportunity for someone else to do the work, and over-communicate with them as much as possible to make sure that they are learning whatever it is that they need to learn. If neither you or the other person knows what's going on, then I can over-communicate, so someone hopefully will see your messages and come to the rescue. That happens a lot. There's a lot of really nice and kind people who will come out and tell you how something works, help you fix it.</p><p><strong>ADAM GLICK: If you were to sum up your experience running this release, what would it be?</strong></p><p>JORGE ALARCÓN: It's been super fun and a little bit stressing, to be honest. Being the release lead is definitely amazing. You're kind of sitting at the center of Kubernetes.</p><p>You not only see the people who are working on things— the things that are broken, and the users filling out issues, and saying what broke, and the like. But you also get the opportunity to work with a lot of people who do a lot of non-code related work. Docs is one of the most obvious things. There's a lot of work that goes into communications, contributor experience, public relations.</p><p>And being connected, getting to talk with those people mostly every other day, it's really fun. It's a really good experience in terms of becoming a better contributor to the community, but also taking some of that knowledge home with you and applying it somewhere else. If you are a software engineer, if you are a project manager, whatever, it's amazing how much you can learn.</p><p><strong>ADAM GLICK: I know the community likes to rotate around who are the release leads. But if you were given the opportunity to be a release lead for a future release of Kubernetes, would you do it again?</strong></p><p>JORGE ALARCÓN: Yeah, it's a fun job. To be honest, it can be really stressing. Especially, as I mentioned, at some point, most of that work is just going to be talking with people, and talking requires a lot more thought and effort than just sitting down and thinking about things sometimes. And some of that can be really stressful.</p><p>But the job itself, it is definitely fun. And at some distant point in the future, if for some reason it was a possibility, I will think about it. But definitely, as you mentioned, one thing that we try to do is cycle out, because I can have fun in it, and that's all good and nice. And hopefully I can help another release go out the door. But providing the opportunity for other people to learn I think is a lot more important than just being the lead itself.</p><hr><p><em><a href=https://twitter.com/alejandrox135>Jorge Alarcón</a> is a site reliability engineer with Searchable AI and served as the Kubernetes 1.18 release team lead.</em></p><p><em>You can find the <a href=http://www.kubernetespodcast.com/>Kubernetes Podcast from Google</a> at <a href=https://twitter.com/KubernetesPod>@KubernetesPod</a> on Twitter, and you can <a href=https://kubernetespodcast.com/subscribe/>subscribe</a> so you never miss an episode.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-462bb8169b38d42b97315b740fae2e19>Music and math: the Kubernetes 1.17 release interview</h1><div class="td-byline mb-4"><time datetime=2020-07-27 class=text-muted>Monday, July 27, 2020</time></div><p><strong>Author</strong>: Adam Glick (Google)</p><p>Every time the Kubernetes release train stops at the station, we like to ask the release lead to take a moment to reflect on their experience. That takes the form of an interview on the weekly <a href=https://kubernetespodcast.com/>Kubernetes Podcast from Google</a> that I co-host with <a href=https://twitter.com/craigbox>Craig Box</a>. If you're not familiar with the show, every week we summarise the new in the Cloud Native ecosystem, and have an insightful discussion with an interesting guest from the broader Kubernetes community.</p><p>At the time of the 1.17 release in December, we <a href=https://kubernetespodcast.com/episode/083-kubernetes-1.17/>talked to release team lead Guinevere Saenger</a>. We have <a href=https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/>shared</a> <a href=https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/>the</a> <a href=https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/>transcripts</a> of previous interviews on the Kubernetes blog, and we're very happy to share another today.</p><p>Next week we will bring you up to date with the story of Kubernetes 1.18, as we gear up for the release of 1.19 next month. <a href=https://kubernetespodcast.com/subscribe/>Subscribe to the show</a> wherever you get your podcasts to make sure you don't miss that chat!</p><hr><p><strong>ADAM GLICK: You have a nontraditional background for someone who works as a software engineer. Can you explain that background?</strong></p><p>GUINEVERE SAENGER: My first career was as a <a href=https://en.wikipedia.org/wiki/Collaborative_piano>collaborative pianist</a>, which is an academic way of saying "piano accompanist". I was a classically trained pianist who spends most of her time onstage, accompanying other people and making them sound great.</p><p><strong>ADAM GLICK: Is that the piano equivalent of pair-programming?</strong></p><p>GUINEVERE SAENGER: No one has said it to me like that before, but all sorts of things are starting to make sense in my head right now. I think that's a really great way of putting it.</p><p><strong>ADAM GLICK: That's a really interesting background, as someone who also has a background with music. What made you decide to get into software development?</strong></p><p>GUINEVERE SAENGER: I found myself in a life situation where I needed more stable source of income, and teaching music, and performing for various gig opportunities, was really just not cutting it anymore. And I found myself to be working really, really hard with not much to show for it. I had a lot of friends who were software engineers. I live in Seattle. That's sort of a thing that happens to you when you live in Seattle — you get to know a bunch of software engineers, one way or the other.</p><p>The ones I met were all lovely people, and they said, hey, I'm happy to show you how to program in Python. And so I did that for a bit, and then I heard about this program called <a href=https://adadevelopersacademy.org/>Ada Developers Academy</a>. That's a year long coding school, targeted at women and non-binary folks that are looking for a second career in tech. And so I applied for that.</p><p><strong>CRAIG BOX: What can you tell us about that program?</strong></p><p>GUINEVERE SAENGER: It's incredibly selective, for starters. It's really popular in Seattle and has gotten quite a good reputation. It took me three tries to get in. They do two classes a year, and so it was a while before I got my response saying 'congratulations, we are happy to welcome you into Cohort 6'. I think what sets Ada Developers Academy apart from other bootcamp style coding programs are three things, I think? The main important one is that if you get in, you pay no tuition. The entire program is funded by company sponsors.</p><p><strong>CRAIG BOX: Right.</strong></p><p>GUINEVERE SAENGER: The other thing that really convinced me is that five months of the 11-month program are an industry internship, which means you get both practical experience, mentorship, and potential job leads at the end of it.</p><p><strong>CRAIG BOX: So very much like a condensed version of the University of Waterloo degree, where you do co-op terms.</strong></p><p>GUINEVERE SAENGER: Interesting. I didn't know about that.</p><p><strong>CRAIG BOX: Having lived in Waterloo for a while, I knew a lot of people who did that. But what would you say the advantages were of going through such a condensed schooling process in computer science?</strong></p><p>GUINEVERE SAENGER: I'm not sure that the condensed process is necessarily an advantage. I think it's a necessity, though. People have to quit their jobs to go do this program. It's not an evening school type of thing.</p><p><strong>CRAIG BOX: Right.</strong></p><p>GUINEVERE SAENGER: And your internship is basically a full-time job when you do it. One thing that Ada was really, really good at is giving us practical experience that directly relates to the workplace. We learned how to use Git. We learned how to design websites using <a href=https://rubyonrails.org/>Rails</a>. And we also learned how to collaborate, how to pair-program. We had a weekly retrospective, so we sort of got a soft introduction to workflows at a real workplace. Adding to that, the internship, and I think the overall experience is a little bit more 'practical workplace oriented' and a little bit less academic.</p><p>When you're done with it, you don't have to relearn how to be an adult in a working relationship with other people. You come with a set of previous skills. There are Ada graduates who have previously been campaign lawyers, and veterinarians, and nannies, cooks, all sorts of people. And it turns out these skills tend to translate, and they tend to matter.</p><p><strong>ADAM GLICK: With your background in music, what do you think that that allows you to bring to software development that could be missing from, say, standard software development training that people go through?</strong></p><p>GUINEVERE SAENGER: People tend to really connect the dots when I tell them I used to be a musician. Of course, I still consider myself a musician, because you don't really ever stop being a musician. But they say, 'oh, yeah, music and math', and that's just a similar sort of brain. And that makes so much sense. And I think there's a little bit of a point to that. When you learn a piece of music, you have to start recognizing patterns incredibly quickly, almost intuitively.</p><p>And I think that is the main skill that translates into programming— recognizing patterns, finding the things that work, finding the things that don't work. And for me, especially as a collaborative pianist, it's the communicating with people, the finding out what people really want, where something is going, how to figure out what the general direction is that we want to take, before we start writing the first line of code.</p><p><strong>CRAIG BOX: In your experience at Ada or with other experiences you've had, have you been able to identify patterns in other backgrounds for people that you'd recommend, 'hey, you're good at music, so therefore you might want to consider doing something like a course in computer science'?</strong></p><p>GUINEVERE SAENGER: Overall, I think ultimately writing code is just giving a set of instructions to a computer. And we do that in daily life all the time. We give instructions to our kids, we give instructions to our students. We do math, we write textbooks. We give instructions to a room full of people when you're in court as a lawyer.</p><p>Actually, the entrance exam to Ada Developers Academy used to have questions from the <a href=https://en.wikipedia.org/wiki/Law_School_Admission_Test>LSAT</a> on it to see if you were qualified to join the program. They changed that when I applied, but I think that's a thing that happened at one point. So, overall, I think software engineering is a much more varied field than we give it credit for, and that there are so many ways in which you can apply your so-called other skills and bring them under the umbrella of software engineering.</p><p><strong>CRAIG BOX: I do think that programming is effectively half art and half science. There's creativity to be applied. There is perhaps one way to solve a problem most efficiently. But there are many different ways that you can choose to express how you compiled something down to that way.</strong></p><p>GUINEVERE SAENGER: Yeah, I mean, that's definitely true. I think one way that you could probably prove that is that if you write code at work and you're working on something with other people, you can probably tell which one of your co-workers wrote which package, just by the way it's written, or how it is documented, or how it is styled, or any of those things. I really do think that the human character shines through.</p><p><strong>ADAM GLICK: What got you interested in Kubernetes and open source?</strong></p><p>GUINEVERE SAENGER: The honest answer is absolutely nothing. Going back to my programming school— and remember that I had to do a five-month internship as part of my training— the way that the internship works is that sponsor companies for the program get interns in according to how much they sponsored a specific cohort of students.</p><p>So at the time, Samsung and SDS offered to host two interns for five months on their <a href=https://samsung-cnct.github.io/>Cloud Native Computing team</a> and have that be their practical experience. So I go out of a Ruby on Rails full stack web development bootcamp and show up at my internship, and they said, "Welcome to Kubernetes. Try to bring up a cluster." And I said, "Kuber what?"</p><p><strong>CRAIG BOX: We've all said that on occasion.</strong></p><p><strong>ADAM GLICK: Trial by fire, wow.</strong></p><p>GUINEVERE SAENGER: I will say that that entire team was absolutely wonderful, delightful to work with, incredibly helpful. And I will forever be grateful for all of the help and support that I got in that environment. It was a great place to learn.</p><p><strong>CRAIG BOX: You now work on GitHub's Kubernetes infrastructure. Obviously, there was GitHub before there was a Kubernetes, so a migration happened. What can you tell us about the transition that GitHub made to running on Kubernetes?</strong></p><p>GUINEVERE SAENGER: A disclaimer here— I was not at GitHub at the time that the transition to Kubernetes was made. However, to the best of my knowledge, the decision to transition to Kubernetes was made and people decided, yes, we want to try Kubernetes. We want to use Kubernetes. And mostly, the only decision left was, which one of our applications should we move over to Kubernetes?</p><p><strong>CRAIG BOX: I thought GitHub was written on Rails, so there was only one application.</strong></p><p>GUINEVERE SAENGER: [LAUGHING] We have a lot of supplementary stuff under the covers.</p><p><strong>CRAIG BOX: I'm sure.</strong></p><p>GUINEVERE SAENGER: But yes, GitHub is written in Rails. It is still written in Rails. And most of the supplementary things are currently running on Kubernetes. We have a fair bit of stuff that currently does not run on Kubernetes. Mainly, that is GitHub Enterprise related things. I would know less about that because I am on the platform team that helps people use the Kubernetes infrastructure. But back to your question, leadership at the time decided that it would be a good idea to start with GitHub the Rails website as the first project to move to Kubernetes.</p><p><strong>ADAM GLICK: High stakes!</strong></p><p>GUINEVERE SAENGER: The reason for this was that they decided if they were going to not start big, it really wasn't going to transition ever. It was really not going to happen. So they just decided to go all out, and it was successful, for which I think the lesson would probably be commit early, commit big.</p><p><strong>CRAIG BOX: Are there any other lessons that you would take away or that you've learned kind of from the transition that the company made, and might be applicable to other people who are looking at moving their companies from a traditional infrastructure to a Kubernetes infrastructure?</strong></p><p>GUINEVERE SAENGER: I'm not sure this is a lesson specifically, but I was on support recently, and it turned out that, due to unforeseen circumstances and a mix of human error, a bunch of the namespaces on one of our Kubernetes clusters got deleted.</p><p><strong>ADAM GLICK: Oh, my.</strong></p><p>GUINEVERE SAENGER: It should not have affected any customers, I should mention, at this point. But all in all, it took a few of us a few hours to almost completely recover from this event. I think that, without Kubernetes, this would not have been possible.</p><p><strong>CRAIG BOX: Generally, deleting something like that is quite catastrophic. We've seen a number of other vendors suffer large outages when someone's done something to that effect, which is why we get <a href=https://twitter.com/hashtag/hugops>#hugops</a> on Twitter all the time.</strong></p><p>GUINEVERE SAENGER: People did send me #hugops, that is a thing that happened. But overall, something like this was an interesting stress test and sort of proved that it wasn't nearly as catastrophic as a worst case scenario.</p><p><strong>CRAIG BOX: GitHub <a href=https://githubengineering.com/githubs-metal-cloud/>runs its own data centers</a>. Kubernetes was largely built for running on the cloud, but a lot of people do choose to run it on their own, bare metal. How do you manage clusters and provisioning of the machinery you run?</strong></p><p>GUINEVERE SAENGER: When I started, my onboarding project was to deprovision an old cluster, make sure all the traffic got moved to somewhere where it would keep running, provision a new cluster, and then move website traffic onto the new cluster. That was a really exciting onboarding project. At the time, we provisioned bare metal machines using Puppet. We still do that to a degree, but I believe the team that now runs our computing resources actually inserts virtual machines as an extra layer between the bare metal and the Kubernetes nodes.</p><p>Again, I was not intrinsically part of that decision, but my understanding is that it just makes for a greater reliability and reproducibility across the board. We've had some interesting hardware dependency issues come up, and the virtual machines basically avoid those.</p><p><strong>CRAIG BOX: You've been working with Kubernetes for a couple of years now. How did you get involved in the release process?</strong></p><p>GUINEVERE SAENGER: When I first started in the project, I started at the <a href=https://github.com/kubernetes/community/tree/master/sig-contributor-experience#readme>special interest group for contributor experience</a>, namely because one of my co-workers at the time, Aaron Crickenberger, was a big Kubernetes community person. Still is.</p><p><strong>CRAIG BOX: We've <a href=https://kubernetespodcast.com/episode/046-kubernetes-1.14/>had him on the show</a> for one of these very release interviews!</strong></p><p>GUINEVERE SAENGER: In fact, this is true! So Aaron and I actually go way back to Samsung SDS. Anyway, Aaron suggested that I should write up a contribution to the Kubernetes project, and I said, me? And he said, yes, of course. You will be <a href="https://www.youtube.com/watch?v=TkCDUFR6xqw">speaking at KubeCon</a>, so you should probably get started with a PR or something. So I tried, and it was really, really hard. And I complained about it <a href=https://github.com/kubernetes/community/issues/141>in a public GitHub issue</a>, and people said, yeah. Yeah, we know it's hard. Do you want to help with that?</p><p>And so I started getting really involved with the <a href=https://github.com/kubernetes/community/tree/master/contributors/guide>process for new contributors to get started</a> and have successes, kind of getting a foothold into a project that's as large and varied as Kubernetes. From there on, I began to talk to people, get to know people. The great thing about the Kubernetes community is that there is so much mentorship to go around.</p><p><strong>ADAM GLICK: Right.</strong></p><p>GUINEVERE SAENGER: There are so many friendly people willing to help. It's really funny when I talk to other people about it. They say, what do you mean, your coworker? And I said, well, he's really a colleague. He really works for another company.</p><p><strong>CRAIG BOX: He's sort-of officially a competitor.</strong></p><p>GUINEVERE SAENGER: Yeah.</p><p><strong>CRAIG BOX: But we're friends.</strong></p><p>GUINEVERE SAENGER: But he totally helped me when I didn't know how to git patch my borked pull request. So that happened. And eventually, somebody just suggested that I start following along in the release process and shadow someone on their release team role. And that, at the time, was Tim Pepper, who was bug triage lead, and I shadowed him for that role.</p><p><strong>CRAIG BOX: Another <a href=https://kubernetespodcast.com/episode/010-kubernetes-1.11/>podcast guest</a> on the interview train.</strong></p><p>GUINEVERE SAENGER: This is a pattern that probably will make more sense once I explain to you about the shadow process of the release team.</p><p><strong>ADAM GLICK: Well, let's turn to the Kubernetes release and the release process. First up, what's new in this release of 1.17?</strong></p><p>GUINEVERE SAENGER: We have only a very few new things. The one that I'm most excited about is that we have moved <a href=https://github.com/kubernetes/enhancements/issues/563>IPv4 and IPv6 dual stack</a> support to alpha. That is the most major change, and it has been, I think, a year and a half in coming. So this is the very first cut of that feature, and I'm super excited about that.</p><p><strong>CRAIG BOX: The people who have been promised IPv6 for many, many years and still don't really see it, what will this mean for them?</strong></p><p><strong>ADAM GLICK: And most importantly, why did we skip IPv5 support?</strong></p><p>GUINEVERE SAENGER: I don't know!</p><p><strong>CRAIG BOX: Please see <a href=https://softwareengineering.stackexchange.com/questions/185380/ipv4-to-ipv6-where-is-ipv5>the appendix to this podcast</a> for technical explanations.</strong></p><p>GUINEVERE SAENGER: Having a dual stack configuration obviously enables people to have a much more flexible infrastructure and not have to worry so much about making decisions that will become outdated or that may be over-complicated. This basically means that pods can have dual stack addresses, and nodes can have dual stack addresses. And that basically just makes communication a lot easier.</p><p><strong>CRAIG BOX: What about features that didn't make it into the release? We had a conversation with Lachie in the <a href=https://kubernetespodcast.com/episode/072-kubernetes-1.16/>1.16 interview</a>, where he mentioned <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md>sidecar containers</a>. They unfortunately didn't make it into that release. And I see now that they haven't made this one either.</strong></p><p>GUINEVERE SAENGER: They have not, and we are actually currently undergoing an effort of tracking features that flip multiple releases.</p><p>As a community, we need everyone's help. There are a lot of features that people want. There is also a lot of cleanup that needs to happen. And we have started talking at previous KubeCons repeatedly about problems with maintainer burnout, reviewer burnout, have a hard time finding reviews for your particular contributions, especially if you are not an entrenched member of the community. And it has become very clear that this is an area where the entire community needs to improve.</p><p>So the unfortunate reality is that sometimes life happens, and people are busy. This is an open source project. This is not something that has company mandated OKRs. Particularly during the fourth quarter of the year in North America, but around the world, we have a lot of holidays. It is the end of the year. Kubecon North America happened as well. This makes it often hard to find a reviewer in time or to rally the support that you need for your enhancement proposal. Unfortunately, slipping releases is fairly common and, at this point, expected. We started out with having 42 enhancements and <a href="https://docs.google.com/spreadsheets/d/1ebKGsYB1TmMnkx86bR2ZDOibm5KWWCs_UjV3Ys71WIs/edit#gid=0">landed with roughly half of that</a>.</p><p><strong>CRAIG BOX: I was going to ask about the truncated schedule due to the fourth quarter of the year, where there are holidays in large parts of the world. Do you find that the Q4 release on the whole is smaller than others, if not for the fact that it's some week shorter?</strong></p><p>GUINEVERE SAENGER: Q4 releases are shorter by necessity because we are trying to finish the final release of the year before the end of the year holidays. Often, releases are under pressure of KubeCons, during which finding reviewers or even finding the time to do work can be hard to do, if you are attending. And even if you're not attending, your reviewers might be attending.</p><p>It has been brought up last year to make the final release more of a stability release, meaning no new alpha features. In practice, for this release, this is actually quite close to the truth. We have four features graduating to beta and most of our features are graduating to stable. I am hoping to use this as a precedent to change our process to make the final release a stability release from here on out. The timeline fits. The past experience fits this model.</p><p><strong>ADAM GLICK: On top of all of the release work that was going on, there was also KubeCon that happened. And you were involved in the <a href=https://github.com/kubernetes/community/tree/master/events/2019/11-contributor-summit>contributor summit</a>. How was the summit?</strong></p><p>GUINEVERE SAENGER: This was the first contributor summit where we had an organized events team with events organizing leads, and handbooks, and processes. And I have heard from multiple people— this is just word of mouth— that it was their favorite contributor summit ever.</p><p><strong>CRAIG BOX: Was someone allocated to hat production? <a href=https://flickr.com/photos/143247548@N03/49093218951/>Everyone had sailor hats</a>.</strong></p><p>GUINEVERE SAENGER: Yes, the entire event staff had sailor hats with their GitHub handle on them, and it was pretty fantastic. You can probably see me wearing one in some of the pictures from the contributor summit. That literally was something that was pulled out of a box the morning of the contributor summit, and no one had any idea. But at first, I was a little skeptical, but then I put it on and looked at myself in the mirror. And I was like, yes. Yes, this is accurate. We should all wear these.</p><p><strong>ADAM GLICK: Did getting everyone together for the contributor summit help with the release process?</strong></p><p>GUINEVERE SAENGER: It did not. It did quite the opposite, really. Well, that's too strong.</p><p><strong>ADAM GLICK: Is that just a matter of the time taken up?</strong></p><p>GUINEVERE SAENGER: It's just a completely different focus. Honestly, it helped getting to know people face-to-face that I had currently only interacted with on video. But we did have to cancel the release team meeting the day of the contributor summit because there was kind of no sense in having it happen. We moved it to the Tuesday, I believe.</p><p><strong>CRAIG BOX: The role of the release team leader has been described as servant leadership. Do you consider the position proactive or reactive?</strong></p><p>GUINEVERE SAENGER: Honestly, I think that depends on who's the release team lead, right? There are some people who are very watchful and look for trends, trying to detect problems before they happen. I tend to be in that camp, but I also know that sometimes it's not possible to predict things. There will be last minute bugs sometimes, sometimes not. If there is a last minute bug, you have to be ready to be on top of that. So for me, the approach has been I want to make sure that I have my priorities in order and also that I have backups in case I can't be available.</p><p><strong>ADAM GLICK: What was the most interesting part of the release process for you?</strong></p><p>GUINEVERE SAENGER: A release lead has to have served in other roles on the release team prior to being release team lead. To me, it was very interesting to see what other roles were responsible for, ones that I hadn't seen from the inside before, such as docs, CI signal. I had helped out with CI signal for a bit, but I want to give a big shout out to CI signal lead, Alena Varkockova, who was able to communicate effectively and kindly with everyone who was running into broken tests, failing tests. And she was very effective in getting all of our tests up and running.</p><p>So that was actually really cool to see. And yeah, just getting to see more of the workings of the team, for me, it was exciting. The other big exciting thing, of course, was to see all the changes that were going in and all the efforts that were being made.</p><p><strong>CRAIG BOX: The release lead for 1.18 has just been announced as <a href=https://twitter.com/alejandrox135>Jorge Alarcon</a>. What are you going to put in the proverbial envelope as advice for him?</strong></p><p>GUINEVERE SAENGER: I would want Jorge to be really on top of making sure that every Special Interest Group that enters a change, that has an enhancement for 1.18, is on top of the timelines and is responsive. Communication tends to be a problem. And I had hinted at this earlier, but some enhancements slipped simply because there wasn't enough reviewer bandwidth.</p><p>Greater communication of timelines and just giving people more time and space to be able to get in their changes, or at least, seemingly give them more time and space by sending early warnings, is going to be helpful. Of course, he's going to have a slightly longer release, too, than I did. This might be related to a unique Q4 challenge. Overall, I would encourage him to take more breaks, to rely more on his release shadows, and split out the work in a fashion that allows everyone to have a turn and everyone to have a break as well.</p><p><strong>ADAM GLICK: What would your advice be to someone who is hearing your experience and is inspired to get involved with the Kubernetes release or contributer process?</strong></p><p>GUINEVERE SAENGER: Those are two separate questions. So let me tackle the Kubernetes release question first. Kubernetes <a href=https://github.com/kubernetes/sig-release/#readme>SIG Release</a> has, in my opinion, a really excellent onboarding program for new members. We have what is called the <a href=https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md>Release Team Shadow Program</a>. We also have the Release Engineering Shadow Program, or the Release Management Shadow Program. Those are two separate subprojects within SIG Release. And each subproject has a team of roles, and each role can have two to four shadows that are basically people who are part of that role team, and they are learning that role as they are doing it.</p><p>So for example, if I am the lead for bug triage on the release team, I may have two, three or four people that I closely work with on the bug triage tasks. These people are my shadows. And once they have served one release cycle as a shadow, they are now eligible to be lead in that role. We have an application form for this process, and it should probably be going up in January. It usually happens the first week of the release once all the release leads are put together.</p><p><strong>CRAIG BOX: Do you think being a member of the release team is something that is a good first contribution to the Kubernetes project overall?</strong></p><p>GUINEVERE SAENGER: It depends on what your goals are, right? I believe so. I believe, for me, personally, it has been incredibly helpful looking into corners of the project that I don't know very much about at all, like API machinery, storage. It's been really exciting to look over all the areas of code that I normally never touch.</p><p>It depends on what you want to get out of it. In general, I think that being a release team shadow is a really, really great on-ramp to being a part of the community because it has a paved path solution to contributing. All you have to do is show up to the meetings, ask questions of your lead, who is required to answer those questions.</p><p>And you also do real work. You really help, you really contribute. If you go across the issues and pull requests in the repo, you will see, 'Hi, my name is so-and-so. I am shadowing the CI signal lead for the current release. Can you help me out here?' And that's a valuable contribution, and it introduces people to others. And then people will recognize your name. They'll see a pull request by you, and they're like oh yeah, I know this person. They're legit.</p><hr><p><em><a href=https://twitter.com/guincodes>Guinevere Saenger</a> is a software engineer for GitHub and served as the Kubernetes 1.17 release team lead.</em></p><p><em>You can find the <a href=http://www.kubernetespodcast.com/>Kubernetes Podcast from Google</a> at <a href=https://twitter.com/KubernetesPod>@KubernetesPod</a> on Twitter, and you can <a href=https://kubernetespodcast.com/subscribe/>subscribe</a> so you never miss an episode.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4eb578511a6303fc1731bbf4979a273>SIG-Windows Spotlight</h1><div class="td-byline mb-4"><time datetime=2020-06-30 class=text-muted>Tuesday, June 30, 2020</time></div><p><em>This post tells the story of how Kubernetes contributors work together to provide a container orchestrator that works for both Linux and Windows.</em></p><img alt="Image of a computer with Kubernetes logo" width=30% src=KubernetesComputer_transparent.png><p>Most people who are familiar with Kubernetes are probably used to associating it with Linux. The connection makes sense, since Kubernetes ran on Linux from its very beginning. However, many teams and organizations working on adopting Kubernetes need the ability to orchestrate containers on Windows. Since the release of Docker and rise to popularity of containers, there have been efforts both from the community and from Microsoft itself to make container technology as accessible in Windows systems as it is in Linux systems.</p><p>Within the Kubernetes community, those who are passionate about making Kubernetes accessible to the Windows community can find a home in the Windows Special Interest Group. To learn more about SIG-Windows and the future of Kubernetes on Windows, I spoke to co-chairs <a href=https://github.com/marosset>Mark Rossetti</a> and <a href=https://github.com/michmike>Michael Michael</a> about the SIG's goals and how others can contribute.</p><h2 id=intro-to-windows-containers-kubernetes>Intro to Windows Containers & Kubernetes</h2><p>Kubernetes is the most popular tool for orchestrating container workloads, so to understand the Windows Special Interest Group (SIG) within the Kubernetes project, it's important to first understand what we mean when we talk about running containers on Windows.</p><hr><p><em>"When looking at Windows support in Kubernetes," says SIG (Special Interest Group) Co-chairs Mark Rossetti and Michael Michael, "many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between operational limitations and differences between the Windows and Linux operating systems. Windows containers run the Windows operating system and Linux containers run Linux."</em></p><hr><p>In essence, any "container" is simply a process being run on its host operating system, with some key tooling in place to isolate that process and its dependencies from the rest of the environment. The goal is to make that running process safely isolated, while taking up minimal resources from the system to perform that isolation. On Linux, the tooling used to isolate processes to create "containers" commonly boils down to cgroups and namespaces (among a few others), which are themselves tools built in to the Linux Kernel.</p><img alt="A visual analogy using dogs to explain Linux cgroups and namespaces." width=40% src=cgroupsNamespacesComboPic.png><h4 id=if-dogs-were-processes-containerization-would-be-like-giving-each-dog-their-own-resources-like-toys-and-food-using-cgroups-and-isolating-troublesome-dogs-using-namespaces><em>If dogs were processes: containerization would be like giving each dog their own resources like toys and food using cgroups, and isolating troublesome dogs using namespaces.</em></h4><p>Native Windows processes are processes that are or must be run on a Windows operating system. This makes them fundamentally different from a process running on a Linux operating system. Since Linux containers are Linux processes being isolated by the Linux kernel tools known as cgroups and namespaces, containerizing native Windows processes meant implementing similar isolation tools within the Windows kernel itself. Thus, "Windows Containers" and "Linux Containers" are fundamentally different technologies, even though they have the same goals (isolating processes) and in some ways work similarly (using kernel level containerization).</p><p>So when it comes to running containers on Windows, there are actually two very important concepts to consider:</p><ul><li>Native Windows processes running as native Windows Server style containers,</li><li>and traditional Linux containers running on a Linux Kernel, generally hosted on a lightweight Hyper-V Virtual Machine.</li></ul><p>You can learn more about Linux and Windows containers in this <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers>tutorial</a> from Microsoft.</p><h3 id=kubernetes-on-windows>Kubernetes on Windows</h3><p>Kubernetes was initially designed with Linux containers in mind and was itself designed to run on Linux systems. Because of that, much of the functionality of Kubernetes involves unique Linux functionality. The Linux-specific work is intentional--we all want Kubernetes to run optimally on Linux--but there is a growing demand for similar optimization for Windows servers. For cases where users need container orchestration on Windows, the Kubernetes contributor community of SIG-Windows has incorporated functionality for Windows-specific use cases.</p><hr><p><em>"A common question we get is, will I be able to have a Windows-only cluster. The answer is NO. Kubernetes control plane components will continue to be based on Linux, while SIG-Windows is concentrating on the experience of having Windows worker nodes in a Kubernetes cluster."</em></p><hr><p>Rather than separating out the concepts of "Windows Kubernetes," and "Linux Kubernetes," the community of SIG-Windows works toward adding functionality to the main Kubernetes project which allows it to handle use cases for Windows. These Windows capabilities mirror, and in some cases add unique functionality to, the Linux use cases Kubernetes has served since its release in 2014 (want to learn more history? Scroll through this <a href=https://github.com/kubernetes/kubernetes/blob/e2b948dbfbba62b8cb681189377157deee93bb43/DESIGN.md>original design document</a>.</p><h2 id=what-does-sig-windows-do>What Does SIG-Windows Do?</h2><hr><p><em>"SIG-Windows is really the center for all things Windows in Kubernetes,"</em> SIG chairs Mark and Michael said, <em>"We mainly focus on the compute side of things, but really anything related to running Kubernetes on Windows is in scope for SIG-Windows."</em></p><hr><p>In order to best serve users, SIG-Windows works to make the Kubernetes user experience as consistent as possible for users of Windows and Linux. However some use cases simply only apply to one Operating System, and as such, the SIG-Windows group also works to create functionality that is unique to Windows-only workloads.</p><p>Many SIGs, or "Special Interest Groups" within Kubernetes have a narrow focus, allowing members to dive deep on a certain facet of the technology. While specific expertise is welcome, those interested in SIG-Windows will find it to be a great community to build broad understanding across many focus areas of Kubernetes. "Members from our SIG interface with storage, network, testing, cluster-lifecycle and others groups in Kubernetes."</p><h3 id=who-are-sig-windows-users>Who are SIG-Windows' Users?</h3><p>The best way to understand the technology a group makes, is often to understand who their customers or users are.</p><h4 id=a-majority-of-the-users-we-ve-interacted-with-have-business-critical-infrastructure-running-on-windows-developed-over-many-years-and-can-t-move-those-workloads-to-linux-for-various-reasons-cost-time-compliance-etc-the-sig-chairs-shared-by-transporting-those-workloads-into-windows-containers-and-running-them-in-kubernetes-they-are-able-to-quickly-modernize-their-infrastructure-and-help-migrate-it-to-the-cloud>"A majority of the users we've interacted with have business-critical infrastructure running on Windows developed over many years and can't move those workloads to Linux for various reasons (cost, time, compliance, etc)," the SIG chairs shared. "By transporting those workloads into Windows containers and running them in Kubernetes they are able to quickly modernize their infrastructure and help migrate it to the cloud."</h4><p>As anyone in the Kubernetes space can attest, companies around the world, in many different industries, see Kubernetes as their path to modernizing their infrastructure. Often this involves re-architecting or event totally re-inventing many of the ways they've been doing business. With the goal being to make their systems more scalable, more robust, and more ready for anything the future may bring. But not every application or workload can or should change the core operating system it runs on, so many teams need the ability to run containers at scale on Windows, or Linux, or both.</p><p>"Sometimes the driver to Windows containers is a modernization effort and sometimes it’s because of expiring hardware warranties or end-of-support cycles for the current operating system. Our efforts in SIG-Windows enable Windows developers to take advantage of cloud native tools and Kubernetes to build and deploy distributed applications faster. That’s exciting! In essence, users can retain the benefits of application availability while decreasing costs."</p><h2 id=who-are-sig-windows>Who are SIG-Windows?</h2><p>Who are these contributors working on enabling Windows workloads for Kubernetes? It could be you!</p><p>Like with other Kubernetes SIGs, contributors to SIG-Windows can be anyone from independent hobbyists to professionals who work at many different companies. They come from many different parts of the world and bring to the table many different skill sets.</p><img alt="Image of several people chatting pleasantly" width=30% src=PeopleDoodle_transparent.png><p><em>"Like most other Kubernetes SIGs, we are a very welcome and open community," explained the SIG co-chairs Michael Michael and Mark Rosetti.</em></p><h3 id=becoming-a-contributor>Becoming a contributor</h3><p>For anyone interested in getting started, the co-chairs added, "New contributors can view old community meetings on GitHub (we record every single meeting going back three years), read our documentation, attend new community meetings, ask questions in person or on Slack, and file some issues on Github. We also attend all KubeCon conferences and host 1-2 sessions, a contributor session, and meet-the-maintainer office hours."</p><p>The co-chairs also shared a glimpse into what the path looks like to becoming a member of the SIG-Windows community:</p><p>"We encourage new contributors to initially just join our community and listen, then start asking some questions and get educated on Windows in Kubernetes. As they feel comfortable, they could graduate to improving our documentation, file some bugs/issues, and eventually they can be a code contributor by fixing some bugs. If they have long-term and sustained substantial contributions to Windows, they could become a technical lead or a chair of SIG-Windows. You won't know if you love this area unless you get started :) To get started, <a href=https://github.com/kubernetes/community/tree/master/sig-windows>visit this getting-started page</a>. It's a one stop shop with links to everything related to SIG-Windows in Kubernetes."</p><p>When asked if there were any useful skills for new contributors, the co-chairs said,</p><p>"We are always looking for expertise in Go and Networking and Storage, along with a passion for Windows. Those are huge skills to have. However, we don’t require such skills, and we welcome any and all contributors, with varying skill sets. If you don’t know something, we will help you acquire it."</p><p>You can get in touch with the folks at SIG-Windows in their <a href=https://kubernetes.slack.com/archives/C0SJ4AFB7>Slack channel</a> or attend one of their regular meetings - currently 30min long on Tuesdays at 12:30PM EST! You can find links to their regular meetings as well as past meeting notes and recordings from the <a href=https://github.com/kubernetes/community/tree/master/sig-windows#readme>SIG-Windows README</a> on GitHub.</p><p>As a closing message from SIG-Windows:</p><hr><h4 id=we-welcome-you-to-get-involved-and-join-our-community-to-share-feedback-and-deployment-stories-and-contribute-to-code-docs-and-improvements-of-any-kind><em>"We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind."</em></h4><hr></div><div class=td-content style=page-break-before:always><h1 id=pg-dc026ba2ad8a7e732e1a2228a0cdb179>Working with Terraform and Kubernetes</h1><div class="td-byline mb-4"><time datetime=2020-06-29 class=text-muted>Monday, June 29, 2020</time></div><p><strong>Author:</strong> <a href=https://twitter.com/pst418>Philipp Strube</a>, Kubestack</p><p>Maintaining Kubestack, an open-source <a href=https://www.kubestack.com/lp/terraform-gitops-framework>Terraform GitOps Framework</a> for Kubernetes, I unsurprisingly spend a lot of time working with Terraform and Kubernetes. Kubestack provisions managed Kubernetes services like AKS, EKS and GKE using Terraform but also integrates cluster services from Kustomize bases into the GitOps workflow. Think of cluster services as everything that's required on your Kubernetes cluster, before you can deploy application workloads.</p><p>Hashicorp recently announced <a href=https://www.hashicorp.com/blog/deploy-any-resource-with-the-new-kubernetes-provider-for-hashicorp-terraform/>better integration between Terraform and Kubernetes</a>. I took this as an opportunity to give an overview of how Terraform can be used with Kubernetes today and what to be aware of.</p><p>In this post I will however focus only on using Terraform to provision Kubernetes API resources, not Kubernetes clusters.</p><p><a href=https://www.terraform.io/intro/index.html>Terraform</a> is a popular infrastructure as code solution, so I will only introduce it very briefly here. In a nutshell, Terraform allows declaring a desired state for resources as code, and will determine and execute a plan to take the infrastructure from its current state, to the desired state.</p><p>To be able to support different resources, Terraform requires providers that integrate the respective API. So, to create Kubernetes resources we need a Kubernetes provider. Here are our options:</p><h2 id=terraform-kubernetes-provider-official>Terraform <code>kubernetes</code> provider (official)</h2><p>First, the <a href=https://github.com/hashicorp/terraform-provider-kubernetes>official Kubernetes provider</a>. This provider is undoubtedly the most mature of the three. However, it comes with a big caveat that's probably the main reason why using Terraform to maintain Kubernetes resources is not a popular choice.</p><p>Terraform requires a schema for each resource and this means the maintainers have to translate the schema of each Kubernetes resource into a Terraform schema. This is a lot of effort and was the reason why for a long time the supported resources where pretty limited. While this has improved over time, still not everything is supported. And especially <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/>custom resources</a> are not possible to support this way.</p><p>This schema translation also results in some edge cases to be aware of. For example, <code>metadata</code> in the Terraform schema is a list of maps. Which means you have to refer to the <code>metadata.name</code> of a Kubernetes resource like this in Terraform: <code>kubernetes_secret.example.metadata.0.name</code>.</p><p>On the plus side however, having a Terraform schema means full integration between Kubernetes and other Terraform resources. Like for <a href=https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/aws/_modules/eks/ingress.tf#L37>example</a>, using Terraform to create a Kubernetes service of type <code>LoadBalancer</code> and then use the returned ELB hostname in a Route53 record to configure DNS.</p><p>The biggest benefit when using Terraform to maintain Kubernetes resources is integration into the Terraform plan/apply life-cycle. So you can review planned changes before applying them. Also, using <code>kubectl</code>, purging of resources from the cluster is not trivial without manual intervention. Terraform does this reliably.</p><h2 id=terraform-kubernetes-alpha-provider>Terraform <code>kubernetes-alpha</code> provider</h2><p>Second, the new <a href=https://github.com/hashicorp/terraform-provider-kubernetes-alpha>alpha Kubernetes provider</a>. As a response to the limitations of the current Kubernetes provider the Hashicorp team recently released an alpha version of a new provider.</p><p>This provider uses dynamic resource types and server-side-apply to support all Kubernetes resources. I personally think this provider has the potential to be a game changer - even if <a href=https://github.com/hashicorp/terraform-provider-kubernetes-alpha#moving-from-yaml-to-hcl>managing Kubernetes resources in HCL</a> may still not be for everyone. Maybe the Kustomize provider below will help with that.</p><p>The only downside really is, that it's explicitly discouraged to use it for anything but testing. But the more people test it, the sooner it should be ready for prime time. So I encourage everyone to give it a try.</p><h2 id=terraform-kustomize-provider>Terraform <code>kustomize</code> provider</h2><p>Last, we have the <a href=https://github.com/kbst/terraform-provider-kustomize><code>kustomize</code> provider</a>. Kustomize provides a way to do customizations of Kubernetes resources using inheritance instead of templating. It is designed to output the result to <code>stdout</code>, from where you can apply the changes using <code>kubectl</code>. This approach means that <code>kubectl</code> edge cases like no purging or changes to immutable attributes still make full automation difficult.</p><p>Kustomize is a popular way to handle customizations. But I was looking for a more reliable way to automate applying changes. Since this is exactly what Terraform is great at the Kustomize provider was born.</p><p>Not going into too much detail here, but from Terraform's perspective, this provider treats every Kubernetes resource as a JSON string. This way it can handle any Kubernetes resource resulting from the Kustomize build. But it has the big disadvantage that Kubernetes resources can not easily be integrated with other Terraform resources. Remember the load balancer example from above.</p><p>Under the hood, similarly to the new Kubernetes alpha provider, the Kustomize provider also uses the dynamic Kubernetes client and server-side-apply. Going forward, I plan to deprecate this part of the Kustomize provider that overlaps with the new Kubernetes provider and only keep the Kustomize integration.</p><h2 id=conclusion>Conclusion</h2><p>For teams that are already invested into Terraform, or teams that are looking for ways to replace <code>kubectl</code> in automation, Terraform's plan/apply life-cycle has always been a promising option to automate changes to Kubernetes resources. However, the limitations of the official Kubernetes provider resulted in this not seeing significant adoption.</p><p>The new alpha provider removes the limitations and has the potential to make Terraform a prime option to automate changes to Kubernetes resources.</p><p>Teams that have already adopted Kustomize, may find integrating Kustomize and Terraform using the Kustomize provider beneficial over <code>kubectl</code> because it avoids common edge cases. Even if in this set up, Terraform can only easily be used to plan and apply the changes, not to adapt the Kubernetes resources. In the future, this issue may be resolved by combining the Kustomize provider with the new Kubernetes provider.</p><p>If you have any questions regarding these three options, feel free to reach out to me on the Kubernetes Slack in either the <a href=https://app.slack.com/client/T09NY5SBT/CMBCT7XRQ>#kubestack</a> or the <a href=https://app.slack.com/client/T09NY5SBT/C9A5ALABG>#kustomize</a> channel. If you happen to give any of the providers a try and encounter a problem, please file a GitHub issue to help the maintainers fix it.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3b8326c4fb648224fb9cbdd4289cf6e8>A Better Docs UX With Docsy</h1><div class="td-byline mb-4"><time datetime=2020-06-15 class=text-muted>Monday, June 15, 2020</time></div><p><strong>Author:</strong> Zach Corleissen, Cloud Native Computing Foundation</p><p><em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).</em></p><p>I'm pleased to announce that the <a href=https://kubernetes.io>Kubernetes website</a> now features the <a href=https://docsy.dev>Docsy Hugo theme</a>.</p><p>The Docsy theme improves the site's organization and navigability, and opens a path to improved API references. After over 4 years with few meaningful UX improvements, Docsy implements some best practices for technical content. The theme makes the Kubernetes site easier to read and makes individual pages easier to navigate. It gives the site a much-needed facelift.</p><p>For example: adding a right-hand rail for navigating topics on the page. No more scrolling up to navigate!</p><p>The theme opens a path for future improvements to the website. The Docsy functionality I'm most excited about is the theme's <a href=https://www.docsy.dev/docs/adding-content/shortcodes/#swaggerui><code>swaggerui</code> shortcode</a>, which provides native support for generating API references from an OpenAPI spec. The CNCF is partnering with <a href=https://developers.google.com/season-of-docs>Google Season of Docs</a> (GSoD) for staffing to make better API references a reality in Q4 this year. We're hopeful to be chosen, and we're looking forward to Google's list of announced projects on August 16th. Better API references have been a personal goal since I first started working with SIG Docs in 2017. It's exciting to see the goal within reach.</p><p>One of SIG Docs' tech leads, <a href=https://github.com/kbhawkey>Karen Bradshaw</a> did a lot of heavy lifting to fix a wide range of site compatibility issues, including a fix to the last of our <a href=https://github.com/kubernetes/website/pull/21359>legacy pieces</a> when we <a href=2018-05-05-hugo-migration/>migrated from Jekyll to Hugo</a> in 2018. Our other tech leads, <a href=https://github.com/sftim>Tim Bannister</a> and <a href=https://github.com/onlydole>Taylor Dolezal</a> provided extensive reviews.</p><p>Thanks also to <a href=https://bep.is/>Björn-Erik Pedersen</a>, who provided invaluable advice about how to navigate a Hugo upgrade beyond <a href=https://gohugo.io/news/0.60.0-relnotes/>version 0.60.0</a>.</p><p>The CNCF contracted with <a href=https://gearboxbuilt.com/>Gearbox</a> in Victoria, BC to apply the theme to the site. Thanks to Aidan, Troy, and the rest of the team for all their work!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-43dc00bef57d6128b69dd2afe2143f47>Supporting the Evolving Ingress Specification in Kubernetes 1.18</h1><div class="td-byline mb-4"><time datetime=2020-06-05 class=text-muted>Friday, June 05, 2020</time></div><p><strong>Authors:</strong> Alex Gervais (Datawire.io)</p><p>Earlier this year, the Kubernetes team released <a href=https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/>Kubernetes 1.18</a>, which extended Ingress. In this blog post, we’ll walk through what’s new in the new Ingress specification, what it means for your applications, and how to upgrade to an ingress controller that supports this new specification.</p><h3 id=what-is-kubernetes-ingress>What is Kubernetes Ingress</h3><p>When deploying your applications in Kubernetes, one of the first challenges many people encounter is how to get traffic into their cluster. <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Kubernetes ingress</a> is a collection of routing rules that govern how external users access services running in a Kubernetes cluster. There are <a href=https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d>three general approaches</a> for exposing your application:</p><ul><li>Using a <code>NodePort</code> to expose your application on a port across each of your nodes</li><li>Using a <code>LoadBalancer</code> service to create an external load balancer that points to a Kubernetes service in your cluster</li><li>Using a Kubernetes Ingress resource</li></ul><h3 id=what-s-new-in-kubernetes-1-18-ingress>What’s new in Kubernetes 1.18 Ingress</h3><p>There are three significant additions to the Ingress API in Kubernetes 1.18:</p><ul><li>A new <code>pathType</code> field</li><li>A new <code>IngressClass</code> resource</li><li>Support for wildcards in hostnames</li></ul><p>The new <code>pathType</code> field allows you to specify how Ingress paths should match.
The field supports three types: <code>ImplementationSpecific</code> (default), <code>exact</code>, and <code>prefix</code>. Explicitly defining the expected behavior of path matching will allow every ingress-controller to support a user’s needs and will increase portability between ingress-controller implementation solutions.</p><p>The <code>IngressClass</code> resource specifies how Ingresses should be implemented by controllers. This was added to formalize the commonly used but never standardized <code>kubernetes.io/ingress.class</code> annotation and allow for implementation-specific extensions and configuration.</p><p>You can read more about these changes, as well as the support for wildcards in hostnames in more detail in <a href=https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/>a previous blog post</a>.</p><h2 id=supporting-kubernetes-ingress>Supporting Kubernetes ingress</h2><p><a href=https://www.getambassador.io>Ambassador</a> is an open-source Envoy-based ingress controller. We believe strongly in supporting common standards such as Kubernetes ingress, which we adopted and <a href=https://blog.getambassador.io/ambassador-ingress-controller-better-config-reporting-updated-envoy-proxy-99dc9139e28f>announced our initial support for back in 2019</a>.</p><p>Every Ambassador release goes through rigorous testing. Therefore, we also contributed an <a href=https://github.com/kubernetes-sigs/ingress-controller-conformance>open conformance test suite</a>, supporting Kubernetes ingress. We wrote the initial bits of test code and will keep iterating over the newly added features and different versions of the Ingress specification as it evolves to a stable v1 GA release. Documentation and usage samples, is one of our top priorities. We understand how complex usage can be, especially when transitioning from a previous version of an API.</p><p>Following a test-driven development approach, the first step we took in supporting Ingress improvements in Ambassador was to translate the revised specification -- both in terms of API and behavior -- into a comprehensible test suite. The test suite, although still under heavy development and going through multiple iterations, was rapidly added to the Ambassador CI infrastructure and acceptance criteria. This means every change to the Ambassador codebase going forward will be compliant with the Ingress API and be tested end-to-end in a lightweight <a href=https://kind.sigs.k8s.io/>KIND cluster</a>. Using KIND allowed us to make rapid improvements while limiting our cloud provider infrastructure bill and testing out unreleased Kubernetes features with pre-release builds.</p><h3 id=adopting-a-new-specification>Adopting a new specification</h3><p>With a global comprehension of additions to Ingress introduced in Kubernetes 1.18 and a test suite on hand, we tackled the task of adapting the Ambassador code so that it would support translating the high-level Ingress API resources into Envoy configurations and constructs. Luckily Ambassador already supported previous versions of ingress functionalities so the development effort was incremental.</p><p>We settled on a controller name of <code>getambassador.io/ingress-controller</code>. This value, consistent with Ambassador's domain and CRD versions, must be used to tie in an IngressClass <code>spec.controller</code> with an Ambassador deployment. The new IngressClass resource allows for extensibility by setting a <code>spec.parameters</code> field. At the moment Ambassador makes no use of this field and its usage is reserved for future development.</p><p>Paths can now define different matching behaviors using the <code>pathType</code> field. The field will default to a value of <code>ImplementationSpecific</code>, which uses the same matching rules as the <a href=https://www.getambassador.io/docs/latest/topics/using/mappings/>Ambassador Mappings</a> prefix field and previous Ingress specification for backward compatibility reasons.</p><h3 id=kubernetes-ingress-controllers>Kubernetes Ingress Controllers</h3><p>A comprehensive <a href=https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>list of Kubernetes ingress controllers</a> is available in the Kubernetes documentation. Currently, Ambassador is the only ingress controller that supports these new additions to the ingress specification. Powered by the <a href=https://www.envoyproxy.io>Envoy Proxy</a>, Ambassador is the fastest way for you to try out the new ingress specification today.</p><p>Check out the following resources:</p><ul><li>Ambassador on <a href=https://www.github.com/datawire/ambassador>GitHub</a></li><li>The Ambassador <a href=https://www.getambassador.io/docs>documentation</a></li><li><a href=https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/>Improvements to the Ingress API</a></li></ul><p>Or join the community on <a href=http://d6e.co/slack>Slack</a>!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3c57eea59e90455a9ab22078c7fc1c3f>K8s KPIs with Kuberhealthy</h1><div class="td-byline mb-4"><time datetime=2020-05-29 class=text-muted>Friday, May 29, 2020</time></div><p><strong>Authors:</strong> Joshulyne Park (Comcast), Eric Greer (Comcast)</p><h3 id=building-onward-from-kuberhealthy-v2-0-0>Building Onward from Kuberhealthy v2.0.0</h3><p>Last November at KubeCon San Diego 2019, we announced the release of
<a href="https://www.youtube.com/watch?v=aAJlWhBtzqY">Kuberhealthy 2.0.0</a> - transforming Kuberhealthy into a Kubernetes operator
for synthetic monitoring. This new ability granted developers the means to create their own Kuberhealthy check
containers to synthetically monitor their applications and clusters. The community was quick to adopt this new feature and we're grateful for everyone who implemented and tested Kuberhealthy 2.0.0 in their clusters. Thanks to all of you who reported
issues and contributed to discussions on the #kuberhealthy Slack channel. We quickly set to work to address all your feedback
with a newer version of Kuberhealthy. Additionally, we created a guide on how to easily install and use Kuberhealthy in order to capture some helpful synthetic <a href=https://kpi.org/KPI-Basics>KPIs</a>.</p><h3 id=deploying-kuberhealthy>Deploying Kuberhealthy</h3><p>To install Kuberhealthy, make sure you have <a href=https://helm.sh/docs/intro/install/>Helm 3</a> installed. If not, you can use the generated flat spec files located
in this <a href=https://github.com/Comcast/kuberhealthy/tree/master/deploy>deploy folder</a>. You should use <a href=https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus.yaml>kuberhealthy-prometheus.yaml</a> if you don't use the <a href=https://github.com/coreos/prometheus-operator>Prometheus Operator</a>, and <a href=https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus-operator.yaml>kuberhealthy-prometheus-operator.yaml</a> if you do. If you don't use Prometheus at all, you can still use Kuberhealthy with a JSON status page and/or InfluxDB integration using <a href=https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy.yaml>this spec</a>.</p><h4 id=to-install-using-helm-3>To install using Helm 3:</h4><h5 id=1-create-namespace-kuberhealthy-in-the-desired-kubernetes-cluster-context>1. Create namespace "kuberhealthy" in the desired Kubernetes cluster/context:</h5><pre><code>kubectl create namespace kuberhealthy
</code></pre><h5 id=2-set-your-current-namespace-to-kuberhealthy>2. Set your current namespace to "kuberhealthy":</h5><pre><code>kubectl config set-context --current --namespace=kuberhealthy 
</code></pre><h5 id=3-add-the-kuberhealthy-repo-to-helm>3. Add the kuberhealthy repo to Helm:</h5><pre><code>helm repo add kuberhealthy https://comcast.github.io/kuberhealthy/helm-repos
</code></pre><h5 id=4-depending-on-your-prometheus-implementation-install-kuberhealthy-using-the-appropriate-command-for-your-cluster>4. Depending on your Prometheus implementation, install Kuberhealthy using the appropriate command for your cluster:</h5><ul><li>If you use the <a href=https://github.com/coreos/prometheus-operator>Prometheus Operator</a>:</li></ul><pre><code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true,prometheus.serviceMonitor=true
</code></pre><ul><li>If you use Prometheus, but NOT Prometheus Operator:</li></ul><pre><code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true
</code></pre><p>See additional details about configuring the appropriate scrape annotations in the section <a href=#prometheus-integration-details>Prometheus Integration Details</a> below.</p><ul><li>Finally, if you don't use Prometheus:</li></ul><pre><code>helm install kuberhealthy kuberhealthy/kuberhealthy
</code></pre><p>Running the Helm command should automatically install the newest version of Kuberhealthy (v2.2.0) along with a few basic checks. If you run <code>kubectl get pods</code>, you should see two Kuberhealthy pods. These are the pods that create, coordinate, and track test pods. These two Kuberhealthy pods also serve a JSON status page as well as a <code>/metrics</code> endpoint. Every other pod you see created is a checker pod designed to execute and shut down when done.</p><h3 id=configuring-additional-checks>Configuring Additional Checks</h3><p>Next, you can run <code>kubectl get khchecks</code>. You should see three Kuberhealthy checks installed by default:</p><ul><li><a href=https://github.com/Comcast/kuberhealthy/tree/master/cmd/daemonset-check>daemonset</a>: Deploys and tears down a daemonset to ensure all nodes in the cluster are functional.</li><li><a href=https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check>deployment</a>: Creates a deployment and then triggers a rolling update. Tests that the deployment is reachable via a service and then deletes everything. Any problem in this process will cause this check to report a failure.</li><li><a href=https://github.com/Comcast/kuberhealthy/tree/master/cmd/dns-resolution-check>dns-status-internal</a>: Validates that internal cluster DNS is functioning as expected.</li></ul><p>To view other available external checks, check out the <a href=https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECKS_REGISTRY.md>external checks registry</a> where you can find other yaml files you can apply to your cluster to enable various checks.</p><p>Kuberhealthy check pods should start running shortly after Kuberhealthy starts running (1-2 minutes). Additionally, the check-reaper cronjob runs every few minutes to ensure there are no more than 5 completed checker pods left lying around at a time.</p><p>To get status page view of these checks, you'll need to either expose the <code>kuberhealthy</code> service externally by editing the service <code>kuberhealthy</code> and setting <code>Type: LoadBalancer</code> or use <code>kubectl port-forward service/kuberhealthy 8080:80</code>. When viewed, the service endpoint will display a JSON status page that looks like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
    <span style=color:green;font-weight:700>&#34;OK&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
    <span style=color:green;font-weight:700>&#34;Errors&#34;</span>: [],
    <span style=color:green;font-weight:700>&#34;CheckDetails&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;kuberhealthy/daemonset&#34;</span>: {
            <span style=color:green;font-weight:700>&#34;OK&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
            <span style=color:green;font-weight:700>&#34;Errors&#34;</span>: [],
            <span style=color:green;font-weight:700>&#34;RunDuration&#34;</span>: <span style=color:#b44>&#34;22.512278967s&#34;</span>,
            <span style=color:green;font-weight:700>&#34;Namespace&#34;</span>: <span style=color:#b44>&#34;kuberhealthy&#34;</span>,
            <span style=color:green;font-weight:700>&#34;LastRun&#34;</span>: <span style=color:#b44>&#34;2020-04-06T23:20:31.7176964Z&#34;</span>,
            <span style=color:green;font-weight:700>&#34;AuthoritativePod&#34;</span>: <span style=color:#b44>&#34;kuberhealthy-67bf8c4686-mbl2j&#34;</span>,
            <span style=color:green;font-weight:700>&#34;uuid&#34;</span>: <span style=color:#b44>&#34;9abd3ec0-b82f-44f0-b8a7-fa6709f759cd&#34;</span>
        },
        <span style=color:green;font-weight:700>&#34;kuberhealthy/deployment&#34;</span>: {
            <span style=color:green;font-weight:700>&#34;OK&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
            <span style=color:green;font-weight:700>&#34;Errors&#34;</span>: [],
            <span style=color:green;font-weight:700>&#34;RunDuration&#34;</span>: <span style=color:#b44>&#34;29.142295647s&#34;</span>,
            <span style=color:green;font-weight:700>&#34;Namespace&#34;</span>: <span style=color:#b44>&#34;kuberhealthy&#34;</span>,
            <span style=color:green;font-weight:700>&#34;LastRun&#34;</span>: <span style=color:#b44>&#34;2020-04-06T23:20:31.7176964Z&#34;</span>,
            <span style=color:green;font-weight:700>&#34;AuthoritativePod&#34;</span>: <span style=color:#b44>&#34;kuberhealthy-67bf8c4686-mbl2j&#34;</span>,
            <span style=color:green;font-weight:700>&#34;uuid&#34;</span>: <span style=color:#b44>&#34;5f0d2765-60c9-47e8-b2c9-8bc6e61727b2&#34;</span>
        },
        <span style=color:green;font-weight:700>&#34;kuberhealthy/dns-status-internal&#34;</span>: {
            <span style=color:green;font-weight:700>&#34;OK&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
            <span style=color:green;font-weight:700>&#34;Errors&#34;</span>: [],
            <span style=color:green;font-weight:700>&#34;RunDuration&#34;</span>: <span style=color:#b44>&#34;2.43940936s&#34;</span>,
            <span style=color:green;font-weight:700>&#34;Namespace&#34;</span>: <span style=color:#b44>&#34;kuberhealthy&#34;</span>,
            <span style=color:green;font-weight:700>&#34;LastRun&#34;</span>: <span style=color:#b44>&#34;2020-04-06T23:20:44.6294547Z&#34;</span>,
            <span style=color:green;font-weight:700>&#34;AuthoritativePod&#34;</span>: <span style=color:#b44>&#34;kuberhealthy-67bf8c4686-mbl2j&#34;</span>,
            <span style=color:green;font-weight:700>&#34;uuid&#34;</span>: <span style=color:#b44>&#34;c85f95cb-87e2-4ff5-b513-e02b3d25973a&#34;</span>
        }
    },
    <span style=color:green;font-weight:700>&#34;CurrentMaster&#34;</span>: <span style=color:#b44>&#34;kuberhealthy-7cf79bdc86-m78qr&#34;</span>
}
</code></pre></div><p>This JSON page displays all Kuberhealthy checks running in your cluster. If you have Kuberhealthy checks running in different namespaces, you can filter them by adding the <code>GET</code> variable <code>namespace</code> parameter: <code>?namespace=kuberhealthy,kube-system</code> onto the status page URL.</p><h3 id=writing-your-own-checks>Writing Your Own Checks</h3><p>Kuberhealthy is designed to be extended with custom check containers that can be written by anyone to check anything. These checks can be written in any language as long as they are packaged in a container. This makes Kuberhealthy an excellent platform for creating your own synthetic checks!</p><p>Creating your own check is a great way to validate your client library, simulate real user workflow, and create a high level of confidence in your service or system uptime.</p><p>To learn more about writing your own checks, along with simple examples, check the <a href=https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECK_CREATION.md>custom check creation</a> documentation.</p><h3 id=prometheus-integration-details>Prometheus Integration Details</h3><p>When enabling Prometheus (not the operator), the Kuberhealthy service gets the following annotations added:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-.env data-lang=.env>prometheus.io/path: /metrics
prometheus.io/port: <span style=color:#b44>&#34;80&#34;</span>
prometheus.io/scrape: <span style=color:#b44>&#34;true&#34;</span>
</code></pre></div><p>In your prometheus configuration, add the following example scrape_config that scrapes the Kuberhealthy service given the added prometheus annotation:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>- <span style=color:green;font-weight:700>job_name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;kuberhealthy&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scrape_interval</span>:<span style=color:#bbb> </span>1m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>honor_labels</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metrics_path</span>:<span style=color:#bbb> </span>/metrics<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubernetes_sd_configs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>service<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespaces</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>names</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- kuberhealthy<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>relabel_configs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>source_labels</span>:<span style=color:#bbb> </span>[__meta_kubernetes_service_annotation_prometheus_io_scrape]<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>action</span>:<span style=color:#bbb> </span>keep<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>regex</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>You can also specify the target endpoint to be scraped using this example job:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>- <span style=color:green;font-weight:700>job_name</span>:<span style=color:#bbb> </span>kuberhealthy<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scrape_interval</span>:<span style=color:#bbb> </span>1m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>honor_labels</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metrics_path</span>:<span style=color:#bbb> </span>/metrics<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>static_configs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>targets</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- kuberhealthy.kuberhealthy.svc.cluster.local:80<span style=color:#bbb>
</span></code></pre></div><p>Once the appropriate prometheus configurations are applied, you should be able to see the following Kuberhealthy metrics:</p><ul><li><code>kuberhealthy_check</code></li><li><code>kuberhealthy_check_duration_seconds</code></li><li><code>kuberhealthy_cluster_states</code></li><li><code>kuberhealthy_running</code></li></ul><h3 id=creating-key-performance-indicators>Creating Key Performance Indicators</h3><p>Using these Kuberhealthy metrics, our team has been able to collect KPIs based on the following definitions, calculations, and PromQL queries.</p><p><em>Availability</em></p><p>We define availability as the K8s cluster control plane being up and functioning as expected. This is measured by our ability to create a deployment, do a rolling update, and delete the deployment within a set period of time.</p><p>We calculate this by measuring Kuberhealthy's <a href=https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check>deployment check</a> successes and failures.</p><ul><li><p>Availability = Uptime / (Uptime * Downtime)</p></li><li><p>Uptime = Number of Deployment Check Passes * Check Run Interval</p></li><li><p>Downtime = Number of Deployment Check Fails * Check Run Interval</p></li><li><p>Check Run Interval = how often the check runs (<code>runInterval</code> set in your KuberhealthyCheck Spec)</p></li><li><p>PromQL Query (Availability % over the past 30 days):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-promql data-lang=promql><span style=color:#666>1</span><span style=color:#bbb> </span><span style=color:#666>-</span><span style=color:#bbb> </span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>sum</span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>count_over_time</span><span style=color:#666>(</span><span style=color:#b8860b>kuberhealthy_check</span>{<span style=color:#a0a000>check</span><span style=color:#666>=</span>&#34;<span style=color:#b44>kuberhealthy/deployment</span>&#34;,<span style=color:#bbb> </span><span style=color:#a0a000>status</span><span style=color:#666>=</span>&#34;<span style=color:#b44>0</span>&#34;}[<span style=color:#b44>30d</span>]<span style=color:#666>))</span><span style=color:#bbb> </span><span style=color:#b8860b>OR</span><span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>vector</span><span style=color:#666>(</span><span style=color:#666>0</span><span style=color:#666>))</span><span style=color:#bbb> </span><span style=color:#666>/</span><span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>sum</span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>count_over_time</span><span style=color:#666>(</span><span style=color:#b8860b>kuberhealthy_check</span>{<span style=color:#a0a000>check</span><span style=color:#666>=</span>&#34;<span style=color:#b44>kuberhealthy/deployment</span>&#34;,<span style=color:#bbb> </span><span style=color:#a0a000>status</span><span style=color:#666>=</span>&#34;<span style=color:#b44>1</span>&#34;}[<span style=color:#b44>30d</span>]<span style=color:#666>))</span><span style=color:#bbb>
</span></code></pre></div></li></ul><p><em>Utilization</em></p><p>We define utilization as user uptake of product (k8s) and its resources (pods, services, etc.). This is measured by how many nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs are being utilized by our customers.
We calculate this by counting the total number of nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs.</p><p><em>Duration (Latency)</em></p><p>We define duration as the control plane's capacity and utilization of throughput. We calculate this by capturing the average run duration of a Kuberhealthy <a href=https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check>deployment check</a> run.</p><ul><li>PromQL Query (Deployment check average run duration):<div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-promql data-lang=promql><span style=color:#a2f;font-weight:700>avg</span><span style=color:#666>(</span><span style=color:#b8860b>kuberhealthy_check_duration_seconds</span>{<span style=color:#a0a000>check</span><span style=color:#666>=</span>&#34;<span style=color:#b44>kuberhealthy/deployment</span>&#34;}<span style=color:#666>)</span><span style=color:#bbb> 
</span></code></pre></div></li></ul><p><em>Errors / Alerts</em></p><p>We define errors as all k8s cluster and Kuberhealthy related alerts. Every time one of our Kuberhealthy check fails, we are alerted of this failure.</p><h3 id=thank-you>Thank You!</h3><p>Thanks again to everyone in the community for all of your contributions and help! We are excited to see what you build. As always, if you find an issue, have a feature request, or need to open a pull request, please <a href=https://github.com/Comcast/kuberhealthy/issues>open an issue</a> on the Github project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fde3199bf6bbc61752a7fbe6c3468c89>My exciting journey into Kubernetes’ history</h1><div class="td-byline mb-4"><time datetime=2020-05-28 class=text-muted>Thursday, May 28, 2020</time></div><p><strong>Author:</strong> Sascha Grunert, SUSE Software Solutions</p><p><em>Editor's note: Sascha is part of <a href=https://github.com/kubernetes/sig-release>SIG Release</a> and is working on many other
different container runtime related topics. Feel free to reach him out on
Twitter <a href=https://twitter.com/saschagrunert>@saschagrunert</a>.</em></p><hr><blockquote><p>A story of data science-ing 90,000 GitHub issues and pull requests by using
Kubeflow, TensorFlow, Prow and a fully automated CI/CD pipeline.</p></blockquote><ul><li><a href=#introduction>Introduction</a></li><li><a href=#getting-the-data>Getting the Data</a></li><li><a href=#exploring-the-data>Exploring the Data</a><ul><li><a href=#labels-labels-labels>Labels, Labels, Labels</a></li></ul></li><li><a href=#building-the-machine-learning-model>Building the Machine Learning Model</a><ul><li><a href=#doing-some-first-natural-language-processing-nlp>Doing some first Natural Language Processing (NLP)</a></li><li><a href=#creating-the-multi-layer-perceptron-mlp-model>Creating the Multi-Layer Perceptron (MLP) Model</a></li><li><a href=#training-the-model>Training the Model</a></li><li><a href=#a-first-prediction>A first Prediction</a></li></ul></li><li><a href=#automate-everything>Automate Everything</a></li><li><a href=#automatic-labeling-of-new-prs>Automatic Labeling of new PRs</a></li><li><a href=#summary>Summary</a></li></ul><h1 id=introduction>Introduction</h1><p>Choosing the right steps when working in the field of data science is truly no
silver bullet. Most data scientists might have their custom workflow, which
could be more or less automated, depending on their area of work. Using
<a href=https://kubernetes.io>Kubernetes</a> can be a tremendous enhancement when trying to automate
workflows on a large scale. In this blog post, I would like to take you on my
journey of doing data science while integrating the overall workflow into
Kubernetes.</p><p>The target of the research I did in the past few months was to find any
useful information about all those thousands of GitHub issues and pull requests
(PRs) we have in the <a href=https://github.com/kubernetes/kubernetes>Kubernetes repository</a>. What I ended up with was a
fully automated, in Kubernetes running Continuous Integration (CI) and
Deployment (CD) data science workflow powered by <a href=https://www.kubeflow.org>Kubeflow</a> and <a href=https://github.com/kubernetes/test-infra/tree/master/prow>Prow</a>.
You may not know both of them, but we get to the point where I explain what
they’re doing in detail. The source code of my work can be found in the
<a href=https://github.com/kubernetes-analysis/kubernetes-analysis>kubernetes-analysis GitHub repository</a>, which contains everything source
code-related as well as the raw data. But how to retrieve this data I’m talking
about? Well, this is where the story begins.</p><h1 id=getting-the-data>Getting the Data</h1><p>The foundation for my experiments is the raw GitHub API data in plain <a href=https://en.wikipedia.org/wiki/JSON>JSON</a>
format. The necessary data can be retrieved via the <a href=https://developer.github.com/v3/issues>GitHub issues
endpoint</a>, which returns all pull requests as well as regular issues in the
<a href=https://en.wikipedia.org/wiki/Representational_state_transfer>REST</a> API. I exported roughly <strong>91000</strong> issues and pull requests in
the first iteration into a massive <strong>650 MiB</strong> data blob. This took me about <strong>8
hours</strong> of data retrieval time because for sure, the GitHub API is <a href=https://developer.github.com/apps/building-github-apps/understanding-rate-limits-for-github-apps/>rate
limited</a>. To be able to put this data into a GitHub repository, I’d chosen
to compress it via <a href=https://linux.die.net/man/1/xz><code>xz(1)</code></a>. The result was a roundabout <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/data/api.tar.xz>25 MiB sized
tarball</a>, which fits well into the repository.</p><p>I had to find a way to regularly update the dataset because the Kubernetes
issues and pull requests are updated by the users over time as well as new ones
are created. To achieve the continuous update without having to wait 8 hours
over and over again, I now fetch the delta GitHub API data between the
<a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/.update>last update</a> and the current time. This way, a Continuous Integration job
can update the data on a regular basis, whereas I can continue my research with
the latest available set of data.</p><p>From a tooling perspective, I’ve written an <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/main>all-in-one Python executable</a>,
which allows us to trigger the different steps during the data science
experiments separately via dedicated subcommands. For example, to run an export
of the whole data set, we can call:</p><pre><code>&gt; export GITHUB_TOKEN=&lt;MY-SECRET-TOKEN&gt;
&gt; ./main export
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Dumping all issues
INFO | Pulling 90929 items
INFO | 1: Unit test coverage in Kubelet is lousy. (~30%)
INFO | 2: Better error messages if go isn't installed, or if gcloud is old.
INFO | 3: Need real cluster integration tests
INFO | 4: kubelet should know which containers it is managing
… [just wait 8 hours] …
</code></pre><p>To update the data between the last time stamp stored in the repository we can
run:</p><pre><code>&gt; ./main export --update-api
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Retrieving issues and PRs
INFO | Updating API
INFO | Got update timestamp: 2020-05-09T10:57:40.854151
INFO | 90786: Automated cherry pick of #90749: fix: azure disk dangling attach issue
INFO | 90674: Switch core master base images from debian to distroless
INFO | 90086: Handling error returned by request.Request.ParseForm()
INFO | 90544: configurable weight on the CPU and memory
INFO | 87746: Support compiling Kubelet w/o docker/docker
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Updating data
INFO | Updating issue 90786 (updated at 2020-05-09T10:59:43Z)
INFO | Updating issue 90674 (updated at 2020-05-09T10:58:27Z)
INFO | Updating issue 90086 (updated at 2020-05-09T10:58:26Z)
INFO | Updating issue 90544 (updated at 2020-05-09T10:57:51Z)
INFO | Updating issue 87746 (updated at 2020-05-09T11:01:51Z)
INFO | Saving data
</code></pre><p>This gives us an idea of how fast the project is actually moving: On a Saturday
at noon (European time), 5 issues and pull requests got updated within literally 5
minutes!</p><p>Funnily enough, <a href=https://github.com/jbeda>Joe Beda</a>, one of the founders of Kubernetes, created the
first GitHub issue <a href=https://github.com/kubernetes/kubernetes/issues/1>mentioning that the unit test coverage is too low</a>. The
issue has no further description than the title, and no enhanced labeling
applied, like we know from more recent issues and pull requests. But now we have
to explore the exported data more deeply to do something useful with it.</p><h1 id=exploring-the-data>Exploring the Data</h1><p>Before we can start creating machine learning models and train them, we have to
get an idea about how our data is structured and what we want to achieve in
general.</p><p>To get a better feeling about the amount of data, let’s look at how many issues
and pull requests have been created over time inside the Kubernetes repository:</p><pre><code>&gt; ./main analyze --created
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
</code></pre><p>The Python <a href=https://matplotlib.org>matplotlib</a> module should pop up a graph which looks like this:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-all.svg alt="created all"></p><p>Okay, this looks not that spectacular but gives us an impression on how the
project has grown over the past 6 years. To get a better idea about the speed of
development of the project, we can look at the <em>created-vs-closed</em> metric. This
means on our timeline, we add one to the y-axis if an issue or pull request got
created and subtracts one if closed. Now the chart looks like this:</p><pre><code>&gt; ./main analyze --created-vs-closed
</code></pre><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-all.svg alt="created vs closed all"></p><p>At the beginning of 2018, the Kubernetes projects introduced some more enhanced
life-cycle management via the glorious <a href=https://github.com/fejta-bot>fejta-bot</a>. This automatically
closes issues and pull requests after they got stale over a longer period of
time. This resulted in a massive closing of issues, which does not apply to pull
requests in the same amount. For example, if we look at the <em>created-vs-closed</em>
metric only for pull requests.</p><pre><code>&gt; ./main analyze --created-vs-closed --pull-requests
</code></pre><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-pull-requests.svg alt="created vs closed pull requests"></p><p>The overall impact is not that obvious. What we can see is that the increasing
number of peaks in the PR chart indicates that the project is moving faster over
time. Usually, a candlestick chart would be a better choice for showing this kind
of volatility-related information. I’d also like to highlight that it looks like
the development of the project slowed down a bit in the beginning of 2020.</p><p>Parsing raw JSON in every analysis iteration is not the fastest approach to do
in Python. This means that I decided to parse the more important information,
for example the content, title and creation time into dedicated <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/issue.py>issue</a> and
<a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/pull_request.py>PR classes</a>. This data will be <a href=https://docs.python.org/3/library/pickle.html>pickle</a> serialized into the repository
as well, which allows an overall faster startup independently of the JSON blob.</p><p>A pull request is more or less the same as an issue in my analysis, except that
it contains a release note.</p><p>Release notes in Kubernetes are written in the PRs description into a separate
<code>release-note</code> block like this:</p><pre><code>```release-note
I changed something extremely important and you should note that.
```
</code></pre><p>Those release notes are parsed by <a href=https://github.com/kubernetes/release#tools>dedicated Release Engineering Tools like
<code>krel</code></a> during the release creation process and will be part of the various
<a href=https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG>CHANGELOG.md</a> files and the <a href=https://relnotes.k8s.io>Release Notes Website</a>. That seems like a
lot of magic, but in the end, the quality of the overall release notes is much
higher because they’re easy to edit, and the PR reviewers can ensure that we
only document real user-facing changes and nothing else.</p><p>The quality of the input data is a key aspect when doing data science. I decided
to focus on the release notes because they seem to have the highest amount of
overall quality when comparing them to the plain descriptions in issues and PRs.
Besides that, they’re easy to parse, and we would not need to strip away
the <a href=https://github.com/kubernetes/kubernetes/tree/master/.github/ISSUE_TEMPLATE>various issue</a> and <a href=https://github.com/kubernetes/kubernetes/blob/master/.github/PULL_REQUEST_TEMPLATE.md>PR template</a> text noise.</p><h2 id=labels-labels-labels>Labels, Labels, Labels</h2><p>Issues and pull requests in Kubernetes get different labels applied during its
life-cycle. They are usually grouped via a single slash (<code>/</code>). For example, we
have <code>kind/bug</code> and <code>kind/api-change</code> or <code>sig/node</code> and <code>sig/network</code>. An easy
way to understand which label groups exist and how they’re distributed across
the repository is to plot them into a bar chart:</p><pre><code>&gt; ./main analyze --labels-by-group
</code></pre><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-group-all-top-25.svg alt="labels by group all top 25"></p><p>It looks like that <code>sig/</code>, <code>kind/</code> and <code>area/</code> labels are pretty common.
Something like <code>size/</code> can be ignored for now because these labels are
automatically applied based on the amount of the code changes for a pull
request. We said that we want to focus on release notes as input data, which
means that we have to check out the distribution of the labels for the PRs. This
means that the top 25 labels on pull requests are:</p><pre><code>&gt; ./main analyze --labels-by-name --pull-requests
</code></pre><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-name-pull-requests-top-25.svg alt="labels by name pull requests top 25"></p><p>Again, we can ignore labels like <code>lgtm</code> (looks good to me), because every PR
which now should get merged has to look good. Pull requests containing release
notes automatically get the <code>release-note</code> label applied, which enables further
filtering more easily. This does not mean that every PR containing that label
also contains the release notes block. The label could have been applied
manually and the parsing of the release notes block did not exist since the
beginning of the project. This means we will probably loose a decent amount of
input data on one hand. On the other hand we can focus on the highest possible
data quality, because applying labels the right way needs some enhanced maturity
of the project and its contributors.</p><p>From a label group perspective I have chosen to focus on the <code>kind/</code> labels.
Those labels are something which has to be applied manually by the author of the
PR, they are available on a good amount of pull requests and they’re related to
user-facing changes as well. Besides that, the <code>kind/</code> choice has to be done for
every pull request because it is part of the PR template.</p><p>Alright, how does the distribution of those labels look like when focusing only
on pull requests which have release notes?</p><pre><code>&gt; ./main analyze --release-notes-stats
</code></pre><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/release-notes-stats.svg alt="release notes stats"></p><p>Interestingly, we have approximately 7,000 overall pull requests containing
release notes, but only ~5,000 have a <code>kind/</code> label applied. The distribution of
the labels is not equal, and one-third of them are labeled as <code>kind/bug</code>. This
brings me to the next decision in my data science journey: I will build a binary
classifier which, for the sake of simplicity, is only able to distinguish between
bugs (via <code>kind/bug</code>) and non-bugs (where the label is not applied).</p><p>The main target is now to be able to classify newly incoming release notes if
they are related to a bug or not, based on the historical data we already have
from the community.</p><p>Before doing that, I recommend that you play around with the <code>./main analyze -h</code>
subcommand as well to explore the latest set of data. You can also check out all
the <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/assets>continuously updated assets</a> I provide within the analysis repository.
For example, those are the top 25 PR creators inside the Kubernetes repository:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/users-by-created-pull-requests-top-25.svg alt="users by created pull request"></p><h1 id=building-the-machine-learning-model>Building the Machine Learning Model</h1><p>Now we have an idea what the data set is about, and we can start building a first
machine learning model. Before actually building the model, we have to
pre-process all the extracted release notes from the PRs. Otherwise, the model
would not be able to understand our input.</p><h2 id=doing-some-first-natural-language-processing-nlp>Doing some first Natural Language Processing (NLP)</h2><p>In the beginning, we have to define a vocabulary for which we want to train. I
decided to choose the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>TfidfVectorizer</a> from the Python scikit-learn machine
learning library. This vectorizer is able to take our input texts and create a
single huge vocabulary out of it. This is our so-called <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words</a>,
which has a chosen n-gram range of <code>(1, 2)</code> (unigrams and bigrams). Practically
this means that we always use the first word and the next one as a single
vocabulary entry (bigrams). We also use the single word as vocabulary entry
(unigram). The TfidfVectorizer is able to skip words that occur multiple times
(<code>max_df</code>), and requires a minimum amount (<code>min_df</code>) to add a word to the
vocabulary. I decided not to change those values in the first place, just
because I had the intuition that release notes are something unique to a
project.</p><p>Parameters like <code>min_df</code>, <code>max_df</code> and the n-gram range can be seen as some of
our hyperparameters. Those parameters have to be optimized in a dedicated step
after the machine learning model has been built. This step is called
hyperparameter tuning and basically means that we train multiple times with
different parameters and compare the accuracy of the model. Afterwards, we choose
the parameters with the best accuracy.</p><p>During the training, the vectorizer will produce a <code>data/features.json</code> which
contains the whole vocabulary. This gives us a good understanding of how such a
vocabulary may look like:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>[
  <span>…</span>
  <span style=color:#b44>&#34;hostname&#34;</span>,
  <span style=color:#b44>&#34;hostname address&#34;</span>,
  <span style=color:#b44>&#34;hostname and&#34;</span>,
  <span style=color:#b44>&#34;hostname as&#34;</span>,
  <span style=color:#b44>&#34;hostname being&#34;</span>,
  <span style=color:#b44>&#34;hostname bug&#34;</span>,
  <span>…</span>
]
</code></pre></div><p>This produces round about 50,000 entries in the overall bag-of-words, which is
pretty much. Previous analyses between different data sets showed that it is
simply not necessary to take so many features into account. Some general data
sets state that an overall vocabulary of 20,000 is enough and higher amounts do
not influence the accuracy any more. To do so we can use the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html>SelectKBest</a>
feature selector to strip down the vocabulary to only choose the top features.
Anyway, I still decided to stick to the top 50,000 to not negatively influence
the model accuracy. We have a relatively low amount of data (appr. 7,000
samples) and a low number of words per sample (~15) which already made me wonder
if we have enough data at all.</p><p>The vectorizer is not only able to create our bag-of-words, but it is also able to
encode the features in <a href=https://en.wikipedia.org/wiki/Tf%e2%80%93idf>term frequency–inverse document frequency (tf-idf)</a>
format. That is where the vectorizer got its name, whereas the output of that
encoding is something the machine learning model can directly consume. All the
details of the vectorization process can be found in the <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L193-L235>source code</a>.</p><h2 id=creating-the-multi-layer-perceptron-mlp-model>Creating the Multi-Layer Perceptron (MLP) Model</h2><p>I decided to choose a simple MLP based model which is built with the help of the
popular <a href=https://www.tensorflow.org/api_docs/python/tf/keras>TensorFlow</a> framework. Because we do not have that much input data,
we just use two hidden layers, so that the model basically looks like this:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/model.png alt=model></p><p>There have to be <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L95-L100>multiple other</a> hyperparameters to be taken into account
when creating the model. I will not discuss them in detail here, but they’re
important to be optimized also in relation to the number of classes we want to
have in the model (only two in our case).</p><h2 id=training-the-model>Training the Model</h2><p>Before starting the actual training, we have to split up our input data into
training and validation data sets. I’ve chosen to use ~80% of the data for
training and 20% for validation purposes. We have to shuffle our input data as
well to ensure that the model is not affected by ordering issues. The technical
details of the training process can be found in the <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L91-L170>GitHub sources</a>. So now
we’re ready to finally start the training:</p><pre><code>&gt; ./main train
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Training for label 'kind/bug'
INFO | 6980 items selected
INFO | Using 5584 training and 1395 testing texts
INFO | Number of classes: 2
INFO | Vocabulary len: 51772
INFO | Wrote features to file data/features.json
INFO | Using units: 1
INFO | Using activation function: sigmoid
INFO | Created model with 2 layers and 64 units
INFO | Compiling model
INFO | Starting training
Train on 5584 samples, validate on 1395 samples
Epoch 1/1000
5584/5584 - 3s - loss: 0.6895 - acc: 0.6789 - val_loss: 0.6856 - val_acc: 0.6860
Epoch 2/1000
5584/5584 - 2s - loss: 0.6822 - acc: 0.6827 - val_loss: 0.6782 - val_acc: 0.6860
Epoch 3/1000
…
Epoch 68/1000
5584/5584 - 2s - loss: 0.2587 - acc: 0.9257 - val_loss: 0.4847 - val_acc: 0.7728
INFO | Confusion matrix:
[[920  32]
 [291 152]]
INFO | Confusion matrix normalized:
[[0.966 0.034]
 [0.657 0.343]]
INFO | Saving model to file data/model.h5
INFO | Validation accuracy: 0.7727598547935486, loss: 0.48470408514836355
</code></pre><p>The output of the <a href=https://en.wikipedia.org/wiki/Confusion_matrix>Confusion Matrix</a> shows us that we’re pretty good on
training accuracy, but the validation accuracy could be a bit higher. We now
could start a hyperparameter tuning to see if we can optimize the output of the
model even further. I will leave that experiment up to you with the hint to the
<code>./main train --tune</code> flag.</p><p>We saved the model (<code>data/model.h5</code>), the vectorizer (<code>data/vectorizer.pickle</code>)
and the feature selector (<code>data/selector.pickle</code>) to disk to be able to use them
later on for prediction purposes without having a need for additional training
steps.</p><h2 id=a-first-prediction>A first Prediction</h2><p>We are now able to test the model by loading it from disk and predicting some
input text:</p><pre><code>&gt; ./main predict --test
INFO | Testing positive text:

        Fix concurrent map access panic
        Don't watch .mount cgroups to reduce number of inotify watches
        Fix NVML initialization race condition
        Fix brtfs disk metrics when using a subdirectory of a subvolume

INFO | Got prediction result: 0.9940581321716309
INFO | Matched expected positive prediction result
INFO | Testing negative text:

        action required
        1. Currently, if users were to explicitly specify CacheSize of 0 for
           KMS provider, they would end-up with a provider that caches up to
           1000 keys. This PR changes this behavior.
           Post this PR, when users supply 0 for CacheSize this will result in
           a validation error.
        2. CacheSize type was changed from int32 to *int32. This allows
           defaulting logic to differentiate between cases where users
           explicitly supplied 0 vs. not supplied any value.
        3. KMS Provider's endpoint (path to Unix socket) is now validated when
           the EncryptionConfiguration files is loaded. This used to be handled
           by the GRPCService.

INFO | Got prediction result: 0.1251964420080185
INFO | Matched expected negative prediction result
</code></pre><p>Both tests are real-world examples which already exist. We could also try
something completely different, like this random tweet I found a couple of
minutes ago:</p><pre><code>./main predict &quot;My dudes, if you can understand SYN-ACK, you can understand consent&quot;
INFO  | Got prediction result: 0.1251964420080185
ERROR | Result is lower than selected threshold 0.6
</code></pre><p>Looks like it is not classified as bug for a release note, which seems to work.
Selecting a good threshold is also not that easy, but sticking to something >
50% should be the bare minimum.</p><h1 id=automate-everything>Automate Everything</h1><p>The next step is to find some way of automation to continuously update the model
with new data. If I change any source code within my repository, then I’d like
to get feedback about the test results of the model without having a need to run
the training on my own machine. I would like to utilize the GPUs in my
Kubernetes cluster to train faster and automatically update the data set if a PR
got merged.</p><p>With the help of <a href=https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview>Kubeflow pipelines</a> we can fulfill most of these
requirements. The pipeline I built looks like this:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/kubeflow-pipeline.png alt=pipeline></p><p>First, we check out the source code of the PR, which will be passed on as output
artifact to all other steps. Then we incrementally update the API and internal
data before we run the training on an always up-to-date data set. The prediction
test verifies after the training that we did not badly influence the model with
our changes.</p><p>We also built a container image within our pipeline. <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/Dockerfile-deploy>This container image</a>
copies the previously built model, vectorizer, and selector into a container and
runs <code>./main serve</code>. When doing this, we spin up a <a href=https://www.kubeflow.org/docs/components/serving/kfserving>kfserving</a> web server,
which can be used for prediction purposes. Do you want to try it out by yourself? Simply
do a JSON POST request like this and run the prediction against the endpoint:</p><pre><code>&gt; curl https://kfserving.k8s.saschagrunert.de/v1/models/kubernetes-analysis:predict \
    -d '{&quot;text&quot;: &quot;my test text&quot;}'
{&quot;result&quot;: 0.1251964420080185}
</code></pre><p>The <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/kfserver.py>custom kfserving</a> implementation is pretty straightforward, whereas the
deployment utilizes <a href=https://knative.dev/docs/serving>Knative Serving</a> and an <a href=https://istio.io>Istio</a> ingress gateway
under the hood to correctly route the traffic into the cluster and provide the
right set of services.</p><p>The <code>commit-changes</code> and <code>rollout</code> step will only run if the pipeline runs on
the <code>master</code> branch. Those steps make sure that we always have the latest data
set available on the master branch as well as in the kfserving deployment. The
<a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/rollout.py#L30-L51>rollout step</a> creates a new canary deployment, which only accepts 50% of the
incoming traffic in the first place. After the canary got deployed successfully,
it will be promoted as the new main instance of the service. This is a great way
to ensure that the deployment works as intended and allows additional testing
after rolling out the canary.</p><p>But how to trigger Kubeflow pipelines when creating a pull request? Kubeflow has
no feature for that right now. That’s why I decided to use <a href=https://github.com/kubernetes/test-infra/tree/master/prow>Prow</a>,
Kubernetes test-infrastructure project for CI/CD purposes.</p><p>First of all, a <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/ci/config.yaml#L45-L61>24h periodic job</a> ensures that we have at least daily
up-to-date data available within the repository. Then, if we create a pull
request, Prow will run the whole Kubeflow pipeline without committing or rolling
out any changes. If we merge the pull request via Prow, another job runs on the
master and updates the data as well as the deployment. That’s pretty neat, isn’t
it?</p><h1 id=automatic-labeling-of-new-prs>Automatic Labeling of new PRs</h1><p>The prediction API is nice for testing, but now we need a real-world use case.
Prow supports external plugins which can be used to take action on any GitHub
event. I wrote <a href=https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/pkg>a plugin</a> which uses the kfserving API to make predictions
based on new pull requests. This means if we now create a new pull request in
the kubernetes-analysis repository, we will see the following:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-1.png alt="pr 1"></p><hr><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-2.png alt="pr 2"></p><p>Okay cool, so now let’s change the release note based on a real bug from the
already existing dataset:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-3.png alt="pr 3"></p><hr><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-4.png alt="pr 4"></p><p>The bot edits its own comment, predicts it with round about 90% as <code>kind/bug</code>
and automatically adds the correct label! Now, if we change it back to some
different - obviously wrong - release note:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-5.png alt="pr 5"></p><hr><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-6.png alt="pr 6"></p><p>The bot does the work for us, removes the label and informs us what it did!
Finally, if we change the release note to <code>None</code>:</p><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-7.png alt="pr 7"></p><hr><p><img src=/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-8.png alt="pr 8"></p><p>The bot removed the comment, which is nice and reduces the text noise on the PR.
Everything I demonstrated is running inside a single Kubernetes cluster, which
would make it unnecessary at all to expose the kfserving API to the public. This
introduces an indirect API rate limiting because the only usage would be
possible via the Prow bot user.</p><p>If you want to try it out for yourself, feel free to open a <a href="https://github.com/kubernetes-analysis/kubernetes-analysis/issues/new?&template=release-notes-test.md">new test
issue</a> in <code>kubernetes-analysis</code>. This works because I enabled the plugin
also for issues rather than only for pull requests.</p><p>So then, we have a running CI bot which is able to classify new release notes
based on a machine learning model. If the bot would run in the official
Kubernetes repository, then we could correct wrong label predictions manually.
This way, the next training iteration would pick up the correction and result in
a continuously improved model over time. All totally automated!</p><h1 id=summary>Summary</h1><p>Thank you for reading down to here! This was my little data science journey
through the Kubernetes GitHub repository. There are a lot of other things to
optimize, for example introducing more classes (than just <code>kind/bug</code> or nothing)
or automatic hyperparameter tuning with Kubeflows <a href=https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter>Katib</a>. If you have any
questions or suggestions, then feel free to get in touch with me anytime. See you
soon!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d33d7145d9c3587598bfedee38e4dc48>An Introduction to the K8s-Infrastructure Working Group</h1><div class="td-byline mb-4"><time datetime=2020-05-27 class=text-muted>Wednesday, May 27, 2020</time></div><p><strong>Author</strong>: <a href=https://twitter.com/kiran_oliver>Kiran "Rin" Oliver</a> Storyteller, Kubernetes Upstream Marketing Team</p><h1 id=an-introduction-to-the-k8s-infrastructure-working-group>An Introduction to the K8s-Infrastructure Working Group</h1><p><em>Welcome to part one of a new series introducing the K8s-Infrastructure working group!</em></p><p>When Kubernetes was formed in 2014, Google undertook the task of building and maintaining the infrastructure necessary for keeping the project running smoothly. The tools itself were open source, but the Google Cloud Platform project used to run the infrastructure was internal-only, preventing contributors from being able to help out. In August 2018, Google granted the Cloud Native Computing Foundation <a href=https://cloud.google.com/blog/products/gcp/google-cloud-grants-9m-in-credits-for-the-operation-of-the-kubernetes-project>$9M in credits for the operation of Kubernetes</a>. The sentiment behind this was that a project such as Kubernetes should be both maintained and operated by the community itself rather than by a single vendor.</p><p>A group of community members enthusiastically undertook the task of collaborating on the path forward, realizing that there was a <a href=https://github.com/kubernetes/community/issues/2715>more formal infrastructure necessary</a>. They joined together as a cross-team working group with ownership spanning across multiple Kubernetes SIGs (Architecture, Contributor Experience, Release, and Testing). <a href=https://twitter.com/spiffxp>Aaron Crickenberger</a> worked with the Kubernetes Steering Committee to enable the formation of the working group, co-drafting a charter alongside long-time collaborator <a href=https://twitter.com/dims>Davanum Srinivas</a>, and by 2019 the working group was official.</p><h2 id=what-issues-does-the-k8s-infrastructure-working-group-tackle>What Issues Does the K8s-Infrastructure Working Group Tackle?</h2><p>The team took on the complex task of managing the many moving parts of the infrastructure that sustains Kubernetes as a project.</p><p>The need started with necessity: the first problem they took on was a complete migration of all of the project's infrastructure from Google-owned infrastructure to the Cloud Native Computing Foundation (CNCF). This is being done so that the project is self-sustainable without the need of any direct assistance from individual vendors. This breaks down in the following ways:</p><ul><li>Identifying what infrastructure the Kubernetes project depends on.<ul><li>What applications are running?</li><li>Where does it run?</li><li>Where is its source code?</li><li>What is custom built?</li><li>What is off-the-shelf?</li><li>What services depend on each other?</li><li>How is it administered?</li></ul></li><li>Documenting guidelines and policies for how to run the infrastructure as a community.<ul><li>What are our access policies?</li><li>How do we keep track of billing?</li><li>How do we ensure privacy and security?</li></ul></li><li>Migrating infrastructure over to the CNCF as-is.<ul><li>What is the path of least resistance to migration?</li></ul></li><li>Improving the state of the infrastructure for sustainability.<ul><li>Moving from humans running scripts to a more automated GitOps model (YAML all the things!)</li><li>Supporting community members who wish to develop new infrastructure</li></ul></li><li>Documenting the state of our efforts, better defining goals, and completeness indicators.<ul><li>The project and program management necessary to communicate this work to our <a href=https://kubernetes.io/blog/2020/04/21/contributor-communication/>massive community of contributors</a></li></ul></li></ul><h2 id=the-challenge-of-k8s-infrastructure-is-documentation>The challenge of K8s-Infrastructure is documentation</h2><p>The most crucial problem the working group is trying to tackle is that the project is all volunteer-led. This leads to contributors, chairs, and others involved in the project quickly becoming overscheduled. As a result of this, certain areas such as documentation and organization often lack information, and efforts to progress are taking longer than the group would like to complete.</p><p>Some of the infrastructure that is being migrated over hasn't been updated in a while, and its original authors or directly responsible individuals have moved on from working on Kubernetes. While this is great from the perspective of the fact that the code was able to run untouched for a long period of time, from the perspective of trying to migrate, this makes it difficult to identify how to operate these components, and how to move these infrastructure pieces where they need to be effectively.</p><p>The lack of documentation is being addressed head-on by group member <a href=https://twitter.com/bartsmykla>Bart Smykla</a>, but there is a definite need for others to support. If you're looking for a way to <a href=https://github.com/kubernetes/community/labels/wg%2Fk8s-infra>get involved</a> and learn the infrastructure, you can become a new contributor to the working group!</p><h2 id=celebrating-some-working-group-wins>Celebrating some Working Group wins</h2><p>The team has made progress in the last few months that is well worth celebrating.</p><ul><li>The K8s-Infrastructure Working Group released an automated billing report that they start every meeting off by reviewing as a group.</li><li>DNS for k8s.io and kubernetes.io are also fully <a href=https://groups.google.com/g/kubernetes-dev/c/LZTYJorGh7c/m/u-ydk-yNEgAJ>community-owned</a>, with community members able to <a href="https://github.com/kubernetes/k8s.io/issues/new?assignees=&labels=wg%2Fk8s-infra&template=dns-request.md&title=DNS+REQUEST%3A+%3Cyour-dns-record%3E">file issues</a> to manage records.</li><li>The container registry <a href=https://github.com/kubernetes/k8s.io/tree/master/k8s.gcr.io>k8s.gcr.io</a> is also fully community-owned and available for all Kubernetes subprojects to use.</li><li>The Kubernetes <a href=https://github.com/kubernetes/publishing-bot>publishing-bot</a> responsible for keeping k8s.io/kubernetes/staging repositories published to their own top-level repos (For example: <a href=https://github.com/kubernetes/api>kubernetes/api</a>) runs on a community-owned cluster.</li><li>The gcsweb.k8s.io service used to provide anonymous access to GCS buckets for kubernetes artifacts runs on a community-owned cluster.</li><li>There is also an automated process of promoting all our container images. This includes a fully documented infrastructure, managed by the Kubernetes community, with automated processes for provisioning permissions.</li></ul><p>These are just a few of the things currently happening in the K8s Infrastructure working group.</p><p>If you're interested in getting involved, be sure to join the <a href=https://app.slack.com/client/T09NY5SBT/CCK68P2Q2>#wg-K8s-infra Slack Channel</a>. Meetings are 60 minutes long, and are held every other Wednesday at 8:30 AM PT/16:30 UTC.</p><p>Join to help with the documentation, stay to learn about the amazing infrastructure supporting the Kubernetes community.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fe388f92f881375f927332275d292981>WSL+Docker: Kubernetes on the Windows Desktop</h1><div class="td-byline mb-4"><time datetime=2020-05-21 class=text-muted>Thursday, May 21, 2020</time></div><p><strong>Authors</strong>: <a href=https://twitter.com/nunixtech>Nuno do Carmo</a> Docker Captain and WSL Corsair; <a href=https://twitter.com/idvoretskyi>Ihor Dvoretskyi</a>, Developer Advocate, Cloud Native Computing Foundation</p><h1 id=introduction>Introduction</h1><p>New to Windows 10 and WSL2, or new to Docker and Kubernetes? Welcome to this blog post where we will install from scratch Kubernetes in Docker <a href=https://kind.sigs.k8s.io/>KinD</a> and <a href=https://minikube.sigs.k8s.io/docs/>Minikube</a>.</p><h1 id=why-kubernetes-on-windows>Why Kubernetes on Windows?</h1><p>For the last few years, Kubernetes became a de-facto standard platform for running containerized services and applications in distributed environments. While a wide variety of distributions and installers exist to deploy Kubernetes in the cloud environments (public, private or hybrid), or within the bare metal environments, there is still a need to deploy and run Kubernetes locally, for example, on the developer's workstation.</p><p>Kubernetes has been originally designed to be deployed and used in the Linux environments. However, a good number of users (and not only application developers) use Windows OS as their daily driver. When Microsoft revealed WSL - <a href=https://docs.microsoft.com/en-us/windows/wsl/>the Windows Subsystem for Linux</a>, the line between Windows and Linux environments became even less visible.</p><p>Also, WSL brought an ability to run Kubernetes on Windows almost seamlessly!</p><p>Below, we will cover in brief how to install and use various solutions to run Kubernetes locally.</p><h1 id=prerequisites>Prerequisites</h1><p>Since we will explain how to install KinD, we won't go into too much detail around the installation of KinD's dependencies.</p><p>However, here is the list of the prerequisites needed and their version/lane:</p><ul><li>OS: Windows 10 version 2004, Build 19041</li><li><a href=https://docs.microsoft.com/en-us/windows/wsl/wsl2-install>WSL2 enabled</a><ul><li>In order to install the distros as WSL2 by default, once WSL2 installed, run the command <code>wsl.exe --set-default-version 2</code> in Powershell</li></ul></li><li>WSL2 distro installed from the Windows Store - the distro used is Ubuntu-18.04</li><li><a href=https://hub.docker.com/editions/community/docker-ce-desktop-windows>Docker Desktop for Windows</a>, stable channel - the version used is 2.2.0.4</li><li>[Optional] Microsoft Terminal installed from the Windows Store<ul><li>Open the Windows store and type "Terminal" in the search, it will be (normally) the first option</li></ul></li></ul><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-windows-store-terminal.png alt="Windows Store Terminal"></p><p>And that's actually it. For Docker Desktop for Windows, no need to configure anything yet as we will explain it in the next section.</p><h1 id=wsl2-first-contact>WSL2: First contact</h1><p>Once everything is installed, we can launch the WSL2 terminal from the Start menu, and type "Ubuntu" for searching the applications and documents:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-start-menu-search.png alt="Start Menu Search"></p><p>Once found, click on the name and it will launch the default Windows console with the Ubuntu bash shell running.</p><p>Like for any normal Linux distro, you need to create a user and set a password:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-user-password.png alt=User-Password></p><h2 id=optional-update-the-sudoers>[Optional] Update the <code>sudoers</code></h2><p>As we are working, normally, on our local computer, it might be nice to update the <code>sudoers</code> and set the group <code>%sudo</code> to be password-less:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Edit the sudoers with the visudo command</span>
sudo visudo

<span style=color:#080;font-style:italic># Change the %sudo group to be password-less</span>
%sudo   <span style=color:#b8860b>ALL</span><span style=color:#666>=(</span>ALL:ALL<span style=color:#666>)</span> NOPASSWD: ALL

<span style=color:#080;font-style:italic># Press CTRL+X to exit</span>
<span style=color:#080;font-style:italic># Press Y to save</span>
<span style=color:#080;font-style:italic># Press Enter to confirm</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-visudo.png alt=visudo></p><h2 id=update-ubuntu>Update Ubuntu</h2><p>Before we move to the Docker Desktop settings, let's update our system and ensure we start in the best conditions:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Update the repositories and list of the packages available</span>
sudo apt update
<span style=color:#080;font-style:italic># Update the system based on the packages installed &gt; the &#34;-y&#34; will approve the change automatically</span>
sudo apt upgrade -y
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-apt-update-upgrade.png alt=apt-update-upgrade></p><h1 id=docker-desktop-faster-with-wsl2>Docker Desktop: faster with WSL2</h1><p>Before we move into the settings, let's do a small test, it will display really how cool the new integration with Docker Desktop is:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Try to see if the docker cli and daemon are installed</span>
docker version
<span style=color:#080;font-style:italic># Same for kubectl</span>
kubectl version
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-error.png alt=kubectl-error></p><p>You got an error? Perfect! It's actually good news, so let's now move on to the settings.</p><h2 id=docker-desktop-settings-enable-wsl2-integration>Docker Desktop settings: enable WSL2 integration</h2><p>First let's start Docker Desktop for Windows if it's not still the case. Open the Windows start menu and type "docker", click on the name to start the application:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-start.png alt=docker-start></p><p>You should now see the Docker icon with the other taskbar icons near the clock:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-taskbar.png alt=docker-taskbar></p><p>Now click on the Docker icon and choose settings. A new window will appear:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-general.png alt=docker-settings-general></p><p>By default, the WSL2 integration is not active, so click the "Enable the experimental WSL 2 based engine" and click "Apply & Restart":</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-wsl2-activated.png alt=docker-settings-wsl2></p><p>What this feature did behind the scenes was to create two new distros in WSL2, containing and running all the needed backend sockets, daemons and also the CLI tools (read: docker and kubectl command).</p><p>Still, this first setting is still not enough to run the commands inside our distro. If we try, we will have the same error as before.</p><p>In order to fix it, and finally be able to use the commands, we need to tell the Docker Desktop to "attach" itself to our distro also:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-resources-wsl-integration.png alt=docker-resources-wsl></p><p>Let's now switch back to our WSL2 terminal and see if we can (finally) launch the commands:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Try to see if the docker cli and daemon are installed</span>
docker version
<span style=color:#080;font-style:italic># Same for kubectl</span>
kubectl version
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-success.png alt=docker-kubectl-success></p><blockquote><p>Tip: if nothing happens, restart Docker Desktop and restart the WSL process in Powershell: <code>Restart-Service LxssManager</code> and launch a new Ubuntu session</p></blockquote><p>And success! The basic settings are now done and we move to the installation of KinD.</p><h1 id=kind-kubernetes-made-easy-in-a-container>KinD: Kubernetes made easy in a container</h1><p>Right now, we have Docker that is installed, configured and the last test worked fine.</p><p>However, if we look carefully at the <code>kubectl</code> command, it found the "Client Version" (1.15.5), but it didn't find any server.</p><p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install KinD and create our first cluster.</p><p>And as sources are always important to mention, we will follow (partially) the how-to on the <a href=https://kind.sigs.k8s.io/docs/user/quick-start/>official KinD website</a>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Download the latest version of KinD</span>
curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-linux-amd64
<span style=color:#080;font-style:italic># Make the binary executable</span>
chmod +x ./kind
<span style=color:#080;font-style:italic># Move the binary to your executable path</span>
sudo mv ./kind /usr/local/bin/
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install.png alt=kind-install></p><h2 id=kind-the-first-cluster>KinD: the first cluster</h2><p>We are ready to create our first cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Check if the KUBECONFIG is not set</span>
<span style=color:#a2f>echo</span> <span style=color:#b8860b>$KUBECONFIG</span>
<span style=color:#080;font-style:italic># Check if the .kube directory is created &gt; if not, no need to create it</span>
ls <span style=color:#b8860b>$HOME</span>/.kube
<span style=color:#080;font-style:italic># Create the cluster and give it a name (optional)</span>
kind create cluster --name wslkind
<span style=color:#080;font-style:italic># Check if the .kube has been created and populated with files</span>
ls <span style=color:#b8860b>$HOME</span>/.kube
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create.png alt=kind-cluster-create></p><blockquote><p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed</p></blockquote><p>The cluster has been successfully created, and because we are using Docker Desktop, the network is all set for us to use "as is".</p><p>So we can open the <code>Kubernetes master</code> URL in our Windows browser:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-k8s-master.png alt=kind-browser-k8s-master></p><p>And this is the real strength from Docker Desktop for Windows with the WSL2 backend. Docker really did an amazing integration.</p><h2 id=kind-counting-1-2-3>KinD: counting 1 - 2 - 3</h2><p>Our first cluster was created and it's the "normal" one node cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Check how many nodes it created</span>
kubectl get nodes
<span style=color:#080;font-style:italic># Check the services for the whole cluster</span>
kubectl get all --all-namespaces
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-nodes-services.png alt=kind-list-nodes-services></p><p>While this will be enough for most people, let's leverage one of the coolest feature, multi-node clustering:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Delete the existing cluster</span>
kind delete cluster --name wslkind
<span style=color:#080;font-style:italic># Create a config file for a 3 nodes cluster</span>
cat <span style=color:#b44>&lt;&lt; EOF &gt; kind-3nodes.yaml
</span><span style=color:#b44>kind: Cluster
</span><span style=color:#b44>apiVersion: kind.x-k8s.io/v1alpha4
</span><span style=color:#b44>nodes:
</span><span style=color:#b44>  - role: control-plane
</span><span style=color:#b44>  - role: worker
</span><span style=color:#b44>  - role: worker
</span><span style=color:#b44>EOF</span>
<span style=color:#080;font-style:italic># Create a new cluster with the config file</span>
kind create cluster --name wslkindmultinodes --config ./kind-3nodes.yaml
<span style=color:#080;font-style:italic># Check how many nodes it created</span>
kubectl get nodes
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create-multinodes.png alt=kind-cluster-create-multinodes></p><blockquote><p>Tip: depending on how fast we run the "get nodes" command, it can be that not all the nodes are ready, wait few seconds and run it again, everything should be ready</p></blockquote><p>And that's it, we have created a three-node cluster, and if we look at the services one more time, we will see several that have now three replicas:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Check the services for the whole cluster</span>
kubectl get all --all-namespaces
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-services-multinodes.png alt=wsl2-kind-list-services-multinodes></p><h2 id=kind-can-i-see-a-nice-dashboard>KinD: can I see a nice dashboard?</h2><p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.</p><p>For that, the <a href=https://github.com/kubernetes/dashboard>Kubernetes Dashboard</a> project has been created. The installation and first connection test is quite fast, so let's do it:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Install the Dashboard application into our cluster</span>
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
<span style=color:#080;font-style:italic># Check the resources it created based on the new namespace created</span>
kubectl get all -n kubernetes-dashboard
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install-dashboard.png alt=kind-install-dashboard></p><p>As it created a service with a ClusterIP (read: internal network address), we cannot reach it if we type the URL in our Windows browser:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-error.png alt=kind-browse-dashboard-error></p><p>That's because we need to create a temporary proxy:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Start a kubectl proxy</span>
kubectl proxy
<span style=color:#080;font-style:italic># Enter the URL on your browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-success.png alt=kind-browse-dashboard-success></p><p>Finally to login, we can either enter a Token, which we didn't create, or enter the <code>kubeconfig</code> file from our Cluster.</p><p>If we try to login with the <code>kubeconfig</code>, we will get the error "Internal error (500): Not enough data to create auth info structure". This is due to the lack of credentials in the <code>kubeconfig</code> file.</p><p>So to avoid you ending with the same error, let's follow the <a href=https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md>recommended RBAC approach</a>.</p><p>Let's open a new WSL2 session:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Create a new ServiceAccount</span>
kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: ServiceAccount
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: admin-user
</span><span style=color:#b44>  namespace: kubernetes-dashboard
</span><span style=color:#b44>EOF</span>
<span style=color:#080;font-style:italic># Create a ClusterRoleBinding for the ServiceAccount</span>
kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>apiVersion: rbac.authorization.k8s.io/v1
</span><span style=color:#b44>kind: ClusterRoleBinding
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: admin-user
</span><span style=color:#b44>roleRef:
</span><span style=color:#b44>  apiGroup: rbac.authorization.k8s.io
</span><span style=color:#b44>  kind: ClusterRole
</span><span style=color:#b44>  name: cluster-admin
</span><span style=color:#b44>subjects:
</span><span style=color:#b44>- kind: ServiceAccount
</span><span style=color:#b44>  name: admin-user
</span><span style=color:#b44>  namespace: kubernetes-dashboard
</span><span style=color:#b44>EOF</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-rbac-serviceaccount.png alt=kind-browse-dashboard-rbac-serviceaccount></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Get the Token for the ServiceAccount</span>
kubectl -n kubernetes-dashboard describe secret <span style=color:#a2f;font-weight:700>$(</span>kubectl -n kubernetes-dashboard get secret | grep admin-user | awk <span style=color:#b44>&#39;{print $1}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
<span style=color:#080;font-style:italic># Copy the token and copy it into the Dashboard login and press &#34;Sign in&#34;</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-login-success.png alt=kind-browse-dashboard-login-success></p><p>Success! And let's see our nodes listed also:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-browse-nodes.png alt=kind-browse-dashboard-browse-nodes></p><p>A nice and shiny three nodes appear.</p><h1 id=minikube-kubernetes-from-everywhere>Minikube: Kubernetes from everywhere</h1><p>Right now, we have Docker that is installed, configured and the last test worked fine.</p><p>However, if we look carefully at the <code>kubectl</code> command, it found the "Client Version" (1.15.5), but it didn't find any server.</p><p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install Minikube and create our first cluster.</p><p>And as sources are always important to mention, we will follow (partially) the how-to from the <a href=https://kubernetes.io/docs/tasks/tools/install-minikube/>Kubernetes.io website</a>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Download the latest version of Minikube</span>
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
<span style=color:#080;font-style:italic># Make the binary executable</span>
chmod +x ./minikube
<span style=color:#080;font-style:italic># Move the binary to your executable path</span>
sudo mv ./minikube /usr/local/bin/
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install.png alt=minikube-install></p><h2 id=minikube-updating-the-host>Minikube: updating the host</h2><p>If we follow the how-to, it states that we should use the <code>--driver=none</code> flag in order to run Minikube directly on the host and Docker.</p><p>Unfortunately, we will get an error about "conntrack" being required to run Kubernetes v 1.18:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Create a minikube one node cluster</span>
minikube start --driver<span style=color:#666>=</span>none
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error.png alt=minikube-start-error></p><blockquote><p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed</p></blockquote><p>So let's fix the issue by installing the missing package:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Install the conntrack package</span>
sudo apt install -y conntrack
</code></pre></div><p>![minikube-install-conntrack](/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install conntrack.png)</p><p>Let's try to launch it again:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Create a minikube one node cluster</span>
minikube start --driver<span style=color:#666>=</span>none
<span style=color:#080;font-style:italic># We got a permissions error &gt; try again with sudo</span>
sudo minikube start --driver<span style=color:#666>=</span>none
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error-systemd.png alt=minikube-start-error-systemd></p><p>Ok, this error cloud be problematic ... in the past. Luckily for us, there's a solution</p><h2 id=minikube-enabling-systemd>Minikube: enabling SystemD</h2><p>In order to enable SystemD on WSL2, we will apply the <a href=https://forum.snapcraft.io/t/running-snaps-on-wsl2-insiders-only-for-now/13033>scripts</a> from <a href=https://twitter.com/diddledan>Daniel Llewellyn</a>.</p><p>I invite you to read the full blog post and how he came to the solution, and the various iterations he did to fix several issues.</p><p>So in a nutshell, here are the commands:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Install the needed packages</span>
sudo apt install -yqq daemonize dbus-user-session fontconfig
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-packages.png alt=minikube-systemd-packages></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Create the start-systemd-namespace script</span>
sudo vi /usr/sbin/start-systemd-namespace
<span style=color:#080;font-style:italic>#!/bin/bash</span>

<span style=color:#b8860b>SYSTEMD_PID</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>ps -ef | grep <span style=color:#b44>&#39;/lib/systemd/systemd --system-unit=basic.target$&#39;</span> | grep -v unshare | awk <span style=color:#b44>&#39;{print $2}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
<span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> -z <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span> <span style=color:#666>||</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> !<span style=color:#666>=</span> <span style=color:#b44>&#34;1&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
    <span style=color:#a2f>export</span> <span style=color:#b8860b>PRE_NAMESPACE_PATH</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#b8860b>$PATH</span><span style=color:#b44>&#34;</span>
    <span style=color:#666>(</span><span style=color:#a2f>set</span> -o posix; <span style=color:#a2f>set</span><span style=color:#666>)</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^BASH&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^DIRSTACK=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^EUID=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^GROUPS=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^HOME=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^HOSTNAME=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^HOSTTYPE=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^IFS=&#39;.*&#34;</span><span style=color:#b44>$&#39;\n&#39;</span><span style=color:#b44>&#34;&#39;&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^LANG=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^LOGNAME=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^MACHTYPE=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^NAME=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^OPTERR=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^OPTIND=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^OSTYPE=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^PIPESTATUS=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^POSIXLY_CORRECT=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^PPID=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^PS1=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^PS4=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^SHELL=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^SHELLOPTS=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^SHLVL=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^SYSTEMD_PID=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^UID=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^USER=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        grep -v <span style=color:#b44>&#34;^_=&#34;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>        cat - &gt; <span style=color:#b44>&#34;</span><span style=color:#b8860b>$HOME</span><span style=color:#b44>/.systemd-env&#34;</span>
    <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;PATH=&#39;</span><span style=color:#b8860b>$PATH</span><span style=color:#b44>&#39;&#34;</span> &gt;&gt; <span style=color:#b44>&#34;</span><span style=color:#b8860b>$HOME</span><span style=color:#b44>/.systemd-env&#34;</span>
    <span style=color:#a2f>exec</span> sudo /usr/sbin/enter-systemd-namespace <span style=color:#b44>&#34;</span><span style=color:#b8860b>$BASH_EXECUTION_STRING</span><span style=color:#b44>&#34;</span>
<span style=color:#a2f;font-weight:700>fi</span>
<span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> -n <span style=color:#b44>&#34;</span><span style=color:#b8860b>$PRE_NAMESPACE_PATH</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
    <span style=color:#a2f>export</span> <span style=color:#b8860b>PATH</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#b8860b>$PRE_NAMESPACE_PATH</span><span style=color:#b44>&#34;</span>
<span style=color:#a2f;font-weight:700>fi</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Create the enter-systemd-namespace</span>
sudo vi /usr/sbin/enter-systemd-namespace
<span style=color:#080;font-style:italic>#!/bin/bash</span>

<span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$UID</span><span style=color:#b44>&#34;</span> !<span style=color:#666>=</span> <span style=color:#666>0</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
    <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;You need to run </span><span style=color:#b8860b>$0</span><span style=color:#b44> through sudo&#34;</span>
    <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
<span style=color:#a2f;font-weight:700>fi</span>

<span style=color:#b8860b>SYSTEMD_PID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>ps -ef | grep <span style=color:#b44>&#39;/lib/systemd/systemd --system-unit=basic.target$&#39;</span> | grep -v unshare | awk <span style=color:#b44>&#39;{print $2}&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
<span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> -z <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
    /usr/sbin/daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit<span style=color:#666>=</span>basic.target
    <span style=color:#a2f;font-weight:700>while</span> <span style=color:#666>[</span> -z <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>do</span>
        <span style=color:#b8860b>SYSTEMD_PID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>ps -ef | grep <span style=color:#b44>&#39;/lib/systemd/systemd --system-unit=basic.target$&#39;</span> | grep -v unshare | awk <span style=color:#b44>&#39;{print $2}&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
    <span style=color:#a2f;font-weight:700>done</span>
<span style=color:#a2f;font-weight:700>fi</span>

<span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> -n <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span> <span style=color:#666>&amp;&amp;</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> !<span style=color:#666>=</span> <span style=color:#b44>&#34;1&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>[</span> -n <span style=color:#b44>&#34;</span><span style=color:#b8860b>$1</span><span style=color:#b44>&#34;</span> <span style=color:#666>]</span> <span style=color:#666>&amp;&amp;</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$1</span><span style=color:#b44>&#34;</span> !<span style=color:#666>=</span> <span style=color:#b44>&#34;bash --login&#34;</span> <span style=color:#666>]</span> <span style=color:#666>&amp;&amp;</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$1</span><span style=color:#b44>&#34;</span> !<span style=color:#666>=</span> <span style=color:#b44>&#34;/bin/bash --login&#34;</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>then</span>
        <span style=color:#a2f>exec</span> /usr/bin/nsenter -t <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> -a <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>            /usr/bin/sudo -H -u <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SUDO_USER</span><span style=color:#b44>&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>            /bin/bash -c <span style=color:#b44>&#39;set -a; source &#34;$HOME/.systemd-env&#34;; set +a; exec bash -c &#39;</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span><span style=color:#a2f>printf</span> <span style=color:#b44>&#34;%q&#34;</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$@</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
    <span style=color:#a2f;font-weight:700>else</span>
        <span style=color:#a2f>exec</span> /usr/bin/nsenter -t <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SYSTEMD_PID</span><span style=color:#b44>&#34;</span> -a <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>            /bin/login -p -f <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SUDO_USER</span><span style=color:#b44>&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>            <span style=color:#a2f;font-weight:700>$(</span>/bin/cat <span style=color:#b44>&#34;</span><span style=color:#b8860b>$HOME</span><span style=color:#b44>/.systemd-env&#34;</span> | grep -v <span style=color:#b44>&#34;^PATH=&#34;</span><span style=color:#a2f;font-weight:700>)</span>
    <span style=color:#a2f;font-weight:700>fi</span>
    <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;Existential crisis&#34;</span>
<span style=color:#a2f;font-weight:700>fi</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Edit the permissions of the enter-systemd-namespace script</span>
sudo chmod +x /usr/sbin/enter-systemd-namespace
<span style=color:#080;font-style:italic># Edit the bash.bashrc file</span>
sudo sed -i 2a<span style=color:#b44>&#34;# Start or enter a PID namespace in WSL2\nsource /usr/sbin/start-systemd-namespace\n&#34;</span> /etc/bash.bashrc
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-files.png alt=minikube-systemd-files></p><p>Finally, exit and launch a new session. You <strong>do not</strong> need to stop WSL2, a new session is enough:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-enabled.png alt=minikube-systemd-enabled></p><h2 id=minikube-the-first-cluster>Minikube: the first cluster</h2><p>We are ready to create our first cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Check if the KUBECONFIG is not set</span>
<span style=color:#a2f>echo</span> <span style=color:#b8860b>$KUBECONFIG</span>
<span style=color:#080;font-style:italic># Check if the .kube directory is created &gt; if not, no need to create it</span>
ls <span style=color:#b8860b>$HOME</span>/.kube
<span style=color:#080;font-style:italic># Check if the .minikube directory is created &gt; if yes, delete it</span>
ls <span style=color:#b8860b>$HOME</span>/.minikube
<span style=color:#080;font-style:italic># Create the cluster with sudo</span>
sudo minikube start --driver<span style=color:#666>=</span>none
</code></pre></div><p>In order to be able to use <code>kubectl</code> with our user, and not <code>sudo</code>, Minikube recommends running the <code>chown</code> command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Change the owner of the .kube and .minikube directories</span>
sudo chown -R <span style=color:#b8860b>$USER</span> <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.minikube
<span style=color:#080;font-style:italic># Check the access and if the cluster is running</span>
kubectl cluster-info
<span style=color:#080;font-style:italic># Check the resources created</span>
kubectl get all --all-namespaces
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-fixed.png alt=minikube-start-fixed></p><p>The cluster has been successfully created, and Minikube used the WSL2 IP, which is great for several reasons, and one of them is that we can open the <code>Kubernetes master</code> URL in our Windows browser:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master.png alt=minikube-browse-k8s-master></p><p>And the real strength of WSL2 integration, the port <code>8443</code> once open on WSL2 distro, it actually forwards it to Windows, so instead of the need to remind the IP address, we can also reach the <code>Kubernetes master</code> URL via <code>localhost</code>:</p><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master-localhost.png alt=minikube-browse-k8s-master-localhost></p><h2 id=minikube-can-i-see-a-nice-dashboard>Minikube: can I see a nice dashboard?</h2><p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.</p><p>For that, Minikube embeded the <a href=https://github.com/kubernetes/dashboard>Kubernetes Dashboard</a>. Thanks to it, running and accessing the Dashboard is very simple:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Enable the Dashboard service</span>
sudo minikube dashboard
<span style=color:#080;font-style:italic># Access the Dashboard from a browser on Windows side</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard.png alt=minikube-browse-dashboard></p><p>The command creates also a proxy, which means that once we end the command, by pressing <code>CTRL+C</code>, the Dashboard will no more be accessible.</p><p>Still, if we look at the namespace <code>kubernetes-dashboard</code>, we will see that the service is still created:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Get all the services from the dashboard namespace</span>
kubectl get all --namespace kubernetes-dashboard
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-get-all.png alt=minikube-dashboard-get-all></p><p>Let's edit the service and change it's type to <code>LoadBalancer</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Edit the Dashoard service</span>
kubectl edit service/kubernetes-dashboard --namespace kubernetes-dashboard
<span style=color:#080;font-style:italic># Go to the very end and remove the last 2 lines</span>
status:
  loadBalancer: <span style=color:#666>{}</span>
<span style=color:#080;font-style:italic># Change the type from ClusterIO to LoadBalancer</span>
  type: LoadBalancer
<span style=color:#080;font-style:italic># Save the file</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-type-loadbalancer.png alt=minikube-dashboard-type-loadbalancer></p><p>Check again the Dashboard service and let's access the Dashboard via the LoadBalancer:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># Get all the services from the dashboard namespace</span>
kubectl get all --namespace kubernetes-dashboard
<span style=color:#080;font-style:italic># Access the Dashboard from a browser on Windows side with the URL: localhost:&lt;port exposed&gt;</span>
</code></pre></div><p><img src=/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard-loadbalancer.png alt=minikube-browse-dashboard-loadbalancer></p><h1 id=conclusion>Conclusion</h1><p>It's clear that we are far from done as we could have some LoadBalancing implemented and/or other services (storage, ingress, registry, etc...).</p><p>Concerning Minikube on WSL2, as it needed to enable SystemD, we can consider it as an intermediate level to be implemented.</p><p>So with two solutions, what could be the "best for you"? Both bring their own advantages and inconveniences, so here an overview from our point of view solely:</p><table><thead><tr><th>Criteria</th><th>KinD</th><th>Minikube</th></tr></thead><tbody><tr><td>Installation on WSL2</td><td>Very Easy</td><td>Medium</td></tr><tr><td>Multi-node</td><td>Yes</td><td>No</td></tr><tr><td>Plugins</td><td>Manual install</td><td>Yes</td></tr><tr><td>Persistence</td><td>Yes, however not designed for</td><td>Yes</td></tr><tr><td>Alternatives</td><td>K3d</td><td>Microk8s</td></tr></tbody></table><p>We hope you could have a real taste of the integration between the different components: WSL2 - Docker Desktop - KinD/Minikube. And that gave you some ideas or, even better, some answers to your Kubernetes workflows with KinD and/or Minikube on Windows and WSL2.</p><p>See you soon for other adventures in the Kubernetes ocean.</p><p><a href=https://twitter.com/nunixtech>Nuno</a> & <a href=https://twitter.com/idvoretskyi>Ihor</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9cfa9082e191b21fc55a408bdb127095>How Docs Handle Third Party and Dual Sourced Content</h1><div class="td-byline mb-4"><time datetime=2020-05-06 class=text-muted>Wednesday, May 06, 2020</time></div><p><strong>Author:</strong> Zach Corleissen, Cloud Native Computing Foundation</p><p><em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).</em></p><p>Late last summer, SIG Docs started a community conversation about third party content in Kubernetes docs. This conversation became a <a href=https://github.com/kubernetes/enhancements/pull/1327>Kubernetes Enhancement Proposal</a> (KEP) and, after five months for review and comment, SIG Architecture approved the KEP as a <a href=/docs/contribute/style/content-guide/>content guide</a> for Kubernetes docs.</p><p>Here's how Kubernetes docs handle third party content now:</p><blockquote><p>Links to active content in the Kubernetes project (projects in the kubernetes and kubernetes-sigs GitHub orgs) are always allowed.</p><p>Kubernetes requires some third party content to function. Examples include container runtimes (containerd, CRI-O, Docker), networking policy (CNI plugins), Ingress controllers, and logging.</p><p>Docs can link to third party open source software (OSS) outside the Kubernetes project if it’s necessary for Kubernetes to function.</p></blockquote><p>These common sense guidelines make sure that Kubernetes docs document Kubernetes.</p><h2 id=keeping-the-docs-focused>Keeping the docs focused</h2><p>Our goal is for Kubernetes docs to be a trustworthy guide to Kubernetes features. To achieve this goal, SIG Docs is <a href=https://github.com/kubernetes/website/issues/20232>tracking third party content</a> and removing any third party content that isn't both in the Kubernetes project <em>and</em> required for Kubernetes to function.</p><h3 id=re-homing-content>Re-homing content</h3><p>Some content will be removed that readers may find helpful. To make sure readers have continous access to information, we're giving stakeholders until the <a href=https://github.com/kubernetes/sig-release/tree/master/releases/release-1.19>1.19 release deadline for docs</a>, <strong>July 9th, 2020</strong> to re-home any content slated for removal.</p><p>Over the next few months you'll see less third party content in the docs as contributors open PRs to remove content.</p><h2 id=background>Background</h2><p>Over time, SIG Docs observed increasing vendor content in the docs. Some content took the form of vendor-specific implementations that aren't required for Kubernetes to function in-project. Other content was thinly-disguised advertising with minimal to no feature content. Some vendor content was new; other content had been in the docs for years. It became clear that the docs needed clear, well-bounded guidelines for what kind of third party content is and isn't allowed. The <a href=https://kubernetes.io/docs/contribute/content-guide/>content guide</a> emerged from an extensive period for review and comment from the community.</p><p>Docs work best when they're accurate, helpful, trustworthy, and remain focused on features. In our experience, vendor content dilutes trust and accuracy.</p><p>Put simply: feature docs aren't a place for vendors to advertise their products. Our content policy keeps the docs focused on helping developers and cluster admins, not on marketing.</p><h2 id=dual-sourced-content>Dual sourced content</h2><p>Less impactful but also important is how Kubernetes docs handle <em>dual-sourced content</em>. Dual-sourced content is content published in more than one location, or from a non-canonical source.</p><p>From the <a href=https://kubernetes.io/docs/contribute/style/content-guide/#dual-sourced-content>Kubernetes content guide</a>:</p><blockquote><p>Wherever possible, Kubernetes docs link to canonical sources instead of hosting dual-sourced content.</p></blockquote><p>Minimizing dual-sourced content streamlines the docs and makes content across the Web more searchable. We're working to consolidate and redirect dual-sourced content in the Kubernetes docs as well.</p><h2 id=ways-to-contribute>Ways to contribute</h2><p>We're tracking third-party content in an <a href=https://github.com/kubernetes/website/issues/20232>issue in the Kubernetes website repository</a>. If you see third party content that's out of project and isn't required for Kubernetes to function, please comment on the tracking issue.</p><p>Feel free to open a PR that removes non-conforming content once you've identified it!</p><h2 id=want-to-know-more>Want to know more?</h2><p>For more information, read the issue description for <a href=https://github.com/kubernetes/website/issues/20232>tracking third party content</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-966d134b4e43eb259c24922ba4635311>Introducing PodTopologySpread</h1><div class="td-byline mb-4"><time datetime=2020-05-05 class=text-muted>Tuesday, May 05, 2020</time></div><p><strong>Author:</strong> Wei Huang (IBM), Aldo Culquicondor (Google)</p><p>Managing Pods distribution across a cluster is hard. The well-known Kubernetes
features for Pod affinity and anti-affinity, allow some control of Pod placement
in different topologies. However, these features only resolve part of Pods
distribution use cases: either place unlimited Pods to a single topology, or
disallow two Pods to co-locate in the same topology. In between these two
extreme cases, there is a common need to distribute the Pods evenly across the
topologies, so as to achieve better cluster utilization and high availability of
applications.</p><p>The PodTopologySpread scheduling plugin (originally proposed as EvenPodsSpread)
was designed to fill that gap. We promoted it to beta in 1.18.</p><h2 id=api-changes>API changes</h2><p>A new field <code>topologySpreadConstraints</code> is introduced in the Pod's spec API:</p><pre><code>spec:
  topologySpreadConstraints:
  - maxSkew: &lt;integer&gt;
    topologyKey: &lt;string&gt;
    whenUnsatisfiable: &lt;string&gt;
    labelSelector: &lt;object&gt;
</code></pre><p>As this API is embedded in Pod's spec, you can use this feature in all the
high-level workload APIs, such as Deployment, DaemonSet, StatefulSet, etc.</p><p>Let's see an example of a cluster to understand this API.</p><p><img src=/images/blog/2020-05-05-introducing-podtopologyspread/api.png alt=API></p><ul><li><strong>labelSelector</strong> is used to find matching Pods. For each topology, we count
the number of Pods that match this label selector. In the above example, given
the labelSelector as "app: foo", the matching number in "zone1" is 2; while
the number in "zone2" is 0.</li><li><strong>topologyKey</strong> is the key that defines a topology in the Nodes' labels. In
the above example, some Nodes are grouped into "zone1" if they have the label
"zone=zone1" label; while other ones are grouped into "zone2".</li><li><strong>maxSkew</strong> describes the maximum degree to which Pods can be unevenly
distributed. In the above example:<ul><li>if we put the incoming Pod to "zone1", the skew on "zone1" will become 3 (3
Pods matched in "zone1"; global minimum of 0 Pods matched on "zone2"), which
violates the "maxSkew: 1" constraint.</li><li>if the incoming Pod is placed to "zone2", the skew on "zone2" is 0 (1 Pod
matched in "zone2"; global minimum of 1 Pod matched on "zone2" itself),
which satisfies the "maxSkew: 1" constraint. Note that the skew is
calculated per each qualified Node, instead of a global skew.</li></ul></li><li><strong>whenUnsatisfiable</strong> specifies, when "maxSkew" can't be satisfied, what
action should be taken:<ul><li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it. It's a
hard constraint.</li><li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing
Nodes that reduce the skew. It's a soft constraint.</li></ul></li></ul><h2 id=advanced-usage>Advanced usage</h2><p>As the feature name "PodTopologySpread" implies, the basic usage of this feature
is to run your workload with an absolute even manner (maxSkew=1), or relatively
even manner (maxSkew>=2). See the <a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>official
document</a>
for more details.</p><p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.</p><h3 id=usage-along-with-nodeselector-nodeaffinity>Usage along with NodeSelector / NodeAffinity</h3><p>You may have found that we didn't have a "topologyValues" field to limit which
topologies the Pods are going to be scheduled to. By default, it is going to
search all Nodes and group them by "topologyKey". Sometimes this may not be the
ideal case. For instance, suppose there is a cluster with Nodes tagged with
"env=prod", "env=staging" and "env=qa", and now you want to evenly place Pods to
the "qa" environment across zones, is it possible?</p><p>The answer is yes. You can leverage the NodeSelector or NodeAffinity API spec.
Under the hood, the PodTopologySpread feature will <strong>honor</strong> that and calculate
the spread constraints among the nodes that satisfy the selectors.</p><p><img src=/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-1.png alt=Advanced-Usage-1></p><p>As illustrated above, you can specify <code>spec.affinity.nodeAffinity</code> to limit the
"searching scope" to be "qa" environment, and within that scope, the Pod will be
scheduled to one zone which satisfies the topologySpreadConstraints. In this
case, it's "zone2".</p><h3 id=multiple-topologyspreadconstraints>Multiple TopologySpreadConstraints</h3><p>It's intuitive to understand how one single TopologySpreadConstraint works.
What's the case for multiple TopologySpreadConstraints? Internally, each
TopologySpreadConstraint is calculated independently, and the result sets will
be merged to generate the eventual result set - i.e., suitable Nodes.</p><p>In the following example, we want to schedule a Pod to a cluster with 2
requirements at the same time:</p><ul><li>place the Pod evenly with Pods across zones</li><li>place the Pod evenly with Pods across nodes</li></ul><p><img src=/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-2.png alt=Advanced-Usage-2></p><p>For the first constraint, there are 3 Pods in zone1 and 2 Pods in zone2, so the
incoming Pod can be only put to zone2 to satisfy the "maxSkew=1" constraint. In
other words, the result set is nodeX and nodeY.</p><p>For the second constraint, there are too many Pods in nodeB and nodeX, so the
incoming Pod can be only put to nodeA and nodeY.</p><p>Now we can conclude the only qualified Node is nodeY - from the intersection of
the sets {nodeX, nodeY} (from the first constraint) and {nodeA, nodeY} (from the
second constraint).</p><p>Multiple TopologySpreadConstraints is powerful, but be sure to understand the
difference with the preceding "NodeSelector/NodeAffinity" example: one is to
calculate result set independently and then interjoined; while the other is to
calculate topologySpreadConstraints based on the filtering results of node
constraints.</p><p>Instead of using "hard" constraints in all topologySpreadConstraints, you can
also combine using "hard" constraints and "soft" constraints to adhere to more
diverse cluster situations.</p><blockquote class="note callout"><div><strong>Note:</strong> If two TopologySpreadConstraints are being applied for the same {topologyKey,
whenUnsatisfiable} tuple, the Pod creation will be blocked returning a
validation error.</div></blockquote><h2 id=podtopologyspread-defaults>PodTopologySpread defaults</h2><p>PodTopologySpread is a Pod level API. As such, to use the feature, workload
authors need to be aware of the underlying topology of the cluster, and then
specify proper <code>topologySpreadConstraints</code> in the Pod spec for every workload.
While the Pod-level API gives the most flexibility it is also possible to
specify cluster-level defaults.</p><p>The default PodTopologySpread constraints allow you to specify spreading for all
the workloads in the cluster, tailored for its topology. The constraints can be
specified by an operator/admin as PodTopologySpread plugin arguments in the
<a href=/docs/reference/scheduling/profiles/>scheduling profile configuration
API</a> when starting
kube-scheduler.</p><p>A sample configuration could look like this:</p><pre><code>apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration
profiles:
  pluginConfig:
  - name: PodTopologySpread
    args:
      defaultConstraints:
      - maxSkew: 1
        topologyKey: example.com/rack
        whenUnsatisfiable: ScheduleAnyway
</code></pre><p>When configuring default constraints, label selectors must be left empty.
kube-scheduler will deduce the label selectors from the membership of the Pod to
Services, ReplicationControllers, ReplicaSets or StatefulSets. Pods can
always override the default constraints by providing their own through the
PodSpec.</p><blockquote class="note callout"><div><strong>Note:</strong> When using default PodTopologySpread constraints, it is recommended to disable
the old DefaultTopologySpread plugin.</div></blockquote><h2 id=wrap-up>Wrap-up</h2><p>PodTopologySpread allows you to define spreading constraints for your workloads
with a flexible and expressive Pod-level API. In the past, workload authors used
Pod AntiAffinity rules to force or hint the scheduler to run a single Pod per
topology domain. In contrast, the new PodTopologySpread constraints allow Pods
to specify skew levels that can be required (hard) or desired (soft). The
feature can be paired with Node selectors and Node affinity to limit the
spreading to specific domains. Pod spreading constraints can be defined for
different topologies such as hostnames, zones, regions, racks, etc.</p><p>Lastly, cluster operators can define default constraints to be applied to all
Pods. This way, Pods don't need to be aware of the underlying topology of the
cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bba5d56256c14a660449365ac3803ece>Two-phased Canary Rollout with Open Source Gloo</h1><div class="td-byline mb-4"><time datetime=2020-04-22 class=text-muted>Wednesday, April 22, 2020</time></div><p><strong>Author:</strong> Rick Ducott | <a href=https://github.com/rickducott/>GitHub</a> | <a href=https://twitter.com/ducott>Twitter</a></p><p>Every day, my colleagues and I are talking to platform owners, architects, and engineers who are using <a href=https://github.com/solo-io/gloo>Gloo</a> as an API gateway
to expose their applications to end users. These applications may span legacy monoliths, microservices, managed cloud services, and Kubernetes
clusters. Fortunately, Gloo makes it easy to set up routes to manage, secure, and observe application traffic while
supporting a flexible deployment architecture to meet the varying production needs of our users.</p><p>Beyond the initial set up, platform owners frequently ask us to help design the operational workflows within their organization:
How do we bring a new application online? How do we upgrade an application? How do we divide responsibilities across our
platform, ops, and development teams?</p><p>In this post, we're going to use Gloo to design a two-phased canary rollout workflow for application upgrades:</p><ul><li>In the first phase, we'll do canary testing by shifting a small subset of traffic to the new version. This allows you to safely perform smoke and correctness tests.</li><li>In the second phase, we'll progressively shift traffic to the new version, allowing us to monitor the new version under load, and eventually, decommission the old version.</li></ul><p>To keep it simple, we're going to focus on designing the workflow using <a href=https://github.com/solo-io/gloo>open source Gloo</a>, and we're going to deploy the gateway and
application to Kubernetes. At the end, we'll talk about a few extensions and advanced topics that could be interesting to explore in a follow up.</p><h2 id=initial-setup>Initial setup</h2><p>To start, we need a Kubernetes cluster. This example doesn't take advantage of any cloud specific
features, and can be run against a local test cluster such as <a href=https://kubernetes.io/docs/tasks/tools/install-minikube/>minikube</a>.
This post assumes a basic understanding of Kubernetes and how to interact with it using <code>kubectl</code>.</p><p>We'll install the latest <a href=https://github.com/solo-io/gloo>open source Gloo</a> to the <code>gloo-system</code> namespace and deploy
version <code>v1</code> of an example application to the <code>echo</code> namespace. We'll expose this application outside the cluster
by creating a route in Gloo, to end up with a picture like this:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png alt=Setup></p><h3 id=deploying-gloo>Deploying Gloo</h3><p>We'll install gloo with the <code>glooctl</code> command line tool, which we can download and add to the <code>PATH</code> with the following
commands:</p><pre><code>curl -sL https://run.solo.io/gloo/install | sh
export PATH=$HOME/.gloo/bin:$PATH
</code></pre><p>Now, you should be able to run <code>glooctl version</code> to see that it is installed correctly:</p><pre><code>➜ glooctl version
Client: {&quot;version&quot;:&quot;1.3.15&quot;}
Server: version undefined, could not find any version of gloo running
</code></pre><p>Now we can install the gateway to our cluster with a simple command:</p><pre><code>glooctl install gateway
</code></pre><p>The console should indicate the install finishes successfully:</p><pre><code>Creating namespace gloo-system... Done.
Starting Gloo installation...

Gloo was successfully installed!

</code></pre><p>Before long, we can see all the Gloo pods running in the <code>gloo-system</code> namespace:</p><pre><code>➜ kubectl get pod -n gloo-system
NAME                             READY   STATUS    RESTARTS   AGE
discovery-58f8856bd7-4fftg       1/1     Running   0          13s
gateway-66f86bc8b4-n5crc         1/1     Running   0          13s
gateway-proxy-5ff99b8679-tbp65   1/1     Running   0          13s
gloo-66b8dc8868-z5c6r            1/1     Running   0          13s
</code></pre><h3 id=deploying-the-application>Deploying the application</h3><p>Our <code>echo</code> application is a simple container (thanks to our friends at HashiCorp) that will
respond with the application version, to help demonstrate our canary workflows as we start testing and
shifting traffic to a <code>v2</code> version of the application.</p><p>Kubernetes gives us a lot of flexibility in terms of modeling this application. We'll adopt the following
conventions:</p><ul><li>We'll include the version in the deployment name so we can run two versions of the application
side-by-side and manage their lifecycle differently.</li><li>We'll label pods with an app label (<code>app: echo</code>) and a version label (<code>version: v1</code>) to help with our canary rollout.</li><li>We'll deploy a single Kubernetes <code>Service</code> for the application to set up networking. Instead of updating
this or using multiple services to manage routing to different versions, we'll manage the rollout with Gloo configuration.</li></ul><p>The following is our <code>v1</code> echo application:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo-v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Shout out to our friends at Hashi for this useful test server</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>hashicorp/http-echo<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;-text=version:v1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- -listen=:8080<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo-v1<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span></code></pre></div><p>And here is the <code>echo</code> Kubernetes <code>Service</code> object:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span></code></pre></div><p>For convenience, we've published this yaml in a repo so we can deploy it with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/echo.yaml
</code></pre><p>We should see the following output:</p><pre><code>namespace/echo created
deployment.apps/echo-v1 created
service/echo created
</code></pre><p>And we should be able to see all the resources healthy in the <code>echo</code> namespace:</p><pre><code>➜ kubectl get all -n echo
NAME                           READY   STATUS    RESTARTS   AGE
pod/echo-v1-66dbfffb79-287s5   1/1     Running   0          6s

NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/echo   ClusterIP   10.55.252.216   &lt;none&gt;        80/TCP    6s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/echo-v1   1/1     1            1           7s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/echo-v1-66dbfffb79   1         1         1       7s
</code></pre><h3 id=exposing-outside-the-cluster-with-gloo>Exposing outside the cluster with Gloo</h3><p>We can now expose this service outside the cluster with Gloo. First, we'll model the application as a Gloo
<a href=https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#upstreams>Upstream</a>, which is Gloo's abstraction
for a traffic destination:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gloo.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Upstream<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kube</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>serviceNamespace</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>servicePort</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>subsetSpec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>selectors</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>keys</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- version<span style=color:#bbb>
</span></code></pre></div><p>Here, we're setting up subsets based on the <code>version</code> label. We don't have to use this in our routes, but later
we'll start to use it to support our canary workflow.</p><p>We can now create a route to this upstream in Gloo by defining a
<a href=https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#virtual-services>Virtual Service</a>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span></code></pre></div><p>We can apply these resources with the following commands:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/upstream.yaml
kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/vs.yaml
</code></pre><p>Once we apply these two resources, we can start to send traffic to the application through Gloo:</p><pre><code>➜ curl $(glooctl proxy url)/
version:v1
</code></pre><p>Our setup is complete, and our cluster now looks like this:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png alt=Setup></p><h2 id=two-phased-rollout-strategy>Two-Phased Rollout Strategy</h2><p>Now we have a new version <code>v2</code> of the echo application that we wish to roll out. We know that when the
rollout is complete, we are going to end up with this picture:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png alt="End State"></p><p>However, to get there, we may want to perform a few rounds of testing to ensure the new version of the application
meets certain correctness and/or performance acceptance criteria. In this post, we'll introduce a two-phased approach to
canary rollout with Gloo, that could be used to satisfy the vast majority of acceptance tests.</p><p>In the first phase, we'll perform smoke and correctness tests by routing a small segment of the traffic to the new version
of the application. In this demo, we'll use a header <code>stage: canary</code> to trigger routing to the new service, though in
practice it may be desirable to make this decision based on another part of the request, such as a claim in a verified JWT.</p><p>In the second phase, we've already established correctness, so we are ready to shift all of the traffic over to the new
version of the application. We'll configure weighted destinations, and shift the traffic while monitoring certain business
metrics to ensure the service quality remains at acceptable levels. Once 100% of the traffic is shifted to the new version,
the old version can be decommissioned.</p><p>In practice, it may be desirable to only use one of the phases for testing, in which case the other phase can be
skipped.</p><h2 id=phase-1-initial-canary-rollout-of-v2>Phase 1: Initial canary rollout of v2</h2><p>In this phase, we'll deploy <code>v2</code>, and then use a header <code>stage: canary</code> to start routing a small amount of specific
traffic to the new version. We'll use this header to perform some basic smoke testing and make sure <code>v2</code> is working the
way we'd expect:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/subset-routing.png alt="Subset Routing"></p><h3 id=setting-up-subset-routing>Setting up subset routing</h3><p>Before deploying our <code>v2</code> service, we'll update our virtual service to only route to pods that have the subset label
<code>version: v1</code>, using a Gloo feature called <a href=https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/subsets/>subset routing</a>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></code></pre></div><p>We can apply them to the cluster with the following commands:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-1.yaml
</code></pre><p>The application should continue to function as before:</p><pre><code>➜ curl $(glooctl proxy url)/
version:v1
</code></pre><h3 id=deploying-echo-v2>Deploying echo v2</h3><p>Now we can safely deploy <code>v2</code> of the echo application:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo-v2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>hashicorp/http-echo<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;-text=version:v2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- -listen=:8080<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo-v2<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span></code></pre></div><p>We can deploy with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/echo-v2.yaml
</code></pre><p>Since our gateway is configured to route specifically to the <code>v1</code> subset, this should have no effect. However, it does enable
<code>v2</code> to be routable from the gateway if the <code>v2</code> subset is configured for a route.</p><p>Make sure <code>v2</code> is running before moving on:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>➜ kubectl get pod -n <span style=color:#a2f>echo</span>
NAME                       READY   STATUS    RESTARTS   AGE
echo-v1-66dbfffb79-2qw86   1/1     Running   <span style=color:#666>0</span>          5m25s
echo-v2-86584fbbdb-slp44   1/1     Running   <span style=color:#666>0</span>          93s
</code></pre></div><p>The application should continue to function as before:</p><pre><code>➜ curl $(glooctl proxy url)/
version:v1
</code></pre><h3 id=adding-a-route-to-v2-for-canary-testing>Adding a route to v2 for canary testing</h3><p>We'll route to the <code>v2</code> subset when the <code>stage: canary</code> header is supplied on the request. If the header isn't
provided, we'll continue to route to the <code>v1</code> subset as before.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>headers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>stage<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></code></pre></div><p>We can deploy with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-2.yaml
</code></pre><h3 id=canary-testing>Canary testing</h3><p>Now that we have this route, we can do some testing. First let's ensure that the existing route is working as expected:</p><pre><code>➜ curl $(glooctl proxy url)/
version:v1
</code></pre><p>And now we can start to canary test our new application version:</p><pre><code>➜ curl $(glooctl proxy url)/ -H &quot;stage: canary&quot;
version:v2
</code></pre><h3 id=advanced-use-cases-for-subset-routing>Advanced use cases for subset routing</h3><p>We may decide that this approach, using user-provided request headers, is too open. Instead, we may
want to restrict canary testing to a known, authorized user.</p><p>A common implementation of this that we've seen is for the canary route to require a valid JWT that contains
a specific claim to indicate the subject is authorized for canary testing. Enterprise Gloo has out of the box
support for verifying JWTs, updating the request headers based on the JWT claims, and recomputing the
routing destination based on the updated headers. We'll save that for a future post covering more advanced use
cases in canary testing.</p><h2 id=phase-2-shifting-all-traffic-to-v2-and-decommissioning-v1>Phase 2: Shifting all traffic to v2 and decommissioning v1</h2><p>At this point, we've deployed <code>v2</code>, and created a route for canary testing. If we are satisfied with the
results of the testing, we can move on to phase 2 and start shifting the load from <code>v1</code> to <code>v2</code>. We'll use
<a href=https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/multi_destination/>weighted destinations</a>
in Gloo to manage the load during the migration.</p><h3 id=setting-up-the-weighted-destinations>Setting up the weighted destinations</h3><p>We can change the Gloo route to route to both of these destinations, with weights to decide how much of the traffic should
go to the <code>v1</code> versus the <code>v2</code> subset. To start, we're going to set it up so 100% of the traffic continues to get routed to the
<code>v1</code> subset, unless the <code>stage: canary</code> header was provided as before.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># We&#39;ll keep our route from before if we want to continue testing with this header</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>headers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>stage<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Now we&#39;ll route the rest of the traffic to the upstream, load balanced across the two subsets.</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>multi</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>destinations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></code></pre></div><p>We can apply this virtual service update to the cluster with the following commands:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-1.yaml
</code></pre><p>Now the cluster looks like this, for any request that doesn't have the <code>stage: canary</code> header:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/init-traffic-shift.png alt="Initialize Traffic Shift"></p><p>With the initial weights, we should see the gateway continue to serve <code>v1</code> for all traffic.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>➜ curl <span style=color:#a2f;font-weight:700>$(</span>glooctl proxy url<span style=color:#a2f;font-weight:700>)</span>/
version:v1
</code></pre></div><h3 id=commence-rollout>Commence rollout</h3><p>To simulate a load test, let's shift half the traffic to <code>v2</code>:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/load-test.png alt="Load Test"></p><p>This can be expressed on our virtual service by adjusting the weights:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>headers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>stage<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>multi</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>destinations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:#080;font-style:italic># Update the weight so 50% of the traffic hits v1</span><span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:#080;font-style:italic># And 50% is routed to v2</span><span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></code></pre></div><p>We can apply this to the cluster with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-2.yaml
</code></pre><p>Now when we send traffic to the gateway, we should see half of the requests return <code>version:v1</code> and the
other half return <code>version:v2</code>.</p><pre><code>➜ curl $(glooctl proxy url)/
version:v1
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v1
</code></pre><p>In practice, during this process it's likely you'll be monitoring some performance and business metrics
to ensure the traffic shift isn't resulting in a decline in the overall quality of service. We can even
leverage operators like <a href=https://github.com/weaveworks/flagger>Flagger</a> to help automate this Gloo
workflow. Gloo Enterprise integrates with your metrics backend and provides out of the box and dynamic,
upstream-based dashboards that can be used to monitor the health of the rollout.
We will save these topics for a future post on advanced canary testing use cases with Gloo.</p><h3 id=finishing-the-rollout>Finishing the rollout</h3><p>We will continue adjusting weights until eventually, all of the traffic is now being routed to <code>v2</code>:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/final-shift.png alt="Final Shift"></p><p>Our virtual service will look like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>headers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>stage<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>multi</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>destinations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:#080;font-style:italic># No traffic will be sent to v1 anymore</span><span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>destination</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                      </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:#080;font-style:italic># Now all the traffic will be routed to v2</span><span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></code></pre></div><p>We can apply that to the cluster with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-3.yaml
</code></pre><p>Now when we send traffic to the gateway, we should see all of the requests return <code>version:v2</code>.</p><pre><code>➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
</code></pre><h3 id=decommissioning-v1>Decommissioning v1</h3><p>At this point, we have deployed the new version of our application, conducted correctness tests using subset routing,
conducted load and performance tests by progressively shifting traffic to the new version, and finished
the rollout. The only remaining task is to clean up our <code>v1</code> resources.</p><p>First, we'll clean up our routes. We'll leave the subset specified on the route so we are all setup for future upgrades.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>gateway.solo.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VirtualService<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>virtualHost</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>routes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>prefix</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>routeAction</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>single</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>upstream</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>echo<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>gloo-system<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>subset</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span>v2<span style=color:#bbb>
</span></code></pre></div><p>We can apply this update with the following command:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/4-decommissioning-v1/vs.yaml
</code></pre><p>And we can delete the <code>v1</code> deployment, which is no longer serving any traffic.</p><pre><code>kubectl delete deploy -n echo echo-v1
</code></pre><p>Now our cluster looks like this:</p><p><img src=/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png alt="End State"></p><p>And requests to the gateway return this:</p><pre><code>➜ curl $(glooctl proxy url)/
version:v2
</code></pre><p>We have now completed our two-phased canary rollout of an application update using Gloo!</p><h2 id=other-advanced-topics>Other Advanced Topics</h2><p>Over the course of this post, we collected a few topics that could be a good starting point for advanced exploration:</p><ul><li>Using the <strong>JWT</strong> filter to verify JWTs, extract claims onto headers, and route to canary versions depending on a claim value.</li><li>Looking at <strong>Prometheus metrics</strong> and <strong>Grafana dashboards</strong> created by Gloo to monitor the health of the rollout.</li><li>Automating the rollout by integrating <strong>Flagger</strong> with <strong>Gloo</strong>.</li></ul><p>A few other topics that warrant further exploration:</p><ul><li>Supporting <strong>self-service</strong> upgrades by giving teams ownership over their upstream and route configuration</li><li>Utilizing Gloo's <strong>delegation</strong> feature and Kubernetes <strong>RBAC</strong> to decentralize the configuration management safely</li><li>Fully automating the continuous delivery process by applying <strong>GitOps</strong> principles and using tools like <strong>Flux</strong> to push config to the cluster</li><li>Supporting <strong>hybrid</strong> or <strong>non-Kubernetes</strong> application use-cases by setting up Gloo with a different deployment pattern</li><li>Utilizing <strong>traffic shadowing</strong> to begin testing the new version with realistic data before shifting production traffic to it</li></ul><h2 id=get-involved-in-the-gloo-community>Get Involved in the Gloo Community</h2><p>Gloo has a large and growing community of open source users, in addition to an enterprise customer base. To learn more about
Gloo:</p><ul><li>Check out the <a href=https://github.com/solo-io/gloo>repo</a>, where you can see the code and file issues</li><li>Check out the <a href=https://docs.solo.io/gloo/latest>docs</a>, which have an extensive collection of guides and examples</li><li>Join the <a href=http://slack.solo.io/>slack channel</a> and start chatting with the Solo engineering team and user community</li></ul><p>If you'd like to get in touch with me (feedback is always appreciated!), you can find me on the
<a href=http://slack.solo.io/>Solo slack</a> or email me at <strong><a href=mailto:rick.ducott@solo.io>rick.ducott@solo.io</a></strong>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e62fc55b46cc494898cd6d42cd437646>Cluster API v1alpha3 Delivers New Features and an Improved User Experience</h1><div class="td-byline mb-4"><time datetime=2020-04-21 class=text-muted>Tuesday, April 21, 2020</time></div><p><strong>Author:</strong> Daniel Lipovetsky (D2IQ)</p><img src=/images/blog/2020-04-21-Cluster-API-v1alpha3-Delivers-New-Features-and-an-Improved-User-Experience/kubernetes-cluster-logos_final-02.svg align=right width=25% alt="Cluster API Logo: Turtles All The Way Down"><p>The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. It provides optional, additive functionality on top of core Kubernetes to manage the lifecycle of a Kubernetes cluster.</p><p>Following the v1alpha2 release in October 2019, many members of the Cluster API community met in San Francisco, California, to plan the next release. The project had just gone through a major transformation, delivering a new architecture that promised to make the project easier for users to adopt, and faster for the community to build. Over the course of those two days, we found our common goals: To implement the features critical to managing production clusters, to make its user experience more intuitive, and to make it a joy to develop.</p><p>The v1alpha3 release of Cluster API brings significant features for anyone running Kubernetes in production and at scale. Among the highlights:</p><ul><li><a href=#declarative-control-plane-management>Declarative Control Plane Management</a></li><li><a href=#distributing-control-plane-nodes-to-reduce-risk>Support for Distributing Control Plane Nodes Across Failure Domains To Reduce Risk</a></li><li><a href=#automated-replacement-of-unhealthy-nodes>Automated Replacement of Unhealthy Nodes</a></li><li><a href=#infrastructure-managed-node-groups>Support for Infrastructure-Managed Node Groups</a></li></ul><p>For anyone who wants to understand the API, or prizes a simple, but powerful, command-line interface, the new release brings:</p><ul><li><a href=#clusterctl>Redesigned clusterctl, a command-line tool (and go library) for installing and managing the lifecycle of Cluster API</a></li><li><a href=#the-cluster-api-book>Extensive and up-to-date documentation in The Cluster API Book</a></li></ul><p>Finally, for anyone extending the Cluster API for their custom infrastructure or software needs:</p><ul><li><a href=#end-to-end-test-framework>New End-to-End (e2e) Test Framework</a></li><li><a href=#provider-implementer-s-guide>Documentation for integrating Cluster API into your cluster lifecycle stack</a></li></ul><p>All this was possible thanks to the hard work of many contributors.</p><h2 id=declarative-control-plane-management>Declarative Control Plane Management</h2><p><em>Special thanks to <a href=https://github.com/detiber/>Jason DeTiberus</a>, <a href=https://github.com/randomvariable>Naadir Jeewa</a>, and <a href=https://github.com/chuckha>Chuck Ha</a></em></p><p>The Kubeadm-based Control Plane (KCP) provides a declarative API to deploy and scale the Kubernetes control plane, including etcd. This is the feature many Cluster API users have been waiting for! Until now, to deploy and scale up the control plane, users had to create specially-crafted Machine resources. To scale down the control plane, they had to manually remove members from the etcd cluster. KCP automates deployment, scaling, and upgrades.</p><blockquote><p><strong>What is the Kubernetes Control Plane?</strong>
The Kubernetes control plane is, at its core, kube-apiserver and etcd. If either of these are unavailable, no API requests can be handled. This impacts not only core Kubernetes APIs, but APIs implemented with CRDs. Other components, like kube-scheduler and kube-controller-manager, are also important, but do not have the same impact on availability.</p><p>The control plane was important in the beginning because it scheduled workloads. However, some workloads could continue to run during a control plane outage. Today, workloads depend on operators, service meshes, and API gateways, which all use the control plane as a platform. Therefore, the control plane's availability is more important than ever.</p><p>Managing the control plane is one of the most complex parts of cluster operation. Because the typical control plane includes etcd, it is stateful, and operations must be done in the correct sequence. Control plane replicas can and do fail, and maintaining control plane availability means being able to replace failed nodes.</p><p>The control plane can suffer a complete outage (e.g. permanent loss of quorum in etcd), and recovery (along with regular backups) is sometimes the only feasible option.</p><p>For more details, read about <a href=https://kubernetes.io/docs/concepts/overview/components/>Kubernetes Components</a> in the Kubernetes documentation.</p></blockquote><p>Here's an example of a 3-replica control plane for the Cluster API Docker Infrastructure, which the project maintains for testing and development. For brevity, other required resources, like Cluster, and Infrastructure Template, referenced by its name and namespace, are not shown.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>controlplane.cluster.x-k8s.io/v1alpha3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeadmControlPlane<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>infrastructureTemplate</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>infrastructure.cluster.x-k8s.io/v1alpha3<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DockerMachineTemplate<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeadmConfigSpec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>clusterConfiguration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span><span style=color:#666>1.16.3</span><span style=color:#bbb>
</span></code></pre></div><p>Deploy this control plane with kubectl:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f example-docker-control-plane.yaml
</code></pre></div><p>Scale the control plane the same way you scale other Kubernetes resources:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl scale kubeadmcontrolplane example  --replicas<span style=color:#666>=</span><span style=color:#666>5</span>
</code></pre></div><pre><code>kubeadmcontrolplane.controlplane.cluster.x-k8s.io/example scaled
</code></pre><p>Upgrade the control plane to a newer patch of the Kubernetes release:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl patch kubeadmcontrolplane example --type<span style=color:#666>=</span>json -p <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/version&#34;, &#34;value&#34;: &#34;1.16.4&#34;}]&#39;</span>
</code></pre></div><blockquote><p><strong>Number of Control Plane Replicas</strong>
By default, KCP is configured to manage etcd, and requires an odd number of replicas. If KCP is configured to not manage etcd, an odd number is recommended, but not required. An odd number of replicas ensures optimal etcd configuration. To learn why your etcd cluster should have an odd number of members, see the <a href=https://etcd.io/docs/v3.4.0/faq/#why-an-odd-number-of-cluster-members>etcd FAQ</a>.</p></blockquote><p>Because it is a core Cluster API component, KCP can be used with any v1alpha3-compatible Infrastructure Provider that provides a fixed control plane endpoint, i.e., a load balancer or virtual IP. This endpoint enables requests to reach multiple control plane replicas.</p><blockquote><p><strong>What is an Infrastructure Provider?</strong>
A source of computational resources (e.g. machines, networking, etc.). The community maintains providers for AWS, Azure, Google Cloud, and VMWare. For details, see the <a href=https://cluster-api.sigs.k8s.io/reference/providers.html>list of providers</a> in the Cluster API Book.</p></blockquote><h2 id=distributing-control-plane-nodes-to-reduce-risk>Distributing Control Plane Nodes To Reduce Risk</h2><p><em>Special thanks to <a href=https://github.com/vincepri/>Vince Prignano</a>, and <a href=https://github.com/chuckha>Chuck Ha</a></em></p><p>Cluster API users can now deploy nodes in different failure domains, reducing the risk of a cluster failing due to a domain outage. This is especially important for the control plane: If nodes in one domain fail, the cluster can continue to operate as long as the control plane is available to nodes in other domains.</p><blockquote><p><strong>What is a Failure Domain?</strong>
A failure domain is a way to group the resources that would be made unavailable by some failure. For example, in many public clouds, an "availability zone" is the default failure domain. A zone corresponds to a data center. So, if a specific data center is brought down by a power outage or natural disaster, all resources in that zone become unavailable. If you run Kubernetes on your own hardware, your failure domain might be a rack, a network switch, or power distribution unit.</p></blockquote><p>The Kubeadm-based ControlPlane distributes nodes across failure domains. To minimize the chance of losing multiple nodes in the event of a domain outage, it tries to distribute them evenly: it deploys a new node in the failure domain with the fewest existing nodes, and it removes an existing node in the failure domain with the most existing nodes.</p><p>MachineDeployments and MachineSets do not distribute nodes across failure domains. To deploy your worker nodes across multiple failure domains, create a MachineDeployment or MachineSet for each failure domain.</p><p>The Failure Domain API works on any infrastructure. That's because every Infrastructure Provider maps failure domains in its own way. The API is optional, so if your infrastructure is not complex enough to need failure domains, you do not need to support it. This example is for the Cluster API Docker Infrastructure Provider. Note that two of the domains are marked as suitable for control plane nodes, while a third is not. The Kubeadm-based ControlPlane will only deploy nodes to domains marked suitable.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>infrastructure.cluster.x-k8s.io/v1alpha3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DockerCluster<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controlPlaneEndpoint</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.4</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>failureDomains</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domain-one</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domain-two</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>domain-three</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></code></pre></div><p>The <a href=https://github.com/kubernetes-sigs/cluster-api-provider-aws>AWS Infrastructure Provider</a> (CAPA), maintained by the Cluster API project, maps failure domains to AWS Availability Zones. Using CAPA, you can deploy a cluster across multiple Availability Zones. First, define subnets for multiple Availability Zones. The CAPA controller will define a failure domain for each Availability Zone. Deploy the control plane with the KubeadmControlPlane: it will distribute replicas across the failure domains. Finally, create a separate MachineDeployment for each failure domain.</p><h2 id=automated-replacement-of-unhealthy-nodes>Automated Replacement of Unhealthy Nodes</h2><p><em>Special thanks to <a href=https://github.com/enxebre>Alberto García Lamela</a>, and <a href=http://github.com/joelspeed>Joel Speed</a></em></p><p>There are many reasons why a node might be unhealthy. The kubelet process may stop. The container runtime might have a bug. The kernel might have a memory leak. The disk may run out of space. CPU, disk, or memory hardware may fail. A power outage may happen. Failures like these are especially common in larger clusters.</p><p>Kubernetes is designed to tolerate them, and to help your applications tolerate them as well. Nevertheless, only a finite number of nodes can be unhealthy before the cluster runs out of resources, and Pods are evicted or not scheduled in the first place. Unhealthy nodes should be repaired or replaced at the earliest opportunity.</p><p>The Cluster API now includes a MachineHealthCheck resource, and a controller that monitors node health. When it detects an unhealthy node, it removes it. (Another Cluster API controller detects the node has been removed and replaces it.) You can configure the controller to suit your needs. You can configure how long to wait before removing the node. You can also set a threshold for the number of unhealthy nodes. When the threshold is reached, no more nodes are removed. The wait can be used to tolerate short-lived outages, and the threshold to prevent too many nodes from being replaced at the same time.</p><p>The controller will remove only nodes managed by a Cluster API MachineSet. The controller does not remove control plane nodes, whether managed by the Kubeadm-based Control Plane, or by the user, as in v1alpha2. For more, see <a href=https://cluster-api.sigs.k8s.io/tasks/healthcheck.html#limitations-and-caveats-of-a-machinehealthcheck>Limits and Caveats of a MachineHealthCheck</a>.</p><p>Here is an example of a MachineHealthCheck. For more details, see <a href=https://cluster-api.sigs.k8s.io/tasks/healthcheck.html>Configure a MachineHealthCheck</a> in the Cluster API book.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>cluster.x-k8s.io/v1alpha3<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>MachineHealthCheck<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-node-unhealthy-5m<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterName</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxUnhealthy</span>:<span style=color:#bbb> </span><span style=color:#666>33</span>%<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeStartupTimeout</span>:<span style=color:#bbb> </span>10m<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodepool</span>:<span style=color:#bbb> </span>nodepool-0<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>unhealthyConditions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Ready<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span>Unknown<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>timeout</span>:<span style=color:#bbb> </span>300s<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Ready<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>timeout</span>:<span style=color:#bbb> </span>300s<span style=color:#bbb>
</span></code></pre></div><h2 id=infrastructure-managed-node-groups>Infrastructure-Managed Node Groups</h2><p><em>Special thanks to <a href=https://github.com/juan-lee>Juan-Lee Pang</a> and <a href=https://github.com/CecileRobertMichon>Cecile Robert-Michon</a></em></p><p>If you run large clusters, you need to create and destroy hundreds of nodes, sometimes in minutes. Although public clouds make it possible to work with large numbers of nodes, having to make a separate API request to create or delete every node may scale poorly. For example, API requests may have to be delayed to stay within rate limits.</p><p>Some public clouds offer APIs to manage groups of nodes as one single entity. For example, AWS has AutoScaling Groups, Azure has Virtual Machine Scale Sets, and GCP has Managed Instance Groups. With this release of Cluster API, Infrastructure Providers can add support for these APIs, and users can deploy groups of Cluster API Machines by using the MachinePool Resource. For more information, see the <a href=https://github.com/kubernetes-sigs/cluster-api/blob/bf51a2502f9007b531f6a9a2c1a4eae1586fb8ca/docs/proposals/20190919-machinepool-api.md>proposal</a> in the Cluster API repository.</p><blockquote><p><strong>Experimental Feature</strong>
The MachinePool API is an experimental feature that is not enabled by default. Users are encouraged to try it and report on how well it meets their needs.</p></blockquote><h2 id=the-cluster-api-user-experience-reimagined>The Cluster API User Experience, Reimagined</h2><h3 id=clusterctl>clusterctl</h3><p><em>Special thanks to <a href=https://github.com/fabriziopandini>Fabrizio Pandini</a></em></p><p>If you are new to Cluster API, your first experience will probably be with the project's command-line tool, clusterctl. And with the new Cluster API release, it has been re-designed to be more pleasing to use than before. The tool is all you need to deploy your first <a href="https://cluster-api.sigs.k8s.io/reference/glossary.html?highlight=pool#workload-cluster">workload cluster</a> in just a few steps.</p><p>First, use <code>clusterctl init</code> to <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/init.html>fetch the configuration</a> for your Infrastructure and Bootstrap Providers and deploy all of the components that make up the Cluster API. Second, use <code>clusterctl config cluster</code> to <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/config-cluster.html>create the workload cluster manifest</a>. This manifest is just a collection of Kubernetes objects. To create the workload cluster, just <code>kubectl apply</code> the manifest. Don't be surprised if this workflow looks familiar: Deploying a workload cluster with Cluster API is just like deploying an application workload with Kubernetes!</p><p>Clusterctl also helps with the "day 2" operations. Use <code>clusterctl move</code> to <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html>migrate Cluster API custom resources</a>, such as Clusters, and Machines, from one <a href=https://cluster-api.sigs.k8s.io/reference/glossary.html#management-cluster>Management Cluster</a> to another. This step--also known as a <a href=https://cluster-api.sigs.k8s.io/reference/glossary.html#pivot>pivot</a>--is necessary to create a workload cluster that manages itself with Cluster API. Finally, use <code>clusterctl upgrade</code> to <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/upgrade.html>upgrade all of the installed components</a> when a new Cluster API release becomes available.</p><p>One more thing! Clusterctl is not only a command-line tool. It is also a Go library! Think of the library as an integration point for projects that build on top of Cluster API. All of clusterctl's command-line functionality is available in the library, making it easy to integrate into your stack. To get started with the library, please read its <a href="https://pkg.go.dev/sigs.k8s.io/cluster-api@v0.3.1/cmd/clusterctl/client?tab=doc">documentation</a>.</p><h3 id=the-cluster-api-book>The Cluster API Book</h3><p><em>Thanks to many contributors!</em></p><p>The <a href=https://cluster-api.sigs.k8s.io/>project's documentation</a> is extensive. New users should get some background on the <a href=https://cluster-api.sigs.k8s.io/user/concepts.html>architecture</a>, and then create a cluster of their own with the <a href=https://cluster-api.sigs.k8s.io/user/quick-start.html>Quick Start</a>. The clusterctl tool has its own <a href=https://cluster-api.sigs.k8s.io/clusterctl/overview.html>reference</a>. The <a href=https://cluster-api.sigs.k8s.io/developer/guide.html>Developer Guide</a> has plenty of information for anyone interested in contributing to the project.</p><p>Above and beyond the content itself, the project's documentation site is a pleasure to use. It is searchable, has an outline, and even supports different color themes. If you think the site a lot like the documentation for a different community project, <a href=https://book.kubebuilder.io/>Kubebuilder</a>, that is no coincidence! Many thanks to Kubebuilder authors for creating a great example of documentation. And many thanks to the <a href=https://github.com/rust-lang/mdBook>mdBook</a> authors for creating a great tool for building documentation.</p><h2 id=integrate-customize>Integrate & Customize</h2><h3 id=end-to-end-test-framework>End-to-End Test Framework</h3><p><em>Special thanks to <a href=https://github.com/chuckha>Chuck Ha</a></em></p><p>The Cluster API project is designed to be extensible. For example, anyone can develop their own Infrastructure and Bootstrap Providers. However, it's important that Providers work in a uniform way. And, because the project is still evolving, it takes work to make sure that Providers are up-to-date with new releases of the core.</p><p>The End-to-End Test Framework provides a set of standard tests for developers to verify that their Providers integrate correctly with the current release of Cluster API, and help identify any regressions that happen after a new release of the Cluster API, or the Provider.</p><p>For more details on the Framework, see <a href="https://cluster-api.sigs.k8s.io/developer/testing.html?highlight=e2e#running-the-end-to-end-tests">Testing</a> in the Cluster API Book, and the <a href=https://github.com/kubernetes-sigs/cluster-api/tree/master/test/framework>README</a> in the repository.</p><h3 id=provider-implementer-s-guide>Provider Implementer's Guide</h3><p><em>Thanks to many contributors!</em></p><p>The community maintains <a href=https://cluster-api.sigs.k8s.io/reference/providers.html>Infrastructure Providers</a> for a many popular infrastructures. However, if you want to build your own Infrastructure or Bootstrap Provider, the <a href=https://cluster-api.sigs.k8s.io/developer/providers/implementers-guide/overview.html>Provider Implementer's</a> guide explains the entire process, from creating a git repository, to creating CustomResourceDefinitions for your Providers, to designing, implementing, and testing the controllers.</p><blockquote><p><strong>Under Active Development</strong>
The Provider Implementer's Guide is actively under development, and may not yet reflect all of the changes in the v1alpha3 release.</p></blockquote><h2 id=join-us>Join Us!</h2><p>The Cluster API project is a very active project, and covers many areas of interest. If you are an infrastructure expert, you can contribute to one of the Infrastructure Providers. If you like building controllers, you will find opportunities to innovate. If you're curious about testing distributed systems, you can help develop the project's end-to-end test framework. Whatever your interests and background, you can make a real impact on the project.</p><p>Come introduce yourself to the community at our weekly meeting, where we dedicate a block of time for a Q&A session. You can also find maintainers and users on the Kubernetes Slack, and in the Kubernetes forum. Please check out the links below. We look forward to seeing you!</p><ul><li>Chat with us on the Kubernetes <a href=http://slack.k8s.io/>Slack</a>:<a href=https://kubernetes.slack.com/archives/C8TSNPY4T> #cluster-api</a></li><li>Join the <a href=https://groups.google.com/forum/>sig-cluster-lifecycle</a> Google Group to receive calendar invites and gain access to documents</li><li>Join our <a href=https://zoom.us/j/861487554>Zoom meeting</a>, every Wednesday at 10:00 Pacific Time</li><li>Post to the <a href=https://discuss.kubernetes.io/c/contributors/cluster-api>Cluster API community forum</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cf21d879935f0508e0410a1deb1e1339>How Kubernetes contributors are building a better communication process</h1><div class="td-byline mb-4"><time datetime=2020-04-21 class=text-muted>Tuesday, April 21, 2020</time></div><p><strong>Authors:</strong> Paris Pittman</p><blockquote><p>"Perhaps we just need to use a different word. We may need to use community development or project advocacy as a word in the open source realm as opposed to marketing, and perhaps then people will realize that they need to do it."
~ <a href=https://todogroup.org/www.linkedin.com/in/nithyaruff/><em>Nithya Ruff</em></a> (from <a href=https://todogroup.org/guides/marketing-open-source-projects/><em>TODO Group</em></a>)</p></blockquote><p>A common way to participate in the Kubernetes contributor community is
to be everywhere.</p><p>We have an active <a href=https://slack.k8s.io>Slack</a>, many mailing lists, Twitter account(s), and
dozens of community-driven podcasts and newsletters that highlight all
end-user, contributor, and ecosystem topics. And to add on to that, we also have <a href=http://github.com/kubernetes/community>repositories of amazing documentation</a>, tons of <a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&ctz=America%2FLos_Angeles">meetings</a> that drive the project forward, and <a href="https://www.youtube.com/watch?v=yqB_le-N6EE">recorded code deep dives</a>. All of this information is incredibly valuable,
but it can be too much.</p><p>Keeping up with thousands of contributors can be a challenge for anyone,
but this task of consuming information straight from the firehose is
particularly challenging for new community members. It's no secret that
the project is vast for contributors and users alike.</p><p>To paint a picture with numbers:</p><ul><li>43,000 contributors</li><li>6,579 members in #kubernetes-dev slack channel</li><li>52 mailing lists (kubernetes-dev@ has thousands of members; sig-networking@ has 1000 alone)</li><li>40 <a href=https://github.com/kubernetes/community/blob/master/governance.md#community-groups>community groups</a></li><li>30 <a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&ctz=America%2FLos_Angeles">upstream meetings</a> <em>this</em> week alone</li></ul><p>All of these numbers are only growing in scale, and with that comes the need to simplify how contributors get the information right information front-and-center.</p><h2 id=how-we-got-here>How we got here</h2><p>Kubernetes (K8s for short) communication grew out of a need for people
to connect in our growing community. With the best of intentions, the
community spun up channels for people to connect. This energy was part
of what helped Kubernetes grow so fast, and it also had us in sprawling out far and wide. As adoption grew, <a href=https://github.com/kubernetes/community/issues/2466>contributors knew there was a need for standardization</a>.</p><p>This new attention to how the community communicates led to the discovery
of a complex web of options. There were so many options, and it was a
challenge for anyone to be sure they were in the right place to receive
the right information. We started taking immediate action combining communication streams and thinking about how to reach out best to serve our community. We also asked for feedback from all our
contributors directly via <a href=https://github.com/kubernetes/community/tree/master/sig-contributor-experience/surveys><strong>annual surveys</strong></a>
to see where folks were actually reading the news that influences their
experiences here in our community.</p><p><img src=https://user-images.githubusercontent.com/1744971/79478603-3a3a1980-7fd1-11ea-8b7a-d36aac7a097b.png alt="Kubernetes channel access"></p><p>With over 43,000 contributors, our contributor base is larger than many enterprise companies. You can imagine what it's like getting important messages across to make sure they are landing and folks are taking action.</p><h2 id=contributing-to-better-communication>Contributing to better communication</h2><p>Think about how your company/employer solves for this kind of communication challenge. Many have done so
by building internal marketing and communication focus areas in
marketing departments. So that's what we are doing. This has also been
applied <a href=https://fedoraproject.org/wiki/Marketing>at Fedora</a> and at a smaller
scale in our very <a href=https://github.com/kubernetes/sig-release/tree/master/release-team>own release</a> and <a href=https://github.com/kubernetes/community/blob/d0fd6c16f7ee754b08082cc15658eb8db7afeaf8/events/events-team/marketing/README.md>contributor summit</a> planning
teams as roles.</p><p>We have hit the accelerator on an <strong>upstream marketing group</strong> under SIG
Contributor Experience and we want to tackle this challenge straight on.
We've learned in other contributor areas that creating roles for
contributors is super helpful - onboarding, breaking down work, and
ownership. <a href=https://github.com/kubernetes/community/tree/master/communication/marketing-team>Here's our team charting the course</a>.</p><p>Journey your way through our other documents like our <a href=https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md>charter</a> if you are
interested in our mission and scope.</p><p>Many of you close to the ecosystem might be scratching your head - isn't
this what CNCF does?</p><p>Yes and no. The CNCF has 40+ other projects that need to be marketed to
a countless number of different types of community members in distinct
ways and they aren't responsible for the day to day operations of their
projects. They absolutely do partner with us to highlight what we need
and when we need it, and they do a fantastic job of it (one example is
the <a href=https://twitter.com/kubernetesio><em>@kubernetesio Twitter account</em></a> and its 200,000
followers).</p><p>Where this group differs is in its scope: we are entirely
focused on elevating the hard work being done throughout the Kubernetes
community by its contributors.</p><h2 id=what-to-expect-from-us>What to expect from us</h2><p>You can expect to see us on the Kubernetes <a href=https://github.com/kubernetes/community/tree/master/communication>communication channels</a> supporting you by:</p><ul><li>Finding ways of adding our human touch to potentially overwhelming
quantities of info by storytelling and other methods - we want to
highlight the work you are doing and provide useful information!</li><li>Keeping you in the know of the comings and goings of contributor
community events, activities, mentoring initiatives, KEPs, and more.</li><li>Creating a presence on Twitter specifically for contributors via
@k8scontributors that is all about being a contributor in all its
forms.</li></ul><p>What does this look like in the wild? Our <a href=https://kubernetes.io/blog/2020/03/19/join-sig-scalability/>first post</a> in a series about our 36 community groups landed recently. Did you see it?
More articles like this and additional themes of stories to flow through
<a href=https://github.com/kubernetes/community/tree/master/communication/marketing-team#purpose>our storytellers</a>.</p><p>We will deliver this with an <a href=https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md#ethosvision>ethos</a> behind us aligned to the Kubernetes project as a whole, and we're
committed to using the same tools as all the other SIGs to do so. Check out our <a href=https://github.com/orgs/kubernetes/projects/39>project board</a> to view our roadmap of upcoming work.</p><h2 id=join-us-and-be-part-of-the-story>Join us and be part of the story</h2><p>This initiative is in an early phase and we still have important roles to fill to make it successful.</p><p>If you are interested in open sourcing marketing functions – it's a fun
ride – join us! Specific immediate roles include storytelling through
blogs and as a designer. We also have plenty of work in progress on our project board.
Add a comment to any open issue to let us know you're interested in getting involved.</p><p>Also, if you're reading this, you're exactly the type of person we are
here to support. We would love to hear about how to improve, feedback,
or how we can work together.</p><p>Reach out at one of the contact methods listed on <a href=https://github.com/kubernetes/community/tree/master/communication/marketing-team#contact-us>our README</a>. We would love to hear from you.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b0f1df51dcc400e9f59126cd3120fb45>API Priority and Fairness Alpha</h1><div class="td-byline mb-4"><time datetime=2020-04-06 class=text-muted>Monday, April 06, 2020</time></div><p><strong>Authors:</strong> Min Kim (Ant Financial), Mike Spreitzer (IBM), Daniel Smith (Google)</p><p>This blog describes “API Priority And Fairness”, a new alpha feature in Kubernetes 1.18. API Priority And Fairness permits cluster administrators to divide the concurrency of the control plane into different weighted priority levels. Every request arriving at a kube-apiserver will be categorized into one of the priority levels and get its fair share of the control plane’s throughput.</p><h2 id=what-problem-does-this-solve>What problem does this solve?</h2><p>Today the apiserver has a simple mechanism for protecting itself against CPU and memory overloads: max-in-flight limits for mutating and for readonly requests. Apart from the distinction between mutating and readonly, no other distinctions are made among requests; consequently, there can be undesirable scenarios where one subset of the requests crowds out other requests.</p><p>In short, it is far too easy for Kubernetes workloads to accidentally DoS the apiservers, causing other important traffic--like system controllers or leader elections---to fail intermittently. In the worst cases, a few broken nodes or controllers can push a busy cluster over the edge, turning a local problem into a control plane outage.</p><h2 id=how-do-we-solve-the-problem>How do we solve the problem?</h2><p>The new feature “API Priority and Fairness” is about generalizing the existing max-in-flight request handler in each apiserver, to make the behavior more intelligent and configurable. The overall approach is as follows.</p><ol><li>Each request is matched by a <em>Flow Schema</em>. The Flow Schema states the Priority Level for requests that match it, and assigns a “flow identifier” to these requests. Flow identifiers are how the system determines whether requests are from the same source or not.</li><li>Priority Levels may be configured to behave in several ways. Each Priority Level gets its own isolated concurrency pool. Priority levels also introduce the concept of queuing requests that cannot be serviced immediately.</li><li>To prevent any one user or namespace from monopolizing a Priority Level, they may be configured to have multiple queues. <a href=https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/#What_is_shuffle_sharding.3F>“Shuffle Sharding”</a> is used to assign each flow of requests to a subset of the queues.</li><li>Finally, when there is capacity to service a request, a <a href=https://en.wikipedia.org/wiki/Fair_queuing>“Fair Queuing”</a> algorithm is used to select the next request. Within each priority level the queues compete with even fairness.</li></ol><p>Early results have been very promising! Take a look at this <a href=https://github.com/kubernetes/kubernetes/pull/88177#issuecomment-588945806>analysis</a>.</p><h2 id=how-do-i-try-this-out>How do I try this out?</h2><p>You are required to prepare the following things in order to try out the feature:</p><ul><li>Download and install a kubectl greater than v1.18.0 version</li><li>Enabling the new API groups with the command line flag <code>--runtime-config="flowcontrol.apiserver.k8s.io/v1alpha1=true"</code> on the kube-apiservers</li><li>Switch on the feature gate with the command line flag <code>--feature-gates=APIPriorityAndFairness=true</code> on the kube-apiservers</li></ul><p>After successfully starting your kube-apiservers, you will see a few default FlowSchema and PriorityLevelConfiguration resources in the cluster. These default configurations are designed for a general protection and traffic management for your cluster.
You can examine and customize the default configuration by running the usual tools, e.g.:</p><ul><li><code>kubectl get flowschemas</code></li><li><code>kubectl get prioritylevelconfigurations</code></li></ul><h2 id=how-does-this-work-under-the-hood>How does this work under the hood?</h2><p>Upon arrival at the handler, a request is assigned to exactly one priority level and exactly one flow within that priority level. Hence understanding how FlowSchema and PriorityLevelConfiguration works will be helping you manage the request traffic going through your kube-apiservers.</p><ul><li><p>FlowSchema: FlowSchema will identify a PriorityLevelConfiguration object and the way to compute the request’s “flow identifier”. Currently we support matching requests according to: the identity making the request, the verb, and the target object. The identity can match in terms of: a username, a user group name, or a ServiceAccount. And as for the target objects, we can match by apiGroup, resource[/subresource], and namespace.</p><ul><li>The flow identifier is used for shuffle sharding, so it’s important that requests have the same flow identifier if they are from the same source! We like to consider scenarios with “elephants” (which send many/heavy requests) vs “mice” (which send few/light requests): it is important to make sure the elephant’s requests all get the same flow identifier, otherwise they will look like many different mice to the system!</li><li>See the API Documentation <a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io>here</a>!</li></ul></li><li><p>PriorityLevelConfiguration: Defines a priority level.</p><ul><li>For apiserver self requests, and any reentrant traffic (e.g., admission webhooks which themselves make API requests), a Priority Level can be marked “exempt”, which means that no queueing or limiting of any sort is done. This is to prevent priority inversions.</li><li>Each non-exempt Priority Level is configured with a number of "concurrency shares" and gets an isolated pool of concurrency to use. Requests of that Priority Level run in that pool when it is not full, never anywhere else. Each apiserver is configured with a total concurrency limit (taken to be the sum of the old limits on mutating and readonly requests), and this is then divided among the Priority Levels in proportion to their concurrency shares.</li><li>A non-exempt Priority Level may select a number of queues and a "hand size" to use for the shuffle sharding. Shuffle sharding maps flows to queues in a way that is better than consistent hashing. A given flow has access to a small collection of queues, and for each incoming request the shortest queue is chosen. When a Priority Level has queues, it also sets a limit on queue length. There is also a limit placed on how long a request can wait in its queue; this is a fixed fraction of the apiserver's request timeout. A request that cannot be executed and cannot be queued (any longer) is rejected.</li><li>Alternatively, a non-exempt Priority Level may select immediate rejection instead of waiting in a queue.</li><li>See the <a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io>API documentation</a> for this feature.</li></ul></li></ul><h2 id=what-s-missing-when-will-there-be-a-beta>What’s missing? When will there be a beta?</h2><p>We’re already planning a few enhancements based on alpha and there will be more as users send feedback to our community. Here’s a list of them:</p><ul><li>Traffic management for WATCH and EXEC requests</li><li>Adjusting and improving the default set of FlowSchema/PriorityLevelConfiguration</li><li>Enhancing observability on how this feature works</li><li>Join the discussion <a href=https://github.com/kubernetes/enhancements/pull/1632>here</a></li></ul><p>Possibly treat LIST requests differently depending on an estimate of how big their result will be.</p><h2 id=how-can-i-get-involved>How can I get involved?</h2><p>As always! Reach us on slack <a href=https://kubernetes.slack.com/messages/sig-api-machinery>#sig-api-machinery</a>, or through the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery>mailing list</a>. We have lots of exciting features to build and can use all sorts of help.</p><p>Many thanks to the contributors that have gotten this feature this far: Aaron Prindle, Daniel Smith, Jonathan Tomer, Mike Spreitzer, Min Kim, Bruce Ma, Yu Liao, Mengyi Zhou!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e4bdb6be4946e6f9bbea5e9b7a7aeb45>Introducing Windows CSI support alpha for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2020-04-03 class=text-muted>Friday, April 03, 2020</time></div><p><strong>Authors:</strong> Authors: Deep Debroy [Docker], Jing Xu [Google], Krishnakumar R (KK) [Microsoft]</p><p><em>The alpha version of <a href=https://github.com/kubernetes-csi/csi-proxy>CSI Proxy</a> for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.</em></p><h2 id=background>Background</h2><p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. All new storage features will utilize CSI, therefore it is important to get CSI drivers to work on Windows.</p><p>A CSI driver in Kubernetes has two main components: a controller plugin and a node plugin. The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services (e.g. cloud storage service). The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. This was previously not possible for containers on Windows. With the release of <a href=https://github.com/kubernetes-csi/csi-proxy>CSIProxy</a>, CSI drivers can now perform storage operations on the node. This inturn enables containerized CSI Drivers to run on Windows.</p><h2 id=csi-support-for-windows-clusters>CSI support for Windows clusters</h2><p>CSI drivers (e.g. AzureDisk, GCE PD, etc.) are recommended to be deployed as containers. CSI driver’s node plugin typically runs on every worker node in the cluster (as a DaemonSet). Node plugin containers need to run with elevated privileges to perform storage related operations. However, Windows currently does not support privileged containers. To solve this problem, <a href=https://github.com/kubernetes-csi/csi-proxy>CSIProxy</a> makes it so that node plugins can now be deployed as unprivileged pods and then use the proxy to perform privileged storage operations on the node.</p><h2 id=node-plugin-interactions-with-csiproxy>Node plugin interactions with CSIProxy</h2><p>The design of the CSI proxy is captured in this <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190714-windows-csi-support.md>KEP</a>. The following diagram depicts the interactions with the CSI node plugin and CSI proxy.</p><p align=center><img src=/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyOverview.png></p><p>The CSI proxy runs as a process directly on the host on every windows node - very similar to kubelet. The CSI code in kubelet interacts with the <a href=https://kubernetes-csi.github.io/docs/node-driver-registrar.html>node driver registrar</a> component and the CSI node plugin. The node driver registrar is a community maintained CSI project which handles the registration of vendor specific node plugins. The kubelet initiates CSI gRPC calls like NodeStageVolume/NodePublishVolume on the node plugin as described in the figure. Node plugins interface with the CSIProxy process to perform local host OS storage related operations such as creation/enumeration of volumes, mounting/unmounting, etc.</p><h2 id=csi-proxy-architecture-and-implementation>CSI proxy architecture and implementation</h2><p align=center><img src=/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyArchitecture.png></p><p>In the alpha release, CSIProxy supports the following API groups:</p><ol><li>Filesystem</li><li>Disk</li><li>Volume</li><li>SMB</li></ol><p>CSI proxy exposes each API group via a Windows named pipe. The communication is performed using gRPC over these pipes. The client library from the CSI proxy project uses these pipes to interact with the CSI proxy APIs. For example, the filesystem APIs are exposed via a pipe like <code>\.\pipe\csi-proxy-filesystem-v1alpha1</code> and volume APIs under the <code>\.\pipe\csi-proxy-volume-v1alpha1</code>, and so on.</p><p>From each API group service, the calls are routed to the host API layer. The host API calls into the host Windows OS by either Powershell or Go standard library calls. For example, when the filesystem API <a href=https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.proto>Rmdir</a> is called the API group service would decode the grpc structure <a href=https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.pb.go>RmdirRequest</a> and find the directory to be removed and call into the Host APIs layer. This would result in a call to <a href=https://github.com/kubernetes-csi/csi-proxy/blob/master/internal/os/filesystem/api.go>os.Remove</a>, a Go standard library call, to perform the remove operation.</p><h2 id=control-flow-details>Control flow details</h2><p>The following figure uses CSI call NodeStageVolume as an example to explain the interaction between kubelet, CSI plugin, and CSI proxy for provisioning a fresh volume. After the node plugin receives a CSI RPC call, it makes a few calls to CSIproxy accordingly. As a result of the NodeStageVolume call, first the required disk is identified using either of the Disk API calls: ListDiskLocations (in AzureDisk driver) or GetDiskNumberByName (in GCE PD driver). If the disk is not partitioned, then the PartitionDisk (Disk API group) is called. Subsequently, Volume API calls such as ListVolumesOnDisk, FormatVolume and MountVolume are called to perform the rest of the required operations. Similar operations are performed in case of NodeUnstageVolume, NodePublishVolume, NodeUnpublishedVolume, etc.</p><p align=center><img src=/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyControlFlow.png></p><h2 id=current-support>Current support</h2><p>CSI proxy is now available as alpha. You can find more details on the <a href=https://github.com/kubernetes-csi/csi-proxy>CSIProxy</a> GitHub repository. There are currently two cloud providers that provide alpha support for CSI drivers on Windows: Azure and GCE.</p><h2 id=future-plans>Future plans</h2><p>One key area of focus in beta is going to be Windows based build and CI/CD setup to improve the stability and quality of the code base. Another area is using Go based calls directly instead of Powershell commandlets to improve performance. Enhancing debuggability and adding more tests are other areas which the team will be looking into.</p><h2 id=how-to-get-involved>How to get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p>For those interested in more details, the <a href=https://github.com/kubernetes-csi/csi-proxy>CSIProxy</a> GitHub repository is a good place to start. In addition, the <a href=https://kubernetes.slack.com/messages/csi-windows>#csi-windows</a> channel on kubernetes slack is available for discussions specific to the CSI on Windows.</p><h2 id=acknowledgments>Acknowledgments</h2><p>We would like to thank Michelle Au for guiding us throughout this journey to alpha. We would like to thank Jean Rougé for contributions during the initial CSI proxy effort. We would like to thank Saad Ali for all the guidance with respect to the project and review/feedback on a draft of this blog. We would like to thank Patrick Lang and Mark Rossetti for helping us with Windows specific questions and details. Special thanks to Andy Zhang for reviews and guidance with respect to Azuredisk and Azurefile work. A big thank you to Paul Burt and Karen Chu for the review and suggestions on improving this blog post.</p><p>Last but not the least, we would like to thank the broader Kubernetes community who contributed at every step of the project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c1a4f74c03f8c8a0605e152bb006a5e1>Improvements to the Ingress API in Kubernetes 1.18</h1><div class="td-byline mb-4"><time datetime=2020-04-02 class=text-muted>Thursday, April 02, 2020</time></div><p><strong>Authors:</strong> Rob Scott (Google), Christopher M Luciano (IBM)</p><p>The Ingress API in Kubernetes has enabled a large number of controllers to provide simple and powerful ways to manage inbound network traffic to Kubernetes workloads. In Kubernetes 1.18, we've made 3 significant additions to this API:</p><ul><li>A new <code>pathType</code> field that can specify how Ingress paths should be matched.</li><li>A new <code>IngressClass</code> resource that can specify how Ingresses should be implemented by controllers.</li><li>Support for wildcards in hostnames.</li></ul><h2 id=better-path-matching-with-path-types>Better Path Matching With Path Types</h2><p>The new concept of a path type allows you to specify how a path should be matched. There are three supported types:</p><ul><li><strong>ImplementationSpecific (default):</strong> With this path type, matching is up to the controller implementing the <code>IngressClass</code>. Implementations can treat this as a separate <code>pathType</code> or treat it identically to the <code>Prefix</code> or <code>Exact</code> path types.</li><li><strong>Exact:</strong> Matches the URL path exactly and with case sensitivity.</li><li><strong>Prefix:</strong> Matches based on a URL path prefix split by <code>/</code>. Matching is case sensitive and done on a path element by element basis.</li></ul><h2 id=extended-configuration-with-ingress-classes>Extended Configuration With Ingress Classes</h2><p>The Ingress resource was designed with simplicity in mind, providing a simple set of fields that would be applicable in all use cases. Over time, as use cases evolved, implementations began to rely on a long list of custom annotations for further configuration. The new <code>IngressClass</code> resource provides a way to replace some of those annotations.</p><p>Each <code>IngressClass</code> specifies which controller should implement Ingresses of the class and can reference a custom resource with additional parameters.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;networking.k8s.io/v1beta1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IngressClass&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;external-lb&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example.com/ingress-controller&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;k8s.example.com/v1alpha&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IngressParameters&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;external-lb&#34;</span><span style=color:#bbb>
</span></code></pre></div><h3 id=specifying-the-class-of-an-ingress>Specifying the Class of an Ingress</h3><p>A new <code>ingressClassName</code> field has been added to the Ingress spec that is used to reference the <code>IngressClass</code> that should be used to implement this Ingress.</p><h3 id=deprecating-the-ingress-class-annotation>Deprecating the Ingress Class Annotation</h3><p>Before the <code>IngressClass</code> resource was added in Kubernetes 1.18, a similar concept of Ingress class was often specified with a <code>kubernetes.io/ingress.class</code> annotation on the Ingress. Although this annotation was never formally defined, it was widely supported by Ingress controllers, and should now be considered formally deprecated.</p><h3 id=setting-a-default-ingressclass>Setting a Default IngressClass</h3><p>It’s possible to mark a specific <code>IngressClass</code> as default in a cluster. Setting the
<code>ingressclass.kubernetes.io/is-default-class</code> annotation to true on an
IngressClass resource will ensure that new Ingresses without an <code>ingressClassName</code> specified will be assigned this default <code>IngressClass</code>.</p><h2 id=support-for-hostname-wildcards>Support for Hostname Wildcards</h2><p>Many Ingress providers have supported wildcard hostname matching like <code>*.foo.com</code> matching <code>app1.foo.com</code>, but until now the spec assumed an exact FQDN match of the host. Hosts can now be precise matches (for example “<code>foo.bar.com</code>”) or a wildcard (for example “<code>*.foo.com</code>”). Precise matches require that the http host header matches the Host setting. Wildcard matches require the http host header is equal to the suffix of the wildcard rule.</p><table><thead><tr><th>Host</th><th>Host header</th><th>Match?</th></tr></thead><tbody><tr><td><code>*.foo.com</code></td><td><code>bar.foo.com</code></td><td>Matches based on shared suffix</td></tr><tr><td><code>*.foo.com</code></td><td><code>baz.bar.foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr><tr><td><code>*.foo.com</code></td><td><code>foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr></tbody></table><h3 id=putting-it-all-together>Putting it All Together</h3><p>These new Ingress features allow for much more configurability. Here’s an example of an Ingress that makes use of pathType, <code>ingressClassName</code>, and a hostname wildcard:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;networking.k8s.io/v1beta1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Ingress&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-ingress&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingressClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;external-lb&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;*.example.com&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/example&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Prefix&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-service&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>servicePort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div><h3 id=ingress-controller-support>Ingress Controller Support</h3><p>Since these features are new in Kubernetes 1.18, each Ingress controller implementation will need some time to develop support for these new features. Check the documentation for your preferred Ingress controllers to see when they will support this new functionality.</p><h2 id=the-future-of-ingress>The Future of Ingress</h2><p>The Ingress API is on pace to graduate from beta to a stable API in Kubernetes 1.19. It will continue to provide a simple way to manage inbound network traffic for Kubernetes workloads. This API has intentionally been kept simple and lightweight, but there has been a desire for greater configurability for more advanced use cases.</p><p>Work is currently underway on a new highly configurable set of APIs that will provide an alternative to Ingress in the future. These APIs are being referred to as the new “Service APIs”. They are not intended to replace any existing APIs, but instead provide a more configurable alternative for complex use cases. For more information, check out the <a href=http://github.com/kubernetes-sigs/service-apis>Service APIs repo on GitHub</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4771ce69b566c519a6f2d40cece40049>Kubernetes 1.18 Feature Server-side Apply Beta 2</h1><div class="td-byline mb-4"><time datetime=2020-04-01 class=text-muted>Wednesday, April 01, 2020</time></div><p><strong>Authors:</strong> Antoine Pelisse (Google)</p><h2 id=what-is-server-side-apply>What is Server-side Apply?</h2><p>Server-side Apply is an important effort to migrate “kubectl apply” to the apiserver. It was started in 2018 by the Apply working group.</p><p>The use of kubectl to declaratively apply resources has exposed the following challenges:</p><ul><li><p>One needs to use the kubectl go code, or they have to shell out to kubectl.</p></li><li><p>Strategic merge-patch, the patch format used by kubectl, grew organically and was challenging to fix while maintaining compatibility with various api-server versions.</p></li><li><p>Some features are hard to implement directly on the client, for example, unions.</p></li></ul><p>Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field.</p><h2 id=how-does-it-work-what-s-managedfields>How does it work, what’s managedFields?</h2><p>Server-side Apply works by keeping track of which actor of the system has changed each field of an object. It does so by diffing all updates to objects, and recording all the fields that have changed as well the time of the operation. All this information is stored in the managedFields in the metadata of objects. Since objects can have many fields, this field can be quite large.</p><p>When someone applies, we can then use the information stored within managedFields to report relevant conflicts and help the merge algorithm to do the right thing.</p><h2 id=wasn-t-it-already-beta-before-1-18>Wasn’t it already Beta before 1.18?</h2><p>Yes, Server-side Apply has been Beta since 1.16, but it didn’t track the owner for fields associated with objects that had not been applied. This means that most objects didn’t have the managedFields metadata stored, and conflicts for these objects cannot be resolved. With Kubernetes 1.18, all new objects will have the managedFields attached to them and provide accurate information on conflicts.</p><h2 id=how-do-i-use-it>How do I use it?</h2><p>The most common way to use this is through kubectl: <code>kubectl apply --server-side</code>. This is likely to show conflicts with other actors, including client-side apply. When that happens, conflicts can be forced by using the <code>--force-conflicts</code> flag, which will grab the ownership for the fields that have changed.</p><h2 id=current-limitations>Current limitations</h2><p>We have two important limitations right now, especially with sub-resources. The first is that if you apply with a status, the status is going to be ignored. We are still going to try and acquire the fields, which may lead to invalid conflicts. The other is that we do not update the managedFields on some sub-resources, including scale, so you may not see information about a horizontal pod autoscaler changing the number of replicas.</p><h2 id=what-s-next>What’s next?</h2><p>We are working hard to improve the experience of using server-side apply with kubectl, and we are trying to make it the default. As part of that, we want to improve the migration from client-side to server-side.</p><h2 id=can-i-help>Can I help?</h2><p>Of course! The working-group apply is available on slack #wg-apply, through the <a href=https://groups.google.com/forum/#!forum/kubernetes-wg-apply>mailing list</a> and we also meet every other Tuesday at 9.30 PT on Zoom. We have lots of exciting features to build and can use all sorts of help.</p><p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this new beta possible:</p><ul><li>Daniel Smith</li><li>Jenny Buckley</li><li>Joe Betz</li><li>Julian Modesto</li><li>Kevin Wiesmüller</li><li>Maria Ntalla</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d1357e1de94f06acc5d502535391b75b>Kubernetes Topology Manager Moves to Beta - Align Up!</h1><div class="td-byline mb-4"><time datetime=2020-04-01 class=text-muted>Wednesday, April 01, 2020</time></div><p><strong>Authors:</strong> Kevin Klues (NVIDIA), Victor Pickard (Red Hat), Conor Nolan (Intel)</p><p>This blog post describes the <strong><code>TopologyManager</code></strong>, a beta feature of Kubernetes in release 1.18. The <strong><code>TopologyManager</code></strong> feature enables NUMA alignment of CPUs and peripheral devices (such as SR-IOV VFs and GPUs), allowing your workload to run in an environment optimized for low-latency.</p><p>Prior to the introduction of the <strong><code>TopologyManager</code></strong>, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications. With the introduction of the <strong><code>TopologyManager</code></strong>, we now have a way to avoid this.</p><p>This blog post covers:</p><ol><li>A brief introduction to NUMA and why it is important</li><li>The policies available to end-users to ensure NUMA alignment of CPUs and devices</li><li>The internal details of how the <strong><code>TopologyManager</code></strong> works</li><li>Current limitations of the <strong><code>TopologyManager</code></strong></li><li>Future directions of the <strong><code>TopologyManager</code></strong></li></ol><h2 id=so-what-is-numa-and-why-do-i-care>So, what is NUMA and why do I care?</h2><p>The term NUMA stands for Non-Uniform Memory Access. It is a technology available on multi-cpu systems that allows different CPUs to access different parts of memory at different speeds. Any memory directly connected to a CPU is considered "local" to that CPU and can be accessed very fast. Any memory not directly connected to a CPU is considered "non-local" and will have variable access times depending on how many interconnects must be passed through in order to reach it. On modern systems, the idea of having "local" vs. "non-local" memory can also be extended to peripheral devices such as NICs or GPUs. For high performance, CPUs and devices should be allocated such that they have access to the same local memory.</p><p>All memory on a NUMA system is divided into a set of "NUMA nodes", with each node representing the local memory for a set of CPUs or devices. We talk about an individual CPU as being part of a NUMA node if its local memory is associated with that NUMA node.</p><p>We talk about a peripheral device as being part of a NUMA node based on the shortest number of interconnects that must be passed through in order to reach it.</p><p>For example, in Figure 1, CPUs 0-3 are said to be part of NUMA node 0, whereas CPUs 4-7 are part of NUMA node 1. Likewise GPU 0 and NIC 0 are said to be part of NUMA node 0 because they are attached to Socket 0, whose CPUs are all part of NUMA node 0. The same is true for GPU 1 and NIC 1 on NUMA node 1.</p><p align=center><img height=300 src=/images/blog/2020-03-25-kubernetes-1.18-release-announcement/example-numa-system.png></p><p><strong>Figure 1:</strong> An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1.</p><p>Although the example above shows a 1-1 mapping of NUMA Node to Socket, this is not necessarily true in the general case. There may be multiple sockets on a single NUMA node, or individual CPUs of a single socket may be connected to different NUMA nodes. Moreover, emerging technologies such as Sub-NUMA Clustering (<a href=https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview>available on recent intel CPUs</a>) allow single CPUs to be associated with multiple NUMA nodes so long as their memory access times to both nodes are the same (or have a negligible difference).</p><p>The <strong><code>TopologyManager</code></strong> has been built to handle all of these scenarios.</p><h2 id=align-up-it-s-a-team-effort>Align Up! It's a TeaM Effort!</h2><p>As previously stated, the <strong><code>TopologyManager</code></strong> allows users to align their CPU and peripheral device allocations by NUMA node. There are several policies available for this:</p><ul><li><strong><code>none:</code></strong> this policy will not attempt to do any alignment of resources. It will act the same as if the <strong><code>TopologyManager</code></strong> were not present at all. This is the default policy.</li><li><strong><code>best-effort:</code></strong> with this policy, the <strong><code>TopologyManager</code></strong> will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node.</li><li><strong><code>restricted:</code></strong> this policy is the same as the <strong><code>best-effort</code></strong> policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the <strong><code>single-numa-node</code></strong> policy, some allocations may come from multiple NUMA nodes if it is impossible to <em>ever</em> satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes).</li><li><strong><code>single-numa-node:</code></strong> this policy is the most restrictive and will only allow a pod to be admitted if <em>all</em> requested CPUs and devices can be allocated from exactly one NUMA node.</li></ul><p>It is important to note that the selected policy is applied to each container in a pod spec individually, rather than aligning resources across all containers together.</p><p>Moreover, a single policy is applied to <em>all</em> pods on a node via a global <strong><code>kubelet</code></strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis). We hope to relax this restriction in the future.</p><p>The <strong><code>kubelet</code></strong> flag to set one of these policies can be seen below:</p><pre><code>--topology-manager-policy=
    [none | best-effort | restricted | single-numa-node]
</code></pre><p>Additionally, the <strong><code>TopologyManager</code></strong> is protected by a feature gate. This feature gate has been available since Kubernetes 1.16, but has only been enabled by default since 1.18.</p><p>The feature gate can be enabled or disabled as follows (as described in more detail <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/>here</a>):</p><pre><code>--feature-gates=&quot;...,TopologyManager=&lt;true|false&gt;&quot;
</code></pre><p>In order to trigger alignment according to the selected policy, a user must request CPUs and peripheral devices in their pod spec, according to a certain set of requirements.</p><p>For peripheral devices, this means requesting devices from the available resources provided by a device plugin (e.g. <strong><code>intel.com/sriov</code></strong>, <strong><code>nvidia.com/gpu</code></strong>, etc.). This will only work if the device plugin has been extended to integrate properly with the <strong><code>TopologyManager</code></strong>. Currently, the only plugins known to have this extension are the <a href=https://github.com/NVIDIA/k8s-device-plugin/blob/5cb45d52afdf5798a40f8d0de049bce77f689865/nvidia.go#L74>Nvidia GPU device plugin</a>, and the <a href=https://github.com/intel/sriov-network-device-plugin/blob/30e33f1ce2fc7b45721b6de8c8207e65dbf2d508/pkg/resources/pciNetDevice.go#L80>Intel SRIOV network device plugin</a>. Details on how to extend a device plugin to integrate with the <strong><code>TopologyManager</code></strong> can be found <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager>here</a>.</p><p>For CPUs, this requires that the <strong><code>CPUManager</code></strong> has been configured with its <strong><code>--static</code></strong> policy enabled and that the pod is running in the Guaranteed QoS class (i.e. all CPU and memory <strong><code>limits</code></strong> are equal to their respective CPU and memory <strong><code>requests</code></strong>). CPUs must also be requested in whole number values (e.g. <strong><code>1</code></strong>, <strong><code>2</code></strong>, <strong><code>1000m</code></strong>, etc). Details on how to set the <strong><code>CPUManager</code></strong> policy can be found <a href=https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies>here</a>.</p><p>For example, assuming the <strong><code>CPUManager</code></strong> is running with its <strong><code>--static</code></strong> policy enabled and the device plugins for <strong><code>gpu-vendor.com</code></strong>, and <strong><code>nic-vendor.com</code></strong> have been extended to integrate with the <strong><code>TopologyManager</code></strong> properly, the pod spec below is sufficient to trigger the <strong><code>TopologyManager</code></strong> to run its selected policy:</p><pre><code>spec:
   containers:
   - name: numa-aligned-container
     image: alpine
     resources:
         limits:
             cpu: 2
             memory: 200Mi
             gpu-vendor.com/gpu: 1
             nic-vendor.com/nic: 1
</code></pre><p>Following Figure 1 from the previous section, this would result in one of the following aligned allocations:</p><pre><code>{cpu: {0, 1}, gpu: 0, nic: 0}
{cpu: {0, 2}, gpu: 0, nic: 0}
{cpu: {0, 3}, gpu: 0, nic: 0}
{cpu: {1, 2}, gpu: 0, nic: 0}
{cpu: {1, 3}, gpu: 0, nic: 0}
{cpu: {2, 3}, gpu: 0, nic: 0}

{cpu: {4, 5}, gpu: 1, nic: 1}
{cpu: {4, 6}, gpu: 1, nic: 1}
{cpu: {4, 7}, gpu: 1, nic: 1}
{cpu: {5, 6}, gpu: 1, nic: 1}
{cpu: {5, 7}, gpu: 1, nic: 1}
{cpu: {6, 7}, gpu: 1, nic: 1}
</code></pre><p>And that’s it! Just follow this pattern to have the <strong><code>TopologyManager</code></strong> ensure NUMA alignment across containers that request topology-aware devices and exclusive CPUs.</p><p>**NOTE:** if a pod is rejected by one of the **<code>TopologyManager</code>** policies, it will be placed in a **<code>Terminated</code>** state with a pod admission error and a reason of "<strong><code>TopologyAffinityError</code><strong>". Once a pod is in this state, the Kubernetes scheduler will not attempt to reschedule it. It is therefore recommended to use a </strong><code>Deployment</code></strong><a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment></a> with replicas to trigger a redeploy of the pod on such a failure. An <a href=https://kubernetes.io/docs/concepts/architecture/controller/>external control loop</a> can also be implemented to trigger a redeployment of pods that have a <strong><code>TopologyAffinityError</code></strong>.</p><h2 id=this-is-great-so-how-does-it-work-under-the-hood>This is great, so how does it work under the hood?</h2><p>Pseudocode for the primary logic carried out by the <strong><code>TopologyManager</code></strong> can be seen below:</p><pre><code>for container := range append(InitContainers, Containers...) {
    for provider := range HintProviders {
        hints += provider.GetTopologyHints(container)
    }

    bestHint := policy.Merge(hints)

    for provider := range HintProviders {
        provider.Allocate(container, bestHint)
    }
}
</code></pre><p>The following diagram summarizes the steps taken during this loop:</p><p align=center><img weight=200 height=200 src=/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-steps-during-loop.png></p><p>The steps themselves are:</p><ol><li>Loop over all containers in a pod.</li><li>For each container, gather "<strong><code>TopologyHints</code></strong>" from a set of "<strong><code>HintProviders</code></strong>" for each topology-aware resource type requested by the container (e.g. <strong><code>gpu-vendor.com/gpu</code></strong>, <strong><code>nic-vendor.com/nic</code></strong>, <strong><code>cpu</code></strong>, etc.).</li><li>Using the selected policy, merge the gathered <strong><code>TopologyHints</code></strong> to find the "best" hint that aligns resource allocations across all resource types.</li><li>Loop back over the set of hint providers, instructing them to allocate the resources they control using the merged hint as a guide.</li><li>This loop runs at pod admission time and will fail to admit the pod if any of these steps fail or alignment cannot be satisfied according to the selected policy. Any resources allocated before the failure are cleaned up accordingly.</li></ol><p>The following sections go into more detail on the exact structure of <strong><code>TopologyHints</code></strong> and <strong><code>HintProviders</code></strong>, as well as some details on the merge strategies used by each policy.</p><h3 id=topologyhints>TopologyHints</h3><p>A <strong><code>TopologyHint</code></strong> encodes a set of constraints from which a given resource request can be satisfied. At present, the only constraint we consider is NUMA alignment. It is defined as follows:</p><pre><code>type TopologyHint struct {
    NUMANodeAffinity bitmask.BitMask
    Preferred bool
}
</code></pre><p>The <strong><code>NUMANodeAffinity</code></strong> field contains a bitmask of NUMA nodes where a resource request can be satisfied. For example, the possible masks on a system with 2 NUMA nodes include:</p><pre><code>{00}, {01}, {10}, {11}
</code></pre><p>The <strong><code>Preferred</code></strong> field contains a boolean that encodes whether the given hint is "preferred" or not. With the <strong><code>best-effort</code></strong> policy, preferred hints will be given preference over non-preferred hints when generating a "best" hint. With the <strong><code>restricted</code></strong> and <strong><code>single-numa-node</code></strong> policies, non-preferred hints will be rejected.</p><p>In general, <strong><code>HintProviders</code></strong> generate <strong><code>TopologyHints</code></strong> by looking at the set of currently available resources that can satisfy a resource request. More specifically, they generate one <strong><code>TopologyHint</code></strong> for every possible mask of NUMA nodes where that resource request can be satisfied. If a mask cannot satisfy the request, it is omitted. For example, a <strong><code>HintProvider</code></strong> might provide the following hints on a system with 2 NUMA nodes when being asked to allocate 2 resources. These hints encode that both resources could either come from a single NUMA node (either 0 or 1), or they could each come from different NUMA nodes (but we prefer for them to come from just one).</p><pre><code>{01: True}, {10: True}, {11: False}
</code></pre><p>At present, all <strong><code>HintProviders</code></strong> set the <strong><code>Preferred</code></strong> field to <strong><code>True</code></strong> if and only if the <strong><code>NUMANodeAffinity</code></strong> encodes a <em>minimal</em> set of NUMA nodes that can satisfy the resource request. Normally, this will only be <strong><code>True</code></strong> for <strong><code>TopologyHints</code></strong> with a single NUMA node set in their bitmask. However, it may also be <strong><code>True</code></strong> if the only way to <em>ever</em> satisfy the resource request is to span multiple NUMA nodes (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes):</p><pre><code>{0011: True}, {0111: False}, {1011: False}, {1111: False}
</code></pre><p><strong>NOTE:</strong> Setting of the <strong><code>Preferred</code></strong> field in this way is <em>not</em> based on the set of currently available resources. It is based on the ability to physically allocate the number of requested resources on some minimal set of NUMA nodes.</p><p>In this way, it is possible for a <strong><code>HintProvider</code></strong> to return a list of hints with <em>all</em> <strong><code>Preferred</code></strong> fields set to <strong><code>False</code></strong> if an actual preferred allocation cannot be satisfied until other containers release their resources. For example, consider the following scenario from the system in Figure 1:</p><ol><li>All but 2 CPUs are currently allocated to containers</li><li>The 2 remaining CPUs are on different NUMA nodes</li><li>A new container comes along asking for 2 CPUs</li></ol><p>In this case, the only generated hint would be <strong><code>{11: False}</code></strong> and not <strong><code>{11: True}</code></strong>. This happens because it <em>is</em> possible to allocate 2 CPUs from the same NUMA node on this system (just not right now, given the current allocation state). The idea being that it is better to fail pod admission and retry the deployment when the minimal alignment can be satisfied than to allow a pod to be scheduled with sub-optimal alignment.</p><h3 id=hintproviders>HintProviders</h3><p>A <strong><code>HintProvider</code></strong> is a component internal to the <strong><code>kubelet</code></strong> that coordinates aligned resource allocations with the <strong><code>TopologyManager</code></strong>. At present, the only <strong><code>HintProviders</code></strong> in Kubernetes are the <strong><code>CPUManager</code></strong> and the <strong><code>DeviceManager</code></strong>. We plan to add support for <strong><code>HugePages</code></strong> soon.</p><p>As discussed previously, the <strong><code>TopologyManager</code></strong> both gathers <strong><code>TopologyHints</code></strong> from <strong><code>HintProviders</code></strong> as well as triggers aligned resource allocations on them using a merged "best" hint. As such, <strong><code>HintProviders</code></strong> implement the following interface:</p><pre><code>type HintProvider interface {
    GetTopologyHints(*v1.Pod, *v1.Container) map[string][]TopologyHint
    Allocate(*v1.Pod, *v1.Container) error
}
</code></pre><p>Notice that the call to <strong><code>GetTopologyHints()</code></strong> returns a <strong><code>map[string][]TopologyHint</code></strong>. This allows a single <strong><code>HintProvider</code></strong> to provide hints for multiple resource types instead of just one. For example, the <strong><code>DeviceManager</code></strong> requires this in order to pass hints back for every resource type registered by its plugins.</p><p>As <strong><code>HintProviders</code></strong> generate their hints, they only consider how alignment could be satisfied for <em>currently</em> available resources on the system. Any resources already allocated to other containers are not considered.</p><p>For example, consider the system in Figure 1, with the following two containers requesting resources from it:</p><table><tr><td align=center><strong><code>Container0</code></strong></td><td align=center><strong><code>Container1</code></strong></td></tr><tr><td><pre>
spec:
    containers:
    - name: numa-aligned-container0
      image: alpine
      resources:
          limits:
              cpu: 2
              memory: 200Mi
              gpu-vendor.com/gpu: 1
              nic-vendor.com/nic: 1
</pre></td><td><pre>
spec:
    containers:
    - name: numa-aligned-container1
      image: alpine
      resources:
          limits:
              cpu: 2
              memory: 200Mi
              gpu-vendor.com/gpu: 1
              nic-vendor.com/nic: 1
</pre></td></tr></table><p>If <strong><code>Container0</code></strong> is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec.</p><pre><code>               cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
</code></pre><p>With a resulting aligned allocation of:</p><pre><code>{cpu: {0, 1}, gpu: 0, nic: 0}
</code></pre><p align=center><img height=300 src=/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider1.png></p><p>When considering <strong><code>Container1</code></strong> these resources are then presumed to be unavailable, and thus only the following set of hints will be generated:</p><pre><code>               cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{10: True}}
nic-vendor.com/nic: {{10: True}}
</code></pre><p>With a resulting aligned allocation of:</p><pre><code>{cpu: {4, 5}, gpu: 1, nic: 1}
</code></pre><p align=center><img height=300 src=/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider2.png></p><p><strong>NOTE:</strong> Unlike the pseudocode provided at the beginning of this section, the call to <strong><code>Allocate()</code></strong> does not actually take a parameter for the merged "best" hint directly. Instead, the <strong><code>TopologyManager</code></strong> implements the following <strong><code>Store</code></strong> interface that <strong><code>HintProviders</code></strong> can query to retrieve the hint generated for a particular container once it has been generated:</p><pre><code>type Store interface {
    GetAffinity(podUID string, containerName string) TopologyHint
}
</code></pre><p>Separating this out into its own API call allows one to access this hint outside of the pod admission loop. This is useful for debugging as well as for reporting generated hints in tools such as <strong><code>kubectl</code></strong>(not yet available).</p><h3 id=policy-merge>Policy.Merge</h3><p>The merge strategy defined by a given policy dictates how it combines the set of <strong><code>TopologyHints</code></strong> generated by all <strong><code>HintProviders</code></strong> into a single <strong><code>TopologyHint</code></strong> that can be used to inform aligned resource allocations.</p><p>The general merge strategy for all supported policies begins the same:</p><ol><li>Take the cross-product of <strong><code>TopologyHints</code></strong> generated for each resource type</li><li>For each entry in the cross-product, <strong><code>bitwise-and</code></strong> the NUMA affinities of each <strong><code>TopologyHint</code></strong> together. Set this as the NUMA affinity in a resulting "merged" hint.</li><li>If all of the hints in an entry have <strong><code>Preferred</code></strong> set to <strong><code>True</code></strong> , set <strong><code>Preferred</code></strong> to <strong><code>True</code></strong> in the resulting "merged" hint.</li><li>If even one of the hints in an entry has <strong><code>Preferred</code></strong> set to <strong><code>False</code></strong> , set <strong><code>Preferred</code></strong> to <strong><code>False</code></strong> in the resulting "merged" hint. Also set <strong><code>Preferred</code></strong> to <strong><code>False</code></strong> in the "merged" hint if its NUMA affinity contains all 0s.</li></ol><p>Following the example from the previous section with hints for <strong><code>Container0</code></strong> generated as:</p><pre><code>               cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
</code></pre><p>The above algorithm results in the following set of cross-product entries and "merged" hints:</p><table><tr><td align=center>cross-product entry<p><strong><code>{cpu, gpu-vendor.com/gpu, nic-vendor.com/nic}</code></strong></p></td><td align=center>"merged" hint</td></tr><tr><td align=center><strong><code>{{01: True}, {01: True}, {01: True}}</code></strong></td><td align=center><strong><code>{01: True}</code></strong></td></tr><tr><td align=center><strong><code>{{01: True}, {01: True}, {10: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{01: True}, {10: True}, {01: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{01: True}, {10: True}, {10: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td></td><td></td></tr><tr><td align=center><strong><code>{{10: True}, {01: True}, {01: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{10: True}, {01: True}, {10: True}}</code></strong></td align="center"><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{10: True}, {10: True}, {01: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{10: True}, {10: True}, {10: True}}</code></strong></td><td align=center><strong><code>{01: True}</code></strong></td></tr><tr><td></td><td></td></tr><tr><td align=center><strong><code>{{11: False}, {01: True}, {01: True}}</code></strong></td><td align=center><strong><code>{01: False}</code></strong></td></tr><tr><td align=center><strong><code>{{11: False}, {01: True}, {10: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{11: False}, {10: True}, {01: True}}</code></strong></td><td align=center><strong><code>{00: False}</code></strong></td></tr><tr><td align=center><strong><code>{{11: False}, {10: True}, {10: True}}</code></strong></td><td align=center><strong><code>{10: False}</code></strong></td></tr></table><p>Once this list of "merged" hints has been generated, it is the job of the specific <strong><code>TopologyManager</code></strong> policy in use to decide which one to consider as the "best" hint.</p><p>In general, this involves:</p><ol><li>Sorting merged hints by their "narrowness". Narrowness is defined as the number of bits set in a hint’s NUMA affinity mask. The fewer bits set, the narrower the hint. For hints that have the same number of bits set in their NUMA affinity mask, the hint with the most low order bits set is considered narrower.</li><li>Sorting merged hints by their <strong><code>Preferred</code></strong> field. Hints that have <strong><code>Preferred</code></strong> set to <strong><code>True</code></strong> are considered more likely candidates than hints with <strong><code>Preferred</code></strong> set to <strong><code>False</code></strong>.</li><li>Selecting the narrowest hint with the best possible setting for <strong><code>Preferred</code></strong>.</li></ol><p>In the case of the <strong><code>best-effort</code></strong> policy this algorithm will always result in <em>some</em> hint being selected as the "best" hint and the pod being admitted. This "best" hint is then made available to <strong><code>HintProviders</code></strong> so they can make their resource allocations based on it.</p><p>However, in the case of the <strong><code>restricted</code></strong> and <strong><code>single-numa-node</code></strong> policies, any selected hint with <strong><code>Preferred</code></strong> set to <strong><code>False</code></strong> will be rejected immediately, causing pod admission to fail and no resources to be allocated. Moreover, the <strong><code>single-numa-node</code></strong> will also reject a selected hint that has more than one NUMA node set in its affinity mask.</p><p>In the example above, the pod would be admitted by all policies with a hint of <strong><code>{01: True}</code></strong>.</p><h2 id=upcoming-enhancements>Upcoming enhancements</h2><p>While the 1.18 release and promotion to Beta brings along some great enhancements and fixes, there are still a number of limitations, described <a href=https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#known-limitations>here</a>. We are already underway working to address these limitations and more.</p><p>This section walks through the set of enhancements we plan to implement for the <strong><code>TopologyManager</code></strong> in the near future. This list is not exhaustive, but it gives a good idea of the direction we are moving in. It is ordered by the timeframe in which we expect to see each enhancement completed.</p><p>If you would like to get involved in helping with any of these enhancements, please <a href=https://github.com/kubernetes/community/tree/master/sig-node>join the weekly Kubernetes SIG-node meetings</a> to learn more and become part of the community effort!</p><h3 id=supporting-device-specific-constraints>Supporting device-specific constraints</h3><p>Currently, NUMA affinity is the only constraint considered by the <strong><code>TopologyManager</code></strong> for resource alignment. Moreover, the only scalable extensions that can be made to a <strong><code>TopologyHint</code></strong> involve <em>node-level</em> constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any <em>device-specific</em> constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices).</p><p>As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the <strong><code>TopologyManager</code></strong> can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions.</p><p>Details of this proposal can be found <a href=https://github.com/kubernetes/enhancements/pull/1121>here</a>, and should be available as soon as Kubernetes 1.19.</p><h3 id=numa-alignment-for-hugepages>NUMA alignment for hugepages</h3><p>As stated previously, the only two <strong><code>HintProviders</code></strong> currently available to the <strong><code>TopologyManager</code></strong> are the <strong><code>CPUManager</code></strong> and the <strong><code>DeviceManager</code></strong>. However, work is currently underway to add support for hugepages as well. With the completion of this work, the <strong><code>TopologyManager</code></strong> will finally be able to allocate memory, hugepages, CPUs and PCI devices all on the same NUMA node.</p><p>A <a href=https://github.com/kubernetes/enhancements/blob/253f1e5bdd121872d2d0f7020a5ac0365b229e30/keps/sig-node/20200203-memory-manager.md>KEP</a> for this work is currently under review, and a prototype is underway to get this feature implemented very soon.</p><h3 id=scheduler-awareness>Scheduler awareness</h3><p>Currently, the <strong><code>TopologyManager</code></strong> acts as a Pod Admission controller. It is not directly involved in the scheduling decision of where a pod will be placed. Rather, when the kubernetes scheduler (or whatever scheduler is running in the deployment), places a pod on a node to run, the <strong><code>TopologyManager</code></strong> will decide if the pod should be "admitted" or "rejected". If the pod is rejected due to lack of available NUMA aligned resources, things can get a little interesting. This kubernetes <a href=https://github.com/kubernetes/kubernetes/issues/84869>issue</a> highlights and discusses this situation well.</p><p>So how do we go about addressing this limitation? We have the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md>Kubernetes Scheduling Framework</a> to the rescue! This framework provides a new set of plugin APIs that integrate with the existing Kubernetes Scheduler and allow scheduling features, such as NUMA alignment, to be implemented without having to resort to other, perhaps less appealing alternatives, including writing your own scheduler, or even worse, creating a fork to add your own scheduler secret sauce.</p><p>The details of how to implement these extensions for integration with the <strong><code>TopologyManager</code></strong> have not yet been worked out. We still need to answer questions like:</p><ul><li>Will we require duplicated logic to determine device affinity in the <strong><code>TopologyManager</code></strong> and the scheduler?</li><li>Do we need a new API to get <strong><code>TopologyHints</code></strong> from the <strong><code>TopologyManager</code></strong> to the scheduler plugin?</li></ul><p>Work on this feature should begin in the next couple of months, so stay tuned!</p><h3 id=per-pod-alignment-policy>Per-pod alignment policy</h3><p>As stated previously, a single policy is applied to <em>all</em> pods on a node via a global <strong><code>kubelet</code></strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis).</p><p>While we agree that this would be a great feature to have, there are quite a few hurdles that need to be overcome before it is achievable. The biggest hurdle being that this enhancement will require an API change to be able to express the desired alignment policy in either the Pod spec or its associated <strong><code><a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a></code></strong>.</p><p>We are only now starting to have serious discussions around this feature, and it is still a few releases away, at the best, from being available.</p><h2 id=conclusion>Conclusion</h2><p>With the promotion of the <strong><code>TopologyManager</code></strong> to Beta in 1.18, we encourage everyone to give it a try and look forward to any feedback you may have. Many fixes and enhancements have been worked on in the past several releases, greatly improving the functionality and reliability of the <strong><code>TopologyManager</code></strong> and its <strong><code>HintProviders</code></strong>. While there are still a number of limitations, we have a set of enhancements planned to address them, and look forward to providing you with a number of new features in upcoming releases.</p><p>If you have ideas for additional enhancements or a desire for certain features, don’t hesitate to let us know. The team is always open to suggestions to enhance and improve the <strong><code>TopologyManager</code></strong>.</p><p>We hope you have found this blog informative and useful! Let us know if you have any questions or comments. And, happy deploying…..Align Up!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b510a26fb64945b86595c4992b051dbd>Kubernetes 1.18: Fit & Finish</h1><div class="td-byline mb-4"><time datetime=2020-03-25 class=text-muted>Wednesday, March 25, 2020</time></div><p><strong>Authors:</strong> <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md>Kubernetes 1.18 Release Team</a></p><p>We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.</p><p>Kubernetes 1.18 is a "fit and finish" release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.</p><h2 id=major-themes>Major Themes</h2><h3 id=kubernetes-topology-manager-moves-to-beta-align-up>Kubernetes Topology Manager Moves to Beta - Align Up!</h3><p>A beta feature of Kubernetes in release 1.18, the <a href=https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md>Topology Manager feature</a> enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.</p><h3 id=serverside-apply-introduces-beta-2>Serverside Apply Introduces Beta 2</h3><p>Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.</p><h3 id=extending-ingress-with-and-replacing-a-deprecated-annotation-with-ingressclass>Extending Ingress with and replacing a deprecated annotation with IngressClass</h3><p>In Kubernetes 1.18, there are two significant additions to Ingress: A new <code>pathType</code> field and a new <code>IngressClass</code> resource. The <code>pathType</code> field allows specifying how paths should be matched. In addition to the default <code>ImplementationSpecific</code> type, there are new <code>Exact</code> and <code>Prefix</code> path types.</p><p>The <code>IngressClass</code> resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new <code>ingressClassName</code> field on Ingresses. This new resource and field replace the deprecated <code>kubernetes.io/ingress.class</code> annotation.</p><h3 id=sig-cli-introduces-kubectl-alpha-debug>SIG-CLI introduces kubectl alpha debug</h3><p>SIG-CLI was debating the need for a debug utility for quite some time already. With the development of <a href=https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/>ephemeral containers</a>, it became more obvious how we can support developers with tooling built on top of <code>kubectl exec</code>. The addition of the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md><code>kubectl alpha debug</code> command</a> (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.</p><h3 id=introducing-windows-csi-support-alpha-for-kubernetes>Introducing Windows CSI support alpha for Kubernetes</h3><p>The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.</p><h2 id=other-updates>Other Updates</h2><h3 id=graduated-to-stable>Graduated to Stable 💯</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/166>Taint Based Eviction</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/491><code>kubectl diff</code></a></li><li><a href=https://github.com/kubernetes/enhancements/issues/565>CSI Block storage support</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/576>API Server dry run</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/603>Pass Pod information in CSI calls</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/670>Support Out-of-Tree vSphere Cloud Provider</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/689>Support GMSA for Windows workloads</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/770>Skip attach for non-attachable CSI volumes</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/989>PVC cloning</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1020>Moving kubectl package code to staging</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1043>RunAsUserName for Windows</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1507>AppProtocol for Services and Endpoints</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1539>Extending Hugepage Feature</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1601>client-go signature refactor to standardize options and context handling</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1024>Node-local DNS cache</a></li></ul><h3 id=major-changes>Major Changes</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/752>EndpointSlice API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1020>Moving kubectl package code to staging</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1513>CertificateSigningRequest API</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1539>Extending Hugepage Feature</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1601>client-go signature refactor to standardize options and context handling</a></li></ul><h3 id=release-notes>Release Notes</h3><p>Check out the full details of the Kubernetes 1.18 release in our <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md>release notes</a>.</p><h3 id=availability>Availability</h3><p>Kubernetes 1.18 is available for download on <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0>GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a> or run local Kubernetes clusters using Docker container “nodes” with <a href=https://kind.sigs.k8s.io/>kind</a>. You can also easily install 1.18 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h3 id=release-team>Release Team</h3><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md>release team</a> led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 individual contributors</a> to date and an active community of more than 3,000 people.</p><h3 id=release-logo>Release Logo</h3><p><img src=/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png alt="Kubernetes 1.18 Release Logo"></p><h4 id=why-the-lhc>Why the LHC?</h4><p>The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! "A Bit Quarky" as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.</p><h4 id=about-the-designer>About the designer</h4><p>Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: <a href=https://marulango.com>https://marulango.com</a></p><h3 id=user-highlights>User Highlights</h3><ul><li>Ericsson is using Kubernetes and other cloud native technology to deliver a <a href=https://www.cncf.io/case-study/ericsson/>highly demanding 5G network</a> that resulted in up to 90 percent CI/CD savings.</li><li>Zendesk is using Kubernetes to <a href=https://www.cncf.io/case-study/zendesk/>run around 70% of its existing applications</a>. It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.</li><li>LifeMiles has <a href=https://www.cncf.io/case-study/lifemiles/>reduced infrastructure spending by 50%</a> because of its move to Kubernetes. It has also allowed them to double its available resource capacity.</li></ul><h3 id=ecosystem-updates>Ecosystem Updates</h3><ul><li>The CNCF published the results of its <a href=https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/>annual survey</a> showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.</li><li>The “Introduction to Kubernetes” course hosted by the CNCF <a href=https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/>surpassed 100,000 registrations</a>.</li></ul><h3 id=project-velocity>Project Velocity</h3><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.</p><p>This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h3 id=event-update>Event Update</h3><p>Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/>Novel Coronavirus Update page</a>.</p><h3 id=upcoming-release-webinar>Upcoming Release Webinar</h3><p>Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: <a href=https://www.cncf.io/webinars/kubernetes-1-18/>https://www.cncf.io/webinars/kubernetes-1-18/</a>.</p><h3 id=get-involved>Get Involved</h3><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p><ul><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li><li>Read more about what’s happening with Kubernetes on the <a href=https://kubernetes.io/blog/>blog</a></li><li>Learn more about the <a href=https://github.com/kubernetes/sig-release/tree/master/release-team>Kubernetes Release Team</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2b274af3e065e40387815705125ab62>Join SIG Scalability and Learn Kubernetes the Hard Way</h1><div class="td-byline mb-4"><time datetime=2020-03-19 class=text-muted>Thursday, March 19, 2020</time></div><p><strong>Authors:</strong> Alex Handy</p><p>Contributing to SIG Scalability is a great way to learn Kubernetes in all its depth and breadth, and the team would love to have you <a href=https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group>join as a contributor</a>. I took a look at the value of learning the hard way and interviewed the current SIG chairs to give you an idea of what contribution feels like.</p><h2 id=the-value-of-learning-the-hard-way>The value of Learning The Hard Way</h2><p>There is a belief in the software development community that pushes for the most challenging and rigorous possible method of learning a new language or system. These tend to go by the moniker of "Learn __ the Hard Way." Examples abound: Learn Code the Hard Way, Learn Python the Hard Way, and many others originating with Zed Shaw's courses in the topic.</p><p>While there are folks out there who offer you a "Learn Kubernetes the Hard Way" type experience (most notably <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kelsey Hightower's</a>), any "Hard Way" project should attempt to cover every aspect of the core topic's principles.</p><p>Therefore, the real way to "Learn Kubernetes the Hard Way," is to join the CNCF and get involved in the project itself. And there is only one SIG that could genuinely offer a full-stack learning experience for Kubernetes: SIG Scalability.</p><p>The team behind SIG Scalability is responsible for detecting and dealing with issues that arise when Kubernetes clusters are working with upwards of a thousand nodes. Said <a href=https://github.com/wojtek-t>Wojiciech Tyczynski</a>, a staff software engineer at Google and a member of SIG Scalability, the standard size for a test cluster for this SIG is over 5,000 nodes.</p><p>And yet, this SIG is not composed of Ph.D.'s in highly scalable systems designs. Many of the folks working with Tyczynski, for example, joined the SIG knowing very little about these types of issues, and often, very little about Kubernetes.</p><p>Working on SIG Scalability is like jumping into the deep end of the pool to learn to swim, and the SIG is inherently concerned with the entire Kubernetes project. SIG Scalability focuses on how Kubernetes functions as a whole and at scale. The SIG Scalability team members have an impetus to learn about every system and to understand how all systems interact with one another.</p><h2 id=a-complex-and-rewarding-contributor-experience>A complex and rewarding contributor experience</h2><p>While that may sound complicated (and it is!), that doesn't mean it's outside the reach of an average developer, tester, or administrator. Google software developer Matt Matejczyk has only been on the team since the beginning of 2019, and he's been a valued member of the team since then, ferreting out bugs.</p><p>"I am new here," said Matejczyk. "I joined the team in January [2019]. Before that, I worked on AdWords at Google in New York. Why did I join? I knew some people there, so that was one of the decisions for me to move. I thought at that time that Kubernetes is a unique, cutting edge technology. I thought it'd be cool to work on that."</p><p>Matejczyk was correct about the coolness. "It's cool," he said. "So actually, ramping up on scalability is not easy. There are many things you need to understand. You need to understand Kubernetes very well. It can use every part of Kubernetes. I am still ramping up after these 8 months. I think it took me maybe 3 months to get up to decent speed."</p><p>When Matejczyk spoke to what he had worked on during those 8 months, he answered, "An interesting example is a regression I have been working on recently. We noticed the overall slowness of Kubernetes control plane in specific scenarios, and we couldn't attribute it to any particular component. In the end, we realized that everything boiled down to the memory allocation on the golang level. It was very counterintuitive to have two completely separate pieces of code (running as a part of the same binary) affecting the performance of each other only because one of them was allocating memory too fast. But connecting all the dots and getting to the bottom of regression like this gives great satisfaction."</p><p>Tyczynski said that "It's not only debugging regressions, but it's also debugging and finding bottlenecks. In general, those can be regressions, but those can be things we can improve. The other significant area is extending what we want to guarantee to users. Extending SLA and SLO coverage of the system so users can rely on what they can expect from the system in terms of performance and scalability. Matt is doing much work in extending our tests to be more representative and cover more Kubernetes concepts."</p><h2 id=give-sig-scalability-a-try>Give SIG Scalability a try</h2><p>The SIG Scalability team is always in need of new members, and if you're the sort of developer or tester who loves taking on new complex challenges, and perhaps loves learning things the hard way, consider joining this SIG. As the team points out, adding Kubernetes expertise to your resume is never a bad idea, and this is the one SIG where you can learn it all from top to bottom.</p><p>See <a href=https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group>the SIG's documentation</a> to learn about upcoming meetings, its charter, and more. You can also join the <a href=https://kubernetes.slack.com/archives/C09QZTRH7>#sig-scalability Slack channel</a> to see what it's like. We hope to see you join in to take advantage of this great opportunity to learn Kubernetes and contribute back at the same time.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e71e6d28cf7cadef7b3c7f414de2496c>Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2020-03-18 class=text-muted>Wednesday, March 18, 2020</time></div><p><strong>Author:</strong> Kevin Chen, Kong</p><p>Kubernetes has become the de facto way to orchestrate containers and the services within services. But how do we give services outside our cluster access to what is within? Kubernetes comes with the Ingress API object that manages external access to services within a cluster.</p><p>Ingress is a group of rules that will proxy inbound connections to endpoints defined by a backend. However, Kubernetes does not know what to do with Ingress resources without an Ingress controller, which is where an open source controller can come into play. In this post, we are going to use one option for this: the Kong Ingress Controller. The Kong Ingress Controller was open-sourced a year ago and recently reached one million downloads. In the recent 0.7 release, service mesh support was also added. Other features of this release include:</p><ul><li><strong>Built-In Kubernetes Admission Controller</strong>, which validates Custom Resource Definitions (CRD) as they are created or updated and rejects any invalid configurations.</li><li><strong>In-memory Mode</strong> - Each pod’s controller actively configures the Kong container in its pod, which limits the blast radius of failure of a single container of Kong or controller container to that pod only.</li><li><strong>Native gRPC Routing</strong> - gRPC traffic can now be routed via Kong Ingress Controller natively with support for method-based routing.</li></ul><p><img src=/images/blog/Kong-Ingress-Controller-and-Service-Mesh/KIC-gRPC.png alt=K4K-gRPC></p><p>If you would like a deeper dive into Kong Ingress Controller 0.7, please check out the <a href=https://github.com/Kong/kubernetes-ingress-controller>GitHub repository</a>.</p><p>But let’s get back to the service mesh support since that will be the main focal point of this blog post. Service mesh allows organizations to address microservices challenges related to security, reliability, and observability by abstracting inter-service communication into a mesh layer. But what if our mesh layer sits within Kubernetes and we still need to expose certain services beyond our cluster? Then you need an Ingress controller such as the Kong Ingress Controller. In this blog post, we’ll cover how to deploy Kong Ingress Controller as your Ingress layer to an Istio mesh. Let’s dive right in:</p><p><img src=/images/blog/Kong-Ingress-Controller-and-Service-Mesh/k4k8s.png alt="Kong Kubernetes Ingress Controller"></p><h3 id=part-0-set-up-istio-on-kubernetes>Part 0: Set up Istio on Kubernetes</h3><p>This blog will assume you have Istio set up on Kubernetes. If you need to catch up to this point, please check out the <a href=https://istio.io/docs/setup/>Istio documentation</a>. It will walk you through setting up Istio on Kubernetes.</p><h3 id=1-install-the-bookinfo-application>1. Install the Bookinfo Application</h3><p>First, we need to label the namespaces that will host our application and Kong proxy. To label our default namespace where the bookinfo app sits, run this command:</p><pre><code>$ kubectl label namespace default istio-injection=enabled
namespace/default labeled
</code></pre><p>Then create a new namespace that will be hosting our Kong gateway and the Ingress controller:</p><pre><code>$ kubectl create namespace kong
namespace/kong created
</code></pre><p>Because Kong will be sitting outside the default namespace, be sure you also label the Kong namespace with istio-injection enabled as well:</p><pre><code>$ kubectl label namespace kong istio-injection=enabled
namespace/kong labeled
</code></pre><p>Having both namespaces labeled <code>istio-injection=enabled</code> is necessary. Or else the default configuration will not inject a sidecar container into the pods of your namespaces.</p><p>Now deploy your BookInfo application with the following command:</p><pre><code>$ kubectl apply -f http://bit.ly/bookinfoapp
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
</code></pre><p>Let’s double-check our Services and Pods to make sure that we have it all set up correctly:</p><pre><code>$ kubectl get services
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
details       ClusterIP   10.97.125.254    &lt;none&gt;        9080/TCP   29s
kubernetes    ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    29h
productpage   ClusterIP   10.97.62.68      &lt;none&gt;        9080/TCP   28s
ratings       ClusterIP   10.96.15.180     &lt;none&gt;        9080/TCP   28s
reviews       ClusterIP   10.104.207.136   &lt;none&gt;        9080/TCP   28s
</code></pre><p>You should see four new services: details, productpage, ratings, and reviews. None of them have an external IP so we will use the <a href=https://github.com/Kong/kong>Kong gateway</a> to expose the necessary services. And to check pods, run the following command:</p><pre><code>$ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-c5b5f496d-9wm29        2/2     Running   0          101s
productpage-v1-7d6cfb7dfd-5mc96   2/2     Running   0          100s
ratings-v1-f745cf57b-hmkwf        2/2     Running   0          101s
reviews-v1-85c474d9b8-kqcpt       2/2     Running   0          101s
reviews-v2-ccffdd984-9jnsj        2/2     Running   0          101s
reviews-v3-98dc67b68-nzw97        2/2     Running   0          101s
</code></pre><p>This command outputs useful data, so let’s take a second to understand it. If you examine the READY column, each pod has two containers running: the service and an Envoy sidecar injected alongside it. Another thing to highlight is that there are three review pods but only 1 review service. The Envoy sidecar will load balance the traffic to three different review pods that contain different versions, giving us the ability to A/B test our changes. We have one step before we can access the deployed application. We need to add an additional annotation to the <code>productpage</code> service. To do so, run:</p><pre><code>$ kubectl annotate service productpage ingress.kubernetes.io/service-upstream=true
service/productpage annotated
</code></pre><p>Both the API gateway (Kong) and the service mesh (Istio) can handle the load-balancing. Without the additional <code>ingress.kubernetes.io/service-upstream: "true"</code> annotation, Kong will try to load-balance by selecting its own endpoint/target from the productpage service. This causes Envoy to receive that pod’s IP as the upstream local address, instead of the service’s cluster IP. But we want the service's cluster IP so that Envoy can properly load balance.</p><p>With that added, you should now be able to access your product page!</p><pre><code>$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre><h3 id=2-kong-kubernetes-ingress-controller-without-database>2. Kong Kubernetes Ingress Controller Without Database</h3><p>To expose your services to the world, we will deploy Kong as the north-south traffic gateway. <a href=https://github.com/Kong/kong/releases/tag/1.1.2>Kong 1.1</a> released with declarative configuration and DB-less mode. Declarative configuration allows you to specify the desired system state through a YAML or JSON file instead of a sequence of API calls. Using declarative config provides several key benefits to reduce complexity, increase automation and enhance system performance. And with the Kong Ingress Controller, any Ingress rules you apply to the cluster will automatically be configured on the Kong proxy. Let’s set up the Kong Ingress Controller and the actual Kong proxy first like this:</p><pre><code>$ kubectl apply -f https://bit.ly/k4k8s
namespace/kong configured
customresourcedefinition.apiextensions.k8s.io/kongconsumers.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongcredentials.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongingresses.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongplugins.configuration.konghq.com created
serviceaccount/kong-serviceaccount created
clusterrole.rbac.authorization.k8s.io/kong-ingress-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/kong-ingress-clusterrole-nisa-binding created
configmap/kong-server-blocks created
service/kong-proxy created
service/kong-validation-webhook created
deployment.apps/ingress-kong created
</code></pre><p>To check if the Kong pod is up and running, run:</p><pre><code>$ kubectl get pods -n kong
NAME                               READY   STATUS    RESTARTS   AGE
pod/ingress-kong-8b44c9856-9s42v   3/3     Running   0          2m26s
</code></pre><p>There will be three containers within this pod. The first container is the Kong Gateway that will be the Ingress point to your cluster. The second container is the Ingress controller. It uses Ingress resources and updates the proxy to follow rules defined in the resource. And lastly, the third container is the Envoy proxy injected by Istio. Kong will route traffic through the Envoy sidecar proxy to the appropriate service. To send requests into the cluster via our newly deployed Kong Gateway, setup an environment variable with the a URL based on the IP address at which Kong is accessible.</p><pre><code>$ export PROXY_URL=&quot;$(minikube service -n kong kong-proxy --url | head -1)&quot;
$ echo $PROXY_URL
http://192.168.99.100:32728
</code></pre><p>Next, we need to change some configuration so that the side-car Envoy process can route the request correctly based on the host/authority header of the request. Run the following to stop the route from preserving host:</p><pre><code>$ echo &quot;
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
    name: do-not-preserve-host
route:
  preserve_host: false
upstream:
  host_header: productpage.default.svc
&quot; | kubectl apply -f -
kongingress.configuration.konghq.com/do-not-preserve-host created
</code></pre><p>And annotate the existing productpage service to set service-upstream as true:</p><pre><code>$ kubectl annotate svc productpage Ingress.kubernetes.io/service-upstream=&quot;true&quot;
service/productpage annotated
</code></pre><p>Now that we have everything set up, we can look at how to use the Ingress resource to help route external traffic to the services within your Istio mesh. We’ll create an Ingress rule that routes all traffic with the path of <code>/</code> to our productpage service:</p><pre><code>$ echo &quot;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: productpage
  annotations:
    configuration.konghq.com: do-not-preserve-host
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: productpage
          servicePort: 9080
&quot; | kubectl apply -f -
ingress.extensions/productpage created
</code></pre><p>And just like that, the Kong Ingress Controller is able to understand the rules you defined in the Ingress resource and routes it to the productpage service! To view the product page service’s GUI, go to <code>$PROXY_URL/productpage</code> in your browser. Or to test it in your command line, try:</p><pre><code>$ curl $PROXY_URL/productpage
</code></pre><p>That is all I have for this walk-through. If you enjoyed the technologies used in this post, please check out their repositories since they are all open source and would love to have more contributors! Here are their links for your convenience:</p><ul><li>Kong: [<a href=https://github.com/Kong/kubernetes-ingress-controller>GitHub</a>] [<a href=https://twitter.com/thekonginc>Twitter</a>]</li><li>Kubernetes: [<a href=https://github.com/kubernetes/kubernetes>GitHub</a>] [<a href=https://twitter.com/kubernetesio>Twitter</a>]</li><li>Istio: [<a href=https://github.com/istio/istio>GitHub</a>] [<a href=https://twitter.com/IstioMesh>Twitter</a>]</li><li>Envoy: [<a href=https://github.com/envoyproxy/envoy>GitHub</a>] [<a href=https://twitter.com/EnvoyProxy>Twitter</a>]</li></ul><p>Thank you for following along!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-12001c222627cf4a9b495bdceed5ba8a>Contributor Summit Amsterdam Postponed</h1><div class="td-byline mb-4"><time datetime=2020-03-04 class=text-muted>Wednesday, March 04, 2020</time></div><p><strong>Authors:</strong> Dawn Foster (VMware), Jorge Castro (VMware)</p><p>The CNCF has announced that <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/>KubeCon + CloudNativeCon EU has been delayed</a> until July/August of 2020. As a result the Contributor Summit planning team is weighing options for how to proceed. Here’s the current plan:</p><ul><li>There will be an in-person Contributor Summit as planned when KubeCon + CloudNativeCon is rescheduled.</li><li>We are looking at options for having additional virtual contributor activities in the meantime.</li></ul><p>We will communicate via this blog and the usual communications channels on the final plan. Please bear with us as we adapt when we get more information. Thank you for being patient as the team pivots to bring you a great Contributor Summit!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-44e9c6545509d2a79b8cb50de92907dc>Bring your ideas to the world with kubectl plugins</h1><div class="td-byline mb-4"><time datetime=2020-02-28 class=text-muted>Friday, February 28, 2020</time></div><p><strong>Author:</strong> Cornelius Weig (TNG Technology Consulting GmbH)</p><p><code>kubectl</code> is the most critical tool to interact with Kubernetes and has to address multiple user personas, each with their own needs and opinions.
One way to make <code>kubectl</code> do what you need is to build new functionality into <code>kubectl</code>.</p><h2 id=challenges-with-building-commands-into-kubectl>Challenges with building commands into <code>kubectl</code></h2><p>However, that's easier said than done. Being such an important cornerstone of
Kubernetes, any meaningful change to <code>kubectl</code> needs to undergo a Kubernetes
Enhancement Proposal (KEP) where the intended change is discussed beforehand.</p><p>When it comes to implementation, you'll find that <code>kubectl</code> is an ingenious and
complex piece of engineering. It might take a long time to get used to
the processes and style of the codebase to get done what you want to achieve. Next
comes the review process which may go through several rounds until it meets all
the requirements of the Kubernetes maintainers -- after all, they need to take
over ownership of this feature and maintain it from the day it's merged.</p><p>When everything goes well, you can finally rejoice. Your code will be shipped
with the next Kubernetes release. Well, that could mean you need to wait
another 3 months to ship your idea in <code>kubectl</code> if you are unlucky.</p><p>So this was the happy path where everything goes well. But there are good
reasons why your new functionality may never make it into <code>kubectl</code>. For one,
<code>kubectl</code> has a particular look and feel and violating that style will not be
acceptable by the maintainers. For example, an interactive command that
produces output with colors would be inconsistent with the rest of <code>kubectl</code>.
Also, when it comes to tools or commands useful only to a minuscule proportion
of users, the maintainers may simply reject your proposal as <code>kubectl</code> needs to
address common needs.</p><p>But this doesn’t mean you can’t ship your ideas to <code>kubectl</code> users.</p><h2 id=what-if-you-didn-t-have-to-change-kubectl-to-add-functionality>What if you didn’t have to change <code>kubectl</code> to add functionality?</h2><p>This is where <code>kubectl</code> <a href=https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>plugins</a> shine.
Since <code>kubectl</code> v1.12, you can simply
drop executables into your <code>PATH</code>, which follows the naming pattern
<code>kubectl-myplugin</code>. Then you can execute this plugin as <code>kubectl myplugin</code>, and
it will just feel like a normal sub-command of <code>kubectl</code>.</p><p>Plugins give you the opportunity to try out new experiences like terminal UIs,
colorful output, specialized functionality, or other innovative ideas. You can
go creative, as you’re the owner of your own plugin.</p><p>Further, plugins offer safe experimentation space for commands you’d like to
propose to <code>kubectl</code>. By pre-releasing as a plugin, you can push your
functionality faster to the end-users and quickly gather feedback. For example,
the <a href=https://github.com/verb/kubectl-debug>kubectl-debug</a> plugin is proposed
to become a built-in command in <code>kubectl</code> in a
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md>KEP</a>).
In the meanwhile, the plugin author can ship the functionality and collect
feedback using the plugin mechanism.</p><h2 id=how-to-get-started-with-developing-plugins>How to get started with developing plugins</h2><p>If you already have an idea for a plugin, how do you best make it happen?
First you have to ask yourself if you can implement it as a wrapper around
existing <code>kubectl</code> functionality. If so, writing the plugin as a shell script
is often the best way forward, because the resulting plugin will be small,
works cross-platform, and has a high level of trust because it is not
compiled.</p><p>On the other hand, if the plugin logic is complex, a general-purpose language
is usually better. The canonical choice here is Go, because you can use the
excellent <code>client-go</code> library to interact with the Kubernetes API. The Kubernetes
maintained <a href=https://github.com/kubernetes/sample-cli-plugin>sample-cli-plugin</a>
demonstrates some best practices and can be used as a template for new plugin
projects.</p><p>When the development is done, you just need to ship your plugin to the
Kubernetes users. For the best plugin installation experience and discoverability,
you should consider doing so via the
<a href=https://github.com/kubernetes-sigs/krew>krew</a> plugin manager. For an in-depth
discussion about the technical details around <code>kubectl</code> plugins, refer to the
documentation on <a href=https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>kubernetes.io</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a9ad2d376559402c732199cd9182446c>Contributor Summit Amsterdam Schedule Announced</h1><div class="td-byline mb-4"><time datetime=2020-02-18 class=text-muted>Tuesday, February 18, 2020</time></div><p><strong>Authors:</strong> Jeffrey Sica (Red Hat), Amanda Katona (VMware)</p><p>tl;dr <a href=https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/>Registration is open</a> and the <a href=https://kcseu2020.sched.com/>schedule is live</a> so register now and we’ll see you in Amsterdam!</p><h2 id=kubernetes-contributor-summit>Kubernetes Contributor Summit</h2><p><strong>Sunday, March 29, 2020</strong></p><ul><li>Evening Contributor Celebration:
<a href=https://www.zuid-pool.nl/en/>ZuidPool</a></li><li>Address: <a href="https://www.google.com/search?q=KubeCon+Amsterdam+2020&ie=UTF-8&ibp=htl;events&rciv=evn&sa=X&ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 22, 1078 GZ Amsterdam, Netherlands</a></li><li>Time: 18:00 - 21:00</li></ul><p><strong>Monday, March 30, 2020</strong></p><ul><li>All Day Contributor Summit:</li><li><a href=https://www.rai.nl/en/>Amsterdam RAI</a></li><li>Address: <a href="https://www.google.com/search?q=kubecon+amsterdam+2020&oq=kubecon+amste&aqs=chrome.0.35i39j69i57j0l4j69i61l2.3957j1j4&sourceid=chrome&ie=UTF-8&ibp=htl;events&rciv=evn&sa=X&ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 24, 1078 GZ Amsterdam, Netherlands</a></li><li>Time: 09:00 - 17:00 (Breakfast at 08:00)</li></ul><p><img src=/images/blog/2020-02-18-Contributor-Summit-Amsterdam-Schedule-Announced/contribsummit.jpg alt="Contributor Summit"></p><p>Hello everyone and Happy 2020! It’s hard to believe that KubeCon EU 2020 is less than six weeks away, and with that another contributor summit! This year we have the pleasure of being in Amsterdam in early spring, so be sure to pack some warmer clothing. This summit looks to be exciting with a lot of fantastic community-driven content. We received <strong>26</strong> submissions from the CFP. From that, the events team selected <strong>12</strong> sessions. Each of the sessions falls into one of four categories:</p><ul><li>Community</li><li>Contributor Improvement</li><li>Sustainability</li><li>In-depth Technical</li></ul><p>On top of the presentations, there will be a dedicated Docs Sprint as well as the New Contributor Workshop 101 and 201 Sessions. All told, we will have five separate rooms of content throughout the day on Monday. Please <strong><a href=https://kcseu2020.sched.com/>see the full schedule</a></strong> to see what sessions you’d be interested in. We hope between the content provided and the inevitable hallway track, everyone has a fun and enriching experience.</p><p>Speaking of fun, the social Sunday night should be a blast! We’re hosting this summit’s social close to the conference center, at <a href=https://www.zuid-pool.nl/en/>ZuidPool</a>. There will be games, bingo, and unconference sign-up throughout the evening. It should be a relaxed way to kick off the week.</p><p><a href=https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/>Registration is open</a>! Space is limited so it’s always a good idea to register early.</p><p>If you have any questions, reach out to the <a href=https://github.com/kubernetes/community/tree/master/events/2020/03-contributor-summit#team>Amsterdam Team</a> on Slack in the <a href=https://kubernetes.slack.com/archives/C7J893413>#contributor-summit</a> channel.</p><p>Hope to see you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-398cfdd54f0eb2fc9ac98357a2ae424c>Deploying External OpenStack Cloud Provider with Kubeadm</h1><div class="td-byline mb-4"><time datetime=2020-02-07 class=text-muted>Friday, February 07, 2020</time></div><p>This document describes how to install a single control-plane Kubernetes cluster v1.15 with kubeadm on CentOS, and then deploy an external OpenStack cloud provider and Cinder CSI plugin to use Cinder volumes as persistent volumes in Kubernetes.</p><h3 id=preparation-in-openstack>Preparation in OpenStack</h3><p>This cluster runs on OpenStack VMs, so let's create a few things in OpenStack first.</p><ul><li>A project/tenant for this Kubernetes cluster</li><li>A user in this project for Kubernetes, to query node information and attach volumes etc</li><li>A private network and subnet</li><li>A router for this private network and connect it to a public network for floating IPs</li><li>A security group for all Kubernetes VMs</li><li>A VM as a control-plane node and a few VMs as worker nodes</li></ul><p>The security group will have the following rules to open ports for Kubernetes.</p><p><strong>Control-Plane Node</strong></p><table><thead><tr><th>Protocol</th><th>Port Number</th><th>Description</th></tr></thead><tbody><tr><td>TCP</td><td>6443</td><td>Kubernetes API Server</td></tr><tr><td>TCP</td><td>2379-2380</td><td>etcd server client API</td></tr><tr><td>TCP</td><td>10250</td><td>Kubelet API</td></tr><tr><td>TCP</td><td>10251</td><td>kube-scheduler</td></tr><tr><td>TCP</td><td>10252</td><td>kube-controller-manager</td></tr><tr><td>TCP</td><td>10255</td><td>Read-only Kubelet API</td></tr></tbody></table><p><strong>Worker Nodes</strong></p><table><thead><tr><th>Protocol</th><th>Port Number</th><th>Description</th></tr></thead><tbody><tr><td>TCP</td><td>10250</td><td>Kubelet API</td></tr><tr><td>TCP</td><td>10255</td><td>Read-only Kubelet API</td></tr><tr><td>TCP</td><td>30000-32767</td><td>NodePort Services</td></tr></tbody></table><p><strong>CNI ports on both control-plane and worker nodes</strong></p><table><thead><tr><th>Protocol</th><th>Port Number</th><th>Description</th></tr></thead><tbody><tr><td>TCP</td><td>179</td><td>Calico BGP network</td></tr><tr><td>TCP</td><td>9099</td><td>Calico felix (health check)</td></tr><tr><td>UDP</td><td>8285</td><td>Flannel</td></tr><tr><td>UDP</td><td>8472</td><td>Flannel</td></tr><tr><td>TCP</td><td>6781-6784</td><td>Weave Net</td></tr><tr><td>UDP</td><td>6783-6784</td><td>Weave Net</td></tr></tbody></table><p>CNI specific ports are only required to be opened when that particular CNI plugin is used. In this guide, we will use Weave Net. Only the Weave Net ports (TCP 6781-6784 and UDP 6783-6784), will need to be opened in the security group.</p><p>The control-plane node needs at least 2 cores and 4GB RAM. After the VM is launched, verify its hostname and make sure it is the same as the node name in Nova.
If the hostname is not resolvable, add it to <code>/etc/hosts</code>.</p><p>For example, if the VM is called master1, and it has an internal IP 192.168.1.4. Add that to <code>/etc/hosts</code> and set hostname to master1.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;192.168.1.4 master1&#34;</span> &gt;&gt; /etc/hosts

hostnamectl set-hostname master1
</code></pre></div><h3 id=install-docker-and-kubernetes>Install Docker and Kubernetes</h3><p>Next, we'll follow the official documents to install docker and Kubernetes using kubeadm.</p><p>Install Docker following the steps from the <a href=/docs/setup/production-environment/container-runtimes/>container runtime</a> documentation.</p><p>Note that it is a <a href=/docs/setup/production-environment/container-runtimes/#cgroup-drivers>best practice to use systemd as the cgroup driver</a> for Kubernetes.
If you use an internal container registry, add them to the docker config.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># Install Docker CE</span>
<span style=color:#080;font-style:italic>## Set up the repository</span>
<span style=color:#080;font-style:italic>### Install required packages.</span>

yum install yum-utils device-mapper-persistent-data lvm2

<span style=color:#080;font-style:italic>### Add Docker repository.</span>

yum-config-manager <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --add-repo <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  https://download.docker.com/linux/centos/docker-ce.repo

<span style=color:#080;font-style:italic>## Install Docker CE.</span>

yum update <span style=color:#666>&amp;&amp;</span> yum install docker-ce-18.06.2.ce

<span style=color:#080;font-style:italic>## Create /etc/docker directory.</span>

mkdir /etc/docker

<span style=color:#080;font-style:italic># Configure the Docker daemon</span>

cat &gt; /etc/docker/daemon.json <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>{
</span><span style=color:#b44>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span style=color:#b44>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span style=color:#b44>  &#34;log-opts&#34;: {
</span><span style=color:#b44>    &#34;max-size&#34;: &#34;100m&#34;
</span><span style=color:#b44>  },
</span><span style=color:#b44>  &#34;storage-driver&#34;: &#34;overlay2&#34;,
</span><span style=color:#b44>  &#34;storage-opts&#34;: [
</span><span style=color:#b44>    &#34;overlay2.override_kernel_check=true&#34;
</span><span style=color:#b44>  ]
</span><span style=color:#b44>}
</span><span style=color:#b44>EOF</span>

mkdir -p /etc/systemd/system/docker.service.d

<span style=color:#080;font-style:italic># Restart Docker</span>
systemctl daemon-reload
systemctl restart docker
systemctl <span style=color:#a2f>enable</span> docker
</code></pre></div><p>Install kubeadm following the steps from the <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>Installing Kubeadm</a> documentation.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style=color:#b44>[kubernetes]
</span><span style=color:#b44>name=Kubernetes
</span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style=color:#b44>enabled=1
</span><span style=color:#b44>gpgcheck=1
</span><span style=color:#b44>repo_gpgcheck=1
</span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># Set SELinux in permissive mode (effectively disabling it)</span>
<span style=color:#080;font-style:italic># Caveat: In a production environment you may not want to disable SELinux, please refer to Kubernetes documents about SELinux</span>
setenforce <span style=color:#666>0</span>
sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config

yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes

systemctl <span style=color:#a2f>enable</span> --now kubelet

cat <span style=color:#b44>&lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span><span style=color:#b44>EOF</span>
sysctl --system

<span style=color:#080;font-style:italic># check if br_netfilter module is loaded</span>
lsmod | grep br_netfilter

<span style=color:#080;font-style:italic># if not, load it explicitly with </span>
modprobe br_netfilter
</code></pre></div><p>The official document about how to create a single control-plane cluster can be found from the <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Creating a single control-plane cluster with kubeadm</a> documentation.</p><p>We'll largely follow that document but also add additional things for the cloud provider.
To make things more clear, we'll use a <code>kubeadm-config.yml</code> for the control-plane node.
In this config we specify to use an external OpenStack cloud provider, and where to find its config.
We also enable storage API in API server's runtime config so we can use OpenStack volumes as persistent volumes in Kubernetes.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;external&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;v1.15.1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>NodeRestriction<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>runtime-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;storage.k8s.io/v1=true&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>external-cloud-volume-plugin</span>:<span style=color:#bbb> </span>openstack<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;cloud-config&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud-config&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud-config&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>File<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.96.0.0/12&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.224.0.0/16&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dnsDomain</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;cluster.local&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>Now we'll create the cloud config, <code>/etc/kubernetes/cloud-config</code>, for OpenStack.
Note that the tenant here is the one we created for all Kubernetes VMs in the beginning.
All VMs should be launched in this project/tenant.
In addition you need to create a user in this tenant for Kubernetes to do queries.
The ca-file is the CA root certificate for OpenStack's API endpoint, for example <code>https://openstack.cloud:5000/v3</code>
At the time of writing the cloud provider doesn't allow insecure connections (skip CA check).</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=color:#a2f;font-weight:700>[Global]</span>
<span style=color:#b44>region</span><span style=color:#666>=</span><span style=color:#b44>RegionOne</span>
<span style=color:#b44>username</span><span style=color:#666>=</span><span style=color:#b44>username</span>
<span style=color:#b44>password</span><span style=color:#666>=</span><span style=color:#b44>password</span>
<span style=color:#b44>auth-url</span><span style=color:#666>=</span><span style=color:#b44>https://openstack.cloud:5000/v3</span>
<span style=color:#b44>tenant-id</span><span style=color:#666>=</span><span style=color:#b44>14ba698c0aec4fd6b7dc8c310f664009</span>
<span style=color:#b44>domain-id</span><span style=color:#666>=</span><span style=color:#b44>default</span>
<span style=color:#b44>ca-file</span><span style=color:#666>=</span><span style=color:#b44>/etc/kubernetes/ca.pem</span>

<span style=color:#a2f;font-weight:700>[LoadBalancer]</span>
<span style=color:#b44>subnet-id</span><span style=color:#666>=</span><span style=color:#b44>b4a9a292-ea48-4125-9fb2-8be2628cb7a1</span>
<span style=color:#b44>floating-network-id</span><span style=color:#666>=</span><span style=color:#b44>bc8a590a-5d65-4525-98f3-f7ef29c727d5</span>

<span style=color:#a2f;font-weight:700>[BlockStorage]</span>
<span style=color:#b44>bs-version</span><span style=color:#666>=</span><span style=color:#b44>v2</span>

<span style=color:#a2f;font-weight:700>[Networking]</span>
<span style=color:#b44>public-network-name</span><span style=color:#666>=</span><span style=color:#b44>public</span>
<span style=color:#b44>ipv6-support-disabled</span><span style=color:#666>=</span><span style=color:#b44>false</span>
</code></pre></div><p>Next run kubeadm to initiate the control-plane node</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm init --config<span style=color:#666>=</span>kubeadm-config.yml
</code></pre></div><p>With the initialization completed, copy admin config to .kube</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>  mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
  sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div><p>At this stage, the control-plane node is created but not ready. All the nodes have the taint <code>node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule</code> and are waiting to be initialized by the cloud-controller-manager.</p><pre><code class=language-console data-lang=console># kubectl describe no master1
Name:               master1
Roles:              master
......
Taints:             node-role.kubernetes.io/master:NoSchedule
                    node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
......
</code></pre><p>Now deploy the OpenStack cloud controller manager into the cluster, following <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md>using controller manager with kubeadm</a>.</p><p>Create a secret with the cloud-config for the openstack cloud provider.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret -n kube-system generic cloud-config --from-literal<span style=color:#666>=</span>cloud.conf<span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>cat /etc/kubernetes/cloud-config<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span> --dry-run -o yaml &gt; cloud-config-secret.yaml
kubectl apply -f cloud-config-secret.yaml 
</code></pre></div><p>Get the CA certificate for OpenStack API endpoints and put that into <code>/etc/kubernetes/ca.pem</code>.</p><p>Create RBAC resources.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-roles.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml
</code></pre></div><p>We'll run the OpenStack cloud controller manager as a DaemonSet rather than a pod.
The manager will only run on the control-plane node, so if there are multiple control-plane nodes, multiple pods will be run for high availability.
Create <code>openstack-cloud-controller-manager-ds.yaml</code> containing the following manifests, then apply it.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ServiceAccount<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>updateStrategy</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>RollingUpdate<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>node-role.kubernetes.io/master</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>runAsUser</span>:<span style=color:#bbb> </span><span style=color:#666>1001</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node.cloudprovider.kubernetes.io/uninitialized<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node.kubernetes.io/not-ready<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>serviceAccountName</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1.15.0<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- /bin/openstack-cloud-controller-manager<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- --v=1<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- --cloud-config=$(CLOUD_CONFIG)<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- --cloud-provider=openstack<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- --use-service-account-credentials=true<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- --address=127.0.0.1<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>k8s-certs<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/ssl/certs<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-certs<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/config<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-config-volume<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>flexvolume-dir<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>200m<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CLOUD_CONFIG<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/etc/config/cloud.conf<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostNetwork</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>flexvolume-dir<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>k8s-certs<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/etc/ssl/certs<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-certs<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-config-volume<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>cloud-config<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>openstack-ca-cert<span style=color:#bbb>
</span></code></pre></div><p>When the controller manager is running, it will query OpenStack to get information about the nodes and remove the taint. In the node info you'll see the VM's UUID in OpenStack.</p><pre><code class=language-console data-lang=console># kubectl describe no master1
Name:               master1
Roles:              master
......
Taints:             node-role.kubernetes.io/master:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
......
sage:docker: network plugin is not ready: cni config uninitialized
......
PodCIDR:                     10.224.0.0/24
ProviderID:                  openstack:///548e3c46-2477-4ce2-968b-3de1314560a5

</code></pre><p>Now install your favourite CNI and the control-plane node will become ready.</p><p>For example, to install Weave Net, run this command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div><p>Next we'll set up worker nodes.</p><p>Firstly, install docker and kubeadm in the same way as how they were installed in the control-plane node.
To join them to the cluster we need a token and ca cert hash from the output of control-plane node installation.
If it is expired or lost we can recreate it using these commands.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># check if token is expired</span>
kubeadm token list

<span style=color:#080;font-style:italic># re-create token and show join command</span>
kubeadm token create --print-join-command

</code></pre></div><p>Create <code>kubeadm-config.yml</code> for worker nodes with the above token and ca cert hash.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.1.7</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span>0c0z4p.dnafh6vnmouus569<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;sha256:fcb3e956a6880c05fc9d09714424b827f57a6fdc8afc44497180905946527adf&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;external&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span></code></pre></div><p>apiServerEndpoint is the control-plane node, token and caCertHashes can be taken from the join command printed in the output of 'kubeadm token create' command.</p><p>Run kubeadm and the worker nodes will be joined to the cluster.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubeadm join  --config kubeadm-config.yml 
</code></pre></div><p>At this stage we'll have a working Kubernetes cluster with an external OpenStack cloud provider.
The provider tells Kubernetes about the mapping between Kubernetes nodes and OpenStack VMs.
If Kubernetes wants to attach a persistent volume to a pod, it can find out which OpenStack VM the pod is running on from the mapping, and attach the underlying OpenStack volume to the VM accordingly.</p><h3 id=deploy-cinder-csi>Deploy Cinder CSI</h3><p>The integration with Cinder is provided by an external Cinder CSI plugin, as described in the <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md>Cinder CSI</a> documentation.</p><p>We'll perform the following steps to install the Cinder CSI plugin.
Firstly, create a secret with CA certs for OpenStack's API endpoints. It is the same cert file as what we use in cloud provider above.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret -n kube-system generic openstack-ca-cert --from-literal<span style=color:#666>=</span>ca.pem<span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>cat /etc/kubernetes/ca.pem<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span> --dry-run -o yaml &gt; openstack-ca-cert.yaml
kubectl apply -f openstack-ca-cert.yaml
</code></pre></div><p>Then create RBAC resources.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml
</code></pre></div><p>The Cinder CSI plugin includes a controller plugin and a node plugin.
The controller communicates with Kubernetes APIs and Cinder APIs to create/attach/detach/delete Cinder volumes. The node plugin in-turn runs on each worker node to bind a storage device (attached volume) to a pod, and unbind it during deletion.
Create <code>cinder-csi-controllerplugin.yaml</code> and apply it to create csi controller.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-cinder-controller-service<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-controllerplugin<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-controllerplugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dummy<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>12345</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-cinder-controllerplugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;csi-cinder-controller-service&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-controllerplugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-controllerplugin<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>serviceAccount</span>:<span style=color:#bbb> </span>csi-cinder-controller-sa<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-attacher<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/k8scsi/csi-attacher:v1.0.1<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--v=5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--csi-address=$(ADDRESS)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ADDRESS<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IfNotPresent&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-provisioner<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/k8scsi/csi-provisioner:v1.0.1<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--provisioner=csi-cinderplugin&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--csi-address=$(ADDRESS)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ADDRESS<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IfNotPresent&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-snapshotter<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/k8scsi/csi-snapshotter:v1.0.1<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--connection-timeout=15s&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--csi-address=$(ADDRESS)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ADDRESS<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cinder-csi-plugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>docker.io/k8scloudprovider/cinder-csi-plugin:v1.15.0<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args </span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- /bin/cinder-csi-plugin<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--v=5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--nodeid=$(NODE_ID)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--endpoint=$(CSI_ENDPOINT)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--cloud-config=$(CLOUD_CONFIG)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--cluster=$(CLUSTER_NAME)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NODE_ID<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>spec.nodeName<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CSI_ENDPOINT<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>unix://csi/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CLOUD_CONFIG<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/etc/config/cloud.conf<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CLUSTER_NAME<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IfNotPresent&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/csi<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/config<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/csi/sockets/pluginproxy/<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>cloud-config<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>openstack-ca-cert<span style=color:#bbb>
</span></code></pre></div><p>Create <code>cinder-csi-nodeplugin.yaml</code> and apply it to create csi node.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-cinder-nodeplugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-nodeplugin<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>csi-cinder-nodeplugin<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>serviceAccount</span>:<span style=color:#bbb> </span>csi-cinder-node-sa<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostNetwork</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>node-driver-registrar<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/k8scsi/csi-node-driver-registrar:v1.1.0<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--v=5&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--csi-address=$(ADDRESS)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>lifecycle</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>preStop</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>exec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;rm -rf /registration/cinder.csi.openstack.org /registration/cinder.csi.openstack.org-reg.sock&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ADDRESS<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/csi/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>DRIVER_REG_SOCK_PATH<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>KUBE_NODE_NAME<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>spec.nodeName<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IfNotPresent&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/csi<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>registration-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/registration<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cinder-csi-plugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>privileged</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>capabilities</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>add</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;SYS_ADMIN&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>allowPrivilegeEscalation</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>docker.io/k8scloudprovider/cinder-csi-plugin:v1.15.0<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>args </span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- /bin/cinder-csi-plugin<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--nodeid=$(NODE_ID)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--endpoint=$(CSI_ENDPOINT)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;--cloud-config=$(CLOUD_CONFIG)&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NODE_ID<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>spec.nodeName<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CSI_ENDPOINT<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>unix://csi/csi.sock<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>CLOUD_CONFIG<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>/etc/config/cloud.conf<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;IfNotPresent&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/csi<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-mount-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pods<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPropagation</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Bidirectional&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubelet-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/kubelet<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPropagation</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Bidirectional&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-cloud-data<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/cloud/data<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-probe-dir<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/dev<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPropagation</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;HostToContainer&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/config<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>socket-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/kubelet/plugins/cinder.csi.openstack.org<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>registration-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/kubelet/plugins_registry/<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubelet-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/kubelet<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-mount-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pods<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-cloud-data<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/cloud/data<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-probe-dir<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/dev<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>cloud-config<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>openstack-ca-cert<span style=color:#bbb>
</span><span style=color:#bbb>
</span></code></pre></div><p>When they are both running, create a storage class for Cinder.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-sc-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>csi-cinderplugin<span style=color:#bbb>
</span></code></pre></div><p>Then we can create a PVC with this class.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myvol<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>csi-sc-cinderplugin<span style=color:#bbb>
</span><span style=color:#bbb>
</span></code></pre></div><p>When the PVC is created, a Cinder volume is created correspondingly.</p><pre><code class=language-console data-lang=console># kubectl get pvc
NAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
myvol   Bound    pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad   1Gi        RWO            csi-sc-cinderplugin   3s

</code></pre><p>In OpenStack the volume name will match the Kubernetes persistent volume generated name. In this example it would be: <em>pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad</em></p><p>Now we can create a pod with the PVC.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hostPort</span>:<span style=color:#bbb> </span><span style=color:#666>8081</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/usr/share/nginx/html&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>myvol<span style=color:#bbb>
</span></code></pre></div><p>When the pod is running, the volume will be attached to the pod.
If we go back to OpenStack, we can see the Cinder volume is mounted to the worker node where the pod is running on.</p><pre><code class=language-console data-lang=console># openstack volume show 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field                          | Value                                                                                                                                                                                                                                                                                                                          |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| attachments                    | [{u'server_id': u'1c5e1439-edfa-40ed-91fe-2a0e12bc7eb4', u'attachment_id': u'11a15b30-5c24-41d4-86d9-d92823983a32', u'attached_at': u'2019-07-24T05:02:34.000000', u'host_name': u'compute-6', u'volume_id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f', u'device': u'/dev/vdb', u'id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f'}] |
| availability_zone              | nova                                                                                                                                                                                                                                                                                                                           |
| bootable                       | false                                                                                                                                                                                                                                                                                                                          |
| consistencygroup_id            | None                                                                                                                                                                                                                                                                                                                           |
| created_at                     | 2019-07-24T05:02:18.000000                                                                                                                                                                                                                                                                                                     |
| description                    | Created by OpenStack Cinder CSI driver                                                                                                                                                                                                                                                                                         |
| encrypted                      | False                                                                                                                                                                                                                                                                                                                          |
| id                             | 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f                                                                                                                                                                                                                                                                                           |
| migration_status               | None                                                                                                                                                                                                                                                                                                                           |
| multiattach                    | False                                                                                                                                                                                                                                                                                                                          |
| name                           | pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad                                                                                                                                                                                                                                                                                       |
| os-vol-host-attr:host          | rbd:volumes@rbd#rbd                                                                                                                                                                                                                                                                                                            |
| os-vol-mig-status-attr:migstat | None                                                                                                                                                                                                                                                                                                                           |
| os-vol-mig-status-attr:name_id | None                                                                                                                                                                                                                                                                                                                           |
| os-vol-tenant-attr:tenant_id   | 14ba698c0aec4fd6b7dc8c310f664009                                                                                                                                                                                                                                                                                               |
| properties                     | attached_mode='rw', cinder.csi.openstack.org/cluster='kubernetes'                                                                                                                                                                                                                                                              |
| replication_status             | None                                                                                                                                                                                                                                                                                                                           |
| size                           | 1                                                                                                                                                                                                                                                                                                                              |
| snapshot_id                    | None                                                                                                                                                                                                                                                                                                                           |
| source_volid                   | None                                                                                                                                                                                                                                                                                                                           |
| status                         | in-use                                                                                                                                                                                                                                                                                                                         |
| type                           | rbd                                                                                                                                                                                                                                                                                                                            |
| updated_at                     | 2019-07-24T05:02:35.000000                                                                                                                                                                                                                                                                                                     |
| user_id                        | 5f6a7a06f4e3456c890130d56babf591                                                                                                                                                                                                                                                                                               |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

</code></pre><h3 id=summary>Summary</h3><p>In this walk-through, we deployed a Kubernetes cluster on OpenStack VMs and integrated it with OpenStack using an external OpenStack cloud provider. Then on this Kubernetes cluster we deployed Cinder CSI plugin which can create Cinder volumes and expose them in Kubernetes as persistent volumes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e332ba17f4aeb04e399bda303f280be7>KubeInvaders - Gamified Chaos Engineering Tool for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2020-01-22 class=text-muted>Wednesday, January 22, 2020</time></div><p><strong>Authors</strong> Eugenio Marzo, Sourcesense</p><p>Some months ago, I released my latest project called KubeInvaders. The
first time I shared it with the community was during an Openshift
Commons Briefing session. Kubenvaders is a Gamified Chaos Engineering
tool for Kubernetes and Openshift and helps test how resilient your
Kubernetes cluster is, in a fun way.</p><p>It is like Space Invaders, but the aliens are pods.</p><p><img src=https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img1.png alt></p><p>During my presentation at Codemotion Milan 2019, I started saying "of
course you can do it with few lines of Bash, but it is boring."</p><p><img src=https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img2.png alt></p><p>Using the code above you can kill random pods across a Kubernetes cluster, but I
think it is much more fun with the spaceship of KubeInvaders.</p><p>I published the code at
<a href=https://github.com/lucky-sideburn/KubeInvaders>https://github.com/lucky-sideburn/KubeInvaders</a>
and there is a little community that is growing gradually. Some people
love to use it for demo sessions killing pods on a big screen.</p><p><img src=https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img3.png alt></p><h2 id=how-to-install-kubeinvaders>How to install KubeInvaders</h2><p>I defined multiples modes to install it:</p><ol><li><p>Helm Chart
<a href=https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders>https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders</a></p></li><li><p>Manual Installation for Openshift using a template
<a href=https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift>https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift</a></p></li><li><p>Manual Installation for Kubernetes
<a href=https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes>https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes</a></p></li></ol><p>The preferred way, of course, is with a Helm chart:</p><pre><code># Please set target_namespace to set your target namespace!
helm install --set-string target_namespace=&quot;namespace1,namespace2&quot; \
--name kubeinvaders --namespace kubeinvaders ./helm-charts/kubeinvaders
</code></pre><h2 id=how-to-use-kubeinvaders>How to use KubeInvaders</h2><p>Once it is installed on your cluster you can use the following
functionalities:</p><ul><li>Key 'a' — Switch to automatic pilot</li><li>Key 'm' — Switch to manual pilot</li><li>Key 'i' — Show pod's name. Move the ship towards an alien</li><li>Key 'h' — Print help</li><li>Key 'n' — Jump between different namespaces (my favorite feature!)</li></ul><h2 id=tuning-kubeinvaders>Tuning KubeInvaders</h2><p>At Codemotion Milan 2019, my colleagues and I organized a desk with a
game station for playing KubeInvaders. People had to fight with Kubernetes to
win a t-shirt.</p><p>If you have pods that require a few seconds to start, you may lose. It
is possible to set the complexity of the game with these parameters as
environmment variables in the Kubernetes deployment:</p><ul><li>ALIENPROXIMITY — Reduce this value to increase the distance between aliens;</li><li>HITSLIMIT — Seconds of CPU time to wait before shooting;</li><li>UPDATETIME — Seconds to wait before updating pod status (you can set also 0.x Es: 0.5);</li></ul><p>The result is a harder game experience against the machine.</p><h2 id=use-cases>Use cases</h2><p>Adopting chaos engineering strategies for your production environment is
really useful, because it is the only way to test if a system supports
unexpected destructive events.</p><p>KubeInvaders is a game — so please do not take it too seriously! — but it demonstrates
some important use cases:</p><ul><li>Test how resilient Kubernetes clusters are on unexpected pod deletion</li><li>Collect metrics like pod restart time</li><li>Tune readiness probes</li></ul><h2 id=next-steps>Next steps</h2><p>I want to continue to add some cool features and integrate it into a
Kubernetes dashboard because I am planning to transform it into a
"Gamified Chaos Engineering and Development Tool for Kubernetes", to help
developer to interact with deployments in a Kubernetes environment. For
example:</p><ul><li>Point to the aliens to get pod logs</li><li>Deploy Helm charts by shooting some particular objects</li><li>Read messages stored in a specific label present in a deployment</li></ul><p>Please feel free to contribute to
<a href=https://github.com/lucky-sideburn/KubeInvaders>https://github.com/lucky-sideburn/KubeInvaders</a>
and stay updated following #kubeinvaders news <a href=https://twitter.com/luckysideburn>on Twitter</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-199e33ba6883a21dfa31ee59c9b686e2>CSI Ephemeral Inline Volumes</h1><div class="td-byline mb-4"><time datetime=2020-01-21 class=text-muted>Tuesday, January 21, 2020</time></div><p><strong>Author:</strong> Patrick Ohly (Intel)</p><p>Typically, volumes provided by an external storage driver in
Kubernetes are <em>persistent</em>, with a lifecycle that is completely
independent of pods or (as a special case) loosely coupled to the
first pod which uses a volume (<a href=https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode>late binding
mode</a>).
The mechanism for requesting and defining such volumes in Kubernetes
are <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/>Persistent Volume Claim (PVC) and Persistent Volume
(PV)</a>
objects. Originally, volumes that are backed by a Container Storage Interface
(CSI) driver could only be used via this PVC/PV mechanism.</p><p>But there are also use cases for data volumes whose content and
lifecycle is tied to a pod. For example, a driver might populate a
volume with dynamically created secrets that are specific to the
application running in the pod. Such volumes need to be created
together with a pod and can be deleted as part of pod termination
(<em>ephemeral</em>). They get defined as part of the pod spec (<em>inline</em>).</p><p>Since Kubernetes 1.15, CSI drivers can also be used for such
<em>ephemeral inline</em> volumes. The <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/>CSIInlineVolume feature
gate</a>
had to be set to enable it in 1.15 because support was still in alpha
state. In 1.16, the feature reached beta state, which typically means
that it is enabled in clusters by default.</p><p>CSI drivers have to be adapted to support this because although two
existing CSI gRPC calls are used (<code>NodePublishVolume</code> and <code>NodeUnpublishVolume</code>),
the way how they are
used is different and not covered by the CSI spec: for ephemeral
volumes, only <code>NodePublishVolume</code> is invoked by <code>kubelet</code> when asking
the CSI driver for a volume. All other calls
(like <code>CreateVolume</code>, <code>NodeStageVolume</code>, etc.) are skipped. The volume
parameters are provided in the pod spec and from there copied into the
<code>NodePublishVolumeRequest.volume_context</code> field. There are currently
no standardized parameters; even common ones like size must be
provided in a format that is defined by the CSI driver. Likewise, only
<code>NodeUnpublishVolume</code> gets called after the pod has terminated and the
volume needs to be removed.</p><p>Initially, the assumption was that CSI drivers would be specifically
written to provide either persistent or ephemeral volumes. But there
are also drivers which provide storage that is useful in both modes:
for example, <a href=https://github.com/intel/pmem-csi>PMEM-CSI</a> manages
persistent memory (PMEM), a new kind of local storage that is provided
by <a href=https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html>Intel® Optane™ DC Persistent
Memory</a>. Such
memory is useful both as persistent data storage (faster than normal SSDs)
and as ephemeral scratch space (higher capacity than DRAM).</p><p>Therefore the support in Kubernetes 1.16 was extended:</p><ul><li>Kubernetes and users can determine which kind of volumes a driver
supports via the <code>volumeLifecycleModes</code> field in the <a href=https://kubernetes-csi.github.io/docs/csi-driver-object.html#what-fields-does-the-csidriver-object-have><code>CSIDriver</code>
object</a>.</li><li>Drivers can get information about the volume mode by enabling the
<a href=https://kubernetes-csi.github.io/docs/pod-info.html>"pod info on
mount"</a> feature
which then will add the new <code>csi.storage.k8s.io/ephemeral</code> entry to
the <code>NodePublishRequest.volume_context</code>.</li></ul><p>For more information about implementing support of ephemeral inline
volumes in a CSI driver, see the <a href=https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html>Kubernetes-CSI
documentation</a>
and the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md>original design
document</a>.</p><p>What follows in this blog post are usage examples based on real drivers
and a summary at the end.</p><h1 id=examples>Examples</h1><h2 id=pmem-csi-https-github-com-intel-pmem-csi><a href=https://github.com/intel/pmem-csi>PMEM-CSI</a></h2><p>Support for ephemeral inline volumes was added in <a href=https://github.com/intel/pmem-csi/releases/tag/v0.6.0>release
v0.6.0</a>. The
driver can be used on hosts with real Intel® Optane™ DC Persistent
Memory, on <a href=https://github.com/intel/pmem-csi/blob/v0.6.0/examples/gce.md>special machines in
GCE</a> or
with hardware emulated by QEMU. The latter is fully <a href=https://github.com/intel/pmem-csi/tree/v0.6.0#qemu-and-kubernetes>integrated into
the
makefile</a>
and only needs Go, Docker and KVM, so that approach was used for this
example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>git clone --branch release-0.6 https://github.com/intel/pmem-csi
<span style=color:#a2f>cd</span> pmem-csi
<span style=color:#b8860b>TEST_DISTRO</span><span style=color:#666>=</span>clear <span style=color:#b8860b>TEST_DISTRO_VERSION</span><span style=color:#666>=</span><span style=color:#666>32080</span> <span style=color:#b8860b>TEST_PMEM_REGISTRY</span><span style=color:#666>=</span>intel make start
</code></pre></div><p>Bringing up the four-node cluster can take a while but eventually should end with:</p><pre><code>The test cluster is ready. Log in with /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm, run kubectl once logged in.
Alternatively, KUBECONFIG=/work/pmem-csi/_work/pmem-govm/kube.config can also be used directly.

To try out the pmem-csi driver persistent volumes:
...

To try out the pmem-csi driver ephemeral volumes:
   cat deploy/kubernetes-1.17/pmem-app-ephemeral.yaml | /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm kubectl create -f -
</code></pre><p><code>deploy/kubernetes-1.17/pmem-app-ephemeral.yaml</code> specifies one volume:</p><pre><code>kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app-inline-volume
spec:
  containers:
    - name: my-frontend
      image: busybox
      command: [ &quot;sleep&quot;, &quot;100000&quot; ]
      volumeMounts:
      - mountPath: &quot;/data&quot;
        name: my-csi-volume
  volumes:
  - name: my-csi-volume
    csi:
      driver: pmem-csi.intel.com
      fsType: &quot;xfs&quot;
      volumeAttributes:
        size: &quot;2Gi&quot;
        nsmode: &quot;fsdax&quot;
</code></pre><p>Once we have created that pod, we can inspect the result:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl describe pods/my-csi-app-inline-volume
</code></pre></div><pre><code>Name:         my-csi-app-inline-volume
...
Volumes:
  my-csi-volume:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            pmem-csi.intel.com
    FSType:            xfs
    ReadOnly:          false
    VolumeAttributes:      nsmode=fsdax
                           size=2Gi
</code></pre><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl <span style=color:#a2f>exec</span> my-csi-app-inline-volume -- df -h /data
</code></pre></div><pre><code>Filesystem                Size      Used Available Use% Mounted on
/dev/ndbus0region0fsdax/d7eb073f2ab1937b88531fce28e19aa385e93696
                          1.9G     34.2M      1.8G   2% /data
</code></pre><h2 id=image-populator-https-github-com-kubernetes-csi-csi-driver-image-populator><a href=https://github.com/kubernetes-csi/csi-driver-image-populator>Image Populator</a></h2><p>The image populator automatically unpacks a container image and makes
its content available as an ephemeral volume. It's still in
development, but canary images are already available which can be
installed with:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-csidriverinfo.yaml
kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-daemonset.yaml
</code></pre></div><p>This example pod will run nginx and have it serve data that
comes from the <code>kfox1111/misc:test</code> image:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl create -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>apiVersion: v1
</span><span style=color:#b44>kind: Pod
</span><span style=color:#b44>metadata:
</span><span style=color:#b44>  name: nginx
</span><span style=color:#b44>spec:
</span><span style=color:#b44>  containers:
</span><span style=color:#b44>  - name: nginx
</span><span style=color:#b44>    image: nginx:1.16-alpine
</span><span style=color:#b44>    ports:
</span><span style=color:#b44>    - containerPort: 80
</span><span style=color:#b44>    volumeMounts:
</span><span style=color:#b44>    - name: data
</span><span style=color:#b44>      mountPath: /usr/share/nginx/html
</span><span style=color:#b44>  volumes:
</span><span style=color:#b44>  - name: data
</span><span style=color:#b44>    csi:
</span><span style=color:#b44>      driver: image.csi.k8s.io
</span><span style=color:#b44>      volumeAttributes:
</span><span style=color:#b44>          image: kfox1111/misc:test
</span><span style=color:#b44>EOF</span>
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl <span style=color:#a2f>exec</span> nginx -- cat /usr/share/nginx/html/test
</code></pre></div><p>That <code>test</code> file just contains a single word:</p><pre><code>testing
</code></pre><p>Such data containers can be built with Dockerfiles such as:</p><pre><code>FROM scratch
COPY index.html /index.html
</code></pre><h2 id=cert-manager-csi-https-github-com-jetstack-cert-manager-csi><a href=https://github.com/jetstack/cert-manager-csi>cert-manager-csi</a></h2><p>cert-manager-csi works together with
<a href=https://github.com/jetstack/cert-manager>cert-manager</a>. The goal for
this driver is to facilitate requesting and mounting certificate key
pairs to pods seamlessly. This is useful for facilitating mTLS, or
otherwise securing connections of pods with guaranteed present
certificates whilst having all of the features that cert-manager
provides. This project is experimental.</p><h1 id=next-steps>Next steps</h1><p>One of the issues with ephemeral inline volumes is that pods get
scheduled by Kubernetes onto nodes without knowing anything about the
currently available storage on that node. Once the pod has been
scheduled, the CSI driver must make the volume available one that
node. If that is currently not possible, the pod cannot start. This
will be retried until eventually the volume becomes ready. The
<a href=https://github.com/kubernetes/enhancements/pull/1353>storage capacity tracking
KEP</a> is an
attempt to address this problem.</p><p>A related KEP introduces a <a href=https://github.com/kubernetes/enhancements/pull/1409>standardized size
parameter</a>.</p><p>Currently, CSI ephemeral inline volumes stay in beta while issues like
these are getting discussed. Your feedback is needed to decide how to
proceed with this feature. For the KEPs, the two PRs linked to above
is a good place to comment. The SIG Storage also <a href=https://github.com/kubernetes/community/tree/master/sig-storage#meetings>meets
regularly</a>
and can be reached via <a href=https://github.com/kubernetes/community/tree/master/sig-storage#contact>Slack and a mailing
list</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0152ead4467ad6155722ba3071c4c16a>Reviewing 2019 in Docs</h1><div class="td-byline mb-4"><time datetime=2020-01-21 class=text-muted>Tuesday, January 21, 2020</time></div><p><strong>Author:</strong> Zach Corleissen (Cloud Native Computing Foundation)</p><p>Hi, folks! I'm one of the co-chairs for the Kubernetes documentation special interest group (SIG Docs). This blog post is a review of SIG Docs in 2019. Our contributors did amazing work last year, and I want to highlight their successes.</p><p>Although I review 2019 in this post, my goal is to point forward to 2020. I observe some trends in SIG Docs–some good, others troubling. I want to raise visibility before those challenges increase in severity.</p><h2 id=the-good>The good</h2><p>There was much to celebrate in SIG Docs in 2019.</p><p>Kubernetes docs started the year with three localizations in progress. By the end of the year, we ended with ten localizations available, four of which (Chinese, French, Japanese, Korean) are reasonably complete. The Korean and French teams deserve special mentions for their contributions to git best practices across all localizations (Korean team) and help bootstrapping other localizations (French team).</p><p>Despite significant transition over the year, SIG Docs <a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&var-period=w&var-repogroup_name=SIG%20Docs&var-apichange=All&var-size_name=All&var-kind_name=All">improved its review velocity</a>, with a median review time from PR open to merge of just over 24 hours.</p><p>Issue triage improved significantly in both volume and speed, largely due to the efforts of GitHub users @sftim, @tengqm, and @kbhawkey.</p><p>Doc sprints remain valuable at KubeCon contributor days, introducing new contributors to Kubernetes documentation.</p><p>The docs component of Kubernetes quarterly releases improved over 2019, thanks to iterative playbook improvements from release leads and their teams.</p><p>Site traffic increased over the year. The website ended the year with ~6 million page views per month in December, up from ~5M page views in January. The kubernetes.io website had 851k site visitors in October, a new all-time high. Reader satisfaction <a href=https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/>remains general</a>.</p><p>We onboarded a new SIG chair: @jimangel, a Cloud Architect at General Motors. Jim was a docs contributor for a year, during which he led the 1.14 docs release, before stepping up as chair.</p><h2 id=the-not-so-good>The not so good</h2><p>While reader satisfaction is decent, <strong>most respondents indicated dissatisfaction with stale content</strong> in every area: concepts, tasks, tutorials, and reference. Additionally, readers requested more diagrams, advanced conceptual content, and code samples—things that technical writers excel at providing.</p><p>SIG Docs continues to solve how best to handle <a href=https://github.com/kubernetes/enhancements/pull/1327>third-party content</a>. <strong>There's too much vendor content on kubernetes.io</strong>, and guidelines for adding or rejecting third-party content remain unclear. The discussion so far has been powerful, including pushback demanding greater collaborative input—a powerful reminder that Kubernetes is in all ways a communal effort.</p><p>We're in the middle of our third chair transition in 18 months. Each chair transition has been healthy and collegial, but it's still a lot of turnover in a short time. Chairing any open source project is difficult, but especially so with SIG Docs. Chairship of SIG Docs requires a steep learning curve across multiple domains: docs (both written and generated from spec), information architecture, specialized contribution paths (for example, localization), how to run a release cycle, website development, CI/CD, community management, on and on. It's a role that requires multiple people to function successfully without burning people out. Training replacements is time-intensive.</p><p>Perhaps most pressing in the Not So Good category is that SIG Docs currently has only one technical writer dedicated full-time to Kubernetes docs. This has impacts on Kubernetes docs: some obvious, some less so.</p><h2 id=impacts-of-understaffing-on-kubernetes-docs>Impacts of understaffing on Kubernetes docs</h2><blockquote class=twitter-tweet><p lang=en dir=ltr>Me today: <a href=https://t.co/cDpHOWEsjf>pic.twitter.com/cDpHOWEsjf</a></p>&mdash; Benjamin Elder (@BenTheElder) <a href="https://twitter.com/BenTheElder/status/1215453579651104768?ref_src=twsrc%5Etfw">January 10, 2020</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>If Kubernetes continues through 2020 without more technical writers dedicated to the docs, here's what I see as the most likely possibilities.</p><h3 id=but-first-a-disclaimer>But first, a disclaimer</h3><blockquote class="caution callout"><div><strong>Caution:</strong> It is very hard to predict, especially the future.
-Niels Bohr</div></blockquote><p>Some of my predictions are almost certainly wrong. Any errors are mine alone.</p><p>That said...</p><h3 id=effects-in-2020>Effects in 2020</h3><p>Current levels of function aren't self-sustaining. Even with a strong playbook, the release cycle still requires expert support from at least one (and usually two) chairs during every cycle. Without fail, each release breaks in new and unexpected ways, and it requires familiarity and expertise to diagnose and resolve. As chairs continue to cycle—and to be clear, regular transitions are part of a healthy project—we accrue the risks associated with a pool lacking sufficient professional depth and employer support.</p><p>Oddly enough, one of the challenges to staffing is that the docs appear good enough. Based on site analytics and survey responses, readers are pleased with the quality of the docs. When folks visit the site, they generally find what they need and behave like satisfied visitors.</p><p>The danger is that this will change over time: slowly with occasional losses of function, annoying at first, then increasingly critical. The more time passes without adequate staffing, the more difficult and costly fixes will become.</p><p>I suspect this is true because the challenges we face now at decent levels of reader satisfaction are already difficult to fix. API reference generation is complex and brittle; the site's UI is outdated; and our most consistent requests are for more tutorials, advanced concepts, diagrams, and code samples, all of which require ongoing, dedicated time to create.</p><p><strong>Release support remains strong.</strong></p><p>The release team continues a solid habit of leaving each successive team with better support than the previous release. This mostly takes the form of iterative improvements to the <a href=https://github.com/kubernetes/community/tree/master/sig-release#docs-lead>docs release playbook</a>, producing better documentation and reducing siloed knowledge.</p><p><strong>Staleness accelerates.</strong></p><p>Conceptual content becomes less accurate or relevant as features change or deprecate. Tutorial content degrades for the same reason.</p><p>The content structure will also degrade: the categories of concepts, tasks, and tutorials are legacy categories that may not best fit the needs of current readers, let alone future ones.</p><p>Cruft accumulates for both readers and contributors. Reference docs become increasingly brittle without intervention.</p><p><strong>Critical knowledge vanishes.</strong></p><p>As I mentioned previously, SIG Docs has a wide range of functions, some with a steep learning curve. As contributors change roles or jobs, their expertise and availability will diminish or reduce to zero. Contributors with specific knowledge may not be available for consultation, exposing critical vulnerabilities in docs function. Specific examples include reference generation and chair leadership.</p><h3 id=that-s-a-lot-to-take-in>That's a lot to take in</h3><p>It's difficult to strike a balance between the importance of SIG Docs' work to the community and our users, the joy it brings me personally, and the fact that things can't remain as they are without significant negative impacts (eventually). SIG Docs is by no means dying; it's a vibrant community with active contributors doing cool things. It's also a community with some critical knowledge and capacity shortages that can only be remedied with trained, paid staff dedicated to documentation.</p><h2 id=what-the-community-can-do-for-healthy-docs>What the community can do for healthy docs</h2><p>Hire technical writers dedicated to Kubernetes docs. Support advanced content creation, not just release docs and incremental feature updates.</p><p>Thanks, and Happy 2020.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-951c3eb34e5599cb9806fb380774663c>Kubernetes on MIPS</h1><div class="td-byline mb-4"><time datetime=2020-01-15 class=text-muted>Wednesday, January 15, 2020</time></div><p><strong>Authors:</strong> TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)</p><h2 id=background>Background</h2><p><a href=https://en.wikipedia.org/wiki/MIPS_architecture>MIPS</a> (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.</p><p><a href=https://kubernetes.io>Kubernetes</a> has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.</p><h2 id=achievements>Achievements</h2><p>For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.</p><p>Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.</p><p><img src=/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png alt="Kubernetes on MIPS"></p><p><em>Figure 1 Kubernetes on MIPS</em></p><h2 id=k8s-mips-component-build>K8S-MIPS component build</h2><p>Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:</p><ul><li>golang</li><li>docker-ce</li><li>hyperkube</li><li>pause</li><li>etcd</li><li>calico</li><li>coredns</li><li>metrics-server</li></ul><p>Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.</p><p>During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.</p><p>To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.</p><p>After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.</p><table><thead><tr><th>Name</th><th>Version</th><th>MIPS Repository</th></tr></thead><tbody><tr><td>golang on MIPS</td><td>1.12.5</td><td>-</td></tr><tr><td>docker-ce on MIPS</td><td>18.09.8</td><td>-</td></tr><tr><td>metrics-server for CKE on MIPS</td><td>0.3.2</td><td><code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2</code></td></tr><tr><td>etcd for CKE on MIPS</td><td>3.2.26</td><td><code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26</code></td></tr><tr><td>pause for CKE on MIPS</td><td>3.1</td><td><code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1</code></td></tr><tr><td>hyperkube for CKE on MIPS</td><td>1.14.3</td><td><code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3</code></td></tr><tr><td>coredns for CKE on MIPS</td><td>1.6.5</td><td><code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5</code></td></tr><tr><td>calico for CKE on MIPS</td><td>3.8.0</td><td><code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0</code> <code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0</code> <code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0</code> <code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0</code></td></tr></tbody></table><p><strong>Note</strong>: CKE is a Kubernetes-based cloud container engine launched by Inspur</p><p><img src=/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png alt="K8S-MIPS Cluster Components"></p><p><em>Figure 2 K8S-MIPS Cluster Components</em></p><p><img src=/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png alt="CPU Architecture"></p><p><em>Figure 3 CPU Architecture</em></p><p><img src=/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png alt="Cluster Node Information"></p><p><em>Figure 4 Cluster Node Information</em></p><h2 id=run-k8s-conformance-test>Run K8S Conformance Test</h2><p>The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes <a href=https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md>conformance test</a>.</p><p>Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.</p><p>Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from <code>kubernetes/test/images</code>, and the built images are at <code>gcr.io/kubernetes-e2e-test-images</code>. Since there are no MIPS images in the repository, we must first build all needed images to run the test.</p><h3 id=build-needed-images-for-test>Build needed images for test</h3><p>The first step is to find all needed images for the test. We can run <code>sonobuoy images-p e2e</code> command to list all images, or we can find those images in <a href=https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go>/test/utils/image/manifest.go</a>. Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.</p><p>Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of <a href=https://www.alpinelinux.org/>alpine</a>, so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.</p><p>Some images are not in <code>kubernetes/test/images</code>, such as <code>gcr.io/google-samples/gb-frontend:v6</code>. There is no clear documentation explaining where these images are locaated, though we found the source code in repository <a href=https://github.com/GoogleCloudPlatform/kubernetes-engine-samples>github.com/GoogleCloudPlatform/kubernetes-engine-samples</a>. We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as <code>php:5-apache</code>, <code>redis</code>, and <code>perl</code>.</p><p>After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is <code>imagePullPolicy: ifNotPresent</code>.</p><p>Here are some of the images we built：</p><ul><li><code>docker.io/library/busybox:1.29</code></li><li><code>docker.io/library/nginx:1.14-alpine</code></li><li><code>docker.io/library/nginx:1.15-alpine</code></li><li><code>docker.io/library/perl:5.26</code></li><li><code>docker.io/library/httpd:2.4.38-alpine</code></li><li><code>docker.io/library/redis:5.0.5-alpine</code></li><li><code>gcr.io/google-containers/conformance:v1.16.2</code></li><li><code>gcr.io/google-containers/hyperkube:v1.16.2</code></li><li><code>gcr.io/google-samples/gb-frontend:v6</code></li><li><code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6</code></li><li><code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1</code></li><li><code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2</code></li><li><code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/kitten:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2</code></li><li><code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5</code></li><li><code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10</code></li><li><code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0</code></li><li><code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1</code></li><li><code>k8s.gcr.io/etcd:3.3.15</code></li><li><code>k8s.gcr.io/pause:3.1</code></li></ul><p>Finally, we ran the tests and got the test result, include <code>e2e.log</code>, which showed that all test cases passed. Additionally, we submitted our test result to <a href=https://github.com/cncf/k8s-conformance>k8s-conformance</a> as a <a href=https://github.com/cncf/k8s-conformance/pull/779>pull request</a>.</p><p><img src=/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png alt="Pull request for conformance test results"></p><p><em>Figure 5 Pull request for conformance test results</em></p><h2 id=what-s-next>What's next</h2><p>We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.</p><p>In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.</p><p>Contribution plan：</p><ul><li>contribute the source of e2e test images for MIPS</li><li>contribute the source of hyperkube for MIPS</li><li>contribute the source of deploy tools like kubeadm for MIPS</li></ul><hr></div><div class=td-content style=page-break-before:always><h1 id=pg-20eba094421fb566b3bc2a466a227395>Announcing the Kubernetes bug bounty program</h1><div class="td-byline mb-4"><time datetime=2020-01-14 class=text-muted>Tuesday, January 14, 2020</time></div><p><strong>Authors:</strong> Maya Kaczorowski and Tim Allclair, Google, on behalf of the <a href=https://github.com/kubernetes/community/tree/master/committee-product-security>Kubernetes Product Security Committee</a></p><p>Today, the <a href=https://github.com/kubernetes/community/tree/master/committee-product-security>Kubernetes Product Security Committee</a> is launching a <a href=https://hackerone.com/kubernetes>new bug bounty program</a>, funded by the <a href=https://www.cncf.io/>CNCF</a>, to reward researchers finding security vulnerabilities in Kubernetes.</p><h2 id=setting-up-a-new-bug-bounty-program>Setting up a new bug bounty program</h2><p>We aimed to set up this bug bounty program as transparently as possible, with <a href="https://docs.google.com/document/d/1dvlQsOGODhY3blKpjTg6UXzRdPzv5y8V55RD_Pbo7ag/edit#heading=h.7t1efwpev42p">an initial proposal</a>, <a href=https://github.com/kubernetes/kubernetes/issues/73079>evaluation of vendors</a>, and <a href=https://github.com/kubernetes/community/blob/master/contributors/guide/bug-bounty.md>working draft of the components in scope</a>. Once we onboarded the selected bug bounty program vendor, <a href=https://www.hackerone.com/>HackerOne</a>, these documents were further refined based on the feedback from HackerOne, as well as what was learned in the recent <a href=https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf>Kubernetes security audit</a>. The bug bounty program has been in a private release for several months now, with invited researchers able to submit bugs and help us test the triage process. After almost two years since the initial proposal, the program is now ready for all security researchers to contribute!</p><p>What’s exciting is that this is rare: a bug bounty for an open-source infrastructure tool. Some open-source bug bounty programs exist, such as the <a href=https://internetbugbounty.org/>Internet Bug Bounty</a>, this mostly covers core components that are consistently deployed across environments; but most bug bounties are still for hosted web apps. In fact, with more than<a href=https://www.cncf.io/certification/kcsp/> 100 certified distributions of Kubernetes</a>, the bug bounty program needs to apply to the Kubernetes code that powers all of them. By far, the most time-consuming challenge here has been ensuring that the program provider (HackerOne) and their researchers who do the first line triage have the awareness of Kubernetes and the ability to easily test the validity of a reported bug. As part of the bootstrapping process, HackerOne had their team pass the <a href=https://www.cncf.io/certification/cka/>Certified Kubernetes Administrator</a> (CKA) exam.</p><h2 id=what-s-in-scope>What’s in scope</h2><p>The bug bounty scope covers code from the main Kubernetes organizations on GitHub, as well as continuous integration, release, and documentation artifacts. Basically, most content you’d think of as ‘core’ Kubernetes, included at <a href=https://github.com/kubernetes>https://github.com/kubernetes</a>, is in scope. We’re particularly interested in cluster attacks, such as privilege escalations, authentication bugs, and remote code execution in the kubelet or API server. Any information leak about a workload, or unexpected permission changes is also of interest. Stepping back from the cluster admin’s view of the world, you’re also encouraged to look at the Kubernetes supply chain, including the build and release processes, which would allow any unauthorized access to commits, or the ability to publish unauthorized artifacts.</p><p>Notably out of scope is the community management tooling, e.g., the Kubernetes mailing lists or Slack channel. Container escapes, attacks on the Linux kernel, or other dependencies, such as etcd, are also out of scope and should be reported to the appropriate party. We would still appreciate that any Kubernetes vulnerability, even if not in scope for the bug bounty, be <a href=https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability>disclosed privately</a> to the Kubernetes Product Security Committee. See the full scope on the <a href=https://hackerone.com/kubernetes>program reporting page</a>.</p><h2 id=how-kubernetes-handles-vulnerabilities-and-disclosures>How Kubernetes handles vulnerabilities and disclosures</h2><p>Kubernetes’ <a href=https://github.com/kubernetes/community/tree/master/committee-product-security>Product Security Committee</a> is a group of security-focused maintainers who are responsible for receiving and responding to reports of security issues in Kubernetes. This follows the documented <a href=https://kubernetes.io/docs/reference/issues-security/security/>security vulnerability response process</a>, which includes initial triage, assessing impact, generating and rolling out a fix.</p><p>With our bug bounty program, initial triage and initial assessment are handled by the bug bounty provider, in this case, HackerOne, enabling us better scale our limited Kubernetes security experts to handle only valid reports. Nothing else in this process is changing - the Product Security Committee will continue to develop fixes, build private patches, and coordinate special security releases. New releases with security patches will be announced at <a href=https://groups.google.com/forum/#!forum/kubernetes-security-announce>kubernetes-security-announce@googlegroups.com</a>.</p><p>If you want to report a bug, you don’t need to use the bug bounty - you can still follow the <a href=https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability>existing process</a> and report what you’ve found at <a href=mailto:security@kubernetes.io>security@kubernetes.io</a>.</p><h2 id=get-started>Get started</h2><p>Just as many organizations support open source by hiring developers, paying bug bounties directly supports security researchers. This bug bounty is a critical step for Kubernetes to build up its community of security researchers and reward their hard work.</p><p>If you’re a security researcher, and new to Kubernetes, check out these resources to learn more and get started bug hunting:</p><ul><li><strong>Hardening guides</strong><ul><li>Kubernetes.io: <a href=https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/>https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/</a></li></ul></li><li><strong>Frameworks</strong><ul><li>CIS benchmarks: <a href=https://www.cisecurity.org/benchmark/kubernetes/>https://www.cisecurity.org/benchmark/kubernetes/</a></li><li>NIST 800-190: <a href=https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf>https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf</a></li></ul></li><li><strong>Talks</strong><ul><li>The Devil in the Details: Kubernetes’ First Security Assessment (KubeCon NA 2019): <a href="https://www.youtube.com/watch?v=vknE5XEa_Do">https://www.youtube.com/watch?v=vknE5XEa_Do</a></li><li>Crafty Requests: Deep Dive into Kubernetes CVE-2018-1002105 (KubeCon EU 2019): <a href="https://www.youtube.com/watch?v=VjSJqc13PNk">https://www.youtube.com/watch?v=VjSJqc13PNk</a></li><li>A Hacker’s Guide to Kubernetes and the Cloud (KubeCon EU 2018): <a href="https://www.youtube.com/watch?v=dxKpCO2dAy8">https://www.youtube.com/watch?v=dxKpCO2dAy8</a></li><li>Shipping in pirate-infested waters (KubeCon NA 2017): <a href="https://www.youtube.com/watch?v=ohTq0no0ZVU">https://www.youtube.com/watch?v=ohTq0no0ZVU</a></li><li>Hacking and Hardening Kubernetes clusters by example (KubeCon NA 2017): <a href="https://www.youtube.com/watch?v=vTgQLzeBfRU">https://www.youtube.com/watch?v=vTgQLzeBfRU</a></li></ul></li></ul><p>If you find something, please report a security bug to the Kubernetes bug bounty at <a href=https://hackerone.com/kubernetes>https://hackerone.com/kubernetes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0e2b6512216f3a839a62bc7e32719ef3>Remembering Brad Childs</h1><div class="td-byline mb-4"><time datetime=2020-01-10 class=text-muted>Friday, January 10, 2020</time></div><p><strong>Authors:</strong> Paul Morie, Red Hat</p><p>Last year, the Kubernetes family lost one of its own. Brad Childs was a
SIG Storage chair and long time contributor to the project. Brad worked on a
number of features in storage and was known as much for his friendliness and
sense of humor as for his technical contributions and leadership.</p><p>We recently spent time remembering Brad at Kubecon NA:</p><ul><li><a href=https://youtu.be/4eI2PTAJ-sE>A Tribute to Bradley Childs</a></li><li><a href=https://github.com/cncf/memorials/blob/master/bradley-childs.md>CNCF Memorial</a></li></ul><p>Our hearts go out to Brad’s friends and family and others whose lives he touched
inside and outside the Kubernetes community.</p><p>Thank you for everything, Brad. We’ll miss you.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-376a8b22b6e032784b022da0e4b4e12a>Testing of CSI drivers</h1><div class="td-byline mb-4"><time datetime=2020-01-08 class=text-muted>Wednesday, January 08, 2020</time></div><p><strong>Author:</strong> Patrick Ohly (Intel)</p><p>When developing a <a href=https://kubernetes-csi.github.io/docs/>Container Storage Interface (CSI)
driver</a>, it is useful to leverage
as much prior work as possible. This includes source code (like the
<a href=https://github.com/kubernetes-csi/csi-driver-host-path/>sample CSI hostpath
driver</a>) but
also existing tests. Besides saving time, using tests written by
someone else has the advantage that it can point out aspects of the
specification that might have been overlooked otherwise.</p><p>An earlier blog post about <a href=https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/>end-to-end
testing</a>
already showed how to use the <a href=https://github.com/kubernetes/kubernetes/tree/master/test/e2e/storage/testsuites>Kubernetes storage
tests</a>
for testing of a third-party CSI driver. That
approach makes sense when the goal to also add custom E2E tests, but
depends on quite a bit of effort for setting up and maintaining a test
suite.</p><p>When the goal is to merely run the existing tests, then there are
simpler approaches. This blog post introduces those.</p><h2 id=sanity-testing>Sanity testing</h2><p><a href=https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity>csi-test
sanity</a>
ensures that a CSI driver conforms to the CSI specification by calling
the gRPC methods in various ways and checking that the outcome is as
required. Despite
its current hosting under the Kubernetes-CSI organization, it is
completely independent of Kubernetes. Tests connect to a running CSI
driver through its Unix domain socket, so although the tests are
written in Go, the driver itself can be implemented in any language.</p><p>The main
<a href=https://github.com/kubernetes-csi/csi-test/blob/master/pkg/sanity/README.md>README</a>
explains how to include those tests into an existing Go test
suite. The simpler alternative is to just invoke the <a href=https://github.com/kubernetes-csi/csi-test/tree/master/cmd/csi-sanity>csi-sanity</a>
command.</p><h3 id=installation>Installation</h3><p>Starting with csi-test v3.0.0, you can build the <code>csi-sanity</code> command
with <code>go get github.com/kubernetes-csi/csi-test/cmd/csi-sanity</code> and
you'll find the compiled binary in <code>$GOPATH/bin/csi-sanity</code>.</p><p><code>go get</code> always builds the latest revision from the master branch. To
build a certain release, <a href=https://github.com/kubernetes-csi/csi-test/releases>get the source
code</a> and run
<code>make -C cmd/csi-sanity</code>. This produces <code>cmd/csi-sanity/csi-sanity</code>.</p><h3 id=usage>Usage</h3><p>The <code>csi-sanity</code> binary is a full <a href=http://onsi.github.io/ginkgo/>Ginkgo test
suite</a> and thus has the usual <code>-gingko</code>
command line flags. In particular, <code>-ginkgo.focus</code> and
<code>-ginkgo.skip</code> can be used to select which tests are run resp. not
run.</p><p>During a test run, <code>csi-sanity</code> simulates the behavior of a container
orchestrator (CO) by creating staging and target directories as required by the CSI spec
and calling a CSI driver via gRPC. The driver must be started before
invoking <code>csi-sanity</code>. Although the tests currently only check the gRPC
return codes, that might change and so the driver really should make
the changes requested by a call, like mounting a filesystem. That may
mean that it has to run as root.</p><p>At least one <a href=https://github.com/grpc/grpc/blob/master/doc/naming.md>gRPC
endpoint</a> must
be specified via the <code>-csi.endpoint</code> parameter when invoking
<code>csi-sanity</code>, either as absolute path (<code>unix:/tmp/csi.sock</code>) for a Unix
domain socket or as host name plus port (<code>dns:///my-machine:9000</code>) for
TCP. <code>csi-sanity</code> then uses that endpoint for both node and controller
operations. A separate endpoint for controller operations can be
specified with <code>-csi.controllerendpoint</code>. Directories are created in
<code>/tmp</code> by default. This can be changed via <code>-csi.mountdir</code> and
<code>-csi.stagingdir</code>.</p><p>Some drivers cannot be deployed such that everything is guaranteed to
run on the same host. In such a case, custom scripts have to be used
to handle directories: they log into the host where the CSI node
controller runs and create or remove the directories there.</p><p>For example, during CI testing the <a href=https://github.com/kubernetes-csi/csi-driver-host-path>CSI hostpath example
driver</a> gets
deployed on a real Kubernetes cluster before invoking <code>csi-sanity</code> and then
<code>csi-sanity</code> connects to it through port forwarding provided by
<a href=https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/deploy/kubernetes-1.16/hostpath/csi-hostpath-testing.yaml><code>socat</code></a>.
<a href=https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/release-tools/prow.sh#L808-L859>Scripts</a>
are used to create and remove the directories.</p><p>Here's how one can replicate that, using the v1.2.0 release of the CSI hostpath driver:</p><pre><code>$ cd csi-driver-host-path
$ git describe --tags HEAD
v1.2.0
$ kubectl get nodes
NAME        STATUS   ROLES    AGE   VERSION
127.0.0.1   Ready    &lt;none&gt;   42m   v1.16.0

$ deploy/kubernetes-1.16/deploy-hostpath.sh 
applying RBAC rules
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-provisioner/v1.4.0/deploy/kubernetes/rbac.yaml
...
deploying hostpath components
   deploy/kubernetes-1.16/hostpath/csi-hostpath-attacher.yaml
        using           image: quay.io/k8scsi/csi-attacher:v2.0.0
service/csi-hostpath-attacher created
statefulset.apps/csi-hostpath-attacher created
   deploy/kubernetes-1.16/hostpath/csi-hostpath-driverinfo.yaml
csidriver.storage.k8s.io/hostpath.csi.k8s.io created
   deploy/kubernetes-1.16/hostpath/csi-hostpath-plugin.yaml
        using           image: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0
        using           image: quay.io/k8scsi/hostpathplugin:v1.2.0
        using           image: quay.io/k8scsi/livenessprobe:v1.1.0
...
service/hostpath-service created
statefulset.apps/csi-hostpath-socat created
07:38:46 waiting for hostpath deployment to complete, attempt #0
deploying snapshotclass
volumesnapshotclass.snapshot.storage.k8s.io/csi-hostpath-snapclass created

$ cat &gt;mkdir_in_pod.sh &lt;&lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- mktemp -d /tmp/csi-sanity.XXXXXX
EOF

$ cat &gt;rmdir_in_pod.sh &lt;&lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- rmdir &quot;\$@&quot;
EOF

$ chmod u+x *_in_pod.sh
$ csi-sanity -ginkgo.v \
             -csi.endpoint dns:///127.0.0.1:$(kubectl get &quot;services/hostpath-service&quot; -o &quot;jsonpath={..nodePort}&quot;) \
             -csi.createstagingpathcmd ./mkdir_in_pod.sh \
             -csi.createmountpathcmd ./mkdir_in_pod.sh \
             -csi.removestagingpathcmd ./rmdir_in_pod.sh \
             -csi.removemountpathcmd ./rmdir_in_pod.sh

Running Suite: CSI Driver Test Suite
====================================
Random Seed: 1570540138
Will run 72 of 72 specs
...
Controller Service [Controller Server] ControllerGetCapabilities 
  should return appropriate capabilities
  /nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:111
STEP: connecting to CSI driver
STEP: creating mount and staging directories
STEP: checking successful response
•
------------------------------
Controller Service [Controller Server] GetCapacity 
  should return capacity (no optional values added)
  /nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:149
STEP: reusing connection to CSI driver at dns:///127.0.0.1:30056
STEP: creating mount and staging directories
...
Ran 53 of 72 Specs in 148.206 seconds
SUCCESS! -- 53 Passed | 0 Failed | 0 Pending | 19 Skipped
PASS
</code></pre><p>Some comments:</p><ul><li>The source code of these tests is in the <a href=https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity><code>pkg/sanity</code>
package</a>.</li><li>How to determine the external IP address of the node depends on the
cluster. In this example, the cluster was brought up with
<code>hack/local-up-cluster.sh</code> and thus runs on the local host (<code>127.0.0.1</code>).
It uses a port allocated by Kubernetes, obtained above with <code>kubectl get "services/hostpath-service"</code>.
The Kubernetes-CSI CI uses
<a href=https://kind.sigs.k8s.io/docs/user/quick-start/>kind</a> and there <a href=https://github.com/kubernetes-csi/csi-driver-host-path/blob/3488dc7f994e33485629b86b69a6f34ebb7ef2d9/release-tools/prow.sh#L850>a
Docker
command</a>
can be used.</li><li>The create script must print the final directory. Using a
unique directory for each test case has the advantage that if
something goes wrong in one test case, others still start with a
clean slate.</li><li>The "staging directory", aka <code>NodePublishVolumeRequest.target_path</code>
in the CSI spec, must be created and deleted by the CSI driver while
the CO is responsible for the parent directory. <code>csi-sanity</code> handles
that by creating a directory and then giving the CSI driver that
directory path with <code>/target</code> appended at the end. Kubernetes <a href=https://github.com/kubernetes/kubernetes/issues/75535>got
this wrong</a>
and creates the actual <code>target_path</code> directory, so CSI drivers which
want to work with Kubernetes currently have to be lenient and must
not fail when that directory already exists.</li><li>The "mount directory" corresponds to
<code>NodeStageVolumeRequest.staging_target_path</code> and really gets created
by the CO, i.e. <code>csi-sanity</code>.</li></ul><h2 id=end-to-end-testing>End-to-end testing</h2><p>In contrast to <code>csi-sanity</code>, end-to-end testing interacts with the CSI
driver through the Kubernetes API, i.e. it simulates operations from a
normal user, like creating a PersistentVolumeClaim. Support for testing external CSI
drivers was
<a href=https://github.com/kubernetes/kubernetes/commit/6644db9914379a4a7b3d3487b41b2010f226e4dc#diff-5b2d9461c960bc9b146c4ab3d77bcaa5>added</a>
in Kubernetes 1.14.0.</p><h3 id=installation-1>Installation</h3><p>For each Kubernetes release, a test tar archive is published. It's not
listed in the release notes (for example, the ones for
<a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161>1.16</a>),
so one has to know that the full URL is
<code>https://dl.k8s.io/&lt;version>/kubernetes-test-linux-amd64.tar.gz</code> (like
for
<a href=https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz>v1.16.0</a>).</p><p>These include a <code>e2e.test</code> binary for Linux on x86-64. Archives for
other platforms are also available, see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-testing/20190118-breaking-apart-the-kubernetes-test-tarball.md#proposal>this
KEP</a>. The
<code>e2e.test</code> binary is completely self-contained, so one can "install"
it and the <a href=https://onsi.github.io/ginkgo/><code>ginkgo</code> test runner</a> with:</p><pre><code>curl --location https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz | \
  tar --strip-components=3 -zxf - kubernetes/test/bin/e2e.test kubernetes/test/bin/ginkgo
</code></pre><p>Each <code>e2e.test</code> binary contains tests that match the features
available in the corresponding release. In particular, the <code>[Feature: xyz]</code> tags change between releases: they separate tests of alpha
features from tests of non-alpha features. Also, the tests from an
older release might rely on APIs that were removed in more recent
Kubernetes releases. To avoid problems, it's best to simply use the
<code>e2e.test</code> binary that matches the Kubernetes release that is used for
testing.</p><h3 id=usage-1>Usage</h3><p>Not all features of a CSI driver can be discovered through the
Kubernetes API. Therefore a configuration file in YAML or JSON format
is needed which describes the driver that is to be tested. That file
is used to populate <a href=https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/external/external.go#L142-L211>the driverDefinition
struct</a>
and <a href=https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/testsuites/testdriver.go#L139-L185>the DriverInfo
struct</a>
that is embedded inside it. For detailed usage instructions of
individual fields refer to these structs.</p><p>A word of warning: tests are often only run when setting some fields and the
file parser does not warn about unknown fields, so always check that
the file really matches those structs.</p><p>Here is an example that tests the
<a href=https://github.com/kubernetes-csi/csi-driver-host-path><code>csi-driver-host-path</code></a>:</p><pre><code>$ cat &gt;test-driver.yaml &lt;&lt;EOF
StorageClass:
  FromName: true
SnapshotClass:
  FromName: true
DriverInfo:
  Name: hostpath.csi.k8s.io
  Capabilities:
    block: true
    controllerExpansion: true
    exec: true
    multipods: true
    persistence: true
    pvcDataSource: true
    snapshotDataSource: true
InlineVolumes:
- Attributes: {}
EOF
</code></pre><p>At a minimum, you need to define the storage class you want to use in
the test, the name of your driver, and what capabilities you want to
test.
As with <code>csi-sanity</code>, the driver has to be running in the cluster
before testing it.
The actual <code>e2e.test</code> invocation then enables tests for this driver
with <code>-storage.testdriver</code> and selects the storage tests for it with
<code>-ginkgo.focus</code>:</p><pre><code>$ ./e2e.test -ginkgo.v \
             -ginkgo.focus='External.Storage' \
             -storage.testdriver=test-driver.yaml
Oct  8 17:17:42.230: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1008 17:17:42.230210  648569 e2e.go:92] Starting e2e run &quot;90b9adb0-a3a2-435f-80e0-640742d56104&quot; on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1570547861 - Will randomize all specs
Will run 163 of 5060 specs

Oct  8 17:17:42.237: INFO: &gt;&gt;&gt; kubeConfig: /var/run/kubernetes/admin.kubeconfig
Oct  8 17:17:42.241: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
...
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
External Storage [Driver: hostpath.csi.k8s.io] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] 
  should access to two volumes with different volume mode and retain data across pod recreation on the same node
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/multivolume.go:191
[BeforeEach] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
...
</code></pre><p>You can use <code>ginkgo</code> to run some kinds of test in parallel.
Alpha feature tests or those that by design have to be run
sequentially then need to be run separately:</p><pre><code>$ ./ginkgo -p -v \
         -focus='External.Storage' \
         -skip='\[Feature:|\[Disruptive\]|\[Serial\]' \
         ./e2e.test \
         -- \
         -storage.testdriver=test-driver.yaml
$ ./ginkgo -v \
         -focus='External.Storage.*(\[Feature:|\[Disruptive\]|\[Serial\])' \
         ./e2e.test \
         -- \
         -storage.testdriver=test-driver.yaml
</code></pre><h2 id=getting-involved>Getting involved</h2><p>Both the Kubernetes storage tests and the sanity tests are meant to be
applicable to arbitrary CSI drivers. But perhaps tests are based on
additional assumptions and your driver does not pass the testing
although it complies with the CSI specification. If that happens then
please file issues (links below).</p><p>These are open source projects which depend on the help of those
using them, so once a problem has been acknowledged, a pull request
addressing it will be highly welcome.</p><p>The same applies to writing new tests. The following searches in the
issue trackers select issues that have been marked specifically as
something that needs someone's help:</p><ul><li><a href="https://github.com/kubernetes-csi/csi-test/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">csi-test</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Asig%2Fstorage+">Kubernetes</a></li></ul><p>Happy testing! May the issues it finds be few and easy to fix.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9e22e84688a3e1e7622241facd484d8a>Kubernetes 1.17: Stability</h1><div class="td-byline mb-4"><time datetime=2019-12-09 class=text-muted>Monday, December 09, 2019</time></div><p><strong>Authors:</strong> <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md>Kubernetes 1.17 Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.</p><h2 id=major-themes>Major Themes</h2><h3 id=cloud-provider-labels-reach-general-availability>Cloud Provider Labels reach General Availability</h3><p>Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.</p><h3 id=volume-snapshot-moves-to-beta>Volume Snapshot Moves to Beta</h3><p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.</p><h3 id=csi-migration-beta>CSI Migration Beta</h3><p>The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.</p><h2 id=cloud-provider-labels-reach-general-availability-1>Cloud Provider Labels reach General Availability</h2><p>When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.</p><p>Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.</p><p>The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:</p><ul><li><a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type>node.kubernetes.io/instance-type</a></li><li><a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion>topology.kubernetes.io/region</a></li><li><a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone>topology.kubernetes.io/zone</a></li></ul><h2 id=volume-snapshot-moves-to-beta-1>Volume Snapshot Moves to Beta</h2><p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.</p><h3 id=what-is-a-volume-snapshot>What is a Volume Snapshot?</h3><p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).</p><h3 id=why-add-volume-snapshots-to-kubernetes>Why add Volume Snapshots to Kubernetes?</h3><p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.</p><p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.</p><p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.</p><p>By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).</p><p>Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.</p><p>Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.</p><p>You can read more in the blog entry about <a href=https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/>releasing CSI volume snapshots to beta</a>.</p><h2 id=csi-migration-beta-1>CSI Migration Beta</h2><h3 id=why-are-we-migrating-in-tree-plugins-to-csi>Why are we migrating in-tree plugins to CSI?</h3><p>Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.</p><p>As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.
What is CSI migration?</p><p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.</p><p>When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.</p><p>The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.</p><p>You can read more in the blog entry about <a href=https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/>CSI migration going to beta</a>.</p><h2 id=other-updates>Other Updates</h2><h3 id=graduated-to-stable>Graduated to Stable 💯</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/382>Taint Node by Condition</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/495>Configurable Pod Process Namespace Sharing</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/548>Schedule DaemonSet Pods by kube-scheduler</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/554>Dynamic Maximum Volume Count</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/557>Kubernetes CSI Topology Support</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/559>Provide Environment Variables Expansion in SubPath Mount</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/575>Defaulting of Custom Resources</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/589>Move Frequent Kubelet Heartbeats To Lease Api</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/714>Break Apart The Kubernetes Test Tarball</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/956>Add Watch Bookmarks Support</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/960>Behavior-Driven Conformance Testing</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/980>Finalizer Protection For Service Loadbalancers</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1152>Avoid Serializing The Same Object Independently For Every Watcher</a></li></ul><h3 id=major-changes>Major Changes</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/563>Add IPv4/IPv6 Dual Stack Support</a></li></ul><h3 id=other-notable-features>Other Notable Features</h3><ul><li><a href=https://github.com/kubernetes/enhancements/issues/536>Topology Aware Routing of Services (Alpha)</a></li><li><a href=https://github.com/kubernetes/enhancements/issues/1043>RunAsUserName for Windows</a></li></ul><h3 id=availability>Availability</h3><p>Kubernetes 1.17 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a>. You can also easily install 1.17 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h3 id=release-team>Release Team</h3><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md>release team</a> led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39,000 individual contributors</a> to date and an active community of more than 66,000 people.</p><h3 id=webinar>Webinar</h3><p>Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A>here</a>.</p><h3 id=get-involved>Get Involved</h3><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p><ul><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-fffcd16b4766965ab6bdcab60b22457d>Kubernetes 1.17 Feature: Kubernetes Volume Snapshot Moves to Beta</h1><div class="td-byline mb-4"><time datetime=2019-12-09 class=text-muted>Monday, December 09, 2019</time></div><p><strong>Authors:</strong> Xing Yang, VMware & Xiangqian Yu, Google</p><p>The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced <a href=https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/>as alpha</a> in Kubernetes v1.12, with a <a href=https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/>second alpha</a> with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.</p><h2 id=what-is-a-volume-snapshot>What is a Volume Snapshot?</h2><p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).</p><h2 id=why-add-volume-snapshots-to-kubernetes>Why add Volume Snapshots to Kubernetes?</h2><p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.</p><p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.</p><p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.</p><p>By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).</p><p>Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.</p><p>Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.</p><h2 id=what-s-new-in-beta>What’s new in Beta?</h2><p>With the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.</p><p>The move of the Kubernetes Volume Snapshot feature to beta also means:</p><ul><li>A revamp of volume snapshot APIs.</li><li>The CSI external-snapshotter sidecar is split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.</li><li>Deletion secret is added as an annotation to the volume snapshot content.</li><li>A new finalizer is added to the volume snapshot API object to prevent it from being deleted when it is bound to a volume snapshot content API object.</li></ul><h2 id=kubernetes-volume-snapshots-requirements>Kubernetes Volume Snapshots Requirements</h2><p>As mentioned above, with the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.</p><p>In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster:</p><ul><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/53469c21962339229dd150cbba50c34359acec73/config/crd>Kubernetes Volume Snapshot CRDs</a></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller>Volume snapshot controller</a></li><li>CSI Driver supporting Kubernetes volume snapshot beta</li></ul><p>See the deployment section below for details.</p><h2 id=which-drivers-support-kubernetes-volume-snapshots>Which drivers support Kubernetes Volume Snapshots?</h2><p>Kubernetes supports three types of volume plugins: in-tree, Flex, and CSI. See <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Kubernetes Volume Plugin FAQ</a> for details.</p><p>Snapshots are only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.</p><p>Read the “<a href=https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/>Container Storage Interface (CSI) for Kubernetes GA</a>” blog post to learn more about CSI and how to deploy CSI drivers.</p><p>As of the publishing of this blog, the following CSI drivers have been updated to support volume snapshots beta:</p><ul><li><a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>GCE Persistent Disk CSI Driver</a></li><li><a href=https://github.com/libopenstorage/openstorage/tree/master/csi>Portworx CSI Driver</a></li><li><a href=https://github.com/NetApp/trident>NetApp Trident CSI Driver</a></li></ul><p>Beta level Volume Snapshot support for other <a href=https://kubernetes-csi.github.io/docs/drivers.html>CSI drivers</a> is pending, and should be available soon.</p><h2 id=kubernetes-volume-snapshot-beta-api>Kubernetes Volume Snapshot Beta API</h2><p>A number of changes were made to the Kubernetes volume snapshot API between alpha to beta. These changes are not backward compatible. The purpose of these changes was to make API definitions clear and easier to use.</p><p>The following changes are made:</p><ul><li><code>DeletionPolicy</code> is now a required field rather than optional in both <code>VolumeSnapshotClass</code> and <code>VolumeSnapshotContent</code>. This way the user has to explicitly specify it, leaving no room for confusion.</li><li><code>VolumeSnapshotSpec</code> has a new required <code>Source</code> field. <code>Source</code> may be either a <code>PersistentVolumeClaimName</code> (if dynamically provisioning a snapshot) or <code>VolumeSnapshotContentName</code> (if pre-provisioning a snapshot).</li><li><code>VolumeSnapshotContentSpec</code> also has a new required <code>Source</code> field. This <code>Source</code> may be either a <code>VolumeHandle</code> (if dynamically provisioning a snapshot) or a <code>SnapshotHandle</code> (if pre-provisioning volume snapshots).</li><li><code>VolumeSnapshotStatus</code> now contains a <code>BoundVolumeSnapshotContentName</code> to indicate the <code>VolumeSnapshot</code> object is bound to a <code>VolumeSnapshotContent</code>.</li><li><code>VolumeSnapshotContent</code>now contains a <code>Status</code> to indicate the current state of the content. It has a field <code>SnapshotHandle</code> to indicate that the <code>VolumeSnapshotContent</code> represents a snapshot on the storage system.</li></ul><p>The beta Kubernetes VolumeSnapshot API object:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshot <span style=color:#a2f;font-weight:700>struct</span> {
        metav1.TypeMeta
        metav1.ObjectMeta

        Spec VolumeSnapshotSpec
        Status <span style=color:#666>*</span>VolumeSnapshotStatus
}
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotSpec <span style=color:#a2f;font-weight:700>struct</span> {
	Source VolumeSnapshotSource
	VolumeSnapshotClassName <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
}
<span style=color:#080;font-style:italic>// Exactly one of its members MUST be specified
</span><span style=color:#080;font-style:italic></span><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotSource <span style=color:#a2f;font-weight:700>struct</span> {
	<span style=color:#080;font-style:italic>// +optional
</span><span style=color:#080;font-style:italic></span>	PersistentVolumeClaimName <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
	<span style=color:#080;font-style:italic>// +optional
</span><span style=color:#080;font-style:italic></span>	VolumeSnapshotContentName <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
}
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotStatus <span style=color:#a2f;font-weight:700>struct</span> {
	BoundVolumeSnapshotContentName <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
	CreationTime <span style=color:#666>*</span>metav1.Time
	ReadyToUse <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>bool</span>
	RestoreSize <span style=color:#666>*</span>resource.Quantity
	Error <span style=color:#666>*</span>VolumeSnapshotError
}
</code></pre></div><p>The beta Kubernetes VolumeSnapshotContent API object:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotContent <span style=color:#a2f;font-weight:700>struct</span> {
        metav1.TypeMeta
        metav1.ObjectMeta

        Spec VolumeSnapshotContentSpec
        Status <span style=color:#666>*</span>VolumeSnapshotContentStatus
}
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotContentSpec <span style=color:#a2f;font-weight:700>struct</span> {
         VolumeSnapshotRef core_v1.ObjectReference
         Source VolumeSnapshotContentSource
         DeletionPolicy DeletionPolicy
         Driver <span style=color:#0b0;font-weight:700>string</span>
         VolumeSnapshotClassName <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
}
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotContentSource <span style=color:#a2f;font-weight:700>struct</span> {
	<span style=color:#080;font-style:italic>// +optional
</span><span style=color:#080;font-style:italic></span>	VolumeHandle <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
	<span style=color:#080;font-style:italic>// +optional
</span><span style=color:#080;font-style:italic></span>	SnapshotHandle <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
}
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotContentStatus <span style=color:#a2f;font-weight:700>struct</span> {
  CreationTime <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>int64</span>
  ReadyToUse <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>bool</span>
  RestoreSize <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>int64</span>
  Error <span style=color:#666>*</span>VolumeSnapshotError
  SnapshotHandle <span style=color:#666>*</span><span style=color:#0b0;font-weight:700>string</span>
}
</code></pre></div><p>The beta Kubernetes VolumeSnapshotClass API object:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> VolumeSnapshotClass <span style=color:#a2f;font-weight:700>struct</span> {
        metav1.TypeMeta
        metav1.ObjectMeta

        Driver <span style=color:#0b0;font-weight:700>string</span>
        Parameters <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>string</span>
        DeletionPolicy DeletionPolicy
}
</code></pre></div><h3 id=how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster>How do I deploy support for Volume Snapshots on my Kubernetes Cluster?</h3><p>Please note that the Volume Snapshot feature now depends on a new, common <a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller>volume snapshot controller</a> in addition to the volume snapshot CRDs. Both the volume snapshot controller and the CRDs are independent of any CSI driver. Regardless of the number of CSI drivers deployed on the cluster, there must be only one instance of the volume snapshot controller running and one set of volume snapshot CRDs installed per cluster.</p><p>Therefore, it is strongly recommended that Kubernetes distributors bundle and deploy the controller and CRDs as part of their Kubernetes cluster management process (independent of any CSI Driver).</p><p>If your cluster does not come pre-installed with the correct components, you may manually install these components by executing the following steps.</p><h4 id=install-snapshot-beta-crds>Install Snapshot Beta CRDs</h4><ul><li><code>kubectl create -f config/crd</code></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/53469c21962339229dd150cbba50c34359acec73/config/crd>https://github.com/kubernetes-csi/external-snapshotter/tree/53469c21962339229dd150cbba50c34359acec73/config/crd</a></li><li>Do this once per cluster</li></ul><h4 id=install-common-snapshot-controller>Install Common Snapshot Controller</h4><ul><li><code>kubectl create -f deploy/kubernetes/snapshot-controller</code></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller>https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller</a></li><li>Do this once per cluster</li></ul><h4 id=install-csi-driver>Install CSI Driver</h4><p>Follow instructions provided by your CSI Driver vendor.</p><h3 id=how-do-i-use-kubernetes-volume-snapshots>How do I use Kubernetes Volume Snapshots?</h3><p>Assuming all the required components (including CSI driver) are already deployed and running on your cluster, you can create volume snapshots using the VolumeSnapshot API object, and restore them by specifying a VolumeSnapshot data source on a PVC.</p><h4 id=creating-a-new-volume-snapshot-with-kubernetes>Creating a New Volume Snapshot with Kubernetes</h4><p>You can enable creation/deletion of volume snapshots in a Kubernetes cluster, by creating a VolumeSnapshotClass API object pointing to a CSI Driver that support volume snapshots.</p><p>The following VolumeSnapshotClass, for example, tells the Kubernetes cluster that a CSI driver, <code>testdriver.csi.k8s.io</code>, can handle volume snapshots, and that when these snapshots are created, their deletion policy should be to delete.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapclass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>testdriver.csi.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csi.storage.k8s.io/snapshotter-secret-name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csi.storage.k8s.io/snapshotter-secret-namespace</span>:<span style=color:#bbb> </span>mysecretnamespace<span style=color:#bbb>
</span></code></pre></div><p>The common snapshot controller reserves the parameter keys <code>csi.storage.k8s.io/snapshotter-secret-name</code> and <code>csi.storage.k8s.io/snapshotter-secret-namespace</code>. If specified, it fetches the referenced Kubernetes secret and sets it as an annotation on the volume snapshot content object. The CSI external-snapshotter sidecar retrieves it from the content annotation and passes it to the CSI driver during snapshot creation.</p><p>Creation of a volume snapshot is triggered by the creation of a VolumeSnapshot API object.</p><p>The VolumeSnapshot object must specify the following source type:
<code>persistentVolumeClaimName</code> - The name of the PVC to snapshot. Please note that the source PVC, PV, and VolumeSnapshotClass for a VolumeSnapshot object must point to the same CSI driver.</p><p>The following VolumeSnapshot, for example, triggers the creation of a snapshot for a PVC called <code>test-pvc</code> using the VolumeSnapshotClass above.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>test-snapclass<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentVolumeClaimName</span>:<span style=color:#bbb> </span>test-pvc<span style=color:#bbb>
</span></code></pre></div><p>When volume snapshot creation is invoked, the common snapshot controller first creates a VolumeSnapshotContent object with the <code>volumeSnapshotRef</code>, source <code>volumeHandle</code>, <code>volumeSnapshotClassName</code> if specified, <code>driver</code>, and <code>deletionPolicy</code>.</p><p>The CSI external-snapshotter sidecar then passes the VolumeSnapshotClass parameters, the source volume ID, and any referenced secret(s) to the CSI driver (in this case <code>testdriver.csi.k8s.io</code>) via a CSI <code>CreateSnapshot</code> call. In response, the CSI driver creates a new snapshot for the specified volume, and returns the ID for that snapshot. The CSI external-snapshotter sidecar then updates the <code>snapshotHandle</code>, <code>creationTime</code>, <code>restoreSize</code>, and <code>readyToUse</code> in the status field of the VolumeSnapshotContent object that represents the new snapshot. For a storage system that needs to upload the snapshot after it has been cut, the CSI external-snapshotter sidecar will keep calling the CSI <code>CreateSnapshot</code> to check the status until upload is complete and set <code>readyToUse</code> to true.</p><p>The common snapshot controller binds the VolumeSnapshotContent object to the VolumeSnapshot (sets <code>BoundVolumeSnapshotContentName</code>), and updates the <code>creationTime</code>, <code>restoreSize</code>, and <code>readyToUse</code> in the status field of the VolumeSnapshot object based on the status field of the VolumeSnapshotContent object.</p><p>If no <code>volumeSnapshotClassName</code> is specified, one is automatically selected as follows:</p><p>The <code>StorageClass</code> from PVC or PV of the source volume is fetched. The default VolumeSnapshotClass is fetched, if available. A default VolumeSnapshotClass is a snapshot class created by the admin with the <code>snapshot.storage.kubernetes.io/is-default-class</code> annotation. If the <code>Driver</code> field of the default VolumeSnapshotClass is the same as the <code>Provisioner</code> field in the StorageClass, the default VolumeSnapshotClass is used. If there is no default VolumeSnapshotClass or more than one default VolumeSnapshotClass for a snapshot, an error will be returned.</p><p>Please note that the Kubernetes Snapshot API does not provide any consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using some other higher level APIs/controllers.</p><p>You can verify that the VolumeSnapshot object is created and bound with VolumeSnapshotContent by running <code>kubectl describe volumesnapshot</code>:</p><p><code>Bound Volume Snapshot Content Name</code> - field in the <code>Status</code> indicates the volume is bound to the specified VolumeSnapshotContent.
<code>Ready To Use</code> - field in the <code>Status</code> indicates this volume snapshot is ready for use.
<code>Creation Time</code> - field in the <code>Status</code> indicates when the snapshot was actually created (cut).
<code>Restore Size</code> - field in the <code>Status</code> indicates the minimum volume size required when restoring a volume from this snapshot.</p><pre><code>Name:         test-snapshot
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  snapshot.storage.k8s.io/v1beta1
Kind:         VolumeSnapshot
Metadata:
  Creation Timestamp:  2019-11-16T00:36:04Z
  Finalizers:
    snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection
    snapshot.storage.kubernetes.io/volumesnapshot-bound-protection
  Generation:        1
  Resource Version:  1294
  Self Link:         /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/new-snapshot-demo
  UID:               32ceaa2a-3802-4edd-a808-58c4f1bd7869
Spec:
  Source:
    Persistent Volume Claim Name:  test-pvc
  Volume Snapshot Class Name:      test-snapclass
Status:
  Bound Volume Snapshot Content Name:  snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
  Creation Time:                       2019-11-16T00:36:04Z
  Ready To Use:                        true
  Restore Size:                        1Gi
</code></pre><p>As a reminder to any developers building controllers using volume snapshot APIs: before using a VolumeSnapshot API object, validate the bi-directional binding between the VolumeSnpashot and the VolumeSnapshotContent it is bound to, to ensure the binding is complete and correct (not doing so may result in security issues).</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe volumesnapshotcontent
</code></pre></div><pre><code>Name:         snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  snapshot.storage.k8s.io/v1beta1
Kind:         VolumeSnapshotContent
Metadata:
  Creation Timestamp:  2019-11-16T00:36:04Z
  Finalizers:
    snapshot.storage.kubernetes.io/volumesnapshotcontent-bound-protection
  Generation:        1
  Resource Version:  1292
  Self Link:         /apis/snapshot.storage.k8s.io/v1beta1/volumesnapshotcontents/snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
  UID:               7dfdf22e-0b0c-4b71-9ddf-2f1612ca2aed
Spec:
  Deletion Policy:  Delete
  Driver:           testdriver.csi.k8s.io
  Source:
    Volume Handle:             d1b34a5f-0808-11ea-808a-0242ac110003
  Volume Snapshot Class Name:  test-snapclass
  Volume Snapshot Ref:
    API Version:       snapshot.storage.k8s.io/v1beta1
    Kind:              VolumeSnapshot
    Name:              test-snapshot
    Namespace:         default
    Resource Version:  1286
    UID:               32ceaa2a-3802-4edd-a808-58c4f1bd7869
Status:
  Creation Time:    1573864564608810101
  Ready To Use:     true
  Restore Size:     1073741824
  Snapshot Handle:  127c5798-0809-11ea-808a-0242ac110003
Events:             &lt;none&gt;
</code></pre><h4 id=importing-an-existing-volume-snapshot-with-kubernetes>Importing an existing volume snapshot with Kubernetes</h4><p>You can always expose a pre-existing volume snapshot in Kubernetes by manually creating a VolumeSnapshotContent object to represent the existing volume snapshot. Because VolumeSnapshotContent is a non-namespace API object, only a cluster admin may have the permission to create it. By specifying the <code>volumeSnapshotRef</code> the cluster admin specifies exactly which user can use the snapshot.</p><p>The following VolumeSnapshotContent, for example exposes a volume snapshot with the name <code>7bdd0de3-aaeb-11e8-9aae-0242ac110002</code> belonging to a CSI driver called <code>testdriver.csi.k8s.io</code>.</p><p>A VolumeSnapshotContent object should be created by a cluster admin with the following fields to represent an existing snapshot:</p><ul><li><code>driver</code> - CSI driver used to handle this volume. This field is required.</li><li><code>source</code> - Snapshot identifying information</li><li><code>snapshotHandle</code> - name/identifier of the snapshot. This field is required.</li><li><code>volumeSnapshotRef</code> - Pointer to the VolumeSnapshot object this content should bind to.</li><li><code>name</code> and <code>namespace</code> - Specifies the name and namespace of the VolumeSnapshot object which the content is bound to.</li><li><code>deletionPolicy</code> - Valid values are <code>Delete</code> and <code>Retain</code>. If the <code>deletionPolicy</code> is <code>Delete</code>, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the - <code>deletionPolicy</code> is <code>Retain</code>, then both the underlying snapshot and VolumeSnapshotContent remain.</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>manually-created-snapshot-content<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>testdriver.csi.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshotHandle</span>:<span style=color:#bbb> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></code></pre></div><p>Once a VolumeSnapshotContent object is created, a user can create a VolumeSnapshot object pointing to the VolumeSnapshotContent object. The name and namespace of the VolumeSnapshot object must match the name/namespace specified in the volumeSnapshotRef of the VolumeSnapshotContent. It specifies the following fields:
<code>volumeSnapshotContentName</code> - name of the volume snapshot content specified above. This field is required.
<code>volumeSnapshotClassName</code> - name of the volume snapshot class. This field is optional.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>manually-created-snapshot<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeSnapshotContentName</span>:<span style=color:#bbb> </span>test-content<span style=color:#bbb>
</span></code></pre></div><p>Once both objects are created, the common snapshot controller verifies the binding between VolumeSnapshot and VolumeSnapshotContent objects is correct and marks the VolumeSnapshot as ready (if the CSI driver supports the <code>ListSnapshots</code> call, the controller also validates that the referenced snapshot exists). The CSI external-snapshotter sidecar checks if the snapshot exists if ListSnapshots CSI method is implemented, otherwise it assumes the snapshot exists. The external-snapshotter sidecar sets <code>readyToUse</code> to true in the status field of VolumeSnapshotContent. The common snapshot controller marks the snapshot as ready accordingly.</p><h2 id=create-volume-from-snapshot>Create Volume From Snapshot</h2><p>Once you have a bound and ready VolumeSnapshot object, you can use that object to provision a new volume that is pre-populated with data from the snapshot.</p><p>To provision a new volume pre-populated with data from a snapshot, use the <code>dataSource</code> field in the <code>PersistentVolumeClaim</code>. It has three parameters:
<code>name</code> - name of the VolumeSnapshot object representing the snapshot to use as source
<code>kind</code> - must be VolumeSnapshot
<code>apiGroup</code> - must be snapshot.storage.k8s.io</p><p>The namespace of the source VolumeSnapshot object is assumed to be the same as the namespace of the <code>PersistentVolumeClaim</code> object.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-restore<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>demo-namespace<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>testdriver.csi.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>manually-created-snapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></code></pre></div><p>When the <code>PersistentVolumeClaim</code> object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified snapshot.
As a storage vendor, how do I add support for snapshots to my CSI driver?
To implement the snapshot feature, a CSI driver MUST add support for additional controller capabilities <code>CREATE_DELETE_SNAPSHOT</code> and <code>LIST_SNAPSHOTS</code>, and implement additional controller RPCs: <code>CreateSnapshot</code>, <code>DeleteSnapshot</code>, and <code>ListSnapshots</code>. For details, see the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>CSI spec</a> and the <a href=https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html>Kubernetes-CSI Driver Developer Guide</a>.</p><p>Although Kubernetes is as minimally prescriptive on the packaging and deployment of a CSI Volume Driver as possible, it provides a suggested mechanism for deploying an arbitrary containerized CSI driver on Kubernetes to simplify deployment of containerized CSI compatible volume drivers.</p><p>As part of this recommended deployment process, the Kubernetes team provides a number of sidecar (helper) containers, including the <a href=https://kubernetes-csi.github.io/docs/external-snapshotter.html>external-snapshotter sidecar</a> container.</p><p>The external-snapshotter watches the Kubernetes API server for VolumeSnapshotContent object and triggers <code>CreateSnapshot</code> and <code>DeleteSnapshot</code> operations against a CSI endpoint. The CSI <a href=https://kubernetes-csi.github.io/docs/external-provisioner.html>external-provisioner sidecar container</a> has also been updated to support restoring volume from snapshot using the dataSource PVC field.</p><p>In order to support snapshot feature, it is recommended that storage vendors deploy the external-snapshotter sidecar containers in addition to the external provisioner, along with their CSI driver.</p><h2 id=what-are-the-limitations-of-beta>What are the limitations of beta?</h2><p>The beta implementation of volume snapshots for Kubernetes has the following limitations:</p><ul><li>Does not support reverting an existing volume to an earlier state represented by a snapshot (beta only supports provisioning a new volume from a snapshot).</li><li>No snapshot consistency guarantees beyond any guarantees provided by storage system (e.g. crash consistency). These are the responsibility of higher level APIs/controllers</li></ul><h2 id=what-s-next>What’s next?</h2><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to GA in either 1.18 or 1.19. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, volume backups, and more.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>You can also have a look at the <a href=https://github.com/kubernetes-csi/external-snapshotter>external-snapshotter source code repository</a>.</p><p>Check out additional documentation on the snapshot feature <a href=http://k8s.io/docs/concepts/storage/volume-snapshots>here</a> and <a href=https://kubernetes-csi.github.io/docs/>here</a>.</p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach Beta:</p><ul><li>Xing Yang (xing-yang)</li><li>Xiangqian Yu (yuxiangqian)</li><li>Jing Xu (jingxu97)</li><li>Grant Griffiths (ggriffiths)</li><li>Can Zhu (zhucan)</li></ul><p>With special thanks to the following people for their insightful reviews and thorough consideration with the design:</p><ul><li>Michelle Au (msau42)</li><li>Saad Ali (saadali)</li><li>Patrick Ohly (pohly)</li><li>Tim Hockin (thockin)</li><li>Jordan Liggitt (liggitt).</li></ul><p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group (SIG)</a>. We’re rapidly growing and always welcome new contributors.</p><p>We also hold regular <a href="https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing">SIG-Storage Snapshot Working Group meetings</a>. New attendees are welcome to join for design and development discussions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ee4de1e99955b909ad9a400db28557fb>Kubernetes 1.17 Feature: Kubernetes In-Tree to CSI Volume Migration Moves to Beta</h1><div class="td-byline mb-4"><time datetime=2019-12-09 class=text-muted>Monday, December 09, 2019</time></div><p><strong>Authors:</strong> David Zhu, Software Engineer, Google</p><p>The Kubernetes in-tree storage plugin to <a href=https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/>Container Storage Interface (CSI)</a> migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.</p><p>Kubernetes features are generally introduced as alpha and moved to beta (and eventually to stable/GA) over subsequent Kubernetes releases. This process allows Kubernetes developers to get feedback, discover and fix issues, iterate on the designs, and deliver high quality, production grade features.</p><h2 id=why-are-we-migrating-in-tree-plugins-to-csi>Why are we migrating in-tree plugins to CSI?</h2><p>Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.</p><p>As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.</p><h2 id=what-is-csi-migration>What is CSI migration?</h2><p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding <a href=https://kubernetes-csi.github.io/docs/introduction.html>CSI driver</a>. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.</p><p>When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.</p><p>The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.</p><h2 id=how-to-try-out-csi-migration-for-existing-plugins>How to try out CSI migration for existing plugins?</h2><p>If you are Kubernetes distributor that deploys in one of the environments listed below, now would be a good time to start testing the CSI migration and figuring out how to deploy/manage the appropriate CSI driver.</p><p>To try out CSI migration in beta for an existing plugin you must be using Kubernetes v1.17 or higher. First, you must update/create a Kubernetes cluster with the feature flags <code>CSIMigration</code> (on by default in 1.17) and <code>CSIMigration{provider}</code> (off by default) enabled on all Kubernetes components (master and node). Where {provider} is the in-tree cloud provider storage type that is used in your cluster. Please note that during a cluster upgrade you must drain each node (remove running workloads) before updating or changing configuration of your Kubelet. You may also see an optional <code>CSIMigration{provider}Complete</code> flag that you <em>may</em> enable if all of your nodes have CSI migration enabled.</p><p>You must also install the requisite CSI driver on your cluster - instructions for this can generally be found from you provider of choice. CSI migration is available for GCE Persistent Disk and AWS Elastic Block Store in beta as well as for Azure File/Disk and Openstack Cinder in alpha. Kubernetes distributors should look at automating the deployment and management (upgrade, downgrade, etc.) of the CSI Drivers they will depend on.</p><p>To verify the feature flag is enabled and driver installed on a particular node you can get the CSINode object. You should see the in-tree plugin name of the migrated plugin as well as your [installed] driver in the drivers list.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get csinodes -o yaml
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>CSINode<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage.alpha.kubernetes.io/migrated-plugins</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-node<span style=color:#bbb>
</span><span style=color:#bbb>    </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>drivers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pd.csi.storage.gke.io<span style=color:#bbb>
</span><span style=color:#bbb>      </span>...<span style=color:#bbb>
</span></code></pre></div><p>After the above set up is complete you can confirm that your cluster has functioning CSI migration by deploying a stateful workload using the legacy APIs.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-disk<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>standard<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>   </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>       </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/www/html<span style=color:#bbb>
</span><span style=color:#bbb>         </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypvc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>   </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypvc<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>       </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>test-disk<span style=color:#bbb>
</span></code></pre></div><p>Verify that the pod is RUNNING after some time</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods web-server
</code></pre></div><pre><code>NAME         READY   STATUS    RESTARTS   AGE
web-server   1/1     Running   0          39s
</code></pre><p>To confirm that the CSI driver is actually serving your requests it may be prudent to check the container logs of the CSI Driver after exercising the storage management operations. Note that your container logs may look different depending on the provider used.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl logs <span style=color:#666>{</span>CSIdriverPodName<span style=color:#666>}</span> --container<span style=color:#666>={</span>CSIdriverContainerName<span style=color:#666>}</span>
</code></pre></div><pre><code>/csi.v1.Controller/ControllerPublishVolume called with request: ...
Attaching disk ... to ...
ControllerPublishVolume succeeded for disk ... to instance ...
</code></pre><h2 id=current-limitations>Current limitations</h2><p>Although CSI migration is now beta there is one major limitation that prevents us from turning it on by default. Turning on migration still requires a cluster administrator to install a CSI driver before storage functionality is seamlessly handed over. We are currently working with SIG-CloudProvider to provide a frictionless experience of bundling the required CSI Drivers with cloud distributions.</p><h2 id=what-is-the-timeline-status>What is the timeline/status?</h2><p>The timeline for CSI migration is actually set by the cloud provider extraction project. It is part of the effort to remove all cloud provider code from Kubernetes. By migrating cloud storage plugins to external CSI drivers we are able to extract out all the cloud provider dependencies.</p><p>Although the overall feature is beta and not on by default, there is still work to be done on a per-plugin basis. Currently only GCE PD and AWS EBS have gone beta with Migration and yet both are still off by default since they depend on a manual installation of their respective CSI Drivers. Azure File/Disk, OpenStack, and VMWare plugins are currently in less mature states and non-cloud plugins such as NFS, Portworx, RBD etc are still in the planning stages.</p><p>The current and targeted releases for each individual cloud driver is shown in the table below:</p><table><thead><tr><th>Driver</th><th>Alpha</th><th>Beta (in-tree deprecated)</th><th>GA</th><th>Target "in-tree plugin" removal</th></tr></thead><tbody><tr><td>AWS EBS</td><td>1.14</td><td>1.17</td><td>1.19 (Target)</td><td>1.21</td></tr><tr><td>GCE PD</td><td>1.14</td><td>1.17</td><td>1.19 (Target)</td><td>1.21</td></tr><tr><td>OpenStack Cinder</td><td>1.14</td><td>1.18 (Target)</td><td>1.19 (Target)</td><td>1.21</td></tr><tr><td>Azure Disk + File</td><td>1.15</td><td>1.18 (Target)</td><td>1.19 (Target)</td><td>1.21</td></tr><tr><td>VSphere</td><td>1.18 (Target)</td><td>1.19 (Target)</td><td>1.20 (Target)</td><td>1.22</td></tr></tbody></table><h2 id=what-s-next>What's next?</h2><p>Major upcoming work includes implementing and hardening CSI migration for the remaining in-tree plugins, installing CSI Drivers by default in distributions, turning on CSI migration by default, and finally removing all in-tree plugin code as a part of cloud provider extraction. We expect to complete this project including the full switch to “on-by-default” migration by Kubernetes v1.21.</p><h2 id=what-should-i-do-as-a-user>What should I do as a user?</h2><p>Note that all new features for the Kubernetes storage system (like volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the <a href=https://kubernetes-csi.github.io/docs/drivers.html>updated user guides for CSI drivers</a> and use the new CSI APIs.</p><p>However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers.</p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>The Kubernetes Slack channel csi-migration along with any of the standard <a href=https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact>SIG Storage communication channels</a> are great mediums to reach out to the SIG Storage and migration working group teams.</p><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help the project reach Beta:</p><ul><li>David Zhu</li><li>Deep Debroy</li><li>Cheng Pan</li><li>Jan Šafránek</li></ul><p>With special thanks to:</p><ul><li>Michelle Au</li><li>Saad Ali</li><li>Jonathan Basseri</li><li>Fabio Bertinatto</li><li>Ben Elder</li><li>Andrew Sy Kim</li><li>Hemant Kumar</li></ul><p>For fruitful dialogues, insightful reviews, and thorough consideration of CSI migration in other features.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb9814896ba0d24124ab45f0ea3c5c02>When you're in the release team, you're family: the Kubernetes 1.16 release interview</h1><div class="td-byline mb-4"><time datetime=2019-12-06 class=text-muted>Friday, December 06, 2019</time></div><p><b>Author</b>: Craig Box (Google)</p><p>It is a pleasure to co-host the weekly <a href=https://kubernetespodcast.com/>Kubernetes Podcast from Google</a> with Adam Glick. We get to talk to friends old and new from the community, as well as give people a download on the Cloud Native news every week.</p><p>It was also a pleasure to see Lachlan Evenson, the release team lead for Kubernetes 1.16, <a href=https://www.cncf.io/announcement/2019/11/19/cloud-native-computing-foundation-announces-2019-community-awards-winners/>win the CNCF "Top Ambassador" award</a> at KubeCon. We <a href=https://kubernetespodcast.com/episode/072-kubernetes-1.16/>talked with Lachie</a> when 1.16 was released, and as is <a href=https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/>becoming</a> a <a href=https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/>tradition</a>, we are delighted to share an abridged version of that interview with the readers of the Kubernetes Blog.</p><p>If you're paying attention to the release calendar, you'll see 1.17 is due out soon. <a href=https://kubernetespodcast.com/subscribe/>Subscribe to our show</a> in your favourite podcast player for another release interview!</p><hr><p><b>CRAIG BOX: Lachie, I've been looking forward to chatting to you for some time. We first met at KubeCon Berlin in 2017 when you were with Deis. Let's start with a question on everyone's ears-- which part of England are you from?</b></p><p>LACHLAN EVENSON: The prison part! See, we didn't have a choice about going to Australia, but I'd like to say we got the upper hand in the long run. We got that beautiful country, so yes, from Australia, the southern part of England-- the southern tip.</p><p><b>CRAIG BOX: We did set that question up a little bit. I'm actually in Australia this week, and I'll let you know it's quite a nice place. I can't imagine why you would have left.</b></p><p>LACHLAN EVENSON: Yeah, it seems fitting that you're interviewing an Australian from Australia, and that Australian is in San Francisco.</p><p><b>CRAIG BOX: Oh, well, thank you very much for joining us and making it work. This is the third in our occasional series of release lead interviews. We talked to Josh and Tim from Red Hat and VMware, respectively, in <a href=https://kubernetespodcast.com/episode/010-kubernetes-1.11/>episode 10</a>, and we talked to Aaron from Google in <a href=https://kubernetespodcast.com/episode/046-kubernetes-1.14/>episode 46</a>. And we asked all three how their journey in cloud-native started. What was your start in cloud-native?</b></p><p>LACHLAN EVENSON: I remember back in early 2014, I was working for a company called Lithium Technologies. We'd been using containers for quite some time, and my boss at the time had put a challenge out to me-- go and find a way to orchestrate these containers, because they seem to be providing quite a bit of value to our developer velocity.</p><p>He gave me a week, and he said, go and check out both Mesos and Kubernetes. And at the end of that week, I had Kubernetes up and running, and I had workloads scheduled. I was a little bit more challenged on the Mesos side, but Kubernetes was there, and I had it up and running. And from there, I actually went and was offered to speak at the Kubernetes 1.0 launch in OSCOM in Portland in 2014, I believe.</p><p><b>CRAIG BOX: So, a real early adopter?</b></p><p>LACHLAN EVENSON: Really, really early. I remember, I think, I started in 0.8, before CrashLoopBackOff was a thing. I remember writing that thing myself.</p><p>[LAUGHING]</p><p><b>CRAIG BOX: You were contributing to the code at that point as well?</b></p><p>LACHLAN EVENSON: I was just a user. I was part of the community at that point, but from a user perspective. I showed up to things like the community meeting. I remember meeting Sarah Novotny in the very early years of the community meeting, and I spent some time in SIG Apps, so really looking at how people were putting workloads onto Kubernetes-- so going through that whole process.</p><p>It turned out we built some tools like Helm, before Helm existed, to facilitate rollout and putting applications onto Kubernetes. And then, once Helm existed, that's when I met the folks from Deis, and I said, hey, I think you want to get rid of this code that we've built internally and then go and use the open-source code that Helm provided.</p><p>So we got into the Helm ecosystem there, and I subsequently went and worked for Deis, specifically on professional services-- helping people out in the community with their Kubernetes journey. And that was when we actually met, Craig, back in Berlin. It seems, you know, I say container years are like dog years; it's 7:1.</p><p><b>CRAIG BOX: Right.</b></p><p>LACHLAN EVENSON: Seven years ago, we were about 50 years-- much younger.</p><p><b>CRAIG BOX: That sounds like the same ratio as kangaroos to people in Australia.</b></p><p>LACHLAN EVENSON: It's much the same arithmetic, yes.</p><p><b>ADAM GLICK: What was the most interesting implementation that you ran into at that time?</b></p><p>LACHLAN EVENSON: There wasn't a lot of the workload APIs. Back in 1.0, there wasn't even Deployments. There wasn't Ingress. Back in the day, there were a lot of people in those points trying to build those workload APIs on top of Kubernetes, but they didn't actually have any way to extend Kubernetes itself. There were no third-party resources. There were no operators, no custom resources.</p><p>A lot of people are actually trying to figure out how to interact with the Kubernetes API and deliver things like deployments, because you just had-- in those days, you didn't have replica sets. You had a ReplicationController that we called the RC, back in the day. You didn't have a lot of these things that we take for granted today. There wasn't RBAC. There wasn't a lot of the things that we have today.</p><p>So it's great to have seen and been a part of the Kubernetes community from 0.8 to 1.16, and actually leading that release. So I've seen a lot, and it's been a wonderful part of my adventures in open-source.</p><p><b>ADAM GLICK: You were also part of the Deis team that transitioned and became a part of the Microsoft team. What was that transition like, from small startup to joining a large player in the cloud and technology community?</b></p><p>LACHLAN EVENSON: It was fantastic. When we came on board with Microsoft, they didn't have a managed Kubernetes offering, and we were brought on to try and seed that. There was also a bigger part that we were actually building open-source tools to help people in the community integrate. We had the autonomy with-- Brendan Burns was on the team. We had Gabe Monroy. And we really had that top-down autonomy that was believing and placing a bet on open-source and helping us build tools and give us that autonomy to go and solve problems in open-source, along with contributing to things like Kubernetes.</p><p>I'm part of the upstream team from a PM perspective, and we have a bunch of engineers, a bunch of PMs that are actually working on these things in the Cloud Native Compute Foundation to help folks integrate their workloads into things like Kubernetes and build and aid their cloud-native journeys.</p><p><b>CRAIG BOX: There are a number of new tools, and specifications, and so on that are still coming out from Microsoft under the Deis brand. That must be exciting to you as one of the people who joined from Deis initially.</b></p><p>LACHLAN EVENSON: Yeah, absolutely. We really took that Deis brand-- it's now Deis Labs-- but we really wanted this a home to signal to the community that we were building things in the hope to put them out into foundation. You may see things like CNAB, Cloud Native Application Bundles. I know <a href=https://kubernetespodcast.com/episode/061-cnab/>you've had both Ralph and Jeremy on the show before</a> talking about CNAB, SMI - Service Mesh Interface, other tooling in the ecosystem where we want to signal to the community that we want to go give that to a foundation. We really want a neutral place to begin that nascent work, but then things, for example, Virtual Kubelet started there as well, and it went out into the Cloud Native Compute Foundation.</p><p><b>ADAM GLICK: Is there any consternation about the fact that Phippy has become the character people look to rather than the actual "Captain Kube" owl, in the <a href=https://www.cncf.io/phippy/>family of donated characters</a>?</b></p><p>LACHLAN EVENSON: Yes, so it's interesting because I didn't actually work on that project back at Deis, but the Deis folks, Karen Chu and Matt Butcher actually created "The Children's Guide to Kubernetes," which I thought was fantastic.</p><p><b>ADAM GLICK: Totally.</b></p><p>LACHLAN EVENSON: Because I could sit down and read it to my parents, as well, and tell them-- it wasn't for children. It was more for the adults in my life, I like to say. And so when I give out a copy of that book, I'm like, take it home and read it to mum. She might actually understand what you do by the end of that book.</p><p>But it was really a creative way, because this was back in that nascent Kubernetes where people were trying to get their head around those concepts-- what is a pod? What is a secret? What is a namespace? Having that vehicle of a fun set of characters--</p><p><b>ADAM GLICK: Yep.</b></p><p>LACHLAN EVENSON: And Phippy is a PHP app. Remember them? So yeah, it's totally in line with the things that we're seeing people want to containerize and put onto Kubernetes at that. But Phippy is still cute. I was questioned last week about Captain Kube, as well, on the release logo, so we could talk about that a little bit more. But there's a swag of characters in there that are quite cute and illustrate the fun concept behind the Kubernetes community.</p><p><b>CRAIG BOX: <a href=https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/>1.16 has just been released</a>. You were the release team lead for that-- congratulations.</b></p><p>LACHLAN EVENSON: Thank you very much. It was a pleasure to serve the community.</p><p><b>CRAIG BOX: What are the headline announcements in Kubernetes 1.16?</b></p><p>LACHLAN EVENSON: Well, I think there are a few. Custom Resources hit GA. Now, that is a big milestone for extensibility and Kubernetes. I know we've spoken about them for some time-- custom resources were introduced in 1.7, and we've been trying to work through that ecosystem to bring the API up to a GA standard. So it hit GA, and I think a lot of the features that went in as part of the GA release will help people in the community that are writing operators.</p><p>There's a lot of lifecycle management, a lot of tooling that you can put into the APIs themselves. Doing strict dependency checks-- you can do typing, you can do validation, you can do pruning superfluous fields, and allowing for that ecosystem of operators and extensibility in the community to exist on top of Kubernetes.</p><p>It's been a long road to get to GA for Custom Resources, but it's great now that they're here and people can really bank on that being an API they can use to extend Kubernetes. So I'd say that's a large headline feature. The metrics overhaul, as well-- I know this was on the release blog.</p><p>The metrics team have actually tried to standardize the metrics in Kubernetes and put them through the same paces as all other enhancements that go into Kubernetes. So they're really trying to put through, what are the criteria? How do we make them standard? How do we test them? How to make sure that they're extensible? So it was great to see that team actually step up and create stable metrics that everybody can build and stack on.</p><p>Finally, there were some other additions to CSI, as well. Volume resizing was added. This is a maturity story around the Container Storage Interface, which was introduced several releases ago in GA. But really, you've seen volume providers actually build on that interface and that interface get a little bit more broader to adopt things like "I want to resize dynamically at runtime on my storage volume". That's a great story as well, for those providers out there.</p><p>I think they're the big headline features for 1.16, but there are a slew. There were 31 enhancements that went into Kubernetes 1.16. And I know there have been questions out there in the community saying, well, how do we decide what's stable? Eight of those were stable, eight of those were beta, and the rest of those features, the 15 remaining, were actually in alpha. There were quite a few things that went from alpha into beta and beta into stable, so I think that's a good progression for the release, as well.</p><p><b>ADAM GLICK: As you've looked at all these, which of them is your personal favorite?</b></p><p>LACHLAN EVENSON: I probably have two. One is a little bit biased, but I personally worked on, with the <a href=https://kubernetes.io/docs/concepts/services-networking/dual-stack/>dual-stack</a> team in the community. Dual-stack is the ability to give IPv4 and IPv6 addresses to both pods and services. And I think where this is interesting in the community is Kubernetes is becoming a runtime that is going to new spaces. Think IoT, think edge, think cloud edge.</p><p>When you're pushing Kubernetes into these new operational environments, things like addressing may become a problem, where you might want to run thousands and thousands of pods which all need IP addresses. So, having that same crossover point where I can have v4 and v6 at the same time, get comfortable with v6, I think Kubernetes may be an accelerator to v6 adoption through things like IoT workloads on top of Kubernetes.</p><p>The other one is <a href=https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/>Endpoint Slices</a>. Endpoint slices is about scaling. As you may know, services have endpoints attached to them, and endpoints are all the pod IPs that actually match that label selector on a service. Now, when you have large clusters, you can imagine the number of pod IPs being attached to that service growing to tens of thousands. And when you update that, everything that actually watches those service endpoints needs to get an update, which is the delta change over time, which gets rather large as things are being attached, added, and removed, as is the dynamic nature of Kubernetes.</p><p>But what endpoint slices makes available is you can actually slice those endpoints up into groups of 100 and then only update the ones that you really need to worry about, which means as a scaling factor, we don't need to update everybody listening into tens of thousands of updates. We only need to update a subsection. So I'd say they're my two highlights, yeah.</p><p><b>CRAIG BOX: Are there any early stage or alpha features that you're excited to see where they go personally?</b></p><p>LACHLAN EVENSON: Personally, <a href=https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/>ephemeral containers</a>. The tooling that you have available at runtime in a pod is dependent on the constituents or the containers that are part of that pod. And what we've seen in containers being built by scratch and tools like <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> from the folks out of Google, where you can build scratch containers that don't actually have any tooling inside them but just the raw compiled binaries, if you want to go in and debug that at runtime, it's incredibly difficult to insert something in.</p><p>And this is where ephemeral containers come in. I can actually insert a container into a running pod-- and let's just call that a debug container-- that has all my slew of tools that I need to debug that running workload, and I can insert that into a pod at runtime. So I think ephemeral containers is a really interesting feature that's been included in 1.16 in alpha, which allows a greater debugging story for the Kubernetes community.</p><p><b>ADAM GLICK: What feature that slipped do you wish would have made it into the release?</b></p><p>LACHLAN EVENSON: The feature that slipped that I was a little disappointed about was <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md>sidecar containers</a>.</p><p><b>ADAM GLICK: Right.</b></p><p>LACHLAN EVENSON: In the world of service meshes, you may want to order the start of some containers, and it's very specific to things like service meshes in the case of the data plane. I need the Envoy sidecar to start before everything else so that it can wire up the networking.</p><p>The inverse is true as well. I need it to stop last. Sidecar containers gave you that ordered start. And what we see a lot of people doing in the ecosystem is just laying down one sidecar per node as a DaemonSet, and they want that to start before all the other pods on the machine. Or if it's inside the pod, or the context of one pod, they want to say that sidecar needs to stop before all the other containers in a pod. So giving you that ordered guarantee, I think, is really interesting and is really hot, especially given the service mesh ecosystem heating up.</p><p><b>CRAIG BOX: This release <a href=https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/>deprecates a few beta API groups</a>, for things like ReplicaSets and Deployments. That will break deployment for the group of people who have just taken example code off the web and don't really understand it. The GA version of these APIs were released in 1.9, so it's obviously a long time ago. There's been a lot of preparation going into this. But what considerations and concerns have we had about the fact that these are now being deprecated in this particular release?</b></p><p>LACHLAN EVENSON: Let me start by saying that this is the first release that we've had a big API deprecation, so the proof is going to be in the pudding. And we do have an API deprecation policy. So as you mentioned, Craig, the apps/v1 group has been around since 1.9. If you go and read the <a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/>API deprecation policy</a>, you can see that we have a three-release announcement. Around the 1.12, 1.13 time frame, we actually went and announced this deprecation, and over the last few releases, we've been reiterating that.</p><p>But really, what we want to do is get the whole community on those stable APIs because it really starts to become a problem when we're supporting all these many now-deprecated APIs, and people are building tooling around them and trying to build reliable tooling. So this is the first test for us to move people, and I'm sure it will break a lot of tools that depend on things. But I think in the long run, once we get onto those stable APIs, people can actually guarantee that their tools work, and it's going to become easier in the long run.</p><p>So we've put quite a bit of work in announcing this. There was a blog sent out about six months ago by Valerie Lancey in the Kubernetes community which said, hey, go use 'kubectl convert', where you can actually say, I want to convert this resource from this API version to that API version, and it actually makes that really easy. But I think there'll be some problems in the ecosystem, but we need to do this going forward, pruning out the old APIs and making sure that people are on the stable ones.</p><p><b>ADAM GLICK: Congratulations on the release of 1.16. Obviously, that's a big thing. It must have been a lot of work for you. Can you talk a little bit about what went into leading this release?</b></p><p>LACHLAN EVENSON: The job of the release lead is to oversee throughout the process of the release and make sure that the release gets out the door on a specific schedule. So really, what that is is wrangling a lot of different resources and a lot of different people in the community, and making sure that they show up and do the things that they are committed to as part of their duties as either SIG chairs or other roles in the community, and making sure that enhancements are in the right state, and code shows up at the right time, and that things are looking green.</p><p>A lot of it is just making sure you know who to contact and how to contact them, and ask them to actually show up. But when I was asked at the end of the 1.15 release cycle if I would lead, you have to consider how much time it's going to take and the scheduling, where hours a week are dedicated to making sure that this release actually hits the shelves on time and is of a certain quality. So there is lots of pieces to that.</p><p><b>ADAM GLICK: Had you been on the path through the shadow program for release management?</b></p><p>LACHLAN EVENSON: Yeah, I had. I actually joined the shadow program-- so the shadow program for the release team. The Kubernetes release team is tasked with staffing a specific release, and I came in the 1.14 release under the lead of Aaron Crickenberger. And I was an enhancement shadow at that point. I was really interested in how KEPs worked, so the Kubernetes Enhancement Proposal work. I wanted to make sure that I understood that part of the release team, and I came in and helped in that release.</p><p>And then, in 1.15, I was asked if I could be a lead shadow. And the lead shadow is to stand alongside the lead and help the lead fill their duties. So if they're out, if they need people to wrangle different parts of the community, I would go out and do that. I've served on three releases at this point-- 1.14, 1.15, and 1.16.</p><p><b>CRAIG BOX: Thank you for your service.</b></p><p>LACHLAN EVENSON: Absolutely, it's my pleasure.</p><p><b>ADAM GLICK: Release lead emeritus is the next role for you, I assume?</b></p><p>LACHLAN EVENSON: [LAUGHS] Yes. We also have a new role on the release lead team called Emeritus Advisors, which are actually to go back and help answer the questions of, why was this decision made? How can we do better? What was this like in the previous release? So we do have that continuity, and in 1.17, we have the old release lead from 1.15. Claire Lawrence is coming back to fill in as emeritus advisor. So that is something we do take.</p><p>And I think for the shadow program in general, the release team is a really good example of how you can actually build continuity across releases in an open-source fashion. We <a href="https://www.youtube.com/watch?v=ritHCLd2xeE">actually have a session at KubeCon San Diego</a> on how that shadowing program works. But it's really to get people excited about how we can do mentoring in open-source communities and make sure that the project goes on after all of us have rolled on and off the team.</p><p><b>ADAM GLICK: Speaking of the team, <a href=https://github.com/kubernetes/sig-release/tree/master/releases/release-1.16>there were 32 people involved</a>, including yourself, in this release. What is it like to coordinate that group? That sounds like a full time job.</b></p><p>LACHLAN EVENSON: It is a full time job. And let me say that this release team in 1.16 represented five different continents. We can count Antarctica as not having anybody, but we didn't have anybody from South America for that release, which was unfortunate. But we had people from Australia, China, India, Tanzania. We have a good spread-- Europe, North America. It's great to have that spread and that continuity, which allowed for us to get things done throughout the day.</p><p><b>CRAIG BOX: Until you want to schedule a meeting.</b></p><p>LACHLAN EVENSON: Scheduling a meeting was extremely difficult. Typically, on the release team, we run one Europe, Western Europe, and North American-friendly meeting, and then we ask the team if they would like to hold another meeting. Now, in the case of 1.16, they didn't want to hold another meeting. We actually put it out to survey. But in previous releases, we held an EU in the morning so that people in India, as well, or maybe even late-night in China, could be involved.</p><p><b>ADAM GLICK: Any interesting facts about the team, besides the incredible geographic diversity that you had, to work around that?</b></p><p>LACHLAN EVENSON: I really appreciate about the release team that we're from all different backgrounds, from all different parts of the world and all different companies. There are people who are doing this on their own time, There are people who are doing this on company time, but we all come together with that shared common goal of shipping that release.</p><p>This release was we had the five continents. It was really exciting in 1.17 that we have in the lead roles, it was represented mainly by women. So 1.17, watch out-- most of the leads for 1.17 are women, which is a great result, and that's through that shadow program that we can foster different types of talent. I'm excited to see future releases benefiting from different diverse groups of people from the Kubernetes community.</p><p><b>CRAIG BOX: What are you going to put in the proverbial envelope for the 1.17 team?</b></p><p>LACHLAN EVENSON: We've had this theme of a lot of roles in the release team being cut and dry, right? We have these release handbooks, so for each of the members of the team, they're cut into different roles. There's seven different roles on the team. There's the lead. There's the CI signal role. There's bug triage. There's comms. There's docs. And there's release notes. And there's also the release branch managers who actually cut the code and make sure that they have shipped and it ends up in all the repositories.</p><p>What we did in the previous 1.15, we actually had a role call the test-infra role. And thanks to the wonderful work of the folks of the test-infra team out of Google-- <a href=https://kubernetespodcast.com/episode/077-eng-prod-and-testing/>Katharine Berry</a>, and <a href=https://kubernetespodcast.com/episode/069-kind/>Ben Elder</a>, and other folks-- they actually automated this role completely that we could get rid of it in the 1.16 release and still have our same-- and be able to get a release out the door.</p><p>I think a lot of these things are ripe for automation, and therefore, we can have a lot less of a footprint going forward. Let's automate the bits of the process that we can and actually refine the process to make sure that the people that are involved are not doing the repetitive tasks over and over again. In the era of enhancements, we could streamline that process. CI signal and bug triage, there are places we could actually go in and automate that as well. I think one place that's been done really well in 1.16 was in the release notes.</p><p>I don't know if you've seen <a href=https://relnotes.k8s.io>relnotes.k8s.io</a>, but you can go and check out the release notes and now, basically, annotated PRs show up as release notes that are searchable and sortable, all through an automated means, whereas that was previously some YAML jockeying to make sure that that would actually happen and be digestible to the users.</p><p><b>CRAIG BOX: Come on, Lachie, all Kubernetes is just YAML jockeying.</b></p><p>[LAUGHING]</p><p>LACHLAN EVENSON: Yeah, but it's great to have an outcome where we can actually make that searchable and get people out of the mundaneness of things like, let's make sure we're copying and pasting YAML from left to right.</p><p><b>ADAM GLICK: After the release, you had a <a href="https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#heading=h.ipohe1hgr315">retrospective meeting</a>. What was the takeaway from that meeting?</b></p><p>LACHLAN EVENSON: At the end of each release, we do have a retrospective. It's during the community meeting. That retrospective, it was good. I was just really excited to see that there were so many positives. It's a typical retrospective where we go, what did we say we were going to do last release? Did we do that? What was great? What can we do better? And some actions out of that.</p><p>It was great to see people giving other people on the team so many compliments. It was really, really deep and rich, saying, thank you for doing this, thank you for doing that. People showed up and pulled their weight in the release team, and other people were acknowledging that. That was great.</p><p>I think one thing we want to do is-- we have a code freeze as part of the release process, which is where we make sure that code basically stops going into master in Kubernetes. Only things destined for the release can actually be put in there. But we don't actually stop the test infrastructure from changing, so the test infrastructure has a lifecycle of its own.</p><p>One of the things that was proposed was that we actually code freeze the test infrastructure as well, to make sure that we're not actually looking at changes in the test-infra causing jobs to fail while we're trying to stabilize the code. I think that's something we have some high level agreement about, but getting down into the low-level nitty-gritty would be great in 1.17 and beyond.</p><p><b>ADAM GLICK: We talked about sidecar containers slipping out of this release. Most of the features are on a release train, and are put in when they're ready. What does it mean for the process of managing a release when those things happen?</b></p><p>LACHLAN EVENSON: Basically, we have an enhancements freeze, and that says that enhancements-- so the KEPs that are backing these enhancements-- so the sidecar containers would have had an enhancement proposal. And the SIG that owns that code would then need to sign off and say that this is in a state called "implementable." When we've agreed on the high-level details, you can go and proceed and implement that.</p><p>Now, that had actually happened in the case of sidecar containers. The challenge was you still need to write the code and get the code actually implemented, and there's a month gap between enhancement freeze and code freeze. If the code doesn't show up, or the code shows up and needs to be reviewed a little bit more, you may miss that deadline.</p><p>I think that's what happened in the case of this specific feature. It went all the way through to code freeze, the code wasn't complete at that time, and we basically had to make a call-- do we want to grant it an exception? In this case, they didn't ask for an exception. They said, let's just move it to 1.17.</p><p>There's still a lot of people and SIGs show up at the start of a new release and put forward the whole release of all the things they want to ship, and obviously, throughout the release, a lot of those things get plucked off. I think we started with something like 60 enhancements, and then what we got out the door was 31. They either fall off as part of the enhancement freeze or as part of the code freeze, and that is absolutely typical of any release.</p><p><b>ADAM GLICK: Do you think that a three-month wait is acceptable for something that might have had a one- or two-week slip, or would you like to see enhancements be able to be released in point releases between the three-month releases?</b></p><p>LACHLAN EVENSON: Yeah, there's back and forth about this in the community, about how can we actually roll things at different cadences, I think, is the high-level question. Tim Hockin actually put out, how about we do stability cycles as well? Because there are a lot of new features going in, and there are a lot of stability features going in. But if you look at it, half of the features were beta or stable, and the other half were alpha, which means we're still introducing a lot more complexity and largely untested code into alpha state-- which, as much as we wouldn't like to admit, it does affect the stability of the system.</p><p>There's talk of LTS. There's talk of stability releases as well. I think they're all things that are interesting now that Kubernetes has that momentum, and you are seeing a lot of things go to GA. People are like, "I don't need to be drinking from the firehose as fast. I have CRDs in GA. I have all these other things in GA. Do I actually need to consume this at the rate?" So I think-- stay tuned. If you're interested in those discussions, the upstream community is having those. Show up there and voice your opinion.</p><p><b>CRAIG BOX: Is this the first release with its own <a href=https://raw.githubusercontent.com/kubernetes/sig-release/master/releases/release-1.16/116_unlimited_breadsticks_for_all.png>release mascot</a>?</b></p><p>LACHLAN EVENSON: I think that release mascot goes back to-- I would like to say 1.11? If you <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/README.md>go back to 1.11</a>, you can actually see the different mascots. I remember 1.11 being "The Hobbit." So it's the Hobbiton front door of Bilbo Baggins with the Kubernetes Helm on the front of it, and that was called 11ty-one--</p><p><b>CRAIG BOX: Uh-huh.</b></p><p>LACHLAN EVENSON: A long-expected release. So they go through from each release, and you can actually go check them out on the SIG release repository upstream.</p><p><b>CRAIG BOX: I do think this is the first time that's managed to make it into a blog post, though.</b></p><p>LACHLAN EVENSON: I do think it is the case. I wanted to have a little bit of fun with the release team, so typically you will see the release teams have a t-shirt. I have, <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.14/README.md>from 1.14, the Caternetes</a>, which Aaron designed, which has a bunch of cats kind of trying to look at a Kubernetes logo.</p><p><b>CRAIG BOX: We had a fun conversation with Aaron about his love of cats.</b></p><p>LACHLAN EVENSON: [LAUGHS] And it becomes a token of, hey, remember this hard work that you put together? It becomes a badge of honor for everybody that participated in the release. I wanted to highlight it as a release mascot. I don't think a lot of people knew that we did have those across the last few releases. But it's just a bit of fun, and I wanted to put my own spin on things just so that the team could come together. A lot of it was around the laughs that we had as a team throughout this release-- and my love of Olive Garden.</p><p><b>CRAIG BOX: Your love of Olive Garden feels like it may have become a meme to a community which might need a little explanation for our audience. For those who are not familiar with American fine dining, can we start with-- what exactly is Olive Garden?</b></p><p>LACHLAN EVENSON: Olive Garden is the finest Italian dining experience you will have in the continental United States. I see everybody's faces saying, is he sure about that? I'm sure.</p><p><b>CRAIG BOX: That might require a slight justification on behalf of some of our Italian-American listeners.</b></p><p><b>ADAM GLICK: Is it the unlimited breadsticks and salad that really does it for you, or is the plastic boat that it comes in?</b></p><p>LACHLAN EVENSON: I think it's a combination of all three things. You know, the tour of Italy, you can't go past. The free breadsticks are fantastic. But Olive Garden just represents the large chain restaurant and that kind of childhood I had growing up and thinking about these large-scale chain restaurants. You don't get to choose your meme. And the legacy-- I would have liked to have had a different mascot.</p><p>But I just had a run with the meme of Olive Garden. And this came about, I would like to say, about three or four months ago. Paris Pittman from Google, who is another member of the Kubernetes community, kind of put out there, what's your favorite sit-down large-scale restaurant? And of course, I pitched in very early and said, it's got to be the Olive Garden.</p><p>And then everybody kind of jumped onto that. And my inbox is full of free Olive Garden gift certificates now, and it's taken on a life of its own. And at this point, I'm just embracing it-- so much so that we might even have the 1.16 release party at an Olive Garden in San Diego, if it can accommodate 10,000 people.</p><p><b>ADAM GLICK: <a href="https://www.youtube.com/watch?v=9ZJF5-EyjXs">When you're there, are you family?</a></b></p><p>LACHLAN EVENSON: Yes. Absolutely, absolutely. And I would have loved to put that. I think the release name was "unlimited breadsticks for all." I would have liked to have done, "When you're here, you're family," but that is, sadly, trademarked.</p><p><b>ADAM GLICK: Aww. What's next for you in the community?</b></p><p>LACHLAN EVENSON: I've really been looking at Cluster API a lot-- so building Kubernetes clusters on top of a declarative approach. I've been taking a look at what we can do in the Cluster API ecosystem. I'm also a chair of SIG PM, so helping foster the KEP process as well-- making sure that that continues to happen and continues to be fruitful for the community.</p><hr><p><i><a href=https://twitter.com/lachlanevenson>Lachlan Evenson</a> is a Principal Program Manager at Microsoft and an Australian living in the US, and most recently served as the Kubernetes 1.16 release team lead.</p><p>You can find the <a href=http://www.kubernetespodcast.com/>Kubernetes Podcast from Google</a> at <a href=https://twitter.com/KubernetesPod>@kubernetespod</a> on Twitter, and you can <a href=https://kubernetespodcast.com/subscribe/>subscribe</a> so you never miss an episode.</i></p></div><div class=td-content style=page-break-before:always><h1 id=pg-545b4843398d50cb878ea56279c98d76>Gardener Project Update</h1><div class="td-byline mb-4"><time datetime=2019-12-02 class=text-muted>Monday, December 02, 2019</time></div><p><strong>Authors:</strong> <a href=mailto:rafael.franzke@sap.com>Rafael Franzke</a> (SAP), <a href=mailto:vasu.chandrasekhara@sap.com>Vasu
Chandrasekhara</a> (SAP)</p><p>Last year, we introduced <a href=https://gardener.cloud>Gardener</a> in the <a href="https://www.youtube.com/watch?v=DpFTcTnBxbM&feature=youtu.be&t=1642">Kubernetes
Community
Meeting</a>
and in a post on the <a href=https://kubernetes.io/blog/2018/05/17/gardener/>Kubernetes
Blog</a>. At SAP, we have been
running Gardener for more than two years, and are successfully managing
thousands of <a href=https://k8s-testgrid.appspot.com/conformance-gardener>conformant</a>
clusters in various versions on all major hyperscalers as well as in numerous
infrastructures and private clouds that typically join an enterprise via
acquisitions.</p><p>We are often asked why a handful of dynamically scalable clusters would not
suffice. We also started our journey into Kubernetes with a similar mindset. But
we realized that applying the architecture and principles of Kubernetes to
productive scenarios, our internal and external customers very quickly required
the rational separation of concerns and ownership, which in most circumstances
led to the use of multiple clusters. Therefore, a scalable and managed
Kubernetes as a service solution is often also the basis for adoption.
Particularly, when a larger organization runs multiple products on different
providers and in different regions, the number of clusters will quickly rise to
the hundreds or even thousands.</p><p>Today, we want to give an update on what we have implemented in the past year
regarding extensibility and customizability, and what we plan to work on for our
next milestone.</p><h2 id=short-recap-what-is-gardener>Short Recap: What Is Gardener?</h2><p>Gardener's main principle is to leverage Kubernetes primitives for all of its
operations, commonly described as inception or kubeception. The feedback from
the community was that initially our <a href=https://github.com/gardener/documentation/wiki/Architecture>architecture
diagram</a> looks
"overwhelming", but after some little digging into the material, everything we
do is the "Kubernetes way". One can re-use all learnings with respect to APIs,
control loops, etc.<br>The essential idea is that so-called <strong>seed</strong> clusters are used to host the
control planes of end-user clusters (botanically named <strong>shoots</strong>).<br>Gardener provides vanilla Kubernetes clusters as a service independent of the
underlying infrastructure provider in a homogenous way, utilizing the upstream
provided <code>k8s.gcr.io/*</code> images as open distribution. The project is built
entirely on top of Kubernetes extension concepts, and as such adds a custom API
server, a controller-manager, and a scheduler to create and manage the lifecycle
of Kubernetes clusters. It extends the Kubernetes API with custom resources,
most prominently the Gardener cluster specification (<code>Shoot</code> resource), that can
be used to "order" a Kubernetes cluster in a declarative way (for day-1, but
also reconcile all management activities for day-2).</p><p>By leveraging Kubernetes as base infrastructure, we were able to devise a
combined <a href=https://github.com/gardener/hvpa-controller>Horizontal and Vertical Pod Autoscaler
(HVPA)</a> that, when configured with
custom heuristics, scales all control plane components up/down or out/in
automatically. This enables a fast scale-out, even beyond the capacity of
typically some fixed number of master nodes. This architectural feature is one
of the main differences compared to many other Kubernetes cluster provisioning
tools. But in our production, Gardener does not only effectively reduce the
total costs of ownership by bin-packing control planes. It also simplifies
implementation of "day-2 operations" (like cluster updates or robustness
qualities). Again, essentially by relying on all the mature Kubernetes features
and capabilities.</p><p>The newly introduced extension concepts for Gardener now enable providers to
only maintain their specific extension without the necessity to develop inside
the core source tree.</p><h2 id=extensibility>Extensibility</h2><p>As result of its growth over the past years, the Kubernetes code base contained
a numerous amount of provider-specific code that is now being externalized from
its core source tree. The same has happened with Project Gardener: over time,
lots of specifics for cloud providers, operating systems, network plugins, etc.
have been accumulated. Generally, this leads to a significant increase of
efforts when it comes to maintainability, testability, or to new releases. Our
community member <a href=https://www.packet.com>Packet</a> contributed <a href=https://www.packet.com/kubernetes/>Gardener
support</a> for their infrastructure in-tree,
and suffered from the mentioned downsides.</p><p>Consequently, similar to how the Kubernetes community decided to move their
cloud-controller-managers out-of-tree, or volumes plugins to CSI, etc., the
Gardener community
<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>proposed</a>
and implemented likewise extension concepts. The Gardener core source-tree is
now devoid of any provider specifics, allowing vendors to solely focus on their
infrastructure specifics, and enabling core contributors becoming more agile
again.</p><p>Typically, setting up a cluster requires a flow of interdependent steps,
beginning with the generation of certificates and preparation of the
infrastructure, continuing with the provisioning of the control plane and the
worker nodes, and ending with the deployment of system components. We would like
to emphasize here that all these steps are necessary (cf. <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kubernetes the Hard
Way</a>) and all
Kubernetes cluster creation tools implement the same steps (automated to some
degree) in one way or another.</p><p>The general idea of Gardener's extensibility concept was to make <a href=https://github.com/gardener/gardener/blob/0.31.1/pkg/controllermanager/controller/shoot/shoot_control_reconcile.go#L69-L298>this
flow</a>
more generic and to carve out custom resources for each step which can serve as
ideal extension points.</p><figure><img src=/images/blog/2019-11-10-gardener-project-update/flow.png alt="Cluster reconciliation flow with extension points"></figure><p><em>Figure 1 Cluster reconciliation flow with extension points.</em></p><p>With Gardener's flow framework we implicitly have a reproducible state machine
for all infrastructures and all possible states of a cluster.</p><p>The Gardener extensibility approach defines custom resources that serve as ideal
extension points for the following categories:</p><ul><li>DNS providers (e.g., Route53, CloudDNS, ...),</li><li>Blob storage providers (e.g., S3, GCS, ABS,...),</li><li>Infrastructure providers (e.g., AWS, GCP, Azure, ...),</li><li>Operating systems (e.g., CoreOS Container Linux, Ubuntu, FlatCar Linux, ...),</li><li>Network plugins (e.g., Calico, Flannel, Cilium, ...),</li><li>Non-essential extensions (e.g., Let's Encrypt certificate service).</li></ul><h3 id=extension-points>Extension Points</h3><p>Besides leveraging custom resource definitions, we also effectively use mutating
/ validating webhooks in the seed clusters. Extension controllers themselves run
in these clusters and react on CRDs and workload resources (like <code>Deployment</code>,
<code>StatefulSet</code>, etc.) they are responsible for. Similar to the <a href=https://cluster-api.sigs.k8s.io>Cluster
API</a>'s approach, these CRDs may also contain
provider specific information.</p><p>The steps 2. - 10. [cf. Figure 1] involve infrastructure specific meta data
referring to infrastructure specific implementations, e.g. for DNS records there
might be <code>aws-route53</code>, <code>google-clouddns</code>, or for isolated networks even
<code>openstack-designate</code>, and many more. We are going to examine the steps 4 and 6
in the next paragraphs as examples for the general concepts (based on the
implementation for AWS). If you're interested you can read up the fully
documented API contract in our <a href=https://github.com/gardener/gardener/tree/master/docs/extensions>extensibility
documents</a>.</p><h3 id=example-infrastructure-crd>Example: <code>Infrastructure</code> CRD</h3><p>Kubernetes clusters on AWS require a certain infrastructure preparation before
they can be used. This includes, for example, the creation of a VPC, subnets,
etc. The purpose of the <code>Infrastructure</code> CRD is to trigger this preparation:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span>kind: Infrastructure<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>name: infrastructure<span style=color:#bbb>
</span><span style=color:#bbb>  </span>namespace: shoot--foobar--aws<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>type: aws<span style=color:#bbb>
</span><span style=color:#bbb>  </span>region: eu-west-1<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretRef</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>name: cloudprovider<span style=color:#bbb>
</span><span style=color:#bbb>    </span>namespace: shoot--foobar—aws<span style=color:#bbb>
</span><span style=color:#bbb>  </span>sshPublicKey: c3NoLXJzYSBBQUFBQ...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>providerConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb>    </span>kind: InfrastructureConfig<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>networks</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>vpc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>cidr: 10.250.0.0/16<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>zones</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>name: eu-west-1a<span style=color:#bbb>
</span><span style=color:#bbb>        </span>internal: 10.250.112.0/22<span style=color:#bbb>
</span><span style=color:#bbb>        </span>public: 10.250.96.0/22<span style=color:#bbb>
</span><span style=color:#bbb>        </span>workers: 10.250.0.0/19<span style=color:#bbb>
</span></code></pre></div><p>Based on the <code>Shoot</code> resource, Gardener creates this <code>Infrastructure</code> resource
as part of its reconciliation flow. The AWS-specific <code>providerConfig</code> is part of
the end-user's configuration in the <code>Shoot</code> resource and not evaluated by
Gardener but just passed to the extension controller in the seed cluster.</p><p>In its current implementation, the AWS extension creates a new VPC and three
subnets in the <code>eu-west-1a</code> zones. Also, it creates a NAT and an internet
gateway, elastic IPs, routing tables, security groups, IAM roles, instances
profiles, and an EC2 key pair.</p><p>After it has completed its tasks it will report the status and some
provider-specific output:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span>kind: Infrastructure<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>name: infrastructure<span style=color:#bbb>
</span><span style=color:#bbb>  </span>namespace: shoot--foobar--aws<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb> </span>...<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>lastOperation</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Reconcile<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>Succeeded<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>providerStatus</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb>    </span>kind: InfrastructureStatus<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ec2</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>keyName: shoot--foobar--aws-ssh-publickey<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>iam</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>instanceProfiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>name: shoot--foobar--aws-nodes<span style=color:#bbb>
</span><span style=color:#bbb>        </span>purpose: nodes<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>roles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>arn: &#34;arn:aws:iam::&lt;accountID&gt;:role/shoot...&#34;<span style=color:#bbb>
</span><span style=color:#bbb>        </span>purpose: nodes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>vpc</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>id: vpc-0815<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>securityGroups</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>id: sg-0246<span style=color:#bbb>
</span><span style=color:#bbb>        </span>purpose: nodes<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>subnets</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>id: subnet-1234<span style=color:#bbb>
</span><span style=color:#bbb>        </span>purpose: nodes<span style=color:#bbb>
</span><span style=color:#bbb>        </span>zone: eu-west-1b<span style=color:#bbb>
</span><span style=color:#bbb>      </span>-<span style=color:#bbb> </span>id: subnet-5678<span style=color:#bbb>
</span><span style=color:#bbb>        </span>purpose: public<span style=color:#bbb>
</span><span style=color:#bbb>        </span>zone: eu-west-1b<span style=color:#bbb>
</span></code></pre></div><p>The information inside the <code>providerStatus</code> can be used in subsequent steps,
e.g. to configure the cloud-controller-manager or to instrument the
machine-controller-manager.</p><h3 id=example-deployment-of-the-cluster-control-plane>Example: Deployment of the Cluster Control Plane</h3><p>One of the major features of Gardener is the homogeneity of the clusters it
manages across different infrastructures. Consequently, it is still in charge of
deploying the provider-independent control plane components into the seed
cluster (like etcd, kube-apiserver). The deployment of provider-specific control
plane components like cloud-controller-manager or CSI controllers is triggered
by a dedicated <code>ControlPlane</code> CRD. In this paragraph, however, we want to focus
on the customization of the standard components.</p><p>Let's focus on both the kube-apiserver and the kube-controller-manager
<code>Deployment</code>s. Our AWS extension for Gardener is not yet using CSI but relying
on the in-tree EBS volume plugin. Hence, it needs to enable the
<code>PersistentVolumeLabel</code> admission plugin and to provide the cloud provider
config to the kube-apiserver. Similarly, the kube-controller-manager will be
instructed to use its in-tree volume plugin.</p><p>The kube-apiserver <code>Deployment</code> incorporates the <code>kube-apiserver</code> container and
is deployed by Gardener like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>- command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>/hyperkube<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>apiserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--enable-admission-plugins=Priority,...,NamespaceLifecycle<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--allow-privileged=true<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--anonymous-auth=false<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></code></pre></div><p>Using a <code>MutatingWebhookConfiguration</code> the AWS extension injects the mentioned
flags and modifies the spec as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>- command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>/hyperkube<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>apiserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--enable-admission-plugins=Priority,...,NamespaceLifecycle,PersistentVolumeLabel<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--allow-privileged=true<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--anonymous-auth=false<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--cloud-provider=aws<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>--endpoint-reconciler-type=none<span style=color:#bbb>
</span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>-<span style=color:#bbb> </span>mountPath: /etc/kubernetes/cloudprovider<span style=color:#bbb>
</span><span style=color:#bbb>    </span>name: cloud-provider-config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>- configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>defaultMode: 420<span style=color:#bbb>
</span><span style=color:#bbb>    </span>name: cloud-provider-config<span style=color:#bbb>
</span><span style=color:#bbb>  </span>name: cloud-provider-config<span style=color:#bbb>
</span></code></pre></div><p>The kube-controller-manager <code>Deployment</code> is handled in a similar way.</p><p>Webhooks in the seed cluster can be used to mutate anything related to the shoot
cluster control plane deployed by Gardener or any other extension. There is a
similar webhook concept for resources in shoot clusters in case extension
controllers need to customize system components deployed by Gardener.</p><h3 id=registration-of-extension-controllers>Registration of Extension Controllers</h3><p>The Gardener API uses two special resources to register and install extensions.
The registration itself is declared via the <code>ControllerRegistration</code> resource.
The easiest option is to define the Helm chart as well as some values to render
the chart, however, any other deployment mechanism is supported via custom code
as well.</p><p>Gardener determines whether an extension controller is required in a specific
seed cluster, and creates a <code>ControllerInstallation</code> that is used to trigger the
deployment.</p><p>To date, every registered extension controller is deployed to every seed cluster
which is not necessary in general. In the future, Gardener will become more
selective to only deploy those extensions required on the specific seed
clusters.</p><p>Our dynamic registration approach allows to add or remove extensions in the
running system - without the necessity to rebuild or restart any component.</p><figure><img src=/images/blog/2019-11-10-gardener-project-update/architecture.png alt="Gardener architecture with extension controllers"></figure><p><em>Figure 2 Gardener architecture with extension controllers.</em></p><h3 id=status-quo>Status Quo</h3><p>We have recently introduced the new <code>core.gardener.cloud</code> API group that
incorporates fully forwards and backwards compatible <code>Shoot</code> resources, and that
allows providers to use Gardener without modifying anything in its core source
tree.</p><p>We have already adapted all controllers to use this new API group and have
deprecated the old API. Eventually, after a few months we will remove it, so
end-users are advised to start migrating to the new API soon.</p><p>Apart from that, we have enabled all relevant extensions to contribute to the
shoot health status and implemented the respective contract. The basic idea is
that the CRDs may have <code>.status.conditions</code> that are picked up by Gardener and
merged with its standard health checks into the <code>Shoot</code> status field.</p><p>Also, we want to implement some easy-to-use library functions facilitating
defaulting and validation webhooks for the CRDs in order to validate the
<code>providerConfig</code> field controlled by end-users.</p><p>Finally, we will split the
<a href=https://github.com/gardener/gardener-extensions><code>gardener/gardener-extensions</code></a>
repository into separate repositories and keep it only for the generic library
functions that can be used to write extension controllers.</p><h2 id=next-steps>Next Steps</h2><p>Kubernetes has externalized many of the infrastructural management challenges.
The inception design solves most of them by delegating lifecycle operations to a
separate management plane (seed clusters). But what if the garden cluster or a
seed cluster goes down? How do we scale beyond tens of thousands of managed
clusters that need to be reconciled in parallel? We are further investing into
hardening the Gardener scalability and disaster recovery features. Let's briefly
highlight three of the features in more detail:</p><h3 id=gardenlet>Gardenlet</h3><p>Right from the beginning of the Gardener Project we started implementing the
<a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/>operator
pattern</a>: We
have a custom controller-manager that acts on our own custom resources. Now,
when you start thinking about the <a href=https://github.com/gardener/documentation/wiki/Architecture>Gardener
architecture</a>, you
will recognize some interesting similarity with respect to the Kubernetes
architecture: Shoot clusters can be compared with pods, and seed clusters can be
seen as worker nodes. Guided by this observation we introduced the
<strong>gardener-scheduler</strong>. Its main task is to find an appropriate seed cluster to
host the control-plane for newly ordered clusters, similar to how the
kube-scheduler finds an appropriate node for newly created pods. By providing
multiple seed clusters for a region (or provider) and distributing the workload,
we reduce the blast-radius of potential hick-ups as well.</p><figure><img src=/images/blog/2019-11-10-gardener-project-update/gardenlet.png alt="Similarities between Kubernetes and Gardener architecture"></figure><p><em>Figure 3 Similarities between Kubernetes and Gardener architecture.</em></p><p>Yet, there is still a significant difference between the Kubernetes and the
Gardener architectures: Kubernetes runs a primary "agent" on every node, the
kubelet, which is mainly responsible for managing pods and containers on its
particular node. Gardener uses its controller-manager which is responsible for
all shoot clusters on all seed clusters, and it is performing its reconciliation
loops centrally from the garden cluster.</p><p>While this works well at scale for thousands of clusters today, our goal is to
enable true scalability following the Kubernetes principles (beyond the capacity
of a single controller-manager): We are now working on distributing the logic
(or the Gardener operator) into the seed cluster and will introduce a
corresponding component, adequately named the <strong>gardenlet</strong>. It will be
Gardener's primary "agent" on every seed cluster and will be only responsible
for shoot clusters located in its particular seed cluster.</p><p>The gardener-controller-manager will still keep its control loops for other
resources of the Gardener API, however, it will no longer talk to seed/shoot
clusters.</p><p>Reversing the control flow will even allow placing seed/shoot clusters behind
firewalls without the necessity of direct accessibility (via VPN tunnels)
anymore.</p><figure><img src=/images/blog/2019-11-10-gardener-project-update/gardenlet-detailed.png alt="Detailed architecture with Gardenlet"></figure><p><em>Figure 4 Detailed architecture with Gardenlet.</em></p><h3 id=control-plane-migration-between-seed-clusters>Control Plane Migration between Seed Clusters</h3><p>When a seed cluster fails, the user's static workload will continue to operate.
However, administrating the cluster won't be possible anymore because the shoot
cluster's API server running in the failed seed is no longer reachable.</p><p>We have implemented the relocation of failed control planes hit by some seed
disaster to another seed and are now working on fully automating this unique
capability. In fact, this approach is not only feasible, we have performed the
fail-over procedure multiple times in our production.</p><p>The automated failover capability will enable us to implement even more
comprehensive disaster recovery and scalability qualities, e.g., the automated
provisioning and re-balancing of seed clusters or automated migrations for all
non-foreseeable cases. Again, think about the similarities with Kubernetes with
respect to pod eviction and node drains.</p><h3 id=gardener-ring>Gardener Ring</h3><p>The Gardener Ring is our novel approach for provisioning and managing Kubernetes
clusters without relying on an external provision tool for the initial cluster.
By using Kubernetes in a recursive manner, we can drastically reduce the
management complexity by avoiding imperative tool sets, while creating new
qualities with a self-stabilizing circular system.</p><p>The Ring approach is conceptually different from self-hosting and static pod
based deployments. The idea is to create a ring of three (or more) shoot
clusters that each host the control plane of its successor.</p><p>An outage of one cluster will not affect the stability and availability of the
Ring, and as the control plane is externalized the failed cluster can be
automatically recovered by Gardener's self-healing capabilities. As long as
there is a quorum of at least <code>n/2+1</code> available clusters the Ring will always
stabilize itself. Running these clusters on different cloud providers (or at
least in different regions / data centers) reduces the potential for quorum
losses.</p><figure><img src=/images/blog/2019-11-10-gardener-project-update/ring.png alt="Self-stabilizing ring of Kubernetes clusters"></figure><p><em>Figure 5 Self-stabilizing ring of Kubernetes clusters.</em></p><p>The way how the distributed instances of Gardener can share the same data is by
deploying separate kube-apiserver instances talking to the same etcd cluster.
These kube-apiservers are forming a node-less Kubernetes cluster that can be
used as "data container" for Gardener and its associated applications.</p><p>We are running test landscapes internally protected by the ring and it has saved
us from manual interventions. With the automated control plane migration in
place we can easily bootstrap the Ring and will solve the "initial cluster
problem" as well as improve the overall robustness.</p><h2 id=getting-started>Getting Started!</h2><p>If you are interested in writing an extension, you might want to check out the
following resources:</p><ul><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1: Extensibility proposal
document</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4: New <code>core.gardener.cloud/v1alpha1</code>
API</a></li><li><a href=https://github.com/gardener/gardener-extensions/tree/master/controllers/provider-aws>Example extension controller implementation for
AWS</a></li><li><a href=https://godoc.org/github.com/gardener/gardener-extensions/pkg>Gardener Extensions Golang
library</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extension contract
documentation</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul><p>Of course, any other contribution to our project is very welcome as well! We are
always looking for new community members.</p><p>If you want to try out Gardener, please check out our <a href=https://gardener.cloud/installer/>quick installation
guide</a>. This installer will setup a complete
Gardener environment ready to be used for testing and evaluation within just a
few minutes.</p><h2 id=contributions-welcome>Contributions Welcome!</h2><p>The Gardener project is developed as Open Source and hosted on GitHub:
<a href=https://github.com/gardener>https://github.com/gardener</a></p><p>If you see the potential of the Gardener project, please join us via GitHub.</p><p>We are having a weekly <a href=https://docs.google.com/document/d/1314v8ziVNQPjdBrWp-Y4BYrTDlv7dq2cWDFIa9SMaP4>public community
meeting</a>
scheduled every Friday 10-11 a.m. CET, and a public <a href=https://kubernetes.slack.com/messages/gardener>#gardener
Slack</a> channel in the Kubernetes
workspace. Also, we are planning a <a href="https://docs.google.com/document/d/1EQ_kt70gwybiL7FY8F7Dx--GtiNwdv0oRDwqQqAIYMk/edit#heading=h.a43vkkp847f1">Gardener Hackathon in Q1
2020</a>
and are looking forward meeting you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-601488fd56d4096c09b2a9959d0e7a43>Develop a Kubernetes controller in Java</h1><div class="td-byline mb-4"><time datetime=2019-11-26 class=text-muted>Tuesday, November 26, 2019</time></div><p><strong>Authors:</strong> Min Kim (Ant Financial), Tony Ado (Ant Financial)</p><p>The official <a href=https://github.com/kubernetes-client/java>Kubernetes Java SDK</a> project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.</p><h2 id=overall>Overall</h2><p>Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, <a href=https://github.com/kubernetes-sigs/controller-runtime>controller runtime</a>,
<a href=https://github.com/operator-framework/operator-sdk>operator SDK</a>. These
existing Golang frameworks are relying on the various utilities from the
<a href=https://github.com/kubernetes/client-go>Kubernetes Golang SDK</a> proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.</p><h2 id=backgrounds>Backgrounds</h2><p>Why use Java to implement Kubernetes tooling? You might pick Java for:</p><ul><li><p><strong>Integrating legacy enterprise Java systems</strong>: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.</p></li><li><p><strong>More open-source community resources</strong>: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.</p></li></ul><h2 id=how-to-use>How to use?</h2><p>Take maven project as example, adding the following dependencies into your dependencies:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=color:green;font-weight:700>&lt;dependency&gt;</span>
    <span style=color:green;font-weight:700>&lt;groupId&gt;</span>io.kubernetes<span style=color:green;font-weight:700>&lt;/groupId&gt;</span>
    <span style=color:green;font-weight:700>&lt;artifactId&gt;</span>client-java-extended<span style=color:green;font-weight:700>&lt;/artifactId&gt;</span>
    <span style=color:green;font-weight:700>&lt;version&gt;</span>6.0.1<span style=color:green;font-weight:700>&lt;/version&gt;</span>
<span style=color:green;font-weight:700>&lt;/dependency&gt;</span>
</code></pre></div><p>Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example <a href=https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java>here</a>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#666>...</span>
    Reconciler reconciler <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>new</span> Reconciler<span style=color:#666>()</span> <span style=color:#666>{</span>
      <span style=color:#a2f>@Override</span>
      <span style=color:#a2f;font-weight:700>public</span> Result <span style=color:#00a000>reconcile</span><span style=color:#666>(</span>Request request<span style=color:#666>)</span> <span style=color:#666>{</span>
        V1Node node <span style=color:#666>=</span> nodeLister<span style=color:#666>.</span><span style=color:#b44>get</span><span style=color:#666>(</span>request<span style=color:#666>.</span><span style=color:#b44>getName</span><span style=color:#666>());</span>
        System<span style=color:#666>.</span><span style=color:#b44>out</span><span style=color:#666>.</span><span style=color:#b44>println</span><span style=color:#666>(</span><span style=color:#b44>&#34;triggered reconciling &#34;</span> <span style=color:#666>+</span> node<span style=color:#666>.</span><span style=color:#b44>getMetadata</span><span style=color:#666>().</span><span style=color:#b44>getName</span><span style=color:#666>());</span>
        <span style=color:#a2f;font-weight:700>return</span> <span style=color:#a2f;font-weight:700>new</span> Result<span style=color:#666>(</span><span style=color:#a2f;font-weight:700>false</span><span style=color:#666>);</span>
      <span style=color:#666>}</span>
    <span style=color:#666>};</span>
    Controller controller <span style=color:#666>=</span>
        ControllerBuilder<span style=color:#666>.</span><span style=color:#b44>defaultBuilder</span><span style=color:#666>(</span>informerFactory<span style=color:#666>)</span>
            <span style=color:#666>.</span><span style=color:#b44>watch</span><span style=color:#666>(</span>
                <span style=color:#666>(</span>workQueue<span style=color:#666>)</span> <span style=color:#666>-&gt;</span> ControllerBuilder<span style=color:#666>.</span><span style=color:#b44>controllerWatchBuilder</span><span style=color:#666>(</span>V1Node<span style=color:#666>.</span><span style=color:#b44>class</span><span style=color:#666>,</span> workQueue<span style=color:#666>).</span><span style=color:#b44>build</span><span style=color:#666>())</span>
            <span style=color:#666>.</span><span style=color:#b44>withReconciler</span><span style=color:#666>(</span>nodeReconciler<span style=color:#666>)</span> <span style=color:#080;font-style:italic>// required, set the actual reconciler
</span><span style=color:#080;font-style:italic></span>            <span style=color:#666>.</span><span style=color:#b44>withName</span><span style=color:#666>(</span><span style=color:#b44>&#34;node-printing-controller&#34;</span><span style=color:#666>)</span> <span style=color:#080;font-style:italic>// optional, set name for controller for logging, thread-tracing
</span><span style=color:#080;font-style:italic></span>            <span style=color:#666>.</span><span style=color:#b44>withWorkerCount</span><span style=color:#666>(</span>4<span style=color:#666>)</span> <span style=color:#080;font-style:italic>// optional, set worker thread count
</span><span style=color:#080;font-style:italic></span>            <span style=color:#666>.</span><span style=color:#b44>withReadyFunc</span><span style=color:#666>(</span> nodeInformer<span style=color:#666>::</span>hasSynced<span style=color:#666>)</span> <span style=color:#080;font-style:italic>// optional, only starts controller when the cache has synced up
</span><span style=color:#080;font-style:italic></span>            <span style=color:#666>.</span><span style=color:#b44>build</span><span style=color:#666>();</span>
</code></pre></div><p>If you notice, the new Java controller framework learnt a lot from the design of
<a href=https://github.com/kubernetes-sigs/controller-runtime>controller-runtime</a> which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.</p><p>As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.</p><h2 id=future-steps>Future steps</h2><p>The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo <a href=https://github.com/kubernetes-client/java>kubernetes-client/java</a>.
Feel free to share also your feedback with us, through Issues or <a href=http://kubernetes.slack.com/messages/kubernetes-client/>Slack</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b16b2be554bf6272235c07443e287255>Running Kubernetes locally on Linux with Microk8s</h1><div class="td-byline mb-4"><time datetime=2019-11-26 class=text-muted>Tuesday, November 26, 2019</time></div><p><strong>Authors</strong>: <a href=https://twitter.com/idvoretskyi>Ihor Dvoretskyi</a>, Developer Advocate, Cloud Native Computing Foundation; <a href=https://twitter.com/carminerimi>Carmine Rimi</a></p><p>This article, the second in a <a href=/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/>series</a> about local deployment options on Linux, and covers <a href=https://microk8s.io/>MicroK8s</a>. Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.</p><p>While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses <a href=https://snapcraft.io/>snap</a> packages, an application packaging and isolation technology.</p><p>This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions <a href=https://snapcraft.io/docs/installing-snapd>that support snaps</a>. Most popular Linux distributions are supported.</p><p>Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!</p><p>Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide <a href=https://snapcraft.io/docs/channels>channels</a> that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.</p><p>But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.</p><h2 id=disclaimer>Disclaimer</h2><p>This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official <a href=https://microk8s.io/docs/>webpage</a>, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.</p><h2 id=prerequisites>Prerequisites</h2><p>A Linux distribution that <a href=https://snapcraft.io/docs/installing-snapd>supports snaps</a>, is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out <a href=https://multipass.run>Multipass</a> to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.</p><h2 id=microk8s-installation>MicroK8s installation</h2><p>MicroK8s installation is straightforward:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo snap install microk8s --classic
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png width=600></figure></center><p>The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.</p><p>You may verify the MicroK8s status with the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo microk8s.status
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png width=600></figure></center><h2 id=using-microk8s>Using microk8s</h2><p>Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a <code>kubectl</code> binary, which can be accessed by running the <code>microk8s.kubectl</code> command. As an example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>microk8s.kubectl get nodes
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png width=600></figure></center><p>While using the prefix <code>microk8s.kubectl</code> allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the <code>snap alias</code> command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo snap <span style=color:#a2f>alias</span> microk8s.kubectl kubectl
</code></pre></div><p>This will allow you to simply use <code>kubectl</code> after. You can revert this change using the <code>snap unalias</code> command.</p><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png width=600></figure></center><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png width=600></figure></center><h2 id=microk8s-addons>MicroK8s addons</h2><p>One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.</p><p>The full list of extensions can be checked by running the <code>microk8s.status</code> command:</p><pre><code>sudo microk8s.status
</code></pre><p>As of the time of writing this article, the following add-ons are supported:</p><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png width=600></figure></center><p>More add-ons are being created and contributed by the community all the time, it definitely helps to check often!</p><h2 id=release-channels>Release channels</h2><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo snap info microk8s
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png width=600></figure></center><h2 id=installing-the-sample-application>Installing the sample application</h2><p>In this tutorial we’ll use NGINX as a sample application (<a href=https://hub.docker.com/_/nginx>the official Docker Hub image</a>).</p><p>It will be installed as a Kubernetes deployment:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment nginx --image<span style=color:#666>=</span>nginx
</code></pre></div><p>To verify the installation, let’s run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get deployments
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png width=600></figure></center><p>Also, we can retrieve the full output of all available objects within our Kubernetes cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get all --all-namespaces
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png width=600></figure></center><h2 id=uninstalling-microk8s>Uninstalling MicroK8s</h2><p>Uninstalling your microk8s cluster is so easy as uninstalling the snap:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo snap remove microk8s
</code></pre></div><center><figure><img src=/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png width=600></figure></center><h2 id=screencast>Screencast</h2><p><a href=https://asciinema.org/a/263394><img src=https://asciinema.org/a/263394.svg alt=asciicast></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-199a2412eb47773a4ae2e740cb9d0ded>Grokkin' the Docs</h1><div class="td-byline mb-4"><time datetime=2019-11-05 class=text-muted>Tuesday, November 05, 2019</time></div><p><strong>Author:</strong> <a href=https://www.linkedin.com/in/aimee-ukasick/>Aimee Ukasick</a>, Independent Contributor</p><figure><img src=/images/blog/grokkin-the-docs/grok-definition.png alt="grok: to understand profoundly and intuitively"><figcaption><h4>Definition courtesy of Merriam Webster online dictionary</h4></figcaption></figure><h2 id=intro-observations-of-a-new-sig-docs-contributor>Intro - Observations of a new SIG Docs contributor</h2><p>I began contributing to the SIG Docs community in August 2019. Sometimes I feel
like I am a stranger in a strange land adapting to a new community:
investigating community organization, understanding contributor society,
learning new lessons, and incorporating new jargon. I'm an observer as well as a
contributor.</p><h2 id=observation-1>Observation 01: Read the <em>Contribute</em> pages!</h2><p>I contributed code and documentation to OpenStack, OPNFV, and Acumos, so I
thought contributing to the Kubernetes documentation would be the same. I was
wrong. I should have thoroughly <strong>read</strong> the <a href=https://kubernetes.io/docs/contribute/>Contribute to Kubernetes
docs</a> pages instead of skimming them.</p><p>I am very familiar with the git/gerrit workflow. With those tools, a contributor clones
the <code>master</code> repo and then creates a local branch. Kubernetes uses a different
approach, called <em>Fork and Pull</em>. Each contributor <code>forks</code> the master repo, and
then the contributor pushes work to their fork before creating a pull request. I
created a simple pull request (PR), following the instructions in the <strong>Start
contributing</strong> page's <a href=https://kubernetes.io/docs/contribute/start/#submit-a-pull-request>Submit a pull
request</a>
section. This section describes how to make a documentation change using the
GitHub UI. I learned that this method is fine for a change that requires a
single commit to fix. However, this method becomes complicated when you have to
make additional updates to your PR. GitHub creates a new commit for each change
made using the GitHub UI. The Kubernetes GitHub org requires squashing commits.
The <strong>Start contributing</strong> page didn't mention squashing commits, so I looked at
the GitHub and git documentation. I could not squash my commits using the GitHub
UI. I had to <code>git fetch</code> and <code>git checkout</code> my pull request locally, squash the
commits using the command line, and then push my changes. If the <strong>Start
contributing</strong> had mentioned squashing commits, I would have worked from a local
clone instead of using the GitHub UI.</p><h2 id=observation-2>Observation 02: Reach out and ping someone</h2><p>While working on my first PRs, I had questions about working from a local clone
and about keeping my fork updated from <code>upstream master</code>. I turned to searching
the internet instead of asking on the <a href=http://slack.k8s.io/>Kubernetes Slack</a>
#sig-docs channel. I used the wrong process to update my fork, so I had to <code>git rebase</code> my PRs, which did not go well at all. As a result, I closed those PRs
and submitted new ones. When I asked for help on the #sig-docs channel,
contributors posted useful links, what my local git config file should look
like, and the exact set of git commands to run. The process used by contributors
was different than the one defined in the <strong>Intermediate contributing</strong> page.
I would have saved myself so much time if I had asked what GitHub workflow to
use. The more community knowledge that is documented, the easier it is for new
contributors to be productive quickly.</p><h2 id=observation-3>Observation 03: Don't let conflicting information ruin your day</h2><p>The Kubernetes community has a contributor guide for
<a href=https://github.com/kubernetes/community/tree/master/contributors/guide>code</a>
and another one for <a href=https://kubernetes.io/docs/contribute/>documentation</a>. The
guides contain conflicting information on the same topic. For example, the SIG
Docs GitHub process recommends creating a local branch based on
<code>upstream/master</code>. The <a href=https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md>Kubernetes Community Contributor
Guide</a>
advocates updating your fork from upstream and then creating a local branch
based on your fork. Which process should a new contributor follow? Are the two
processes interchangeable? The best place to ask questions about conflicting
information is the #sig-docs or #sig-contribex channels. I asked for
clarification about the GitHub workflows in the #sig-contribex channel.
@cblecker provided an extremely detailed response, which I used to update the
<strong>Intermediate contributing</strong> page.</p><h2 id=observation-4>Observation 04: Information may be scattered</h2><p>It's common for large open source projects to have information scattered around
various repos or duplicated between repos. Sometimes groups work in silos, and
information is not shared. Other times, a person leaves to work on a
different project without passing on specialized knowledge.
Documentation gaps exist and may never be rectified because of higher priority
items. So new contributors may have difficulty finding basic information, such
as meeting details.</p><p>Attending SIG Docs meetings is a great way to become involved. However, people
have had a hard time locating the meeting URL. Most new contributors ask in the
#sig-docs channel, but I decided to locate the meeting information in the docs.
This required several clicks over multiple pages. How many new contributors miss
meetings because they can't locate the meeting details?</p><h2 id=observation-5>Observation 05: Patience is a virtue</h2><p>A contributor may wait days for feedback on a larger PR. The process from
submission to final approval may take weeks instead of days. There are two
reasons for this: 1) most reviewers work part-time on SIG Docs; and 2) reviewers
want to provide meaningful reviews. "Drive-by reviewing" doesn't happen in SIG
Docs! Reviewers check for the following:</p><ul><li><p>Do the commit message and PR description adequately describe the change?</p></li><li><p>Does the PR follow the guidelines in the style and content guides?</p><ul><li>Overall, is the grammar and punctuation correct?</li><li>Is the content clear, concise, and appropriate for non-native speakers?</li><li>Does the content stylistically fit in with the rest of the documentation?</li><li>Does the flow of the content make sense?</li><li>Can anything be changed to make the content better, such as using a Hugo shortcode?</li><li>Does the content render correctly?</li></ul></li><li><p>Is the content technically correct?</p></li></ul><p>Sometimes the review process made me feel defensive, annoyed, and frustrated. I'm
sure other contributors have felt the same way. Contributors need to be patient!
Writing excellent documentation is an iterative process. Reviewers scrutinize
PRs because they want to maintain a high level of quality in the documentation,
not because they want to annoy contributors!</p><h2 id=observation-6>Observation 06: Make every word count</h2><p>Non-native English speakers read and contribute to the Kubernetes documentation.
When you are writing content, use simple, direct language in clear, concise
sentences. Every sentence you write may be translated into another language, so
remove words that don't add substance. I admit that implementing these
guidelines is challenging at times.</p><p>Issues and pull requests aren't translated into other languages. However, you
should still follow the aforementioned guidelines when you write the description
for an issue or pull request. You should add details and background
information to an issue so the person doing triage doesn't have to apply the
<code>triage/needs-information</code> label. Likewise, when you create a pull request, you
should add enough information about the content change that reviewers don't have
to figure out the reason for the pull request. Providing details in clear,
concise language speeds up the process.</p><h2 id=observation-7>Observation 07: Triaging issues is more difficult than it should be</h2><p>In SIG Docs, triaging issues requires the ability to distinguish between
support, bug, and feature requests not only for the documentation but also for
Kubernetes code projects. How to route, label, and prioritize issues has become
easier week by week. I'm still not 100% clear on which SIG and/or project is
responsible for which parts of the documentation. The SIGs and Working Groups
<a href=https://github.com/kubernetes/community/blob/master/sig-list.md>page</a> helps,
but it is not enough. At a page level in the documentation, it's not
always obvious which SIG or project has domain expertise. The page's front
matter sometimes list reviewers but never lists a SIG or project. Each page should
indicate who is responsible for content, so that SIG Docs triagers know where to
route issues.</p><h2 id=observation-8>Observation 08: SIG Docs is understaffed</h2><p>Documentation is the number one driver of software adoption<sup>1</sup>.</p><p>Many contributors devote a small amount of time to SIG Docs but only a handful
are trained technical writers. Few companies have hired tech writers to work on
Kubernetes docs at least half-time. That's very disheartening for online
documentation that has had over 53 million unique page views from readers in 229
countries year to date in 2019.</p><p>SIG Docs faces challenges due to lack of technical writers:</p><ul><li><strong>Maintaining a high quality in the Kubernetes documentation</strong>:
There are over 750 pages of documentation. That's <em>750 pages</em> to check for
stale content on a regular basis. This involves more than running a link
checker against the <code>kubernetes/website</code> repo. This involves people having a
technical understanding of Kubernetes, knowing which code release changes
impact documentation, and knowing where content is located in the
documentation so that <em>all</em> impacted pages and example code files are updated
in a timely fashion. Other SIGs help with this, but based on the number of
issues created by readers, enough people aren't working on keeping the content
fresh.</li><li><strong>Reducing the time to review and merge a PR</strong>:
The larger the size of the PR, the longer it takes to get the <code>lgtm</code> label
and eventual approval. My <code>size/M</code> and larger PRs took from five to thirty
days to approve. Sometimes I politely poked reviewers to review again after
I had pushed updates. Other times I asked on the #sig-docs channel for <em>any
approver</em> to take a look and approve. People are busy. People go on
vacation. People also move on to new roles that don't involve SIG Docs and
forget to remove themselves from the reviewer and approver assignment file.
A large part of the time-to-merge problem is not having enough reviewers and
approvers. The other part is the <a href=https://github.com/kubernetes/community/blob/master/community-membership.md#reviewer>high
barrier</a>
to becoming a reviewer or approver, much higher than what I've seen on other
open source projects. Experienced open source tech writers who want to
contribute to SIG Docs aren't fast-tracked into approver and reviewer roles.
On one hand, that high barrier ensures that those roles are filled by folks
with a minimum level of Kubernetes documentation knowledge; on the other
hand, it might deter experienced tech writers from contributing at all, or
from a company allocating a tech writer to SIG Docs. Maybe SIG Docs should
consider deviating from the Kubernetes community requirements by lowering
the barrier to becoming a reviewer or approver, on a case-by-case basis, of
course.</li><li><strong>Ensuring consistent naming across all pages</strong>:
Terms should be identical to what is used in the <strong>Standardized Glossary</strong>. Being consistent reduces confusion.
Tracking down and fixing these occurrences is time-consuming but worthwhile for readers.</li><li><strong>Working with the Steering Committee to create project documentation guidelines</strong>:
The <a href=https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md>Kubernetes Repository Guidelines</a> don't mention documentation at all. Between a
project's GitHub docs and the Kubernetes docs, some projects have almost
duplicate content, whereas others have conflicting content. Create clear
guidelines so projects know to put roadmaps, milestones, and comprehensive
feature details in the <code>kubernetes/&lt;project></code> repo and to put installation,
configuration, usage details, and tutorials in the Kubernetes docs.</li><li><strong>Removing duplicate content</strong>:
Kubernetes users install Docker, so a good example of duplicate content is
Docker installation instructions. Rather than repeat what's in the Docker
docs, state which version of Docker works with which version of Kubernetes
and link to the Docker docs for installation. Then detail any
Kubernetes-specific configuration. That idea is the same for the container
runtimes that Kubernetes supports.</li><li><strong>Removing third-party vendor content</strong>:
This is tightly coupled to removing duplicate content. Some third-party
content consists of lists or tables detailing external products. Other
third-party content is found in the <strong>Tasks</strong> and <strong>Tutorials</strong> sections.
SIG Docs should not be responsible for verifying that third-party products
work with the latest version of Kubernetes. Nor should SIG Docs be
responsible for maintaining lists of training courses or cloud providers.
Additionally, the Kubernetes documentation isn't the place to pitch vendor
products. If SIG Docs is forced to reverse its policy on not allowing
third-party content, there could be a tidal wave of
vendor-or-commercially-oriented pull requests. Maintaining that content
places an undue burden on SIG Docs.</li><li><strong>Indicating which version of Kubernetes works with each task and tutorial</strong>:
This means reviewing each task and tutorial for every release. Readers
assume if a task or tutorial is in the latest version of the docs, it works
with the latest version of Kubernetes.</li><li><strong>Addressing issues</strong>:
There are 470 open issues in the <code>kubernetes/website</code> repo. It's hard to keep up with all the issues that are created. We encourage
those creating simpler issues to submit PRs: some do; most do not.</li><li><strong>Creating more detailed content</strong>:
Readers
<a href=https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey>indicated</a>
they would like to see more detailed content across all sections of the
documentation, including tutorials.</li></ul><p>Kubernetes has seen unparalleled growth since its first release in 2015. Like
any fast-growing project, it has growing pains. Providing consistently
high-quality documentation is one of those pains, and one incredibly important
to an open source project. SIG Docs needs a larger core team of tech writers who
are allocated at least 50%. SIG Docs can then better achieve goals, move forward
with new content, update existing content, and address open issues in a timely fashion.</p><h2 id=observation-9>Observation 09: Contributing to technical documentation projects requires, on average, more skills than developing software</h2><p>When I said that to my former colleagues, the response was a healthy dose of
skepticism and lots of laughter. It seems that many developers, as well as
managers, don't fully know what tech writers contributing to open source
projects actually do. Having done both development and technical writing for the
better part of 22 years, I've noticed that tech writers are valued far less than
software developers of comparative standing.</p><p>SIG Docs core team members do far more than write content based on requirements:</p><ul><li>We use some of the same processes and tools as developers, such as the
terminal, git workflow, GitHub, and IDEs like Atom, Golang, and Visual Studio Code; we
also use documentation-specific plugins and tools.</li><li>We possess a good eye for detail as well as design and organization: the big picture <em>and</em> the little picture.</li><li>We provide documentation which has a logical flow; it is not merely content on a page
but the way pages fit into sections and sections fit into the overall structure.</li><li>We write content that is comprehensive and uses language that readers not fluent in English can understand.</li><li>We have a firm grasp of English composition using various markup languages.</li><li>We are technical, sometimes to the level of a Kubernetes admin.</li><li>We read, understand, and occasionally write code.</li><li>We are project managers, able to plan new work as well as assign issues to releases.</li><li>We are educators and diplomats with every review we do and with every comment we leave on an issue.</li><li>We use site analytics to plan work based on which pages readers access most often as well as which pages readers say are unhelpful.</li><li>We are surveyors, soliciting feedback from the community on a regular basis.</li><li>We analyze the documentation as a whole, deciding what content should stay and
what content should be removed based on available resources and reader needs.</li><li>We have a working knowledge of Hugo and other frameworks used for
online documentation; we know how to create, use, and debug Hugo shortcodes that
enable content to be more robust than pure Markdown.</li><li>We troubleshoot performance issues not only with Hugo but with Netlify.</li><li>We grapple with the complex problem of API documentation.</li><li>We are dedicated to providing the highest quality documentation that we can.</li></ul><p>If you have any doubts about the complexity of the Kubernetes documentation
project, watch presentations given by SIG Docs Chair Zach Corleissen:</p><ul><li><a href=https://archive.fosdem.org/2019/schedule/event/multikuber/>Multilingual Kubernetes</a> - the kubernetes.io stack, how we got there, and what it took to get there</li><li><a href=https://youtu.be/GXkpHAruNV8>Found in Translation: Lessons from a Year of Open Source Localization</a></li></ul><p>Additionally, <a href=https://youtu.be/JvRd7MmAxPw>Docs as Code: The Missing Manual</a>
(Jennifer Rondeau, Margaret Eker; 2016) is an excellent presentation on the
complexity of documentation projects in general.</p><p>The Write the Docs <a href=http://www.writethedocs.org/>website</a> and <a href=https://www.youtube.com/channel/UCr019846MitZUEhc6apDdcQ>YouTube
channel</a> are
fantastic places to delve into the good, the bad, and the ugly of technical writing.</p><p>Think what an open source project would be without talented, dedicated tech writers!</p><h2 id=observation-10>Observation 10: Community is everything</h2><p>The SIG Docs community, and the larger Kubernetes community, is dedicated,
intelligent, friendly, talented, fun, helpful, and a whole bunch of other
positive adjectives! People welcomed me with open arms, and not only because SIG
Docs needs more technical writers. I have never felt that my ideas and contributions were
dismissed because I was the newbie. Humility and respect go a long way.
Community members have a wealth of knowledge to share. Attend meetings, ask
questions, propose improvements, thank people, and contribute in
every way that you can!</p><p>Big shout out to those who helped me, and put up with me (LOL), during my
break-in period: @zacharaysarah, @sftim, @kbhawkey, @jaypipes, @jrondeau,
@jmangel, @bradtopol, @cody_clark, @thecrudge, @jaredb, @tengqm, @steveperry-53,
@mrbobbytables, @cblecker, and @kbarnard10.</p><h2 id=outro>Outro</h2><p>Do I grok SIG Docs? Not quite yet, but I do understand that SIG Docs needs more
dedicated resources to continue to be successful.</p><h2 id=citations>Citations</h2><p><sup>1</sup> @linuxfoundation. "Megan Byrd-Sanicki, Open Source Strategist, Google @megansanicki - documentation is the #1 driver of software adoption. #ossummit." <em>Twitter</em>, Oct 29, 2019, 3:54 a.m., twitter.com/linuxfoundation/status/1189103201439637510.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-464e22d2595c073a96a61069f2fd202f>Kubernetes Documentation Survey</h1><div class="td-byline mb-4"><time datetime=2019-10-29 class=text-muted>Tuesday, October 29, 2019</time></div><p><strong>Author:</strong> <a href=https://www.linkedin.com/in/aimee-ukasick/>Aimee Ukasick</a> and SIG Docs</p><p>In September, SIG Docs conducted its first survey about the <a href=https://kubernetes.io/docs/>Kubernetes
documentation</a>. We'd like to thank the CNCF's Kim
McMahon for helping us create the survey and access the results.</p><h1 id=key-takeaways>Key takeaways</h1><p>Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.</p><p>74% of respondents would like the Tutorials section to contain advanced content.</p><p>69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.</p><h1 id=survey-methodology-and-respondents>Survey methodology and respondents</h1><p>We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.</p><h2 id=quick-facts-about-respondents>Quick facts about respondents:</h2><ul><li>48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner</li><li>57.58% use Kubernetes in both administrator and developer roles</li><li>64.65% have been using the Kubernetes documentation for more than 12 months</li><li>95.96% read the documentation in English</li></ul><h1 id=question-and-response-highlights>Question and response highlights</h1><h2 id=why-people-access-the-kubernetes-documentation>Why people access the Kubernetes documentation</h2><p>The majority of respondents stated that they access the documentation for the Concepts.</p><figure><img src=/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png alt="Why respondents access the Kubernetes documentation"></figure><p>This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.</p><h2 id=satisfaction-with-the-documentation>Satisfaction with the documentation</h2><p>We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:</p><ul><li>Concepts: 47.96% Moderately Satisfied</li><li>Tasks: 50.54% Moderately Satisfied</li><li>Reference: 40.86% Very Satisfied</li><li>Tutorial: 47.25% Moderately Satisfied</li></ul><h2 id=how-sig-docs-can-improve-each-documentation-section>How SIG Docs can improve each documentation section</h2><p>We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they&#39;re a bucket of separate eels moving in different directions right now
- More diagrams, and more example code
</code></pre></div><p>Respondents used the "Other" text box to record areas causing frustration:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I&#39;ve never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them
</code></pre></div><h2 id=how-sig-docs-can-improve-the-documentation-overall>How SIG Docs can improve the documentation overall</h2><p>We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.
</code></pre></div><p>Other respondents provided feedback on the content:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>-  ...But since we&#39;re talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each
configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
- More in-depth examples and use cases would be great. I often feel that the Kubernetes
documentation scratches the surface of a topic, which might be great for new users, but it leaves
more experienced users without much &#34;official&#34; guidance on how to implement certain things.
- More production like examples in the resource sections (notably secrets) or links to production like
examples
- It would be great to see a very clear &#34;Quick Start&#34; A-&gt;Z up and running like many other tech
projects. There are a handful of almost-quick-starts, but no single guidance. The result is
information overkill.
</code></pre></div><p>A few respondents provided technical suggestions:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>- Make table columns sortable and filterable using a ReactJS or Angular component.
- For most, I think creating documentation with Hugo - a system for static site generation - is not
appropriate. There are better systems for documenting large software project. Specifically, I would
like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy to
learn if you know markdown, it is widely adopted by other projects (e.g. every software project in
readthedocs.io, linux kernel, docs.python.org etc).
</code></pre></div><p>Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.</p><h1 id=where-to-see-more>Where to see more</h1><p>Survey results summary, charts, and raw data are available in <code>kubernetes/community</code> sig-docs <a href=https://github.com/kubernetes/community/tree/master/sig-docs/survey>survey</a> directory.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fb4706cbf52d410fa926ccb78d30b84f>Contributor Summit San Diego Schedule Announced!</h1><div class="td-byline mb-4"><time datetime=2019-10-10 class=text-muted>Thursday, October 10, 2019</time></div><p>Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)</p><p>tl;dr A week ago we announced that <a href=https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/>registration is open</a> for the contributor
summit , and we're now live with <a href=https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/>the full Contributor Summit schedule!</a>
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. (<a href=https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/>Register here!</a>)</p><p>There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.</p><p>Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.</p><p><img src=/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg alt=Unconference></p><p>While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:</p><ul><li><strong><a href=https://sched.co/VvMc>Vision</a></strong>: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.</li><li><strong><a href=https://sched.co/VvMj>Security</a></strong>: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.</li><li><strong><a href=https://sched.co/Vv6Z>Prow</a></strong>: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.</li><li><strong><a href=https://sched.co/VvNa>Git</a></strong>: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.</li><li><strong><a href=https://sched.co/VutA>Reviewing</a></strong>: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.</li><li><strong><a href=https://sched.co/VvNJ>End Users</a></strong>: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&A with contributors to strengthen our feedback loop.</li><li><strong><a href=https://sched.co/Vux2>Docs</a></strong>: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.</li></ul><p>We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet & Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.</p><p>Hope to see you all there, and <a href=https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/>make sure you register!</a>
<a href=http://git.k8s.io/community/events/events-team>San Diego team</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-fc6fc922d4286e1fe8e3289d1094bdf8>2019 Steering Committee Election Results</h1><div class="td-byline mb-4"><time datetime=2019-10-03 class=text-muted>Thursday, October 03, 2019</time></div><p><strong>Authors</strong>: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)</p><p>The <a href=https://git.k8s.io/community/events/elections/2019>2019 Steering Committee Election</a> is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.</p><p>Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.</p><h2 id=results><strong>Results</strong></h2><p>The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):</p><ul><li><strong>Christoph Blecker (<a href=https://github.com/cblecker>@cblecker</a>), Red Hat</strong></li><li><strong>Derek Carr (<a href=https://github.com/derekwaynecarr>@derekwaynecarr</a>), Red Hat</strong></li><li><strong>Nikhita Raghunath (<a href=https://github.com/nikhita>@nikhita</a>), Loodse</strong></li><li><strong>Paris Pittman (<a href=https://github.com/parispittman>@parispittman</a>)</strong>, <strong>Google</strong></li></ul><p>They join Aaron Crickenberger (<a href=https://github.com/spiffxp>@spiffxp</a>), Google; Davanum Srinivas (<a href=https://github.com/dims>@dims</a>),
VMware; and Timothy St. Clair (<a href=https://github.com/timothysc>@timothysc</a>), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.</p><h2 id=big-thanks>Big Thanks!</h2><ul><li>Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:<ul><li>Joe Beda (<a href=https://github.com/jbeda>@jbeda</a>), VMware</li><li>Brendan Burns (<a href=https://github.com/brendandburns>@brendandburns</a>), Microsoft</li><li>Clayton Coleman (<a href=https://github.com/smarterclayton>@smarterclayton</a>), Red Hat</li><li>Brian Grant (<a href=https://github.com/bgrant0607>@bgrant0607</a>), Google</li><li>Tim Hockin (<a href=https://github.com/thockin>@thockin</a>), Google</li><li>Sarah Novotny (<a href=https://github.com/sarahnovotny>@sarahnovotny</a>), Microsoft</li><li>Brandon Philips (<a href=https://github.com/philips>@philips</a>), Red Hat</li></ul></li><li>And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:<ul><li>Quinton Hoole (<a href=https://github.com/quinton-hoole>@quinton-hoole</a>), Huawei</li><li>Michelle Noorali (<a href=https://github.com/michelleN>@michelleN</a>), Microsoft</li><li>Phillip Wittrock (<a href=https://github.com/pwittrock>@pwittrock</a>), Google</li></ul></li><li>Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.</li><li>Thanks to all 377 voters who cast a ballot.</li><li>And last but not least…Thanks to Cornell University for hosting <a href=https://civs.cs.cornell.edu/>CIVS</a>!</li></ul><h2 id=get-involved-with-the-steering-committee>Get Involved with the Steering Committee</h2><p>You can follow along with Steering Committee <a href=https://github.com/kubernetes/steering/projects/1>backlog items</a> and weigh in by
filing an issue or creating a PR against their <a href=https://github.com/kubernetes/steering>repo</a>. They meet bi-weekly on
<a href=https://github.com/kubernetes/steering>Wednesdays at 8pm UTC</a> and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list <a href=mailto:steering@kubernetes.io>steering@kubernetes.io</a>.</p><p>Steering Committee Meetings:</p><ul><li><a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-aed805df2d0756cee0216566e206c2e0>Contributor Summit San Diego Registration Open!</h1><div class="td-byline mb-4"><time datetime=2019-09-24 class=text-muted>Tuesday, September 24, 2019</time></div><p><strong>Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)</strong></p><p><a href=https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/>Contributor Summit San Diego 2019 Event Page</a><br>Registration is now open and in record time, we’ve hit capacity for the
<em>new contributor workshop</em> session of the event! Waitlist is now available.</p><p><strong>Sunday, November 17</strong><br>Evening Contributor Celebration:<br><a href=https://quartyardsd.com/>QuartYard</a>*<br>Address: 1301 Market Street, San Diego, CA 92101<br>Time: 6:00PM - 9:00PM</p><p><strong>Monday, November 18</strong><br>All Day Contributor Summit:<br><a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina</a><br>Address: 333 W Harbor Dr, San Diego, CA 92101<br>Time: 9:00AM - 5:00PM</p><p>While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.</p><p>Kubernetes has a large distributed remote contributing team, from <a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&var-period=y&var-metric=contributions&var-repogroup_name=All&var-companies=All">individuals and
organizations</a> all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.</p><p>This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our <a href=https://github.com/kubernetes/community/tree/master/events/events-team>rolebooks, guidelines,
best practices</a> and opened up our <a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">meetings</a> and <a href=https://github.com/orgs/kubernetes/projects/21>project board</a>. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.</p><p>Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the <a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIG Intro and Deep Dive sessions</a> during KubeCon + CloudNativeCon to
participate in Q&A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.</p><p>We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and <a href=https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/>register right now</a>! This event will sell out - here’s your warning.
:smiley:</p><p>Check out past blogs on <a href=https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/>persona building around our events</a> and the <a href=https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/>Barcelona summit story</a>.</p><p><img src=/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG alt="Group Picture in 2018"></p><p>*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! <a href=mailto:community@kubernetes.io>community@kubernetes.io</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-848cbe6754810f86c360ea5906b4e16d>Kubernetes 1.16: Custom Resources, Overhauled Metrics, and Volume Extensions</h1><div class="td-byline mb-4"><time datetime=2019-09-18 class=text-muted>Wednesday, September 18, 2019</time></div><p><strong>Authors:</strong> <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.16/release_team.md>Kubernetes 1.16 Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.16, our third release of 2019! Kubernetes 1.16 consists of 31 enhancements: 8 enhancements moving to stable, 8 enhancements in beta, and 15 enhancements in alpha.</p><h1 id=major-themes>Major Themes</h1><h2 id=custom-resources>Custom resources</h2><p>CRDs are in widespread use as a Kubernetes extensibility mechanism and have been available in beta since the 1.7 release. The 1.16 release marks the graduation of CRDs to general availability (GA).</p><h2 id=overhauled-metrics>Overhauled metrics</h2><p>Kubernetes has previously made extensive use of a global metrics registry to register metrics to be exposed. By implementing a metrics registry, metrics are registered in more transparent means. Previously, Kubernetes metrics have been excluded from any kind of stability requirements.</p><h2 id=volume-extension>Volume Extension</h2><p>There are quite a few enhancements in this release that pertain to volumes and volume modifications. Volume resizing support in CSI specs is moving to beta which allows for any CSI spec volume plugin to be resizable.</p><h1 id=significant-changes-to-the-kubernetes-api>Significant Changes to the Kubernetes API</h1><p>As the Kubernetes API has evolved, we have promoted some API resources to <em>stable</em>, others have been reorganized to different groups. We deprecate older versions of a resource and make newer versions available in accordance with the <a href=https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning>API versioning policy</a>.</p><p>An example of this is the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> resource. This was introduced under the <code>extensions/v1beta1</code> group in 1.6 and as the project changed has been promoted to <code>extensions/v1beta2</code>, <code>apps/v1beta2</code> and finally promoted to <code>stable</code> and moved to <code>apps/v1</code> in 1.9.</p><p>It's important to note that until this release the project has not stopped serving any of the previous versions of the any of the deprecated resources.</p><p>This means that folks interacting with the Kubernetes API have not been <em>required</em> to move to the new version of any of the deprecated API objects.</p><p>In 1.16 if you submit a <code>Deployment</code> to the API server and specify <code>extensions/v1beta1</code> as the API group it will be rejected with:</p><pre><code>error: unable to recognize &quot;deployment&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;
</code></pre><p>With this release we are taking a very important step in the maturity of the Kubernetes API, and are no longer serving the deprecated APIs. Our earlier post <a href=https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/>Deprecated APIs Removed In 1.16: Here’s What You Need To Know</a> tells you more, including which resources are affected.</p><h1 id=additional-enhancements>Additional Enhancements</h1><h2 id=custom-resources-reach-general-availability>Custom Resources Reach General Availability</h2><p>CRDs have become the basis for extensions in the Kubernetes ecosystem. Started as a ground-up redesign of the ThirdPartyResources prototype, they have finally reached GA in 1.16 with apiextensions.k8s.io/v1, as the hard-won lessons of API evolution in Kubernetes have been integrated. As we transition to GA, the focus is on data consistency for API clients.</p><p>As you upgrade to the GA API, you’ll notice that several of the previously optional guard rails have become required and/or default behavior. Things like structural schemas, pruning unknown fields, validation, and protecting the *.k8s.io group are important for ensuring the longevity of your APIs and are now much harder to accidentally miss. Defaulting is another important part of API evolution and that support will be on by default for CRD.v1. The combination of these, along with CRD conversion mechanisms are enough to build stable APIs that evolve over time, the same way that native Kubernetes resources have changed without breaking backward-compatibility.</p><p>Updates to the CRD API won’t end here. We have ideas for features like arbitrary subresources, API group migration, and maybe a more efficient serialization protocol, but the changes from here are expected to be optional and complementary in nature to what’s already here in the GA API. Happy operator writing!</p><p>Details on how to work with custom resources can be found <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/>in the Kubernetes documentation</a>.</p><h2 id=opening-doors-with-windows-enhancements>Opening Doors With Windows Enhancements</h2><h3 id=beta-enhancing-the-workload-identity-options-for-windows-containers>Beta: Enhancing the workload identity options for Windows containers</h3><p>Active Directory Group Managed Service Account (GMSA) support is graduating to beta and certain annotations that were introduced with the alpha support are being deprecated. GMSA is a specific type of Active Directory account that enables Windows containers to carry an identity across the network and communicate with other resources. Windows containers can now gain authenticated access to external resources. In addition, GMSA provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.</p><p>Adding support for RunAsUserName as an alpha release. The RunAsUserName is a string specifying the windows identity (or username) in Windows to run the entrypoint of the container and is a part of the newly introduced windowsOptions component of the securityContext (WindowsSecurityContextOptions).</p><h3 id=alpha-improvements-to-setup-node-join-experience-with-kubeadm>Alpha: Improvements to setup & node join experience with kubeadm</h3><p>Introducing alpha support for kubeadm, enabling Kubernetes users to easily join (and reset) Windows worker nodes to an existing cluster the same way they do for Linux nodes. Users can utilize kubeadm to prepare and add a Windows node to cluster. When the operations are complete, the node will be in a Ready state and able to run Windows containers. In addition, we will also provide a set of Windows-specific scripts to enable the installation of prerequisites and CNIs ahead of joining the node to the cluster.</p><h3 id=alpha-introducing-support-for-container-storage-interface-csi>Alpha: Introducing support for Container Storage Interface (CSI)</h3><p>Introducing CSI plugin support for out-of-tree providers, enabling Windows nodes in a Kubernetes cluster to leverage persistent storage capabilities for Windows-based workloads. This significantly expands the storage options of Windows workloads, adding onto a list that included FlexVolume and in-tree storage plugins. This capability is achieved through a host OS proxy that enables the execution of privileged operations on the Windows node on behalf of containers.</p><h2 id=introducing-endpoint-slices>Introducing Endpoint Slices</h2><p>The release of Kubernetes 1.16 includes an exciting new alpha feature: the EndpointSlice API. This API provides a scalable and extensible alternative to the <a href=https://v1-16.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#endpoints-v1-core>Endpoints</a> resource, which dates back to the very first versions of Kubernetes. Behind the scenes, Endpoints play a big role in network routing within Kubernetes. Each Service endpoint is tracked within these resources - kube-proxy uses them for generating proxy rules that allow pods to communicate with each other so easily in Kubernetes, and many ingress controllers use them to route HTTP traffic directly to pods.</p><h3 id=providing-greater-scalability>Providing Greater Scalability</h3><p>A key goal for EndpointSlices is to enable greater scalability for Kubernetes Services. With the existing Endpoints API, a single instance must include network endpoints representing all pods matching a Service. As Services start to scale to thousands of pods, the corresponding Endpoints resources become quite large. Simply adding or removing one endpoint from a Service at this scale can be quite costly. As the Endpoints instance is updated, every piece of code watching Endpoints will need to be sent a full copy of the resource. With kube-proxy running on every node in a cluster, a copy needs to be sent to every single node. At a small scale, this is not an issue, but it becomes increasingly noticeable as clusters get larger.</p><p><img src=/images/blog/2019-09-18-kubernetes-1-16-release-announcement/endpoint-slices.png alt="Endpoints to Endpoint Slice"></p><p>With EndpointSlices, network endpoints for a Service are split into multiple instances, significantly decreasing the amount of data required for updates at scale. By default, EndpointSlices are limited to 100 endpoints each.</p><p>For example, let’s take a cluster with 10,000 Service endpoints spread over 5,000 nodes. A single Pod update would result in approximately 5GB transmitted with the Endpoints API (that’s enough to fill a DVD). This becomes increasingly significant given how frequently Endpoints can change during events like rolling updates on Deployments. The same update will be much more efficient with EndpointSlices since each one includes only a tiny portion of the total number of Service endpoints. Instead of transferring a big Endpoints object to each node, only the small EndpointSlice that’s been changed has to be transferred. In this example, EndpointSlices would decrease data transferred by approximately 100x.</p><table><tr><td></td><td><strong>Endpoints</strong></td><td><strong>Endpoint Slices</strong></td></tr><tr><td># of resources</td><td><em>1</em></td><td><em>20k / 100 = 200</em></td></tr><tr><td># of network endpoints stored</td><td><em>1 * 20k = 20k</em></td><td><em>200 * 100 = 20k</em></td></tr><tr><td>size of each resource</td><td><em>20k * const = ~2.0 MB</em></td><td><em>100 * const = ~10 kB</em></td></tr><tr><td>watch event data transferred</td><td><em>~2.0MB * 5k = 10GB</em></td><td><em>~10kB * 5k = 50MB</em></td></tr></table><h3 id=providing-greater-extensibility>Providing Greater Extensibility</h3><p>A second goal for EndpointSlices was to provide a resource that would be highly extensible and useful across a wide variety of use cases. One of the key additions with EndpointSlices involves a new topology attribute. By default, this will be populated with the existing topology labels used throughout Kubernetes indicating attributes such as region and zone. Of course, this field can be populated with custom labels as well for more specialized use cases.</p><p>EndpointSlices also include greater flexibility for address types. Each contains a list of addresses. An initial use case for multiple addresses would be to support dual-stack endpoints with both IPv4 and IPv6 addresses. As an example, here’s a simple EndpointSlice showing how one could be represented:</p><pre><code>apiVersion: discovery.k8s.io/v1alpha
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IP
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
    - &quot;10.1.2.3&quot;
    - &quot;2001:db8::1234:5678&quot;
    topology:
      kubernetes.io/hostname: node-1
      topology.kubernetes.io/zone: us-west2-a
</code></pre><h3 id=more-about-endpoint-slices>More About Endpoint Slices</h3><p>EndpointSlices are an alpha feature in Kubernetes 1.16 and not enabled by default. The Endpoints API will continue to be enabled by default, but we’re working to move the largest Endpoints consumers to the new EndpointSlice API. Notably, kube-proxy in Kubernetes 1.16 includes alpha support for EndpointSlices.</p><p>The official Kubernetes documentation contains more information about EndpointSlices as well as how to enable them in your cluster. There’s also a <a href="https://www.youtube.com/watch?v=Y5JOCCbJ_Fg">great KubeCon talk</a> that provides more background on the initial rationale for developing this API.</p><h4 id=notable-feature-updates>Notable Feature Updates</h4><ul><li><a href=https://github.com/kubernetes/enhancements/issues/693>Topology Manager</a>, a new Kubelet component, aims to co-ordinate resource assignment decisions to provide optimized resource allocations.</li><li><a href=https://kubernetes.io/docs//concepts/services-networking/dual-stack/>IPv4/IPv6 dual-stack</a> enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.</li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/20190422-cloud-controller-manager-migration.md>Extensions</a> for Cloud Controller Manager Migration.</li></ul><h2 id=availability>Availability</h2><p>Kubernetes 1.16 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.16.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a>. You can also easily install 1.16 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h2 id=release-team>Release Team</h2><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=http://bit.ly/k8s116-team>release team</a> led by Lachlan Evenson, Principal Program Manager at Microsoft. The 32 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">32,000 individual contributors</a> to date and an active community of more than 66,000 people.</p><h2 id=release-mascot>Release Mascot</h2><p>The Kubernetes 1.16 release crest was loosely inspired by the Apollo 16 mission crest. It represents the hard work of the release-team and the community alike and is an ode to the challenges and fun times we shared as a team throughout the release cycle. Many thanks to Ronan Flynn-Curran of Microsoft for creating this magnificent piece.</p><p><img src=/images/blog/2019-09-18-kubernetes-1-16-release-announcement/mascot.png alt="Kubernetes 1.16 Release Mascot"></p><h1 id=kubernetes-updates>Kubernetes Updates</h1><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://k8s.devstats.cncf.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. This past year, <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&var-period_name=Last%20year&var-metric=contributions">1,147 different companies and over 3,149 individuals</a> contribute to Kubernetes each month. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=ecosystem>Ecosystem</h2><ul><li>The Kubernetes project leadership created the Security Audit Working Group to oversee the very first third-part <a href=https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/>Kubernetes security audit</a>, in an effort to improve the overall security of the ecosystem.</li><li>The <a href=https://www.cncf.io/announcement/2019/07/09/the-cloud-native-computing-foundation-announces-the-kubernetes-certified-service-providers-program-has-reached-100-participants/>Kubernetes Certified Service Providers</a> program (KCSP) reached 100 member companies, ranging from the largest multinational cloud, enterprise software, and consulting companies to tiny startups.</li><li>The first <a href=https://www.cncf.io/blog/2019/08/29/announcing-the-cncf-kubernetes-project-journey-report/>Kubernetes Project Journey Report</a> was released, showcasing the massive growth of the project.</li></ul><h2 id=kubecon-cloudnativecon>KubeCon + CloudNativeCon</h2><p>The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in San Diego, California from November 18-21, 2019. Join Kubernetes, Prometheus, Envoy, CoreDNS, containerd, Fluentd, OpenTracing, gRPC, CNI, Jaeger, Notary, TUF, Vitess, NATS, Linkerd, Helm, Rook, Harbor, etcd, Open Policy Agent, CRI-O, and TiKV as the community gathers for four days to further the education and advancement of cloud native computing. <a href=https://www.cncf.io/community/kubecon-cloudnativecon-events/>Register today</a>!</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.16 release team on Oct 22, 2019 to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/9015681469655/WN_JTLYA0DMRD6Mnm2f64KYMg>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p><ul><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dcfa29bde9d7191277f1eb6654189b99>Announcing etcd 3.4</h1><div class="td-byline mb-4"><time datetime=2019-08-30 class=text-muted>Friday, August 30, 2019</time></div><p><strong>Authors:</strong> Gyuho Lee (Amazon Web Services, @<a href=https://github.com/gyuho>gyuho</a>), Jingyi Hu (Google, @<a href=https://github.com/jingyih>jingyih</a>)</p><p>etcd 3.4 focuses on stability, performance and ease of operation, with features like pre-vote and non-voting member and improvements to storage backend and client balancer.</p><p>Please see <a href=https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.4.md>CHANGELOG</a> for full lists of changes.</p><h2 id=better-storage-backend>Better Storage Backend</h2><p>etcd v3.4 includes a number of performance improvements for large scale Kubernetes workloads.</p><p>In particular, etcd experienced performance issues with a large number of concurrent read transactions even when there is no write (e.g. <code>“read-only range request ... took too long to execute”</code>). Previously, the storage backend commit operation on pending writes blocks incoming read transactions, even when there was no pending write. Now, the commit <a href=https://github.com/etcd-io/etcd/pull/9296>does not block reads</a> which improve long-running read transaction performance.</p><p>We further made <a href=https://github.com/etcd-io/etcd/pull/10523>backend read transactions fully concurrent</a>. Previously, ongoing long-running read transactions block writes and upcoming reads. With this change, write throughput is increased by 70% and P99 write latency is reduced by 90% in the presence of long-running reads. We also ran <a href=https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-scale-performance/1130745634945503235>Kubernetes 5000-node scalability test on GCE</a> with this change and observed similar improvements. For example, in the very beginning of the test where there are a lot of long-running “LIST pods”, the P99 latency of “POST clusterrolebindings” is <a href=https://github.com/etcd-io/etcd/pull/10523#issuecomment-499262001>reduced by 97.4%</a>.</p><p>More improvements have been made to lease storage. We enhanced <a href=https://github.com/etcd-io/etcd/pull/9418>lease expire/revoke performance</a> by storing lease objects more efficiently, and made <a href=https://github.com/etcd-io/etcd/pull/9229>lease look-up operation non-blocking</a> with current lease grant/revoke operation. And etcd v3.4 introduces <a href=https://github.com/etcd-io/etcd/pull/9924>lease checkpoint</a> as an experimental feature to persist remaining time-to-live values through consensus. This ensures short-lived lease objects are not auto-renewed after leadership election. This also prevents lease object pile-up when the time-to-live value is relatively large (e.g. <a href=https://github.com/kubernetes/kubernetes/issues/65497>1-hour TTL never expired in Kubernetes use case</a>).</p><h2 id=improved-raft-voting-process>Improved Raft Voting Process</h2><p>etcd server implements <a href=https://raft.github.io>Raft consensus algorithm</a> for data replication. Raft is a leader-based protocol. Data is replicated from leader to follower; a follower forwards proposals to a leader, and the leader decides what to commit or not. Leader persists and replicates an entry, once it has been agreed by the quorum of cluster. The cluster members elect a single leader, and all other members become followers. The elected leader periodically sends heartbeats to its followers to maintain its leadership, and expects responses from each follower to keep track of its progress.</p><p>In its simplest form, a Raft leader steps down to a follower when it receives a message with higher terms without any further cluster-wide health checks. This behavior can affect the overall cluster availability.</p><p>For instance, a flaky (or rejoining) member drops in and out, and starts campaign. This member ends up with higher terms, ignores all incoming messages with lower terms, and sends out messages with higher terms. When the leader receives this message of a higher term, it reverts back to follower.</p><p>This becomes more disruptive when there’s a network partition. Whenever the partitioned node regains its connectivity, it can possibly trigger the leader re-election. To address this issue, etcd Raft introduces a new node state pre-candidate with the <a href=https://github.com/etcd-io/etcd/pull/9352>pre-vote feature</a>. The pre-candidate first asks other servers whether it's up-to-date enough to get votes. Only if it can get votes from the majority, it increments its term and starts an election. This extra phase improves the robustness of leader election in general. And helps the leader remain stable as long as it maintains its connectivity with the quorum of its peers.</p><p>Similarly, etcd availability can be affected when a restarting node has not received the leader heartbeats in time (e.g. due to slow network), which triggers the leader election. Previously, etcd fast-forwards election ticks on server start, with only one tick left for leader election. For example, when the election timeout is 1-second, the follower only waits 100ms for leader contacts before starting an election. This speeds up initial server start, by not having to wait for the election timeouts (e.g. election is triggered in 100ms instead of 1-second). Advancing election ticks is also useful for cross datacenter deployments with larger election timeouts. However, in many cases, the availability is more critical than the speed of initial leader election. To ensure better availability with rejoining nodes, etcd now <a href=https://github.com/etcd-io/etcd/pull/9415>adjusts election ticks</a> with more than one tick left, thus more time for the leader to prevent a disruptive restart.</p><h2 id=raft-non-voting-member-learner>Raft Non-Voting Member, Learner</h2><p>The challenge with membership reconfiguration is that it often leads to quorum size changes, which are prone to cluster unavailabilities. Even if it does not alter the quorum, clusters with membership change are more likely to experience other underlying problems. To improve the reliability and confidence of reconfiguration, a new role - learner is introduced in etcd 3.4 release.</p><p>A new etcd member joins the cluster with no initial data, requesting all historical updates from the leader until it catches up to the leader’s logs. This means the leader’s network is more likely to be overloaded, blocking or dropping leader heartbeats to followers. In such cases, a follower may experience election-timeout and start a new leader election. That is, a cluster with a new member is more vulnerable to leader election. Both leader election and the subsequent update propagation to the new member are prone to causing periods of cluster unavailability (see <em>Figure 1</em>).</p><p><img src=/images/blog/2019-08-30-announcing-etcd-3.4/figure-1.png alt=learner-figure-1></p><p>The worst case is a misconfigured membership add. Membership reconfiguration in etcd is a two-step process: <code>etcdctl member add</code> with peer URLs, and starting a new etcd to join the cluster. That is, <code>member add</code> command is applied whether the peer URL value is invalid or not. If the first step is to apply the invalid URLs and change the quorum size, it is possible that the cluster already loses the quorum until the new node connects. Since the node with invalid URLs will never become online and there’s no leader, it is impossible to revert the membership change (see <em>Figure 2</em>).</p><p><img src=/images/blog/2019-08-30-announcing-etcd-3.4/figure-2.png alt=learner-figure-2></p><p>This becomes more complicated when there are partitioned nodes (see the <a href=https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md>design document</a> for more).</p><p>In order to address such failure modes, etcd introduces a <a href=https://github.com/etcd-io/etcd/issues/10537>new node state “Learner”</a>, which joins the cluster as a non-voting member until it catches up to leader’s logs. This means the learner still receives all updates from leader, while it does not count towards the quorum, which is used by the leader to evaluate peer activeness. The learner only serves as a standby node until promoted. This relaxed requirements for quorum provides the better availability during membership reconfiguration and operational safety (see <em>Figure 3</em>).</p><p><img src=/images/blog/2019-08-30-announcing-etcd-3.4/figure-3.png alt=learner-figure-3></p><p>We will further improve learner robustness, and explore auto-promote mechanisms for easier and more reliable operation. Please read our <a href=https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md>learner design documentation</a> and <a href=https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#add-a-new-member-as-learner>runtime-configuration document</a> for user guides.</p><h2 id=new-client-balancer>New Client Balancer</h2><p>etcd is designed to tolerate various system and network faults. By design, even if one node goes down, the cluster “appears” to be working normally, by providing one logical cluster view of multiple servers. But, this does not guarantee the liveness of the client. Thus, etcd client has implemented a different set of intricate protocols to guarantee its correctness and high availability under faulty conditions.</p><p>Historically, etcd client balancer heavily relied on old gRPC interface: every gRPC dependency upgrade broke client behavior. A majority of development and debugging efforts were devoted to fixing those client behavior changes. As a result, its implementation has become overly complicated with bad assumptions on server connectivity. The primary goal was to <a href=https://github.com/etcd-io/etcd/pull/9860>simplify balancer failover logic in etcd v3.4 client</a>; instead of maintaining a list of unhealthy endpoints, which may be stale, simply roundrobin to the next endpoint whenever client gets disconnected from the current endpoint. It does not assume endpoint status. Thus, no more complicated status tracking is needed.</p><p>Furthermore, the new client now creates its own credential bundle to <a href=https://github.com/etcd-io/etcd/pull/10911>fix balancer failover against secure endpoints</a>. This resolves the <a href=https://github.com/kubernetes/kubernetes/issues/72102>year-long bug</a>, where kube-apiserver loses its connectivity to etcd cluster when the first etcd server becomes unavailable.</p><p>Please see <a href=https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md>client balancer design documentation</a> for more.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-87d6b9119a0096eaf905a06844242333>OPA Gatekeeper: Policy and Governance for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2019-08-06 class=text-muted>Tuesday, August 06, 2019</time></div><p><strong>Authors:</strong> Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)</p><p>The <a href=https://github.com/open-policy-agent/gatekeeper>Open Policy Agent Gatekeeper</a> project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.</p><p>The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:</p><ul><li><a href=https://youtu.be/Yup1FUc2Qn0>Intro: Open Policy Agent Gatekeeper</a></li><li><a href=https://youtu.be/n94_FNhuzy4>Deep Dive: Open Policy Agent</a></li></ul><h2 id=motivations>Motivations</h2><p>If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?</p><p>For example, you can enforce policies like:</p><ul><li>All images must be from approved repositories</li><li>All ingress hostnames must be globally unique</li><li>All pods must have resource limits</li><li>All namespaces must have a label that lists a point-of-contact</li></ul><p>Kubernetes allows decoupling policy decisions from the API server by means of <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>admission controller webhooks</a> to intercept admission requests before they are persisted as objects in Kubernetes. <a href=https://github.com/open-policy-agent/gatekeeper>Gatekeeper</a> was created to enable users to customize admission control via configuration, not code and to bring awareness of the cluster’s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the <a href=https://www.openpolicyagent.org>Open Policy Agent (OPA)</a>, a policy engine for Cloud Native environments hosted by CNCF.</p><h2 id=evolution>Evolution</h2><p>Before we dive into the current state of Gatekeeper, let’s take a look at how the Gatekeeper project has evolved.</p><ul><li>Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.</li><li>Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.</li><li>Gatekeeper v3.0 - The admission controller is integrated with the <a href=https://github.com/open-policy-agent/frameworks/tree/master/constraint>OPA Constraint Framework</a> to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for <a href=https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/>Rego</a> policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.</li></ul><p><img src=/images/blog/2019-08-06-opa-gatekeeper/v3.png alt></p><h2 id=gatekeeper-v3-0-features>Gatekeeper v3.0 Features</h2><p>Now let’s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the object’s labels. How can you do this with Gatekeeper?</p><h3 id=validating-admission-control>Validating Admission Control</h3><p>Once all the Gatekeeper components have been <a href=https://github.com/open-policy-agent/gatekeeper>installed</a> in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.</p><p>During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.</p><h3 id=policies-and-constraints>Policies and Constraints</h3><p>With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.</p><p>Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.</p><p>For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>templates.gatekeeper.sh/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConstraintTemplate<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>k8srequiredlabels<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>crd</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>names</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>K8sRequiredLabels<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>listKind</span>:<span style=color:#bbb> </span>K8sRequiredLabelsList<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>plural</span>:<span style=color:#bbb> </span>k8srequiredlabels<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>singular</span>:<span style=color:#bbb> </span>k8srequiredlabels<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>validation</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Schema for the `parameters` field</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>openAPIV3Schema</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>array<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targets</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>target</span>:<span style=color:#bbb> </span>admission.k8s.gatekeeper.sh<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>rego</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>        package k8srequiredlabels
</span><span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>        deny[{&#34;msg&#34;: msg, &#34;details&#34;: {&#34;missing_labels&#34;: missing}}] {
</span><span style=color:#b44;font-style:italic>          provided := {label | input.review.object.metadata.labels[label]}
</span><span style=color:#b44;font-style:italic>          required := {label | label := input.parameters.labels[_]}
</span><span style=color:#b44;font-style:italic>          missing := required - provided
</span><span style=color:#b44;font-style:italic>          count(missing) &gt; 0
</span><span style=color:#b44;font-style:italic>          msg := sprintf(&#34;you must provide labels: %v&#34;, [missing])
</span><span style=color:#b44;font-style:italic>        }</span><span style=color:#bbb>        
</span></code></pre></div><p>Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label <code>hr</code> to be present on all namespaces.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>constraints.gatekeeper.sh/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>K8sRequiredLabels<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ns-must-have-hr<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>match</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;Namespace&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;hr&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>Similarly, another Constraint CRD that requires the label <code>finance</code> to be present on all namespaces can easily be created from the same Constraint template.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>constraints.gatekeeper.sh/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>K8sRequiredLabels<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ns-must-have-finance<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>match</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;Namespace&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;finance&#34;</span>]<span style=color:#bbb>
</span></code></pre></div><p>As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.</p><h3 id=audit>Audit</h3><p>The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as <code>violations</code> listed in the <code>status</code> field of the relevant Constraint.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>constraints.gatekeeper.sh/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>K8sRequiredLabels<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ns-must-have-hr<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>match</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kinds</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;Namespace&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;hr&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>auditTimestamp</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2019-08-06T01:46:13Z&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>byPod</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>enforced</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>id</span>:<span style=color:#bbb> </span>gatekeeper-controller-manager-0<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>violations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>enforcementAction</span>:<span style=color:#bbb> </span>deny<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Namespace<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>message: &#39;you must provide labels</span>:<span style=color:#bbb> </span>{<span style=color:#b44>&#34;hr&#34;</span>}<span style=color:#b44>&#39;
</span><span style=color:#b44>    name: default
</span><span style=color:#b44>  - enforcementAction: deny
</span><span style=color:#b44>    kind: Namespace
</span><span style=color:#b44>    message: &#39;</span><span style=color:green;font-weight:700>you must provide labels</span>:<span style=color:#bbb> </span>{<span style=color:#b44>&#34;hr&#34;</span>}<span style=color:#b44>&#39;
</span><span style=color:#b44>    name: gatekeeper-system
</span><span style=color:#b44>  - enforcementAction: deny
</span><span style=color:#b44>    kind: Namespace
</span><span style=color:#b44>    message: &#39;</span><span style=color:green;font-weight:700>you must provide labels</span>:<span style=color:#bbb> </span>{<span style=color:#b44>&#34;hr&#34;</span>}<span style=color:#b44>&#39;
</span><span style=color:#b44>    name: kube-public
</span><span style=color:#b44>  - enforcementAction: deny
</span><span style=color:#b44>    kind: Namespace
</span><span style=color:#b44>    message: &#39;</span><span style=color:green;font-weight:700>you must provide labels</span>:<span style=color:#bbb> </span>{<span style=color:#b44>&#34;hr&#34;</span>}&#39;<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></code></pre></div><h3 id=data-replication>Data Replication</h3><p>Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.</p><p>To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>config.gatekeeper.sh/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Config<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;gatekeeper-system&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>sync</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>syncOnly</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>group</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;v1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Namespace&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>group</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;v1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Pod&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=planned-for-future>Planned for Future</h2><p>The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.</p><p>If you are interested in learning more about the project, check out the <a href=https://github.com/open-policy-agent/gatekeeper>Gatekeeper</a> repo. If you are interested in helping define the direction of Gatekeeper, join the <a href=https://openpolicyagent.slack.com/messages/CDTN970AX>#kubernetes-policy</a> channel on OPA Slack, and join our <a href=https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit>weekly meetings</a> to discuss development, issues, use cases, etc.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0ead202f9f4c8cc2966564a0649a52f6>Get started with Kubernetes (using Python)</h1><div class="td-byline mb-4"><time datetime=2019-07-23 class=text-muted>Tuesday, July 23, 2019</time></div><p><strong>Author</strong>: Jason Haley (Independent Consultant)</p><p>So, you know you want to run your application in Kubernetes but don’t know where to start. Or maybe you’re getting started but still don’t know what you don’t know. In this blog you’ll walk through how to containerize an application and get it running in Kubernetes.</p><p>This walk-through assumes you are a developer or at least comfortable with the command line (preferably bash shell).</p><h2 id=what-we-ll-do>What we’ll do</h2><ol><li>Get the code and run the application locally</li><li>Create an image and run the application in Docker</li><li>Create a deployment and run the application in Kubernetes</li></ol><h2 id=prerequisites>Prerequisites</h2><ul><li>A Kubernetes service - I'm using <a href=https://www.docker.com/products/kubernetes>Docker Desktop with Kubernetes</a> in this walkthrough, but you can use one of the others. See <a href=https://kubernetes.io/docs/setup/>Getting Started</a> for a full listing.</li><li><a href=https://www.python.org/>Python 3.7</a> installed</li><li><a href=https://git-scm.com/downloads>Git</a> installed</li></ul><h2 id=containerizing-an-application>Containerizing an application</h2><p>In this section you’ll take some source code, verify it runs locally, and then create a Docker image of the application. The sample application used is a very simple Flask web application; if you want to test it locally, you’ll need Python installed. Otherwise, you can skip to the "Create a Dockerfile" section.</p><h3 id=get-the-application-code>Get the application code</h3><p>Use git to clone the repository to your local machine:</p><pre><code>git clone https://github.com/JasonHaley/hello-python.git
</code></pre><p>Change to the app directory:</p><pre><code>cd hello-python/app
</code></pre><p>There are only two files in this directory. If you look at the main.py file, you’ll see the application prints out a hello message. You can learn more about Flask on the <a href=http://flask.pocoo.org/>Flask website</a>.</p><pre><code>from flask import Flask
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    return &quot;Hello from Python!&quot;

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0')
</code></pre><p>The requirements.txt file contains the list of packages needed by the main.py and will be used by <a href=https://pip.pypa.io/en/stable/>pip</a> to install the Flask library.</p><blockquote class="note callout"><div><strong>Note:</strong> When you start writing more advanced Python, you'll find it's not always recommended to use <code>pip install</code> and may want to use <code>virtualenv</code> (or <code>pyenv</code>) to install your dependencies in a virtual environment.</div></blockquote><h3 id=run-locally>Run locally</h3><p>Manually run the installer and application using the following commands:</p><pre><code>pip install -r requirements.txt
python main.py
</code></pre><p>This will start a development web server hosting your application, which you will be able to see by navigating to http://localhost:5000. Because port 5000 is the default port for the development server, we didn’t need to specify it.</p><h3 id=create-a-dockerfile>Create a Dockerfile</h3><p>Now that you have verified the source code works, the first step in containerizing the application is to create a Dockerfile.</p><p>In the hello-python/app directory, create a file named Dockerfile with the following contents and save it:</p><pre><code>FROM python:3.7

RUN mkdir /app
WORKDIR /app
ADD . /app/
RUN pip install -r requirements.txt

EXPOSE 5000
CMD [&quot;python&quot;, &quot;/app/main.py&quot;]
</code></pre><p>This file is a set of instructions Docker will use to build the image. For this simple application, Docker is going to:</p><ol><li>Get the official <a href=https://hub.docker.com/_/python/>Python Base Image</a> for version 3.7 from Docker Hub.</li><li>In the image, create a directory named app.</li><li>Set the working directory to that new app directory.</li><li>Copy the local directory’s contents to that new folder into the image.</li><li>Run the pip installer (just like we did earlier) to pull the requirements into the image.</li><li>Inform Docker the container listens on port 5000.</li><li>Configure the starting command to use when the container starts.</li></ol><h3 id=create-an-image>Create an image</h3><p>At your command line or shell, in the hello-python/app directory, build the image with the following command:</p><pre><code>docker build -f Dockerfile -t hello-python:latest .
</code></pre><blockquote class="note callout"><div><strong>Note:</strong> I'm using the :latest tag in this example, if you are not familiar with what it is you may want to read <a href=https://container-solutions.com/docker-latest-confusion/>Docker: The latest Confusion</a>.</div></blockquote><p>This will perform those seven steps listed above and create the image. To verify the image was created, run the following command:</p><pre><code>docker image ls
</code></pre><p><img src=/images/blog/get-started-with-kubernetes-using-python/docker-image-ls.png alt="Docker image listing"></p><p>The application is now containerized, which means it can now run in Docker and Kubernetes!</p><h2 id=running-in-docker>Running in Docker</h2><p>Before jumping into Kubernetes, let’s verify it works in Docker.
Run the following command to have Docker run the application in a container and map it to port 5001:</p><pre><code>docker run -p 5001:5000 hello-python
</code></pre><p>Now navigate to http://localhost:5001, and you should see the “Hello form Python!” message.</p><h3 id=more-info>More info</h3><ul><li><a href=https://docs.docker.com/get-started/>Get started with Docker</a></li><li><a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/>Best practices for writing Dockerfiles</a></li><li><a href=https://www.docker.com/sites/default/files/Docker_CheatSheet_08.09.2016_0.pdf>Docker Cheat Sheet</a> (pdf)</li></ul><h2 id=running-in-kubernetes>Running in Kubernetes</h2><p>You are finally ready to get the application running in Kubernetes. Because you have a web application, you will create a service and a deployment.</p><p>First verify your kubectl is configured. At the command line, type the following:</p><pre><code>kubectl version
</code></pre><p>If you don’t see a reply with a Client and Server version, you’ll need to <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>install</a> and configure it.</p><p>If you are running on Windows or Mac, make sure it is using the Docker for Desktop context by running the following:</p><pre><code>kubectl config use-context docker-for-desktop
</code></pre><p>Now you are working with Kubernetes! You can see the node by typing:</p><pre><code>kubectl get nodes
</code></pre><p>Now let’s have it run the application. Create a file named deployment.yaml and add the following contents to it and then save it:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello-python-service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>hello-python<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;TCP&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>5000</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello-python<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>hello-python<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>4</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>hello-python<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello-python<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>hello-python:latest<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>5000</span><span style=color:#bbb>
</span></code></pre></div><p>This YAML file is the instructions to Kubernetes for what you want running. It is telling Kubernetes the following:</p><ul><li>You want a load-balanced service exposing port 6000</li><li>You want four instances of the hello-python container running</li></ul><p>Use kubectl to send the YAML file to Kubernetes by running the following command:</p><pre><code>kubectl apply -f deployment.yaml
</code></pre><p>You can see the pods are running if you execute the following command:</p><pre><code>kubectl get pods
</code></pre><p><img src=/images/blog/get-started-with-kubernetes-using-python/kubectl-get-pods.png alt="Pod listing"></p><p>Now navigate to http://localhost:6000, and you should see the “Hello form Python!” message.</p><p>That’s it! The application is now running in Kubernetes!</p><h3 id=more-info-1>More Info</h3><ul><li><a href=https://azure.microsoft.com/en-us/topic/kubernetes/>Learn Kubernetes Basics</a></li><li><a href=https://kubernetes.io/docs/reference/kubectl/cheatsheet/>kubectl Cheat Sheet</a></li><li><a href=https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/>kubectl for Docker Users</a></li></ul><h2 id=summary>Summary</h2><p>In this walk-through, we containerized an application, and got it running in Docker and in Kubernetes. This simple application only scratches the surface of what’s possible (and what you’ll need to learn).</p><h3 id=next-steps>Next steps</h3><p>If you are just getting started and this walk-through was useful to you, then the following resources should be good next steps for you to further expand your Kubernetes knowledge:</p><ul><li><a href="https://www.youtube.com/watch?v=1xo-0gCVhTU">Introduction to Microservices, Docker, and Kubernetes</a> - 55-minute video by James Quigley<ul><li>This is a great place to start because it provides more information than I could here.</li></ul></li><li><a href=https://github.com/PacktPublishing/Containerize-your-Apps-with-Docker-and-Kubernetes>Containerize your Apps with Docker and Kubernetes</a> - free e-book by Dr Gabriel N Schenker<ul><li>This is my favorite book on Docker and Kubernetes.</li></ul></li><li><a href=https://aka.ms/LearnKubernetes>Kubernetes Learning Path: 50 days from zero to hero with Kubernetes</a> - on Microsoft’s site<ul><li>This is a 10-page pdf that has tons of links to videos (with Brendan Burns), documentation sites, and a really good workshop for Azure Kubernetes Service.</li></ul></li></ul><h2 id=how-to-enable-kubernetes-in-docker-desktop>How to enable Kubernetes in Docker Desktop</h2><p>Once you have Docker Desktop installed, open the Settings:</p><p><img src=/images/blog/get-started-with-kubernetes-using-python/docker-settings-menu.png alt="Docker settings menu"></p><p>Select the <strong>Kubernetes</strong> menu item on the left and verify that the <strong>Enable Kubernetes</strong> is checked. If it isn’t, <strong>check it</strong> and click the <strong>Apply</strong> button at the bottom right:</p><p><img src=/images/blog/get-started-with-kubernetes-using-python/kubernetes-tab.png alt="Kubernetes tab"></p></div><div class=td-content style=page-break-before:always><h1 id=pg-21fa818ae63e4f4893fae6a94d6d008e>Deprecated APIs Removed In 1.16: Here’s What You Need To Know</h1><div class="td-byline mb-4"><time datetime=2019-07-18 class=text-muted>Thursday, July 18, 2019</time></div><p><strong>Author</strong>: Vallery Lancey (Lyft)</p><p>As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old API is deprecated and eventually removed.</p><p>The <strong>v1.16</strong> release will stop serving the following deprecated API versions in favor of newer and more stable API versions:</p><ul><li>NetworkPolicy in the <strong>extensions/v1beta1</strong> API version is no longer served<ul><li>Migrate to use the <strong>networking.k8s.io/v1</strong> API version, available since v1.8.
Existing persisted data can be retrieved/updated via the new version.</li></ul></li><li>PodSecurityPolicy in the <strong>extensions/v1beta1</strong> API version<ul><li>Migrate to use the <strong>policy/v1beta1</strong> API, available since v1.10.
Existing persisted data can be retrieved/updated via the new version.</li></ul></li><li>DaemonSet in the <strong>extensions/v1beta1</strong> and <strong>apps/v1beta2</strong> API versions is no longer served<ul><li>Migrate to use the <strong>apps/v1</strong> API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.</li><li>Notable changes:<ul><li><code>spec.templateGeneration</code> is removed</li><li><code>spec.selector</code> is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades</li><li><code>spec.updateStrategy.type</code> now defaults to <code>RollingUpdate</code> (the default in <code>extensions/v1beta1</code> was <code>OnDelete</code>)</li></ul></li></ul></li><li>Deployment in the <strong>extensions/v1beta1</strong>, <strong>apps/v1beta1</strong>, and <strong>apps/v1beta2</strong> API versions is no longer served<ul><li>Migrate to use the <strong>apps/v1</strong> API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.</li><li>Notable changes:<ul><li><code>spec.rollbackTo</code> is removed</li><li><code>spec.selector</code> is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades</li><li><code>spec.progressDeadlineSeconds</code> now defaults to <code>600</code> seconds (the default in <code>extensions/v1beta1</code> was no deadline)</li><li><code>spec.revisionHistoryLimit</code> now defaults to <code>10</code> (the default in <code>apps/v1beta1</code> was <code>2</code>, the default in <code>extensions/v1beta1</code> was to retain all)</li><li><code>maxSurge</code> and <code>maxUnavailable</code> now default to <code>25%</code> (the default in <code>extensions/v1beta1</code> was <code>1</code>)</li></ul></li></ul></li><li>StatefulSet in the <strong>apps/v1beta1</strong> and <strong>apps/v1beta2</strong> API versions is no longer served<ul><li>Migrate to use the <strong>apps/v1</strong> API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.</li><li>Notable changes:<ul><li><code>spec.selector</code> is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades</li><li><code>spec.updateStrategy.type</code> now defaults to <code>RollingUpdate</code> (the default in <code>apps/v1beta1</code> was <code>OnDelete</code>)</li></ul></li></ul></li><li>ReplicaSet in the <strong>extensions/v1beta1</strong>, <strong>apps/v1beta1</strong>, and <strong>apps/v1beta2</strong> API versions is no longer served<ul><li>Migrate to use the <strong>apps/v1</strong> API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.</li><li>Notable changes:<ul><li><code>spec.selector</code> is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades</li></ul></li></ul></li></ul><p>The <strong>v1.22</strong> release will stop serving the following deprecated API versions in favor of newer and more stable API versions:</p><ul><li>Ingress in the <strong>extensions/v1beta1</strong> API version will no longer be served<ul><li>Migrate to use the <strong>networking.k8s.io/v1beta1</strong> API version, available since v1.14.
Existing persisted data can be retrieved/updated via the new version.</li></ul></li></ul><h1 id=what-to-do>What To Do</h1><p>Kubernetes 1.16 is due to be released in September 2019, so be sure to audit
your configuration and integrations now!</p><ul><li>Change YAML files to reference the newer APIs</li><li>Update custom integrations and controllers to call the newer APIs</li><li>Update third party tools (ingress controllers, continuous delivery systems)
to call the newer APIs</li></ul><p>Migrating to the new Ingress API will only require changing the API path - the
API fields remain the same. However, migrating other resources (EG Deployments)
will require some updates based on changed fields. You can use the
<code>kubectl convert</code> command to automatically convert an existing object:
<code>kubectl convert -f &lt;file> --output-version &lt;group>/&lt;version></code>.</p><p>For example, to convert
an older Deployment to apps/v1, you can run:
<code>kubectl convert -f ./my-deployment.yaml --output-version apps/v1</code>
Note that this may use non-ideal default values. To learn more about a specific
resource, check the Kubernetes <a href=https://kubernetes.io/docs/reference/#api-reference>api reference</a>.</p><p>You can test your clusters by starting an apiserver with the above resources
disabled, to simulate the upcoming removal. Add the following flag to the
apiserver startup arguments:</p><p><code>--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false</code></p><h1 id=want-to-know-more>Want To Know More?</h1><p>Deprecations are announced in the Kubernetes release notes. You can see these
announcements in
<a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.14.md#deprecations>1.14</a>
and <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#deprecations-and-removals>1.15</a>.</p><p>You can read more <a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api>in our deprecation policy document</a>
about the deprecation policies for Kubernetes APIs, and other Kubernetes components.
Deprecation policies vary by component (for example, the primary APIs vs.
admin CLIs) and by maturity (alpha, beta, or GA).</p><p>These details were also <a href=https://groups.google.com/forum/#!topic/kubernetes-dev/je0rjyfTVyc>previously announced</a>
on the kubernetes-dev mailing list, along with the releases of Kubernetes 1.14
and 1.15. From Jordan Liggitt:</p><pre><code>In case you missed it in the 1.15.0 release notes, the timelines for deprecated resources in the extensions/v1beta1, apps/v1beta1, and apps/v1beta2 API groups to no longer be served by default have been updated:

* NetworkPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the networking.k8s.io/v1 API, available since v1.8. Existing persisted data can be retrieved/updated via the networking.k8s.io/v1 API.
* PodSecurityPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the policy/v1beta1 API, available since v1.10. Existing persisted data can be retrieved/updated via the policy/v1beta1 API.
* DaemonSet, Deployment, StatefulSet, and ReplicaSet resources will no longer be served from extensions/v1beta1, apps/v1beta1, or apps/v1beta2 by default in v1.16. Migrate to the apps/v1 API, available since v1.9. Existing persisted data can be retrieved/updated via the apps/v1 API.

To start a v1.15.0 API server with these resources disabled to flush out dependencies on these deprecated APIs, and ensure your application/manifests will work properly against the v1.16 release, use the following --runtime-config argument:

--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-5b056cc37326bb16c7334dca184370f4>Recap of Kubernetes Contributor Summit Barcelona 2019</h1><div class="td-byline mb-4"><time datetime=2019-06-25 class=text-muted>Tuesday, June 25, 2019</time></div><p><strong>Author</strong>: Jonas Rosland (VMware)</p><p>First of all, <strong>THANK YOU</strong> to everyone who made the Kubernetes Contributor Summit in Barcelona possible. We had an amazing team of volunteers tasked with planning and executing the event, and it was so much fun meeting and talking to all new and current contributors during the main event and the pre-event celebration.</p><p>Contributor Summit in Barcelona kicked off KubeCon + CloudNativeCon in a big way as it was the <strong>largest contributor summit</strong> to date with 331 people signed up, and only 9 didn't pick up their badges!</p><h2 id=contributor-celebration>Contributor Celebration</h2><p>Sunday evening before the main event we held a <strong>Contributor Celebration</strong>, which was very well attended. We hope that all new and current contributors felt welcome and enjoyed the food, the music, and the company.</p><p><img src=https://live.staticflickr.com/65535/46981485515_561bb324b2_z.jpg alt=contributor-celebration2>
<img src=https://live.staticflickr.com/65535/46981484655_8122564557_z.jpg alt=contributor-celebration></p><h2 id=new-contributor-workshops>New Contributor Workshops</h2><p>We had over <strong>130 people registered</strong> for the New Contributor Workshops. This year the workshops were divided into <em>101-level content</em> for people who were not familiar with contributing to an open source project, and <em>201-level content</em> for those who were.</p><p>The workshops contained overviews of what SIGs are, deep-dives into the codebase, test builds of the Kubernetes project, and real contributions.</p><p>Did you miss something during the workshops? We now have them <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP2WTJ6P8sQenhf0RY-JqF5L">published on YouTube</a>, with added closed captioning!</p><p><img src=https://live.staticflickr.com/65535/47897543541_a57d3b9ac9_z.jpg alt=img>
<img src=https://live.staticflickr.com/65535/47108247174_5d60b60846_z.jpg alt=img></p><h2 id=sig-face-to-face>SIG Face-to-Face</h2><p>We also tried a new thing for Barcelona, the SIG Face-to-Face meetings. We had <strong>over 170 people</strong> registered to attend the 11 SIG and one subproject meetings throughout the day, going over what they're working on and what they want to do in the near future.</p><p><img src=https://live.staticflickr.com/65535/47108254124_248a80ef1a_z.jpg alt=img>
<img src=https://live.staticflickr.com/65535/47108250434_88afdf930a_z.jpg alt=img></p><h2 id=sig-meet-and-greet>SIG Meet and Greet</h2><p>At the end of the summit, both new and current contributors had a chance to sit down with SIG chairs and members. The goal of this was to make sure that contributors got to know even more individuals in the project, hear what some of the SIGs actually do, and sign up to be a part of them and learn more.</p><p><img src=https://live.staticflickr.com/65535/47108248464_97eb2bbbb6_k.jpg alt=img>
<img src=https://live.staticflickr.com/65535/47845452032_a3d478beb9_k.jpg alt=img></p><h2 id=join-us>Join us!</h2><p>Interested in attending the Contributor Summit in San Diego? <a href=https://events.linuxfoundation.org/events/contributor-summit-north-america-2019/>You can get more information on our event page</a>, sign up and we will notify you when registration opens.</p><h2 id=thanks>Thanks!</h2><p>Again, thank you to everyone for making this an amazing event, and we're looking forward to seeing you next time!</p><p>To our Barcelona crew, you ROCK! 🥁</p><p>Paris Pittman, Bob Killen, Guinevere Saenger, Tim Pepper, Deb Giles, Ihor Dvoretskyi, Jorge Castro, Noah Kantrowitz, Dawn Foster, Ruben Orduz, Josh Berkus, Kiran Mova, Bart Smykla, Rostislav Georgiev, Jeffrey Sica, Rael Garcia, Silvia Moura Pina, Arnaud Meukam, Jason DeTiberius, Andy Goldstein, Suzanne Ambiel, Jonas Rosland</p><p>You can see many more pictures from the event <a href=https://www.flickr.com/photos/143247548@N03/sets/72157680323974628>over on CNCF's Flickr</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0f38c170d1a63c8632eb3ec160e4f619>Automated High Availability in kubeadm v1.15: Batteries Included But Swappable</h1><div class="td-byline mb-4"><time datetime=2019-06-24 class=text-muted>Monday, June 24, 2019</time></div><p><strong>Authors</strong>:</p><ul><li>Lucas Käldström, <a href=https://github.com/luxas>@luxas</a>, SIG Cluster Lifecycle co-chair & kubeadm subproject owner, Weaveworks</li><li>Fabrizio Pandini, <a href=https://github.com/fabriziopandini>@fabriziopandini</a>, kubeadm subproject owner, Independent</li></ul><p><a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a> is a tool that enables Kubernetes administrators
to quickly and easily bootstrap minimum viable clusters that are fully compliant with
<a href=https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md>Certified Kubernetes</a> guidelines.
It’s been under active development by <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG Cluster Lifecycle</a>
since 2016 and graduated it from beta to
<a href=https://kubernetes.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/>generally available (GA) at the end of 2018</a>.</p><p>After this important milestone, the kubeadm team is now focused on the stability of the core feature set and working on
maturing existing features.</p><p>With this post, we are introducing the improvements made in the v1.15 release of kubeadm.</p><h2 id=the-scope-of-kubeadm>The scope of kubeadm</h2><p>kubeadm is focused on performing the actions necessary to get a minimum viable, secure cluster up and running in a
user-friendly way. kubeadm's scope is limited to the local machine’s filesystem and the Kubernetes API, and it is
intended to be a <em>composable building block for higher-level tools</em>.</p><p>The core of the kubeadm interface is quite simple: new control plane nodes are created by you running
<strong><code>kubeadm init</code></strong>, worker nodes are joined to the control plane by you running
<strong><code>kubeadm join</code></strong>. Also included are common utilities for managing already bootstrapped
clusters, such as control plane upgrades, token and certificate renewal.</p><p>To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of scope:</p><ul><li>Infrastructure provisioning</li><li>Third-party networking</li><li>Non-critical add-ons, e.g. monitoring, logging, and visualization</li><li>Specific cloud provider integrations</li></ul><p>Those tasks are addressed by other SIG Cluster Lifecycle projects, such as the
<a href=https://github.com/kubernetes-sigs/cluster-api>Cluster API</a> for infrastructure provisioning and management.</p><p>Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the
<a href=/docs/concepts/overview/components/#control-plane-components>control plane</a>.</p><p><img src=/images/blog/2019-06-24-kubeadm-ha-v115/overview.png alt="Cluster Lifecycle Layers"></p><h2 id=what-s-new-in-kubeadm-v1-15>What’s new in kubeadm v1.15?</h2><h3 id=high-availability-to-beta>High Availability to Beta</h3><p>We are delighted to announce that automated support for High Availability clusters is graduating to <strong>Beta</strong> in kubeadm v1.15. Let’s give a great shout out to all the contributors that helped in this effort and to the early adopter users for the great feedback received so far!</p><p>But how does automated High Availability work in kubeadm?</p><p>The great news is that you can use the familiar <code>kubeadm init</code> or <code>kubeadm join</code> workflow for creating high availability cluster as well, with the only difference that you have to pass the <code>--control-plane</code> flag to <code>kubeadm join</code> when adding more control plane nodes.</p><p>A 3-minute screencast of this feature is here:</p><p><a href=https://asciinema.org/a/252343><img src=https://asciinema.org/a/252343.svg alt=asciicast></a></p><p>In a nutshell:</p><ol><li><p><strong>Set up a Load Balancer.</strong> You need an <em>external load balancer</em>; providing this however, is out of scope of kubeadm.</p><ul><li>The community will provide a set of reference implementations for this task though</li><li>HAproxy, Envoy, or a similar Load Balancer from a cloud provider work well</li></ul></li><li><p><strong>Run kubeadm init</strong> on the first control plane node, with small modifications:</p><ul><li>Create a <a href=https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>kubeadm Config File</a></li><li>In the config file, set the <code>controlPlaneEndpoint</code> field to where your Load Balancer can be reached at.</li><li>Run init, with the <code>--upload-certs</code> flag like this: <code>sudo kubeadm init --config=kubeadm-config.yaml --upload-certs</code></li></ul></li><li><p><strong>Run kubeadm join --control-plane</strong> at any time when you want to expand the set of control plane nodes</p><ul><li>Both control-plane- and normal nodes can be joined in any order, at any time</li><li>The command to run will be given by <code>kubeadm init</code> above, and is of the form:</li></ul><pre><code>kubeadm join [LB endpoint] \
   --token ... \                                                                                               
   --discovery-token-ca-cert-hash sha256:... \                                                             
   --control-plane --certificate-key ...  
</code></pre></li></ol><p>For those interested in the details, there are many things that make this functionality possible. Most notably:</p><ul><li><p><strong>Automated certificate transfer</strong>. kubeadm implements an automatic certificate copy feature to automate the distribution of all the certificate authorities/keys that must be shared across all the control-planes nodes in order to get your cluster to work. This feature can be activated by passing <code>--upload-certs</code> to <code>kubeadm init</code>; see <a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/>configure and deploy an HA control plane</a> for more details. This is an explicit opt-in feature, you can also distribute the certificates manually in your preferred way. \</p></li><li><p><strong>Dynamically-growing etcd cluster</strong>. When you're not providing an external etcd cluster, kubeadm automatically adds a new etcd member, running as a static pod. All the etcd members are joined in a “stacked” etcd cluster that grows together with your high availability control-plane \</p></li><li><p><strong>Concurrent joining</strong>. Similarly to what already implemented for worker nodes, you join control-plane nodes whenever, in any order, or even in parallel. \</p></li><li><p><strong>Upgradable</strong>. The kubeadm upgrade workflow was improved in order to properly handle the HA scenario, and, after starting the upgrade with <code>kubeadm upgrade apply</code> as usual, users can now complete the upgrade process by using <code>kubeadm upgrade node</code> both on the remaining control-plane nodes and worker nodes</p></li></ul><p>Finally, it is also worthy to notice that an entirely new test suite has been created specifically for ensuring High Availability in kubeadm will stay stable over time.</p><h3 id=certificate-management>Certificate Management</h3><p>Certificate management has become more simple and robust in kubeadm v1.15.</p><p>If you perform Kubernetes version upgrades regularly, kubeadm will now take care of keeping your cluster up to date and reasonably secure by <a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal>automatically rotating all your certificates</a> at <code>kubeadm upgrade</code> time.</p><p>If instead, you prefer to renew your certificates manually, you can opt out from the automatic certificate renewal by passing <code>--certificate-renewal=false</code> to <code>kubeadm upgrade</code> commands. Then you can perform <a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal>manual certificate renewal</a> with the <code>kubeadm alpha certs renew</code> command.</p><p>But there is more.</p><p>A new command <code>kubeadm alpha certs check-expiration</code> was introduced to allow users to
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#check-certificate-expiration>check certificate expiration</a>. The output is similar to this:</p><pre><code class=language-console data-lang=console>CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 May 15, 2020 13:03 UTC   364d            false
apiserver                  May 15, 2020 13:00 UTC   364d            false
apiserver-etcd-client      May 15, 2020 13:00 UTC   364d            false
apiserver-kubelet-client   May 15, 2020 13:00 UTC   364d            false
controller-manager.conf    May 15, 2020 13:03 UTC   364d            false
etcd-healthcheck-client    May 15, 2020 13:00 UTC   364d            false
etcd-peer                  May 15, 2020 13:00 UTC   364d            false
etcd-server                May 15, 2020 13:00 UTC   364d            false
front-proxy-client         May 15, 2020 13:00 UTC   364d            false
scheduler.conf             May 15, 2020 13:03 UTC   364d            false
</code></pre><p>You should expect also more work around certificate management in kubeadm in the next releases, with the introduction of ECDSA keys and with improved support for CA key rotation. Additionally, the commands staged under <code>kubeadm alpha</code> are expected to move top-level soon.</p><h3 id=improved-configuration-file-format>Improved Configuration File Format</h3><p>You can argue that there are hardly two Kubernetes clusters that are configured equally, and hence there is a need to customize how the cluster is set up depending on the environment. One way of configuring a component is via flags. However, this has some scalability limitations:</p><ul><li><strong>Hard to maintain.</strong> When a component’s flag set grows over 30+ flags, configuring it becomes really painful.</li><li><strong>Complex upgrades</strong>. When flags are removed, deprecated or changed, you need to upgrade of the binary at the same time as the arguments.</li><li><strong>Key-value limited</strong>. There are simply many types of configuration you can’t express with the <code>--key=value</code> syntax.</li><li><strong>Imperative</strong>. In contrast to Kubernetes API objects themselves that are declaratively specified, flag arguments are imperative by design.</li></ul><p>This is a key problem for Kubernetes components in general, as some components have 150+ flags. With kubeadm we’re pioneering the ComponentConfig effort, and providing users with a small set of flags, but most importantly, a <strong>declarative and versioned configuration file</strong> for advance use-cases. We call this <em>ComponentConfig</em>. It has the following characteristics:</p><ul><li><strong>Upgradable</strong>: You can upgrade the binary, and still use the existing, older schema. Automatic migrations.</li><li><strong>Programmable</strong>. Configuration expressed in JSON/YAML allows for consistent, and programmable manipulation</li><li><strong>Expressible</strong>. Advanced patterns of configuration can be used and applied.</li><li><strong>Declarative</strong>. OpenAPI information can easily be exposed / used for doc generation</li></ul><p>In kubeadm v1.15, we have improved the structure and are releasing the new <strong>v1beta2</strong> format. Important to note that the existing <strong>v1beta1</strong> format released in v1.13 will still continue to work for several releases. This means you can upgrade kubeadm to v1.15, and still use your existing v1beta1 configuration files. When you’re ready to take advantage of the improvements made in v1beta2, you can perform an automatic schema migration using the <code>kubeadm config migrate</code> command.</p><p>During the course of the year, we’re looking forward to graduate the schema to General Availability <code>v1</code>.` If you’re interested in this effort, you can also join <a href=https://github.com/kubernetes/community/tree/master/wg-component-standard>WG Component Standard</a>.</p><h2 id=what-s-next>What’s next?</h2><h3 id=2019-plans>2019 plans</h3><p>We are focusing our efforts around graduating the configuration file format to GA (<code>kubeadm.k8s.io/v1</code>)`, graduating this super-easy High Availability flow to stable, and providing better tools around rotating certificates needed for running the cluster automatically.</p><p>In addition to these three key milestones of our charter, we want to improve the following areas:</p><ul><li>Support joining Windows nodes to a kubeadm cluster (with end-to-end tests)</li><li>Improve the upstream CI signal, mainly for HA and upgrades</li><li>Consolidate how Kubernetes artifacts are built and installed</li><li>Utilize Kustomize to allow for advanced, layered and declarative configuration</li></ul><p>We make no guarantees that these deliverables will ship this year though, as this is a community effort. If you want to see these things happen, please join our SIG and start contributing! The ComponentConfig issues in particular need more attention.</p><h3 id=kubeadm-now-has-a-logo>kubeadm now has a logo!</h3><p><a href=https://github.com/dankohn>Dan Kohn</a> offered CNCF’s help with creating a logo for kubeadm in this cycle.
<a href=https://github.com/alexcontini>Alex Contini</a> created 19 (!) different logo options for the community to vote on. The public poll
was active for around a week, and we got 386 answers. The winning option got 17.4% of the votes. In other words, now we have an
official logo!</p><p><img src=/images/blog/2019-06-24-kubeadm-ha-v115/logo.png alt="kubeadm's logo"></p><h2 id=contributing>Contributing</h2><p>If this all sounds exciting, <strong>join us</strong>!</p><p><a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG Cluster Lifecycle</a> has many different
subprojects, where kubeadm is one of them. In the following picture you can see that there are many pieces in the
puzzle, and we have a lot still to do.</p><p><img src=/images/blog/2019-06-24-kubeadm-ha-v115/projects.png alt="SIG Cluster Lifecycle Projects"></p><p>Some handy links if you want to start contribute:</p><ul><li>You can watch the SIG Cluster Lifecycle <a href="https://www.youtube.com/watch?v=Bof9aveB3rA">New Contributor Onboarding</a> session on YouTube.</li><li>Look out for “good first issue”, “help wanted” and “sig/cluster-lifecycle” labeled issues in our repositories
(e.g. <a href=https://github.com/kubernetes/kubeadm>kubernetes/kubeadm</a>)</li><li>Join <a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>#sig-cluster-lifecycle</a>, <a href=https://kubernetes.slack.com/messages/kubeadm>#kubeadm</a>, <a href=https://kubernetes.slack.com/messages/cluster-api>#cluster-api</a>, <a href=https://kubernetes.slack.com/messages/minikube>#minikube</a>, <a href=https://kubernetes.slack.com/messages/kind>#kind</a>, etc. in Slack</li><li>Join our public, bi-weekly SIG Cluster Lifecycle Zoom meeting at Tuesdays 9am PT<ul><li>Check out the <a href=https://docs.google.com/document/d/1Gmc7LyCIL_148a9Tft7pdhdee0NBHdOfHS1SAF0duI4/edit>Meeting Notes</a> to join</li></ul></li><li>Join our public, weekly kubeadm Office Hours Zoom meeting at Wednesdays 9am PT<ul><li>Check out the <a href=https://docs.google.com/document/d/130_kiXjG7graFNSnIAgtMS1G8zPDwpkshgfRYS0nggo/edit>Meeting Notes</a> to join</li></ul></li><li>Check out the <a href=https://youtu.be/bA2M41J4wvg>SIG Cluster Lifecycle Intro</a> or the
<a href=https://youtu.be/spXSSIbZTqM>kubeadm Deep Dive</a> sessions from KubeCon Barcelona</li></ul><h3 id=thank-you>Thank You</h3><p>This release wouldn’t have been possible without the help of the great people that have been contributing to SIG Cluster Lifecycle
and kubeadm. We would like to thank all the kubeadm contributors and companies making it possible for their developers to work
on Kubernetes!</p><p>In particular, we would like to thank the <a href=https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/OWNERS>kubeadm subproject owners</a> that made this possible:</p><ul><li>Tim St. Clair , <a href=https://github.com/timothysc>@timothysc</a>, SIG Cluster Lifecycle co-chair, VMware</li><li>Lucas Käldström, <a href=https://github.com/luxas>@luxas</a>, SIG Cluster Lifecycle co-chair, Weaveworks</li><li>Fabrizio Pandini, <a href=https://github.com/fabriziopandini>@fabriziopandini</a>, Independent</li><li>Lubomir I. Ivanov, <a href=https://github.com/neolit123>@neolit123</a>, VMware</li><li>Rostislav M. Georgiev, <a href=https://github.com/rosti>@rosti</a>, VMware</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b240f0214be444bad86ee5626abfc100>Introducing Volume Cloning Alpha for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2019-06-21 class=text-muted>Friday, June 21, 2019</time></div><p><strong>Author</strong>: John Griffith (Red Hat)</p><p>Kubernetes v1.15 introduces alpha support for volume cloning. This feature allows you to create new volumes using the contents of existing volumes in the user's namespace using the Kubernetes API.</p><h2 id=what-is-a-clone>What is a Clone?</h2><p>Many storage systems provide the ability to create a "clone" of a volume. A clone is a duplicate of an existing volume that is its own unique volume on the system, but the data on the source is duplicated to the destination (clone). A clone is similar to a snapshot in that it's a point in time copy of a volume, however rather than creating a new snapshot object from a volume, we're instead creating a new independent volume, sometimes thought of as pre-populating the newly created volume.</p><h2 id=why-add-cloning-to-kubernetes>Why add cloning to Kubernetes</h2><p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.</p><p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no specific storage device knowledge.</p><p>The <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage SIG</a> identified clone operations as critical functionality for many stateful workloads. For example, a database administrator may want to duplicate a database volume and create another instance of an existing database.</p><p>By providing a standard way to trigger clone operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations). While cloning is similar in behavior to creating a snapshot of a volume, then creating a volume from the snapshot, a clone operation is more streamlined and is more efficient for many backend devices.</p><p>Kubernetes users are now empowered to incorporate clone operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.</p><h2 id=kubernetes-api-and-cloning>Kubernetes API and Cloning</h2><p>The cloning feature in Kubernetes is enabled via the <code>PersistentVolumeClaim.DataSource</code> field. Prior to v1.15 the only valid object type permitted for use as a dataSource was a <code>VolumeSnapshot</code>. The cloning feature extends the allowed <code>PersistentVolumeclaim.DataSource.Kind</code> field to not only allow <code>VolumeSnapshot</code> but also <code>PersistentVolumeClaim</code>. The existing behavior is not changed.</p><p>There are no new objects introduced to enable cloning. Instead, the existing dataSource field in the PersistentVolumeClaim object is expanded to be able to accept the name of an existing PersistentVolumeClaim in the same namespace. It is important to note that from a users perspective a clone is just another PersistentVolume and PersistentVolumeClaim, the only difference being that that PersistentVolume is being populated with the contents of another PersistentVolume at creation time. After creation it behaves exactly like any other Kubernetes PersistentVolume and adheres to the same behaviors and rules.</p><h2 id=which-volume-plugins-support-kubernetes-cloning>Which volume plugins support Kubernetes Cloning?</h2><p>Kubernetes supports three types of volume plugins: in-tree, Flex, and <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>Container Storage Interface</a> (CSI). See <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Kubernetes Volume Plugin FAQ</a> for details.</p><p>Cloning is only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes cloning feature, ensure that a CSI Driver that implements cloning is deployed on your cluster.
For a list of CSI drivers that currently support cloning see the <a href=https://kubernetes-csi.github.io/docs/drivers.html>CSI Drivers doc</a>.</p><h2 id=kubernetes-cloning-requirements>Kubernetes Cloning Requirements</h2><p>Before using Kubernetes Volume Cloning, you must:</p><ul><li>Ensure a CSI driver implementing Cloning is deployed and running on your Kubernetes cluster.</li><li>Enable the Kubernetes Volume Cloning feature via new Kubernetes feature gate (disabled by default for alpha):<ul><li>Set the following flag on the API server binary: <code>--feature-gates=VolumePVCDataSource=true</code></li></ul></li><li>The source and destination claims must be in the same namespace.</li></ul><h2 id=creating-a-clone-with-kubernetes>Creating a clone with Kubernetes</h2><p>To provision a new volume pre-populated with data from an existing Kubernetes Volume, use the dataSource field in the <code>PersistentVolumeClaim</code>. There are three parameters:</p><ul><li>name - name of the <code>PersistentVolumeClaim</code> object to use as source</li><li>kind - must be <code>PersistentVolumeClaim</code></li><li>apiGroup - must be <code>""</code></li></ul><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-clone
  Namespace: demo-namespace
spec:
  storageClassName: csi-storageclass
  dataSource:
    name: src-pvc
    kind: PersistentVolumeClaim 
    apiGroup: &quot;&quot;
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi # NOTE this capacity must be specified and must be &gt;= the capacity of the source volume
</code></pre><p>When the <code>PersistentVolumeClaim</code> object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified <code>dataSource</code> volume. It is the sole responsbility of the CSI Plugin to implement the cloning of volumes.</p><h2 id=as-a-storage-vendor-how-do-i-add-support-for-cloning-to-my-csi-driver>As a storage vendor, how do I add support for cloning to my CSI driver?</h2><p>For more information on how to implement cloning in your CSI Plugin, reference the <a href=https://kubernetes-csi.github.io/docs/developing.html>developing a CSI driver for Kubernetes</a> section of the CSI docs.</p><h2 id=what-are-the-limitations-of-alpha>What are the limitations of alpha?</h2><p>The alpha implementation of cloning for Kubernetes has the following limitations:</p><ul><li>Does not support cloning volumes across different namespaces</li><li>Does not support cloning volumes across different storage classes (backends)</li></ul><h2 id=future>Future</h2><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI cloning implementation to beta in 1.16.</p><p>A common question that users have regarding cloning is "what about cross namespace clones". As we've mentioned, the current release requires that source and destination be in the same namespace. There are however efforts underway to propose a namespace transfer API, future versions of Kubernetes may provide the ability to transfer volume resources from one namespace to another. This feature is still under discussion and design, and may or may not be available in a future release.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>You can find additional documentation on the cloning feature in the <a href=https://k8s.io/docs/concepts/storage/volume-pvc-datasource.md>storage concept docs</a> and also the <a href=https://kubernetes-csi.github.io/docs/volume-cloning.html>CSI docs</a>.</p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>We offer a huge thank you to all the contributors in Kubernetes Storage SIG and CSI community who helped review the design and implementation of the project, including but not limited to the following:</p><ul><li>Saad Ali (<a href=https://github.com/saadali>saadali</a>)</li><li>Tim Hockin (<a href=https://github.com/thockin>thockin</a>)</li><li>Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>msau42</a>)</li><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li></ul><p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a292df47eabb3613fa5e9bf7722de93b>Future of CRDs: Structural Schemas</h1><div class="td-byline mb-4"><time datetime=2019-06-20 class=text-muted>Thursday, June 20, 2019</time></div><p><strong>Authors:</strong> Stefan Schimanski (Red Hat)</p><p>CustomResourceDefinitions were introduced roughly two years ago as the primary way to extend the Kubernetes API with custom resources. From the beginning they stored arbitrary JSON data, with the exception that <code>kind</code>, <code>apiVersion</code> and <code>metadata</code> had to follow the Kubernetes API conventions. In Kubernetes 1.8 CRDs gained the ability to define an optional OpenAPI v3 based validation schema.</p><p>By the nature of OpenAPI specifications though—only describing what must be there, not what shouldn’t, and by being potentially incomplete specifications—the Kubernetes API server never knew the complete structure of CustomResource instances. As a consequence, kube-apiserver—until today—stores all JSON data received in an API request (if it validates against the OpenAPI spec). This especially includes anything that is not specified in the OpenAPI schema.</p><h2 id=the-story-of-malicious-unspecified-data>The story of malicious, unspecified data</h2><p>To understand this, we assume a CRD for maintenance jobs by the operations team, running each night as a service user:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>operations/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>MaintenanceNightlyJob<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb> </span>&gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    grep backdoor /etc/passwd || 
</span><span style=color:#b44;font-style:italic>    echo “backdoor:76asdfh76:/bin/bash” &gt;&gt; /etc/passwd || true</span><span style=color:#bbb>    
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>machines</span>:<span style=color:#bbb> </span>[“az1-master1”,”az1-master2”,”az2-master3”]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>privileged</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>The privileged field is not specified by the operations team. Their controller does not know it, and their validating admission webhook does not know about it either. Nevertheless, kube-apiserver persists this suspicious, but unknown field without ever validating it.</p><p>When run in the night, this job never fails, but because the service user is not able to write <code>/etc/passwd</code>, it will also not cause any harm.</p><p>The maintenance team needs support for privileged jobs. It adds the <code>privileged</code> support, but is super careful to implement authorization for privileged jobs by only allowing those to be created by very few people in the company. That malicious job though has long been persisted to etcd. The next night arrives and the malicious job is executed.</p><h2 id=towards-complete-knowledge-of-the-data-structure>Towards complete knowledge of the data structure</h2><p>This example shows that we cannot trust CustomResource data in etcd. Without having complete knowledge about the JSON structure, the kube-apsierver cannot do anything to prevent persistence of unknown data.</p><p>Kubernetes 1.15 introduces the concept of a (complete) structural OpenAPI schema—an OpenAPI schema with a certain shape, more in a second—which will fill this knowledge gap.</p><p>If the provided OpenAPI validation schema provided by the CRD author is not structural, violations are reported in a <code>NonStructural</code> condition in the CRD.</p><p>A structural schema for CRDs in <code>apiextensions.k8s.io/v1beta1</code> will not be required. But we plan to require structural schemas for every CRD created in <code>apiextensions.k8s.io/v1</code>, targeted for 1.16.</p><p>But now let us see what a structural schema looks like.</p><h2 id=structural-schema>Structural Schema</h2><p>The <strong>core of a structural schema</strong> is an OpenAPI v3 schema made out of</p><ul><li><code>properties</code></li><li><code>items</code></li><li><code>additionalProperties</code></li><li><code>type</code></li><li><code>nullable</code></li><li><code>title</code></li><li><code>descriptions</code>.</li></ul><p>In addition, all types must be non-empty, and in each sub-schema only one of <code>properties</code>, <code>additionalProperties</code> or <code>items</code> may be used.</p><p>Here is an example of our <code>MaintenanceNightlyJob</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb>    </span>properties<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>machines</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>array<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span></code></pre></div><p>This schema is structural because we only use the permitted OpenAPI constructs, and we specify each type.</p><p>Note that we leave out <code>apiVersion</code>, <code>kind</code> and <code>metadata</code>. These are implicitly defined for each object.</p><p>Starting from this structural core of our schema, we might enhance it for value validation purposes with nearly all other OpenAPI constructs, with only a few restrictions, for example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb>    </span>properties<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>minLength</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>                          </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>minLength</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>                          </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>machines</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>array<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>pattern</span>:<span style=color:#bbb> </span>“^[a-z0-9]+(-[a-z0-9]+)*$”<span style=color:#bbb> </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>oneOf</span>:<span style=color:#bbb>                                    </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“command”]                  <span style=color:#bbb> </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“shell”]                    <span style=color:#bbb> </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“spec”]                           <span style=color:#bbb> </span><span style=color:#080;font-style:italic># value validation</span><span style=color:#bbb>
</span></code></pre></div><p>Some notable restrictions for these additional value validations:</p><ul><li>the last 5 of the core constructs are not allowed: <code>additionalProperties</code>, <code>type</code>, <code>nullable</code>, <code>title</code>, <code>description</code></li><li>every properties field mentioned, must also show up in the core (without the blue value validations).</li></ul><p>As you can see also logical constraints using <code>oneOf</code>, <code>allOf</code>, <code>anyOf</code>, <code>not</code> are allowed.</p><p>To sum up, an OpenAPI schema is structural if<br><br></p><ol><li>it has the core as defined above out of <code>properties</code>, <code>items</code>, <code>additionalProperties</code>, <code>type</code>, <code>nullable</code>, <code>title</code>, <code>description</code>,<br></li><li>all types are defined,<br></li><li>the core is extended with value validation following the constraints:<br>(i) inside of value validations no <code>additionalProperties</code>, <code>type</code>, <code>nullable</code>, <code>title</code>, <code>description</code><br>(ii) all fields mentioned in value validation are specified in the core.</li></ol><p>Let us modify our example spec slightly, to make it non-structural:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb>    </span>properties<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>minLength</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>minLength</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>machines</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>array<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>pattern</span>:<span style=color:#bbb> </span>“^[a-z0-9]+(-[a-z0-9]+)*$”<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>oneOf</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“command”]<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“shell”]<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>not</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>properties</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>privileged</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb> </span>[“spec”]<span style=color:#bbb>
</span></code></pre></div><p>This spec is non-structural for many reasons:</p><ul><li><code>type: object</code> at the root is missing (rule 2).</li><li>inside of <code>oneOf</code> it is not allowed to use <code>type</code> (rule 3-i).</li><li>inside of <code>not</code> the property <code>privileged</code> is mentioned, but it is not specified in the core (rule 3-ii).</li></ul><p>Now that we know what a structural schema is, and what is not, let us take a look at our attempt above to forbid <code>privileged</code> as a field. While we have seen that this is not possible in a structural schema, the good news is that we don’t have to explicitly attempt to forbid unwanted fields in advance.</p><h2 id=pruning-don-t-preserve-unknown-fields>Pruning – don’t preserve unknown fields</h2><p>In <code>apiextensions.k8s.io/v1</code> pruning will be the default, with ways to opt-out of it. Pruning in <code>apiextensions.k8s.io/v1beta1</code> is enabled via</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiextensions/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>CustomResourceDefinition<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>…<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>preserveUnknownFields</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></code></pre></div><p>Pruning can only be enabled if the global schema or the schemas of all versions are structural.</p><p>If pruning is enabled, the pruning algorithm</p><ul><li>assumes that the schema is complete, i.e. every field is mentioned and not-mentioned fields can be dropped</li><li>is run on<br>(i) data received via an API request<br>(ii) after conversion and admission requests<br>(iii) when reading from etcd (using the schema version of the data in etcd).</li></ul><p>As we don’t specify <code>privileged</code> in our structural example schema, the malicious field is pruned from before persisting to etcd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>operations/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>MaintenanceNightlyJob<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>shell</span>:<span style=color:#bbb> </span>&gt;<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>    grep backdoor /etc/passwd || 
</span><span style=color:#b44;font-style:italic>    echo “backdoor:76asdfh76:/bin/bash” &gt;&gt; /etc/passwd || true</span><span style=color:#bbb>    
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>machines</span>:<span style=color:#bbb> </span>[“az1-master1”,”az1-master2”,”az2-master3”]<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># pruned: privileged: true</span><span style=color:#bbb>
</span></code></pre></div><h2 id=extensions>Extensions</h2><p>While most Kubernetes-like APIs can be expressed with a structural schema, there are a few exceptions, notably <code>intstr.IntOrString</code>, <code>runtime.RawExtension</code>s and pure JSON fields.</p><p>Because we want CRDs to make use of these types as well, we introduce the following OpenAPI vendor extensions to the permitted core constructs:</p><ul><li><p><code>x-kubernetes-embedded-resource: true</code> — specifies that this is an <code>runtime.RawExtension</code>-like field, with a Kubernetes resource with apiVersion, kind and metadata. The consequence is that those 3 fields are not pruned and are automatically validated.</p></li><li><p><code>x-kubernetes-int-or-string: true</code> — specifies that this is either an integer or a string. No types must be specified, but</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>oneOf</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>integer<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>string<span style=color:#bbb>
</span></code></pre></div><p>is permitted, though optional.</p></li><li><p><code>x-kubernetes-preserve-unknown-fields: true</code> — specifies that the pruning algorithm should not prune any field. This can be combined with <code>x-kubernetes-embedded-resource</code>. Note that within a nested <code>properties</code> or <code>additionalProperties</code> OpenAPI schema the pruning starts again.</p><p>One can use <code>x-kubernetes-preserve-unknown-fields: true</code> at the root of the schema (and inside any <code>properties</code>, <code>additionalProperties</code>) to get the traditional CRD behaviour that nothing is pruned, despite setting <code>spec.preserveUnknownProperties: false</code>.</p></li></ul><h2 id=conclusion>Conclusion</h2><p>With this we conclude the discussion of the structural schema in Kubernetes 1.15 and beyond. To sum up:</p><ul><li>structural schemas are optional in <code>apiextensions.k8s.io/v1beta1</code>. Non-structural CRDs will keep working as before.</li><li>pruning (enabled via <code>spec.preserveUnknownProperties: false</code>) requires a structural schema.</li><li>structural schema violations are signalled via the <code>NonStructural</code> condition in the CRD.</li></ul><p>Structural schemas are the future of CRDs. <code>apiextensions.k8s.io/v1</code> will require them. But</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>object<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>x-kubernetes-preserve-unknown-fields</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></code></pre></div><p>is a valid structural schema that will lead to the old schema-less behaviour.</p><p>Any new feature for CRDs starting from Kubernetes 1.15 will require to have a structural schema:</p><ul><li>publishing of OpenAPI validation schemas and therefore support for kubectl client-side validation, and <code>kubectl explain</code> support (beta in Kubernetes 1.15)</li><li>CRD conversion (beta in Kubernetes 1.15)</li><li>CRD defaulting (alpha in Kubernetes 1.15)</li><li>Server-side apply (alpha in Kubernetes 1.15, CRD support pending).</li></ul><p>Of course <a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema>structural schemas</a> are also described in the Kubernetes documentation for the 1.15 release.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-73c739c23ae254717a9e5f21cb90dc1e>Kubernetes 1.15: Extensibility and Continuous Improvement</h1><div class="td-byline mb-4"><time datetime=2019-06-19 class=text-muted>Wednesday, June 19, 2019</time></div><p><strong>Authors:</strong> The 1.15 <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.15/release_team.md>Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.15, our second release of 2019! Kubernetes 1.15 consists of 25 enhancements: 2 moving to stable, 13 in beta, and 10 in alpha. The main themes of this release are:</p><ul><li>Continuous Improvement<ul><li>Project sustainability is not just about features. Many SIGs have been working on improving test coverage, ensuring the basics stay reliable, and stability of the core feature set and working on maturing existing features and cleaning up the backlog.</li></ul></li><li>Extensibility<ul><li>The community has been asking for continuing support of extensibility, so this cycle features more work around CRDs and API Machinery. Most of the enhancements in this cycle were from SIG API Machinery and related areas.</li></ul></li></ul><p>Let’s dive into the key features of this release:</p><h2 id=extensibility-around-core-kubernetes-apis>Extensibility around core Kubernetes APIs</h2><p>The theme of the new developments around CustomResourceDefinitions is data consistency and native behaviour. A user should not notice whether the interaction is with a CustomResource or with a Golang-native resource. With big steps we are working towards a GA release of CRDs and GA of admission webhooks in one of the next releases.</p><p>In this direction, we have rethought our OpenAPI based validation schemas in CRDs and from 1.15 on we check each schema against a restriction called “structural schema”. This basically enforces non-polymorphic and complete typing of each field in a CustomResource. We are going to require structural schemas in the future, especially for all new features including those listed below, and list violations in a <code>NonStructural</code> condition. Non-structural schemas keep working for the time being in the v1beta1 API group. But any serious CRD application is urged to migrate to structural schemas in the foreseeable future.</p><p>Details about what makes a schema structural will be published in a blog post on kubernetes.io later this week, and it is of course <a href=/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema>documented in the Kubernetes documentation</a>.</p><p><strong>beta: CustomResourceDefinition Webhook Conversion</strong></p><p>CustomResourceDefinitions support multiple versions as beta since 1.14. With Kubernetes 1.15, they gain the ability to convert between different versions on-the-fly, just like users are used to from native resources for long term. Conversions for CRDs are implemented via webhooks, deployed inside the cluster by the cluster admin. This feature is promoted to beta in Kubernetes 1.15, lifting CRDs to a completely new level for serious CRD applications.</p><p><strong>beta: CustomResourceDefinition OpenAPI Publishing</strong></p><p>OpenAPI specs for native types have been served at <code>/openapi/v2</code> by kube-apiserver for a long time, and they are consumed by a number of components, notably kubectl client-side validation, kubectl explain and OpenAPI based client generators.</p><p>OpenAPI publishing for CRDs will be available with Kubernetes 1.15 as beta, yet again only for structural schemas.</p><p><strong>beta: CustomResourceDefinitions Pruning</strong></p><p>Pruning is the automatic removal of unknown fields in objects sent to a Kubernetes API. A field is unknown if it is not specified in the OpenAPI validation schema. This is both a data consistency and security relevant feature. It enforces that only data structures specified by the CRD developer are persisted to etcd. This is the behaviour of native resources, and will be available for CRDs as well, starting as beta in Kubernetes 1.15.</p><p>Pruning is activated via <code>spec.preserveUnknownFields: false</code> in the CustomResourceDefinition. A future apiextensions.k8s.io/v1 variant of CRDs will enforce pruning (with a possible, but explicitly necessary opt-out).</p><p>Pruning requires that CRD developer provides complete, structural validation schemas, either top-level or for all versions of the CRD.</p><p><strong>alpha: CustomResourceDefinition Defaulting</strong></p><p>CustomResourceDefinitions get support for defaulting. Defaults are specified using the <code>default</code> keyword in the OpenAPI validation schema. Defaults are set for unspecified field in an object sent to the API, and when reading from etcd.</p><p>Defaulting will be available as alpha in Kubernetes 1.15 for structural schemas.</p><p><strong>beta: Admission Webhook Reinvocation & Improvements</strong></p><p>Mutating and validating admission webhooks become more and more mainstream for projects extending the Kubernetes API. Until now mutating webhooks were only called once, in alphabetical order. An earlier run webhook cannot react on the output of webhooks called later in the chain. With Kubernetes 1.15 this will change:</p><p>Mutating webhooks can opt-in into at least one re-invocation by specifying <code>reinvocationPolicy: IfNeeded</code>. If a later mutating webhook modifies the object, the earlier webhook will get a second chance.</p><p>This requires that webhooks have an idem-potent-like behaviour which can cope with this second invocation.</p><p>It is not planned to add another round of invocations such that webhook authors still have to be careful about the changes to admitted objects they implement. Finally the validating webhooks are called to verify that promised invariants are fulfilled.</p><p>There are more smaller changes to admission webhook, notably <code>objectSelector</code> to exclude objects with certain labels from admission, arbitrary port (not only 443) for the webhook server.</p><h2 id=cluster-lifecycle-stability-and-usability-improvements>Cluster Lifecycle Stability and Usability Improvements</h2><p>Work on making Kubernetes installation, upgrade and configuration even more robust has been a major focus for this cycle for SIG Cluster Lifecycle (see our last <a href="https://docs.google.com/presentation/d/1QUOsQxfEfHlMq4lPjlK2ewQHsr9peEKymDw5_XwZm8Q/edit?usp=sharing">Community Update</a>). Bug fixes across bare metal tooling and production-ready user stories, such as the high availability use cases have been given priority for 1.15.</p><p><strong>kubeadm</strong>, the cluster lifecycle building block, continues to receive features and stability work required for bootstrapping production clusters efficiently. kubeadm has promoted high availability (HA) capability to beta, allowing users to use the familiar <code>kubeadm init</code> and <code>kubeadm join</code> commands to <a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>configure and deploy an HA control plane</a>. An entire new test suite has been created specifically for ensuring these features will stay stable over time.</p><p>Certificate management has become more robust in 1.15, with kubeadm now seamlessly rotating all your certificates (on upgrades) before they expire. Check the <a href=/docs/reference/setup-tools/kubeadm/kubeadm-alpha/>kubeadm documentation</a> for information on how to manage your certificates.</p><p>The kubeadm configuration file API is moving from v1beta1 to v1beta2 in 1.15.</p><p>Finally, let’s celebrate that kubeadm now <a href=https://github.com/kubernetes/kubeadm/issues/1588>has its own logo</a>!</p><p><img src=/images/blog/2019-06-19-kubernetes-1-15-release-announcement/kubeadm-logo.png alt="kubeadm official logo"></p><h2 id=continued-improvement-of-csi>Continued improvement of CSI</h2><p>In Kubernetes v1.15, SIG Storage continued work to <a href=https://github.com/kubernetes/enhancements/issues/625>enable migration of in-tree volume plugins</a> to Container Storage Interface (CSI). SIG Storage worked on bringing CSI to feature parity with in-tree functionality, including functionality like resizing, inline volumes, and more. SIG Storage introduces new alpha functionality in CSI that doesn't exist in the Kubernetes Storage subsystem yet, like volume cloning.</p><p>Volume cloning enables users to specify another PVC as a "DataSource" when provisioning a new volume. If the underlying storage system supports this functionality and implements the "CLONE_VOLUME" capability in its CSI driver, then the new volume becomes a clone of the source volume.</p><p><strong>Additional Notable Feature Updates</strong></p><ul><li>Support for go modules in Kubernetes Core</li><li>Continued preparation on cloud provider extraction and code organization. The cloud provider code has been moved to <a href=https://github.com/kubernetes/legacy-cloud-providers>kubernetes/legacy-cloud-providers</a> for easier removal later and external consumption.</li><li>Kubectl <a href=https://github.com/kubernetes/enhancements/issues/515>get and describe</a> now work with extensions</li><li>Nodes now support <a href=https://github.com/kubernetes/enhancements/issues/606>third party monitoring plugins</a>.</li><li>A new <a href=https://github.com/kubernetes/enhancements/issues/624>Scheduling Framework</a> for schedule plugins is now Alpha</li><li>ExecutionHook API <a href=https://github.com/kubernetes/enhancements/issues/962>designed to trigger hook commands</a> in the containers for different use cases is now Alpha.</li><li>Continued deprecation of extensions/v1beta1, apps/v1beta1, and apps/v1beta2 APIs; these extensions will be retired in 1.16!</li></ul><p>Check the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#kubernetes-v115-release-notes>release notes</a> for a complete list of notable features and fixes.</p><p><strong>Availability</strong></p><p>Kubernetes 1.15 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.15.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a>. You can also easily install 1.15 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><p><strong>Features Blog Series</strong></p><p>If you’re interested in exploring these features more in depth, check back this week and the next for our Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:</p><ul><li>Future of CRDs: Structural Schemas</li><li>Introducing Volume Cloning Alpha for Kubernetes</li><li>Automated High Availability in Kubeadm</li></ul><p><strong>Release team</strong></p><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://git.k8s.io/sig-release/releases/release-1.15/release_team.md>release team</a> led by Claire Laurence, Senior Technical Program Manager at Pivotal Software. The 38 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">32,000 individual contributors</a> to date and an active community of more than 66,000 people.</p><p><strong>Project Velocity</strong></p><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&from=now-1y&to=now">379 different companies and over 2,715 individuals</a> contribute to Kubernetes each month. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><p><strong>User Highlights</strong></p><p>Established, global organizations are using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at massive scale. Recently published user stories from the community include:</p><ul><li><strong>China Unicom</strong> is using Kubernetes to <a href=https://kubernetes.io/case-studies/chinaunicom/>increase their resource utilization 20-50%</a>, lowering IT infrastructure costs, and cutting deployment time from hours to 10-15 minutes.</li><li><strong>The City of Montreal</strong> is using Kubernetes to <a href=https://kubernetes.io/case-studies/city-of-montreal/>decrease deployments from months to hours</a> and run 200 application components on 8 machines with 5 people operating Kubernetes clusters.</li><li><strong>SLAMTEC</strong> is using Kubernetes along with other CNCF projects to achiever <a href=https://kubernetes.io/case-studies/slamtec/>18+ months of 100% uptime</a> saving 50% time spent on troubleshooting and debugging and 30% time savings on CI/CD efforts.</li><li><strong>ThredUP</strong> has decreased deployment time by <a href=https://kubernetes.io/case-studies/thredup/>about 50% on average for key services</a> and has shrunk lead time for deployment to under 20 minutes.</li></ul><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community.</p><p><strong>Ecosystem Updates</strong></p><ul><li>Kubernetes recently celebrated its <a href=https://www.cncf.io/blog/2019/06/06/reflections-on-the-fifth-anniversary-of-kubernetes/>five-year anniversary</a> at KubeCon + CloudNativeCon Barcelona</li><li>The <a href=https://www.cncf.io/certification/expert/cka/>Certified Kubernetes Administrator (CKA) exam</a> has become one of the most popular Linux Foundation certifications to date with over 9,000 registrations and over 1,700 individuals that passed and received the certification.</li><li>Coming off the heels of a successful <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/>KubeCon + CloudNativeCon Europe 2019</a>, the CNCF announced it has over 400 members with a 130 percent year-over-year growth rate.</li></ul><p><strong>KubeCon</strong></p><p>The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to <a href=https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/>Shanghai</a> (co-located with Open Source Summit) from June 24-26, 2019 and <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/>San Diego</a> from November 18-21. These conferences will feature technical sessions, case studies, developer deep dives, salons, and more! <a href=https://www.cncf.io/community/kubecon-cloudnativecon-events/>Register today</a>!</p><h2 id=webinar><strong>Webinar</strong></h2><p>Join members of the Kubernetes 1.15 release team on July 23 at 10am PDT to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/8415609575308/WN_AtjsGjz5TRqOsLrEFTWlJQ>here</a>.</p><p><strong>Get Involved</strong></p><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p><ul><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-037f0eed4d73549cc2f339290039e718>Join us at the Contributor Summit in Shanghai</h1><div class="td-byline mb-4"><time datetime=2019-06-12 class=text-muted>Wednesday, June 12, 2019</time></div><p><strong>Author</strong>: Josh Berkus (Red Hat)</p><p>![Picture of contributor panel at 2018 Shanghai contributor summit. Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png)</p><p>For the second year, we will have <a href=https://www.lfasiallc.com/events/contributors-summit-china-2019/>a Contributor Summit event</a> the day before <a href=https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/>KubeCon China</a> in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and <a href=https://www.lfasiallc.com/events/contributors-summit-china-2019/register/>register</a>. The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints.</p><h3 id=current-contributor-day>Current Contributor Day</h3><p>After last year's Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule.</p><p>While we do not yet have a full schedule up, the topics covered in the current contributor track will include:</p><ul><li>How to write a KEP (Kubernetes Enhancement Proposal)</li><li>Codebase and repository review</li><li>Local Build & Test troubleshooting session</li><li>Guide to Non-Code Contribution opportunities</li><li>SIG-Azure face-to-face meeting</li><li>SIG-Scheduling face-to-face meeting</li><li>Other SIG face-to-face meetings as we confirm them</li></ul><p>The schedule will be on <a href=https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit>the Community page</a> once it is complete.</p><p>If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact <a href=mailto:jberkus@redhat.com>Josh Berkus</a>.</p><h3 id=new-contributor-workshop>New Contributor Workshop</h3><p>Students at <a href=/blog/2018/12/05/new-contributor-workshop-shanghai/>last year's New Contributor Workshop</a> (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community.</p><blockquote><p>"It's a one-stop-shop for becoming familiar with the community." said one participant.</p></blockquote><p>If you have not contributed to Kubernetes before, or have only done one or two things, please consider <a href=https://www.lfasiallc.com/events/contributors-summit-china-2019/register/>enrolling</a> in the NCW.</p><blockquote><p>"Got to know the process from signing CLA to PR and made friends with other contributors." said another.</p></blockquote><h3 id=documentation-sprints>Documentation Sprints</h3><p>Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please <a href=https://www.lfasiallc.com/events/contributors-summit-china-2019/register/>sign up</a> to help with the Doc Sprints.</p><h3 id=before-you-attend>Before you attend</h3><p>Regardless of where you participate, everyone at the Contributor Summit should <a href=https://git.k8s.io/community/CLA.md#the-contributor-license-agreement>sign the Kubernetes Contributor License Agreement</a> (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e8bd664629095c8948e08f12a40e814c>Kyma - extend and build on Kubernetes with ease</h1><div class="td-byline mb-4"><time datetime=2019-05-23 class=text-muted>Thursday, May 23, 2019</time></div><p><strong>Authors:</strong> Lukasz Gornicki (SAP)</p><p>According to this recently completed <a href=https://www.cncf.io/blog/2018/08/29/cncf-survey-use-of-cloud-native-technologies-in-production-has-grown-over-200-percent/>CNCF Survey</a>, the adoption rate of Cloud Native technologies in production is growing rapidly. Kubernetes is at the heart of this technological revolution. Naturally, the growth of cloud native technologies has been accompanied by the growth of the ecosystem that surrounds it. Of course, the complexity of cloud native technologies have increased as well. Just google for the phrase “Kubernetes is hard”, and you’ll get plenty of articles that explain this complexity problem. The best thing about the CNCF community is that problems like this can be solved by smart people building new tools to enable Kubernetes users: Projects like Knative and its <a href=https://github.com/knative/build>Build resource</a> extension, for example, serve to reduce complexity across a range of scenarios. Even though increasing complexity might seem like the most important issue to tackle, it is not the only challenge you face when transitioning to Cloud Native.</p><h2 id=problems-to-solve>Problems to solve</h2><h3 id=picking-the-right-technology-is-hard>Picking the right technology is hard</h3><p>Now that you understand Kubernetes, your teams are trained and you’ve started building applications on top, it’s time to face a new layer of challenges. Cloud native doesn’t just mean deploying a platform for developers to build on top of. Developers also need storage, backup, monitoring, logging and a service mesh to enforce policies upon data in transit. Each of these individual systems must be properly configured and deployed, as well as logged, monitored and backed up on its own. The CNCF is here to help. We provide a <a href=https://landscape.cncf.io/>landscape</a> overview of all cloud-native technologies, but the list is huge and can be overwhelming.</p><p>This is where <a href=http://kyma-project.io>Kyma</a> will make your life easier. Its mission statement is to enable a flexible and easy way of extending applications.</p><p><img src=/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/kyma-center.png width=40% alt="Kyma in center"></p><p>This project is designed to give you the tools you need to be able to write an end-to-end, production-grade cloud native application. <a href=https://github.com/kyma-project/kyma/>Kyma</a> was donated to the open-source community by <a href=https://www.sap.com>SAP</a>; a company with great experience in writing production-grade cloud native applications. That’s why we’re so excited to -- <a href=https://twitter.com/kymaproject/status/1121426458243678209>announce</a> the first major release of <a href=https://github.com/kyma-project/kyma/releases/tag/1.0.0>Kyma 1.0</a>!</p><h3 id=deciding-on-the-path-from-monolith-to-cloud-native-is-hard>Deciding on the path from monolith to cloud-native is hard</h3><p>Try Googling <code>monolith to cloud native</code> or <code>monolith to microservices</code> and you’ll get a list of plenty of talks and papers that tackle this challenge. There are many different paths available for migrating a monolith to the cloud, and our experience has taught us to be quite opinionated in this area. First, let's answer the question of why you’d want to move from monolith to cloud native. The goals driving this move are typically:</p><ul><li>Increased scalability.</li><li>Faster implementation of new features.</li><li>More flexible approach to extensibility.</li></ul><p>You do not have to rewrite your monolith to achieve these goals. Why spend all that time rewriting functionality that you already have? Just focus on enabling your monolith to support <a href=https://en.wikipedia.org/wiki/Event-driven_architecture>event-driven architecture</a>.</p><h2 id=how-does-kyma-solve-your-challenges>How does Kyma solve your challenges?</h2><h3 id=what-is-kyma>What is Kyma?</h3><p><a href=https://kyma-project.io/docs/root/kyma/#overview-overview>Kyma</a> runs on Kubernetes and consists of a number of different components, three of which are:</p><ul><li><a href=https://kyma-project.io/docs/components/application-connector/>Application connector</a> that you can use to connect any application with a Kubernetes cluster and expose its APIs and Events through the <a href=https://github.com/kubernetes-incubator/service-catalog>Kubernetes Service Catalog</a>.</li><li><a href=https://kyma-project.io/docs/components/serverless/>Serverless</a> which enables you to easily write extensions for your application. Your function code can be triggered by API calls and also by events coming from external system. You can also securely call back the integrated system from your function.</li><li><a href=https://kyma-project.io/docs/components/service-catalog/>Service Catalog</a> is here to expose integrated systems. This integration also enables you to use services from hyperscalers like Azure, AWS or Google Cloud. <a href=https://kyma-project.io/docs/components/service-catalog/#service-brokers-service-brokers>Kyma</a> allows for easy integration of official service brokers maintained by Microsoft and Google.</li></ul><p><img src=/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/ac-s-sc.svg alt="core components"></p><p>You can watch <a href="https://www.youtube.com/watch?v=wJzVWFGkiKk">this video</a> for a short overview of Kyma key features that is based on a real demo scenario.</p><h3 id=we-picked-the-right-technologies-for-you>We picked the right technologies for you</h3><p>You can provide reliable extensibility in a project like Kyma only if it is properly monitored and configured. We decided not to reinvent the wheel. There are many great projects in the CNCF landscape, most with huge communities behind them. We decided to pick the best ones and glue them all together in Kyma. You can see the same architecture diagram that is above but with a focus on the projects we put together to create Kyma:</p><p><img src=/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/arch.png width=70% alt="Kyma architecture"></p><ul><li>Monitoring and alerting is based on <a href=https://prometheus.io/>Prometheus</a> and <a href=https://grafana.com/>Grafana</a></li><li>Logging is based on <a href=https://grafana.com/loki>Loki</a></li><li>Eventing uses <a href=https://github.com/knative/eventing/>Knative</a> and <a href=https://nats.io/>NATS</a></li><li>Asset management uses <a href=https://min.io/>Minio</a> as a storage</li><li>Service Mesh is based on <a href=https://istio.io/>Istio</a></li><li>Tracing is done with <a href=https://www.jaegertracing.io/>Jaeger</a></li><li>Authentication is supported by <a href=https://github.com/dexidp/dex>dex</a></li></ul><p>You don't have to integrate these tools: We made sure they all play together well, and are always up to date ( Kyma is already using Istio 1.1). With our custom <a href=https://github.com/kyma-project/kyma/blob/master/docs/kyma/04-02-local-installation.md>Installer</a> and <a href=https://helm.sh/>Helm</a> charts, we enabled easy installation and easy upgrades to new versions of Kyma.</p><h3 id=do-not-rewrite-your-monoliths>Do not rewrite your monoliths</h3><p>Rewriting is hard, costs a fortune, and in most cases is not needed. At the end of the day, what you need is to be able to write and put new features into production quicker. You can do it by connecting your monolith to Kyma using the <a href=https://kyma-project.io/docs/components/application-connector>Application Connector</a>. In short, this component makes sure that:</p><ul><li>You can securely call back the registered monolith without the need to take care of authorization, as the Application Connector handles this.</li><li>Events sent from your monolith get securely to the Kyma Event Bus.</li></ul><p>At the moment, your monolith can consume three different types of services: REST (with <a href=https://www.openapis.org/>OpenAPI</a> specification) and OData (with Entity Data Model specification) for synchronous communication, and for asynchronous communication you can register a catalog of events based on <a href=https://www.asyncapi.com/>AsyncAPI</a> specification. Your events are later delivered internally using <a href=https://nats.io/>NATS Streaming</a> channel with <a href=https://github.com/knative/eventing/>Knative eventing</a>.</p><p>Once your monolith's services are connected, you can provision them in selected Namespaces thanks to the previously mentioned <a href=https://kyma-project.io/docs/components/service-catalog/>Service Catalog</a> integration. You, as a developer, can go to the catalog and see a list of all the services you can consume. There are services from your monolith, and services from other 3rd party providers thanks to registered Service Brokers, like <a href=https://github.com/Azure/open-service-broker-azure>Azure's OSBA</a>. It is the one single place with everything you need. If you want to stand up a new application, everything you need is already available in Kyma.</p><h3 id=finally-some-code>Finally some code</h3><p>Check out some code I had to write to integrate a monolith with Azure services. I wanted to understand the sentiments shared by customers under the product's review section. On every event with a review comment, I wanted to use machine learning to call a sentiments analysis service, and in the case of a negative comment, I wanted to store it in a database for later review. This is the code of a function created thanks to our <a href=https://kyma-project.io/docs/components/serverless>Serverless</a> component. Pay attention to my code comments:</p><blockquote><p>You can watch <a href="https://www.youtube.com/watch?v=wJzVWFGkiKk">this</a> short video for a full demo of sentiment analysis function.</p></blockquote><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=color:#080;font-style:italic>/* It is a function powered by NodeJS runtime so I have to import some necessary dependencies. I choosed Azure&#39;s CosmoDB that is a Mongo-like database, so I could use a MongoClient */</span>
<span style=color:#a2f;font-weight:700>const</span> axios <span style=color:#666>=</span> require(<span style=color:#b44>&#34;axios&#34;</span>);
<span style=color:#a2f;font-weight:700>const</span> MongoClient <span style=color:#666>=</span> require(<span style=color:#b44>&#39;mongodb&#39;</span>).MongoClient;

module.exports <span style=color:#666>=</span> { main<span style=color:#666>:</span> <span style=color:#a2f;font-weight:700>async</span> <span style=color:#a2f;font-weight:700>function</span> (event, context) {
    <span style=color:#080;font-style:italic>/* My function was triggered because it was subscribed to customer review event. I have access to the payload of the event. */</span>
    <span style=color:#a2f;font-weight:700>let</span> negative <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>await</span> isNegative(event.data.comment)
    
    <span style=color:#a2f;font-weight:700>if</span> (negative) {
      console.log(<span style=color:#b44>&#34;Customer sentiment is negative:&#34;</span>, event.data)
      <span style=color:#a2f;font-weight:700>await</span> mongoInsert(event.data)
    } <span style=color:#a2f;font-weight:700>else</span> {
      console.log(<span style=color:#b44>&#34;This positive comment was not saved:&#34;</span>, event.data) 
    }
}}

<span style=color:#080;font-style:italic>/* Like in case of isNegative function, I focus of usage of the MongoClient API. The necessary information about the database location and an authorization needed to call it is injected into my function and I just need to pick a proper environment variable. */</span>
<span style=color:#a2f;font-weight:700>async</span> <span style=color:#a2f;font-weight:700>function</span> mongoInsert(data) {

    <span style=color:#a2f;font-weight:700>try</span> {
          client <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>await</span> MongoClient.connect(process.env.connectionString, { useNewUrlParser<span style=color:#666>:</span> <span style=color:#a2f;font-weight:700>true</span> });
          db <span style=color:#666>=</span> client.db(<span style=color:#b44>&#39;mycommerce&#39;</span>);
          <span style=color:#a2f;font-weight:700>const</span> collection <span style=color:#666>=</span> db.collection(<span style=color:#b44>&#39;comments&#39;</span>);
          <span style=color:#a2f;font-weight:700>return</span> <span style=color:#a2f;font-weight:700>await</span> collection.insertOne(data);
    } <span style=color:#a2f;font-weight:700>finally</span> {
      client.close();
    }
}
<span style=color:#080;font-style:italic>/* This function calls Azure&#39;s Text Analytics service to get information about the sentiment. Notice process.env.textAnalyticsEndpoint and process.env.textAnalyticsKey part. When I wrote this function I didn&#39;t have to go to Azure&#39;s console to get these details. I had these variables automatically injected into my function thanks to our integration with Service Catalog and our Service Binding Usage controller that pairs the binding with a function. */</span>
<span style=color:#a2f;font-weight:700>async</span> <span style=color:#a2f;font-weight:700>function</span> isNegative(comment) {
    <span style=color:#a2f;font-weight:700>let</span> response <span style=color:#666>=</span> <span style=color:#a2f;font-weight:700>await</span> axios.post(<span style=color:#b44>`</span><span style=color:#b68;font-weight:700>${</span>process.env.textAnalyticsEndpoint<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/sentiment`</span>,
      { documents<span style=color:#666>:</span> [{ id<span style=color:#666>:</span> <span style=color:#b44>&#39;1&#39;</span>, text<span style=color:#666>:</span> comment }] }, {headers<span style=color:#666>:</span>{ <span style=color:#b44>&#39;Ocp-Apim-Subscription-Key&#39;</span><span style=color:#666>:</span> process.env.textAnalyticsKey }})
    <span style=color:#a2f;font-weight:700>return</span> response.data.documents[<span style=color:#666>0</span>].score <span style=color:#666>&lt;</span> <span style=color:#666>0.5</span>
}
</code></pre></div><p>Thanks to Kyma, I don't have to worry about the infrastructure around my function. As I mentioned, I have all the tools needed in Kyma, and they are integrated together. I can quickly get access to my logs through <a href=https://grafana.com/loki>Loki</a>, and I can quickly get access to a preconfigured Grafana dashboard to see the metrics of my Lambda delivered thanks to <a href=https://prometheus.io/>Prometheus</a> and <a href=https://istio.io/>Istio</a>.</p><p><img src=/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/grafana-lambda.png width=70% alt="Grafana with preconfigured lambda dashboard"></p><p>Such an approach gives you a lot of flexibility in adding new functionality. It also gives you time to rethink the need to rewrite old functions.</p><h2 id=contribute-and-give-feedback>Contribute and give feedback</h2><p>Kyma is an open source project, and we would love help it grow. The way that happens is with your help. After reading this post, you already know that we don't want to reinvent the wheel. We stay true to this approach in our work model, which enables community contributors. We work in <a href=https://github.com/kyma-project/community/tree/master/contributing>Special Interest Groups</a> and have publicly recorded meeting that you can join any time, so we have a setup similar to what you know from Kubernetes itself.
Feel free to share also your feedback with us, through <a href=https://twitter.com/kymaproject>Twitter</a> or <a href=http://slack.kyma-project.io>Slack</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e13ad74063938b0e8e09f8920001a9e7>Kubernetes, Cloud Native, and the Future of Software</h1><div class="td-byline mb-4"><time datetime=2019-05-17 class=text-muted>Friday, May 17, 2019</time></div><p><strong>Authors:</strong> Brian Grant (Google), Jaice Singer DuMars (Google)</p><h1 id=kubernetes-cloud-native-and-the-future-of-software>Kubernetes, Cloud Native, and the Future of Software</h1><p>Five years ago this June, Google Cloud announced a new application management technology called Kubernetes. It began with a <a href=https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56>simple open source commit</a>, followed the next day by a <a href=https://cloudplatform.googleblog.com/2014/06/an-update-on-container-support-on-google-cloud-platform.html>one-paragraph blog mention</a> around container support. Later in the week, Eric Brewer <a href="https://www.youtube.com/watch?v=YrxnVKZeqK8">talked about Kubernetes for the first time</a> at DockerCon. And soon the world was watching.</p><p>We’re delighted to see Kubernetes become core to the creation and operation of modern software, and thereby a key part of the global economy. To us, the success of Kubernetes represents even more: A business transition with truly worldwide implications, thanks to the unprecedented cooperation afforded by the open source software movement.</p><p>Like any important technology, Kubernetes has become about more than just itself; it has positively affected the environment in which it arose, changing how software is deployed at scale, how work is done, and how corporations engage with big open-source projects.</p><p>Let’s take a look at how this happened, since it tells us a lot about where we are today, and what might be happening next.</p><p><strong>Beginnings</strong></p><p>The most important precursor to Kubernetes was the rise of application containers. Docker, the first tool to really make containers usable by a broad audience, began as an open source project in 2013. By containerizing an application, developers could achieve easier language runtime management, deployment, and scalability. This triggered a sea change in the application ecosystem. Containers made stateless applications easily scalable and provided an immutable deployment artifact that drastically reduced the number of variables previously encountered between test and production systems.</p><p>While containers presented strong stand-alone value for developers, the next challenge was how to deliver and manage services, applications, and architectures that spanned multiple containers and multiple hosts.</p><p>Google had already encountered similar issues within its own IT infrastructure. Running the world’s most popular search engine (and several other products with millions of users) lead to early innovation around, and adoption of, containers. Kubernetes was inspired by Borg, Google’s internal platform for scheduling and managing the hundreds of millions, and eventually billions, of containers that implement all of our services.</p><p>Kubernetes is more than just “Borg, for everyone” It distills the most successful architectural and API patterns of prior systems and couples them with load balancing, authorization policies, and other features needed to run and manage applications at scale. This in turn provides the groundwork for cluster-wide abstractions that allow true portability across clouds.</p><p>The November 2014 <a href=https://cloudplatform.googleblog.com/2014/11/google-cloud-platform-live-introducing-container-engine-cloud-networking-and-much-more.html>alpha launch</a> of Google Cloud’s <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine (GKE)</a> introduced managed Kubernetes. There was an explosion of innovation around Kubernetes, and companies from the enterprise down to the startup saw barriers to adoption fall away. Google, Red Hat, and others in the community increased their investment of people, experience, and architectural know-how to ensure it was ready for increasingly mission-critical workloads. The response was a wave of adoption that swept it to the forefront of the crowded container management space.</p><p><strong>The Rise of Cloud Native</strong></p><p>Every enterprise, regardless of its core business, is embracing more digital technology. The ability to rapidly adapt is fundamental to continued growth and competitiveness. Cloud-native technologies, and especially Kubernetes, arose to meet this need, providing the automation and observability necessary to manage applications at scale and with high velocity. Organizations previously constrained to quarterly deployments of critical applications can now deploy safely multiple times a day.</p><p>Kubernetes’s declarative, API-driven infrastructure empowers teams to operate independently, and enables them to focus on their business objectives. An inevitable cultural shift in the workplace has come from enabling greater autonomy and productivity and reducing the toil of development teams.</p><p><strong>Increased engagement with open source</strong></p><p>The ability for teams to rapidly develop and deploy new software creates a virtuous cycle of success for companies and technical practitioners alike. Companies have started to recognize that contributing back to the software projects they use not only improves the performance of the software for their use cases, but also builds critical skills and creates challenging opportunities that help them attract and retain new developers.</p><p>The Kubernetes project in particular curates a collaborative culture that encourages contribution and sharing of learning and development with the community. This fosters a positive-sum ecosystem that benefits both contributors and end-users equally.</p><p><strong>What’s Next?</strong></p><p>Where Kubernetes is concerned, five years seems like an eternity. That says much about the collective innovation we’ve seen in the community, and the rapid adoption of the technology.</p><p>In other ways, it is just the start. New applications such as machine learning, edge computing, and the Internet of Things are finding their way into the cloud native ecosystem via projects like Kubeflow. Kubernetes is almost certain to be at the heart of their success.</p><p>Kubernetes may be most successful if it becomes an invisible essential of daily life, like urban plumbing or electrical grids. True standards are dramatic, but they are also taken for granted. As Googler and KubeCon co-chair Janet Kuo said in a <a href="https://www.youtube.com/watch?v=LAO7RuWwfzA">recent keynote</a>, Kubernetes is going to become boring, and that’s a good thing, at least for the majority of people who don’t have to care about container management.</p><p>At Google Cloud, we’re still excited about the project, and we go to work on it every day. Yet it’s all of the solutions and extensions that expand from Kubernetes that will dramatically change the world as we know it.</p><p>So, as we all celebrate the continued success of Kubernetes, remember to take the time and thank someone you see helping make the community better. It’s up to all of us to foster a cloud-native ecosystem that prizes the efforts of everyone who helps maintain and nurture the work we do together.</p><p>And, to everyone who has been a part of the global success of Kubernetes, thank you. You have changed the world.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-61a7d410a8742331c4512c365210431d>Expanding our Contributor Workshops</h1><div class="td-byline mb-4"><time datetime=2019-05-14 class=text-muted>Tuesday, May 14, 2019</time></div><p><strong>Authors:</strong> Guinevere Saenger (GitHub) and Paris Pittman (Google)</p><p><strong>tl;dr</strong> - learn about the contributor community with us and land your first
PR! We have spots available in <a href=https://events.linuxfoundation.org/events/contributor-summit-europe-2019/>Barcelona</a> (registration <strong>closes</strong> on
Wednesday May 15, so grab your spot!) and the upcoming <a href=https://www.lfasiallc.com/events/contributors-summit-china-2019/>Shanghai</a> Summit.
The Barcelona event is poised to be our biggest one yet, with more registered
attendees than ever before!</p><p>Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our community’s many code bases and seen places to improve? We
have a workshop for you!</p><p>KubeCon + CloudNativeCon Barcelona’s <a href=https://events.linuxfoundation.org/events/contributor-summit-europe-2019/>new contributor workshop</a> will be the
fourth one of its kind, and we’re really looking forward to it! The workshop was
kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to
Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
We are constantly updating and improving the workshop content based on feedback
from past sessions. This time, we’re breaking up the participants by their
experience and comfort level with open source and Kubernetes. We’ll have
developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on. In the 201 track, we
will have a codebase walkthrough and local development and test demonstration
for folks who have a bit more experience in open source but may be unfamiliar
with our community’s development tools. For both tracks, you will have a chance
to get your hands dirty and have some fun. Because not every contributor works
with code, and not every contribution is technical, we will spend the beginning
of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.</p><h2 id=mentoring-opportunities>Mentoring Opportunities</h2><p>We will also bring back the SIG Meet-and-Greet where new contributors will have
a chance to mingle with current contributors, perhaps find their dream SIG,
learn what exciting areas they can help with, gain mentors, and make friends.</p><p>PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on
Thursday, May 23. <a href=http://bit.ly/mentor-bcn>Sign up here</a>. 60% of the attendees during the
Seattle event asked contributor questions.</p><h2 id=past-attendee-story-vallery-lancy-engineer-at-lyft>Past Attendee Story - Vallery Lancy, Engineer at Lyft</h2><p>We talked to a few of our past participants in a series of interviews that we
will publish throughout the course of the year. In our first two clips, we meet
Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle
edition of the workshop. She was poking around in the community for a while to
see where she could jump in.</p><p>Watch Vallery talk about her experience here:</p><center><iframe width=560 height=315 src=https://www.youtube.com/embed/uKg5WUcl6WU frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe></center><p>What does Vallery say to folks curious about the workshops, or those attending
the Barcelona edition?</p><center><iframe width=560 height=315 src=https://www.youtube.com/embed/niHiem7JmPA frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe></center><p>Be like Vallery and hundreds of previous New Contributor Workshop attendees:
join us in Barcelona (or Shanghai - or San Diego!) for a unique experience
without digging into our documentation! Have the opportunity to meet with the
experts and go step by step into your journey with your peers around you. We’re
looking forward to seeing you there! <a href=https://events.linuxfoundation.org/events/contributor-summit-europe-2019/>Register here</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-087af74107d4d9256924532b505ac3b0>Cat shirts and Groundhog Day: the Kubernetes 1.14 release interview</h1><div class="td-byline mb-4"><time datetime=2019-05-13 class=text-muted>Monday, May 13, 2019</time></div><p><b>Author</b>: Craig Box (Google)</p><p>Last week we celebrated one year of the <a href=https://kubernetespodcast.com/>Kubernetes Podcast from Google</a>. In this weekly show, my co-host Adam Glick and I focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.</p><p>Every few cycles we check in on the release process for Kubernetes itself. Last year we <a href=https://kubernetespodcast.com/episode/010-kubernetes-1.11/>interviewed the release managers for Kubernetes 1.11</a>, and shared that transcript on the Kubernetes blog. We got such great feedback that we wanted to share the transcript of our recent conversation with Aaron Crickenberger, the release manager for Kubernetes 1.14.</p><p>As always, the canonical version can be enjoyed by listening to <a href=https://kubernetespodcast.com/episode/046-kubernetes-1.14/>the podcast version</a>. If you like what you hear, <a href=https://kubernetespodcast.com/subscribe/>we encourage you to subscribe</a>!</p><hr><p><b>CRAIG BOX: We like to start with our guests into digging into their backgrounds a little bit. Kubernetes is built from contributors from many different companies. You worked on Kubernetes at Samsung SDS before joining Google. Does anything change in your position in the community and the work you do, when you change companies?</b></p><p>AARON CRICKENBERGER: Largely, no. I think the food's a little bit better at the current company! But by and large, I have gotten to work with basically the same people doing basically the same thing. I cared about the community first and Google second before I joined Google, and I kind of still operate that way mostly because I believe that Google's success depends upon the community's success, as does everybody else who depends upon Kubernetes. A good and healthy upstream makes a good and healthy downstream.</p><p>So that was largely why Samsung had me working on Kubernetes in the first place was because we thought the technology was legit. But we needed to make sure that the community and project as a whole was also legit. And so that's why you've seen me continue to advocate for transparency and community empowerment throughout my tenure in Kubernetes.</p><p><b>ADAM GLICK: You co-founded the <a href=https://github.com/kubernetes/community/tree/master/sig-testing>Testing SIG</a>. How did you decide that that was needed, and at what stage in the process did you come to that?</b></p><p>AARON CRICKENBERGER: This was very early on in the Kubernetes project. I'm actually a little hazy on specifically when it happened. But at the time, my boss, Bob Wise, worked with some folks within Google to co-found the Scalability SIG.</p><p>If you remember way, way back when Kubernetes first started, there was concern over whether or not Kubernetes was performance enough. Like, I believe it officially supported something on the order of 100 nodes. And there were some who thought, that's silly. I mean, come on, Google can do way more than that. And who in their right mind is going to use a container orchestrator that only supports 100 nodes?</p><p>And of course the thing is we're being super-conservative. We're trying to iterate, ship early and often. And so we helped push the boundaries to make sure that Kubernetes could prove that it worked up to a thousand nodes before it was even officially supported to say, look, it already does this, we're just trying to make sure we have all of the nuts and bolts tightened.</p><p>OK, so great. We decided we needed to create a thing called a SIG in the very first place to talk about these things and make sure that we were moving in the right direction. I then turned my personal attention to testing as the next thing that I believe needed a SIG. So I believe that testing was the second SIG ever to be created for Kubernetes. It was co-founded initially with <a href=https://github.com/ihmccreery>Ike McCreary</a> who, at the time I believe, was an SRE for Google, and then eventually it was handed over to some folks who work in the engineering productivity part of Google where I think it aligned really well with testing's interests.</p><p>It is like "I don't know what you people are trying to write here with Kubernetes, but I want to help you write it better, faster, and stronger". And so I want to make sure we, as a community and as a project, are making it easier for you to write tests, easier for you to run tests, and most importantly, easier for you to act based on those test results.</p><p>That came down to, let's make sure that Kubernetes gets tested on more than just Google Cloud. That was super important to me, as somebody who operated not in Google Cloud but in other clouds. I think it really helped sell the story and build confidence in Kubernetes as something that worked effectively on multiple clouds. And I also thought it was really helpful to see SIG Testing in the community's advocacy move us to a world today we can use test grids so that everybody see the same set of test results to understand what is allowed to prevent Kubernetes from going out the door.</p><p>The process was basically just saying, let's do it. The process was finding people who were motivated and suggesting that we meet on a recurring basis and we try to rally around a common set of work. This was sort of well before SIG governance was an official thing. And we gradually, after about a year, I think, settled on the pattern that most SIGs follow where you try to make sure you have a meeting agenda, you have a Slack channel, you have a mailing list, you discuss everything out in the open, you try to use sort a consistent set of milestones and move forward.</p><p><b>CRAIG BOX: A couple of things I wanted to ask about your life before Kubernetes. Why is there a <a href=https://www.rockwellcollins.com/Products-and-Services/Defense/Simulation-and-Training/Training-Systems/Transportable-Black-Hawk-Operations-Simulator.aspx>Black Hawk flight simulator in a shipping container?</a></b></p><p>AARON CRICKENBERGER: As you may imagine, Black Hawk helicopters are flown in a variety of places around the world, not just next to a building that happens to have a parking lot next to it. And so in order to keep your pilots fresh, you may want to make sure they have good training hours and flight time, without spending fuel to fly an actual helicopter.</p><p>I was involved in helping make what's called a operation simulator, to train pilots on a bunch of the procedures using the same exact hardware that was deployed in Black Hawk helicopters, complete with motion seats that would shake to simulate movement and a full-fidelity visual system. This was all packed up in two shipping containers so that the simulator could be deployed wherever needed.</p><p>I definitely had a really fun experience working on this simulator in the field at an Air Force base prior to a conference where I got to experience F-16s doing takeoff drills, which was amazing. They would get off the runway, and then just slam the afterburners to max and go straight up into the air. And I got to work on graphic simulation bugs. It was really cool.</p><p><b>CRAIG BOX: And for a lot of people, when you click on the web page they have listed in the GitHub link, you get their resume, or you get the list of open source projects they work on. In your case, there is <a href=https://soundcloud.com/spiffxp>a SoundCloud page</a>. What do people find on that page?</b></p><p>AARON CRICKENBERGER: They get to see me living my whole life. I find that music is a very important part of my life. It's a non-verbal voice that I have developed over time. I needed some place to host that. And then it came down between SoundCloud and Bandcamp, and SoundCloud was a much easier place to host my recordings.</p><p>So you get to hear the results of me having picked up a guitar and noodling with that about five years ago. You get to hear what I've learned messing around with Ableton Live. You get to hear some mixes that I've done of ambient music. And I haven't posted anything in a while there because I'm trying to get my recording of drums just right.</p><p>So if you go to <a href=https://www.youtube.com/channel/UCfnUO-9Q_gMraUXjbk4p50g>my YouTube channel</a>, mostly what you'll see are recordings of the various SIG meetings that I've participated in. But if you go back a little bit earlier than that, you'll see that I do, in fact, play the drums. I'm trying to get those folded into my next songs.</p><p><b>CRAIG BOX: Do you know who <a href=https://en.wikipedia.org/wiki/Hugh_Padgham#The_%22gated_drum%22_sound>Hugh Padgham</a> is?</b></p><p>AARON CRICKENBERGER: I do not.</p><p><b>CRAIG BOX: Hugh Padgham was the recording engineer who did the gated reverb drum sound that basically defined Phil Collins in the 1980s. I think you should call him up if you're having problems with your drum sound.</b></p><p>AARON CRICKENBERGER: That is awesome.</p><p><b>ADAM GLICK: You mentioned you can also find videos of the work that you're doing with the SIG. How did you become the release manager for 1.14?</b></p><p>AARON CRICKENBERGER: I've been involved in the Kubernetes release process way back in the 1.4 days. I started out as somebody who tried to help figure out, how do you write release notes for this thing? How do you take this whole mess and try to describe it in a sane way that makes sense to end users and developers? And I gradually became involved in other aspects of the release over time.</p><p>I helped out with CI Signal. I helped out with issue triage. When I helped out with CI Signal, I wrote the <a href=https://github.com/kubernetes/sig-release/blob/master/release-team/role-handbooks/ci-signal/README.md>very first playbook</a> to describe what it is I do around here. That's the model that has since been used for the rest of the release team, where every role describes what they do in a playbook that is used not just for their own benefit, but to help them train other people.</p><p>Formally how I became release lead was I served as release shadow in 1.13. And when release leads are looking to figure out who's going to lead the next release, they turn around and they look at their shadows, because those are who they have been helping out and training.</p><p><b>CRAIG BOX: If they don't have a shadow, do they have to wait another three months and do a release again?</b></p><p>AARON CRICKENBERGER: They do not. The way it works is the release lead can look at their shadows, then they take a look at the rest of their release team leads to see if there is sufficient experience there. And then if not, they consult with the chairs of SIG release.</p><p>So for example, for Kubernetes v1.15, I ended up in an unfortunate situation where neither of my shadows were available to step up and become the leads for 1.15. I consulted with <a href=https://github.com/claurence>Claire Lawrence</a>, who was my enhancements lead for 1.14 and who was on the release team for two quarters, and so met the requirements to become a release lead that way. So she will be the release lead for v1.15.</p><p><b>CRAIG BOX: That was a fantastic answer to a throwaway <a href=https://en.wikipedia.org/wiki/Groundhog_Day>Groundhog Day</a> joke. I appreciate that.</b></p><p>AARON CRICKENBERGER: [LAUGHS]</p><p><b>ADAM GLICK: You can ask it again and see what the answer is, and then another time, and see how it evolves over time.</b></p><p>AARON CRICKENBERGER: I'm short on my Groundhog Day riffs. I'll come back to you.</p><p><b>ADAM GLICK: What are your responsibilities as the release lead?</b></p><p>AARON CRICKENBERGER: Don't Panic. I mean, essentially, a release lead's job is to make the final call, and then hold the line by making the final call. So what you shouldn't be doing as a release lead is attempting to dive in and fix all of the things, or do all of the things, or second-guess anybody else's work. You are there principally and primarily to listen to everybody else's advice and help them make the best decision. And only in the situations where there's not a clear consensus do you wade in and make the call yourself.</p><p>I feel like I was helped out by a very capable team in this regard, this release cycle. So it was super helpful. But as somebody who has what I like to call an "accomplishment monkey" on my back, it can be very difficult to resist the urge to dive right in and help out, because I have been there before. I have the boots-on-the-ground experience.</p><p>The release lead's job is not to be the boots on the ground, but to help make sure that everybody who is boots on the ground is actually doing what they need to do and unblocked in doing what they need to do. It also involves doing songs and dances and making funny pictures. So I view it more as like it's about effective communication. And doing a lot of songs and dances, and funny pictures, and memes is one way that I do that.</p><p>So one way that I thought it would help people pay attention to the release updates that I gave every week at the Kubernetes community meeting was to make sure that I wore a different cat T-shirt each week. After people riffed and joked out my first cat T-shirt where I said, I really need coffee right "meow", and somebody asked if I got that coffee from a "purr-colator", I decided to up the ante.</p><p>And I've heard that people will await those cat T-shirts. They want to know what the latest one is. I even got a special cat T-shirt just to signify that code freeze was coming.</p><p>We also decided that instead of imposing this crazy process that involved a lot of milestones, and labels, and whatnot that would cause the machinery to impose a bunch of additional friction, I would just post a lot of memes to Twitter about code freeze coming. And that seems to have worked out really well. So by and large, the release lead's job is communication, unblocking, and then doing nothing for as much as possible.</p><p>It's really kind of difficult and terrifying because you always have this feeling that you may have missed something, or that you're just not seeing something that's out there. So I'm sitting in this position with a release that has been extremely stable, and I spent a lot of time thinking, OK, what am I missing? Like, this looks too good. This is too quiet. There's usually something that blows up. Come on, what is it, what is it, what is it? And it's an exercise in keeping that all in and not sharing it with everybody until the release is over.</p><p><b>ADAM GLICK: <a href=https://twitter.com/KubernetesPod/status/1110611630180597760/photo/1>He is here in a cat T-shirt</a>, as well.</p><p>When a new US President takes over the office, it's customary that the outgoing president leaves them a note with advice in it. Aside from the shadow team, is there something similar that exists with Kubernetes release management?</b></p><p>AARON CRICKENBERGER: Yeah, I would say there's a very special-- I don't know what the word is I'm looking for here-- bond, relationship, or something where people who have been release leads in the past are very empathetic and very supportive of those who step into the role as release lead.</p><p>You know, I talked about release lead being a lot of uncertainty and second-guessing yourself, while on the outside you have to pretend like everything is OK. And having the support of people who have been there and who have gone through that experience is tremendously helpful.</p><p>So I was able to reach out to a previous release lead. Not to pull the game with-- what is it, like two envelopes? The first envelope, you blame the outgoing president. The second envelope, you write two letters. It's not quite like that.</p><p>I am totally happy to be blamed for all of the changes we made to the release process that didn't go well, but I'm also happy to help support my successor. I feel like my job as a release lead is, number one, make sure the release gets out the door, number two, make sure I set up my successor for success.</p><p>So I've already been meeting with Claire to describe what I would do as the introductory steps. And I plan on continuing to consult with Claire throughout the release process to make sure that things are going well.</p><p><b>CRAIG BOX: If you want to hear the perspective from some previous release leads, check out <a href=http://kubernetespodcast.com/episode/010-kubernetes-1.11/>episode 10</a>, where we interview Josh Berkus and Tim Pepper.</b></p><p><b>ADAM GLICK: What do you plan to put into that set of notes for Claire?</b></p><p>AARON CRICKENBERGER: That's a really good question. I would tell Claire to trust her team first and trust her gut second. Like I said, I think it is super important to establish trust with your team, because the release is this superhuman effort that involves consuming, or otherwise fielding, or shepherding the work of hundreds of contributors.</p><p>And your team is made up of at least 13 people. You could go all the way up to 40 or 50, if you include all of the people that are being trained by those people. There's so much work out there. It's just more work than any one person can possibly handle.</p><p>It's honestly the same thing I will tell new contributors to Kubernetes is that there's no way you can possibly understand all of it. You will not understand the shape of Kubernetes. You will never be the expert who knows literally all of the things, and that's OK. The important part is to make sure that you have people who, when you don't know the answer, you know who to ask for the answer. And it is really helpful if your team are those people.</p><p><b>CRAIG BOX: The specific version that you've been working on and the release that's just come out is Kubernetes 1.14. What are some of the new things in this release?</b></p><p>AARON CRICKENBERGER: This release of Kubernetes contains more stable enhancements than any other release of Kubernetes ever. And I'm pretty proud of that fact. I know in the past you may have heard other release leads talk about, like, this is the stability release, or this time we're really making things a little more mature. But I feel a lot of confidence in saying that this time around.</p><p>Like, I stood in a room, and it was a leadership summit, I think, back in 2017 where we said, look, we're really going to try and make Kubernetes more stable. And we're going to focus on sort of hardening the core of Kubernetes and defining what the core of Kubernetes is. And we're not going to accept a bunch of new features. And then we kind of went and accepted a bunch of new features. And that was a while ago. And here we are today.</p><p>But I think we are finally starting to see the results of work that was started back then. <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190103-windows-node-support.md>Windows Server Container Support</a> is probably the biggest one. You can hear Michael Michael tell stories about how SIG Windows was started about three years ago. And today, they can finally announce that Windows Server containers have gone GA. That's a huge accomplishment.</p><p>A lot of the heavy lifting for this, I believe, came at the end. It started with a conversation in Kubernetes 1.13, and was really wrapped up this release where we define, what are Windows Server containers, exactly? How do they differ from Docker containers or other container runtimes that run on Linux?</p><p>Because today so much of the assumptions people make about the functionality that Kubernetes offers are also baked in with the functionality that Linux-based containers offer. And so we wanted to enable people to use the awesome Kubernetes orchestration capabilities that they have come to love, but to also use that to orchestrate some applications or capabilities that are only available on Windows.</p><p>So we put together what's called a <a href=https://github.com/kubernetes/enhancements/tree/master/keps>Kubernetes Enhancement Proposal</a> process, or a KEP, for short. And we said that we're going to use these KEPs to describe exactly what the criteria are to call something alpha, or beta, or stable. And so the Windows feature allowed us to use a KEP-- or in getting Windows in here, we used the KEP to describe everything that would and would not work for Windows Server containers. That was super huge. And that really, I think, helped us better understand or define what Kubernetes is in that context.</p><p>But OK, I've spent most of the time answering your question with just one single stable feature.</p><p><b>CRAIG BOX: Well, let's dig a little bit in to the KEP process then, because this is the first release where there's a new rule. It says, all proposed enhancements for this release must have an associated KEP. So that's a Kubernetes Enhancement Proposal, a one-page document that describes it. What has the process been like of A, getting engineers on-board with using that, and then B, building something based on these documents?</b></p><p>AARON CRICKENBERGER: It is a process of continued improvement. So it is by no means done, but it honestly required a lot of talking, and saying the same thing over and over to the same people or to different people, as is often the case when it comes to things that involve communication and process changes. But by and large, everybody was pretty much on-board with this.</p><p>There was a little bit of confusion, though, over how high the bar would be set and how rigorously or rigidly we would be enforcing these criteria. And that's where I feel like we have room to iterate and improve on. But we have collectively agreed that, yeah, we do like having all of the information about a particular enhancement in one place. Right?</p><p>The way the world used to operate before is we would throw around Google Docs, that were these design proposals, and then we'd comment on those a bunch. And then eventually, those were turned into markdown files. And those would end up in the community repo,</p><p>And then we'd have a bunch of associated issues that talked about that. And then maybe somebody would open up another issue that they'd call an umbrella issue. And then a bunch of comments would be put there. And then there's lots of discussion that goes on in the PRs. There's like seven different things that I just rattled off there.</p><p>So KEPs are about focusing all of the discussion about the design and implementation and reasoning behind enhancements in one single place. And I think there, we are fully on board. Do we have room to improve? Absolutely. Humans are involved, and it's a messy process. We could definitely find places to automate this better, structure it better. And I look forward to seeing those improvements happen.</p><p>You know, I think another one of the big things was a lot of these KEPs were mired across three different SIGs. There was sort of SIG architecture who had the technical vision for these. There was SIG PM, who-- you know, pick your P of choice-- product, project, process, program, people who are better about how to shepherd things forward, and then SIG release, who just wanted to figure out, what's landing in the release, and why, and how, and why is it important? And so taking the responsibilities across all of those three SIGs and putting it in the right place, which is SIG PM, I think really will help us iterate properly, moving forward.</p><p><b>CRAIG BOX: The other change in this release is that there is no code slush. <a href=https://github.com/kubernetes/sig-release/issues/269>What is a code slush, and why don't we have one anymore?</a></b></p><p>AARON CRICKENBERGER: That's a really good question. I had 10 different people ask me that question over the past couple of months, quarters, years. Take your pick. And so I finally decided, if nobody knows what a code slush is, why do we even have it?</p><p><b>CRAIG BOX: It's like a thawed freeze, but possibly with sugar?</b></p><p>AARON CRICKENBERGER: [LAUGHING] So code slush is about-- we want to slow the rate of change prior to code freeze. Like, let's accept code freeze as this big deadline where nothing's going to happen after a code freeze.</p><p>So while I really want to assume and aspire to live in a world where developers are super productive, and start their changes early, and get them done when they're done, today, I happen to live in a world where developers are driven by deadlines. And they get distracted. And there's other stuff going on. And then suddenly, they realize there's a code freeze ahead of them.</p><p>And this wonderful feature that they've been thinking about implementing over the past two months, they now have to get done in two weeks. And so suddenly, all sorts of code starts to fly in super fast and super quickly. And OK, that's great. I love empowering people to be productive.</p><p>But what we don't want to have happen is somebody decide to land some massive feature or enhancement that changes absolutely everything. Or maybe they decided they want to refactor the world. And if they do that, then they make everybody else's life super difficult because of merge conflicts and rebases. Or maybe all of the test signal that we had kind-of grown accustomed to and gotten used to, completely changes.</p><p>So code slush was about reminding people, hey, don't be jerks. Be kind of responsible. Please try not to land anything super huge at the last minute. But the way that we enforced this was with, like, make sure your PR has a milestone. And make sure that it has priority critical/urgent. In times past, we were like, make sure there is a label called status approved for milestone.</p><p>We were like, what do all these things even mean? People became obsessed with all the labels, and the milestones, and the process. And they never really paid attention to why we're asking people to pay attention to the fact that code freeze was coming soon.</p><p><b>ADAM GLICK: Process for process sake, they could start to build on top of each other. You mentioned that there is a number of other things in the release. Do you want to talk about some of the other pieces that are in there?</b></p><p>AARON CRICKENBERGER: Sure. I think two of the other stable features that I believe other people will find to be exciting are <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0007-pod-ready%2B%2B.md>readiness gates</a> and <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20190131-pod-priority-preemption.md>Pod priority and preemption</a>. Today, Pods have the concept of liveliness and readiness. A live Pod has an application running in it, but it might not be ready to do anything. And so when a Pod is ready, that means it's ready to receive traffic.</p><p>So if you're thinking of some big application that's scaled out everywhere, you want to make sure your Pods are only handling traffic when they're good and ready to do so. But prior to 1.14, the only ways you could verify that were by using either TCP probes, HTTP probes, or exec probes. Either make sure that ports are open inside of the container, or run a command inside of the container and see what that command says.</p><p>And then you can definitely customize a fair amount there, but that requires that you put all of that information inside of the Pod. And it might be really useful for some cluster operators to signify some more overarching concerns that they have before a Pod could be ready. So just-- I don't know-- make sure a Pod has registered with some other system to make sure that it is authorized to serve traffic, or something of that nature. Pod readiness gates allow that sort of capability to happen-- to transparently extend the conditions that you use to figure out whether a Pod is ready for traffic. We believe this will enable more sophisticated orchestration and deployment mechanisms for people who are trying to manage their applications and services.</p><p>I feel like Pod priority and preemption will be interesting to consumers who like to oversubscribe their Kubernetes clusters. Instead of assuming everything is the same size and is the same priority, and first Pods win, you can now say that certain Pods are more important than other Pods. They get scheduled before other Pods, and maybe even so that they kick out other Pods to make room for the really important Pods.</p><p>You could think of it as if you have any super important agents or daemons that have to run on your cluster. Those should always be there. Now, you can describe them as high-priority to make sure that they are definitely always there and always scheduled before anything else is.</p><p><b>ADAM GLICK: Are there any other new features that are in alpha or beta that you're keeping your eye on?</b></p><p>AARON CRICKENBERGER: Yeah. So I feel like, on the beta side of things, a lot of what I am interested in-- if I go back to my theme of maturity, and stability, and defining the core of Kubernetes, I think that the storage SIG has been doing amazing work. They continue to ship out, quarter, after quarter, after quarter, after quarter, new and progressive enhancements to storage-- mostly these days through the CSI, Container Storage Interface project, which is fantastic. It allows you to plug in arbitrary pieces of storage functionality.</p><p>They have a number of things related to that that are in beta this time around, such as topology support. So you're going to be able to more accurately express how and where your CSI volumes need to live relative to your application. Block storage support is something I've heard a number of people asking for, as well as the ability to define <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190124-local-persistent-volumes.md>durable local volumes</a>.</p><p>Let's say you're running a Pod on a node, and you want to make sure it's writing directly to the node's local volumes. And that way, it could be super performant. Cool. Give it an emptydir. It'll be fine.</p><p>But if you destroy the Pod, then you lose all the data that the Pod wrote. And so again, I go back to the example of maybe it's an agent, and it's writing a bunch of useful, stateful information to disk. And you'd love for the agent to be able to go away and something to replace it, and be able to get all of that information off of disk. Local durable volumes allow you to do that. And you get to do that in the same way that you're used to specifying durable or persistent volumes that are given to you by a cloud provider, for example.</p><p>Since I did co-found SIG testing, I think I have to call out a testing feature that I like. It's really tiny and silly, but it has always bugged me that when you try to download the tests, you download something that's over a gigabyte in size. That's the way things used work for Kubernetes back in the old days for Kubernetes client and server stuff as well. And we have since broken that up into-- you only need to download the binaries that makes sense for your platform.</p><p>So say I'm developing Kubernetes on my MacBook. I probably don't need to download the Linux test binaries, or the Windows test binaries, or the ARM64 test binaries, or the s390x test binaries. Did I mention Kubernetes supports a lot of different architectures?</p><p><b>CRAIG BOX: I hadn't noticed s390 was a supported platform until now.</b></p><p>AARON CRICKENBERGER: It is definitely something that we build binaries for. I'm not sure if we've actually seen a certified conformant Kubernetes that runs on s390, but it is definitely one of the things we build Kubernetes against.</p><p>Not having to download an entire gigabyte plus of binaries just to run some tests is super great. I like to live in a world where I don't have to build the tests from scratch. Can I please just run a program that has all the tests? Maybe I can use that to soak test or sanity test my cluster to make sure that everything is OK. And downloading just the thing that I need is super great.</p><p><b>CRAIG BOX: You're talking about the idea of Kubernetes having a core and the idea of releases and stability. If you think back to Linux distributions maybe even 10 years ago, we didn't care so much about the version number releases of the kernel anymore, but we cared when there was a new feature in a Red Hat release. Do you think we're getting to that point with Kubernetes at the moment?</b></p><p>AARON CRICKENBERGER: I think that is one model that people really hope to see Kubernetes move toward. I'm not sure if it is the model that we will move toward, but I think it is an ongoing discussion. So you know, we've created a working group called <a href=https://github.com/kubernetes/community/tree/master/wg-lts>WG LTS</a>. I like to call it by its longer name-- WG "to LTS, or not to LTS". What does LTS even mean? What are we trying to release and support?</p><p>Because I think that when people think about distributions, they do naturally gravitate towards some distributions have higher velocity release cadences, and others have slower release cadences. And that's cool and great for people who want to live on a piece of software that never ever changes. But those of us who run software at scale find that you can't actually prevent change from happening. There will always be pieces of your infrastructure, or your environment, or your software, that are not under your control.</p><p>And so anything we can do to achieve what I like to call a dynamic stability is probably better for everybody involved. Make the cost of change as low as you possibly can. Make the pain of changing and upgrade as low as you possibly can, and accept that everything will always be changing all the time.</p><p>So yeah. Maybe that's where Linux lives, where the Kernel is always changing. And you can either care about that, or not. And you can go with a distribution that is super up-to-date with the Linux Kernel, or maybe has a slightly longer upgrade cadence. But I think it's about enabling both of those options. Because I think if we try to live in a world where there are only distributions and nothing else, that's going to actually harm everybody in the long term and maybe bring us away from all of these cloud-native ideals that we have, trying to accept change as a constant.</p><p><b>ADAM GLICK: We can't let you go without talking about the Beard. What is SIG Beard, and how critical was it in you becoming the 1.14 release manager?</b></p><p>AARON CRICKENBERGER: I feel like it's a new requirement for all release leads to be a member of SIG Beard. SIG Beard happened because, one day, I realized I had gotten lazy, and I had this just ginormous and magnificent beard. It was really flattering to have Brendan Burns up on stage at KubeCon Seattle compliment my beard in front of an audience of thousands of people. I cannot tell you what that feels like.</p><p>But to be serious for a moment, like OK, I'm a dude. I have a beard. There are a lot of dudes who work in tech, and many dudes are bearded. And this is by no means a way of being exclusionary, or calling that out, or anything like that. It was just noticing that while I was on camera, there seemed to be more beard than face at times. And what is that about?</p><p>And I had somebody start referring to me as "The Beard" in my company. It turns out they read Neil Stevenson's "<a href=https://en.wikipedia.org/wiki/Cryptonomicon>Cryptonomicon</a>," if you're familiar with that book at all.</p><p><b>ADAM GLICK: It's a great book.</b></p><p>AARON CRICKENBERGER: Yeah. It talks about how you have the beard, and you have the suit. The suit is the person who's responsible for doing all the talking, and the beard is responsible for doing all the walking. And I guess I have gained a reputation for doing an awful lot of walking and showing up in an awful lot of places. And so I thought I would embrace that.</p><p>When I showed up to Google my first day at work where I was looking for the name tag that shows what desk is mine, and my name tag was SIG Beard. And I don't know who did it, but I was like, all right, I'm running with it. And so I referred to myself as "Aaron of SIG Beard" from then on.</p><p>And so to me, the beard is not so much about being bearded on my face, but being bearded at heart-- being welcoming, being fun, embracing this community for all of the awesomeness that it has, and encouraging other people to do the same. So in that regard, I would like to see more people be members of SIG Beard. I'm trying to figure out ways to make that happen. And yeah, it's great.</p><hr><p><i><a href=http://twitter.com/spiffxp>Aaron Crickenberger</a> is a senior test engineer with <a href=https://cloud.google.com/>Google Cloud</a>. He co-founded the Kubernetes Testing SIG, has participated in every Kubernetes release since version 1.4, has served on the <a href=https://github.com/kubernetes/steering>Kubernetes steering committee</a> since its inception in 2017, and most recently served as the Kubernetes 1.14 release lead.</p><p>You can find the <a href=http://www.kubernetespodcast.com/>Kubernetes Podcast from Google</a> at <a href=https://twitter.com/KubernetesPod>@kubernetespod</a> on Twitter, and you can <a href=https://kubernetespodcast.com/subscribe/>subscribe</a> so you never miss an episode. Please come and say Hello to us at KubeCon EU!</i></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d6c48cd505e152b46e8e288eb622abf3>Join us for the 2019 KubeCon Diversity Lunch & Hack</h1><div class="td-byline mb-4"><time datetime=2019-05-02 class=text-muted>Thursday, May 02, 2019</time></div><p><strong>Authors:</strong> Kiran Oliver, Podcast Producer, The New Stack</p><p>Join us for the 2019 KubeCon Diversity Lunch & Hack: Building Tech Skills & An Inclusive Community - Sponsored by Google Cloud and VMware</p><p>Registration for the Diversity Lunch opens today, May 2nd, 2019. To register, go to the main <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/schedule/>KubeCon + CloudNativeCon EU schedule</a>, then log in to your Sched account, and confirm your attendance to the Diversity Lunch. Please sign up ASAP once the link is live, as spaces will fill quickly. We filled the event in just a few days last year, and anticipate doing so again this year.</p><p>The 2019 KubeCon Diversity Lunch & Hack will be held at the Fira Gran Via Barcelona Hall 8.0 Room F1 on May 22nd, 2019 from 12:30-14:00.</p><p>If you’ve never attended a Diversity Lunch before, not to worry. All are welcome, and there’s a variety of things to experience and discuss.</p><p>First things first, let’s establish some ground rules:</p><p>This is a safe space. What does that mean? Simple:</p><ol><li>Asking for and using people’s pronouns</li><li>Absolutely no photography</li><li>Awareness of your actions towards others. Do your best to ensure that you contribute towards making this environment welcoming, safe, and inclusive for all.</li><li>Please avoid tech-heavy arbitrary community slang/jargon [keep in mind that not all of us are developers, many are tech-adjacent and/or new to the community]</li><li>Act with care and empathy towards your fellow community members at all times.</li></ol><p>This event also follows the <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/attend/code-of-conduct/>Code of Conduct</a> for all CNCF events.</p><p>We have run a very successful diversity lunch event before. This isn’t a trial run, nor is it a proof of concept. We had a fun, productive, and educational conversation last year in Seattle, and hope to do so again this year. As 2018’s KubeCon + CloudNativeCon in Seattle marked our first Diversity Lunch with pair programming, we hammered out a lot of kinks post-mortem, using that feedback to inform and improve upon our decision making, planning, and organizational process moving forward, to bring you an improved experience at the 2019 KubeCon + CloudNativeCon Diversity Lunch.</p><p>Tables not related to pair-programming or hands-on Kubernetes will be led by a moderator, where notes and feedback will then be taken and shared at the end of the lunch and in a post-mortem discussion after KubeCon+CloudNativeCon Barcelona ends, as part of our continuous improvement process. Some of last year’s tables were dedicated to topics that were submitted at registration, such as: security, D&I, service meshes, and more. You can suggest your own table topic on the registration form this year as well, and we highly encourage you to do so, particularly if you do not see your preferred topic or activity of choice listed. Your suggestions will then be used to determine the discussion table tracks that will be available at this year’s Diversity Lunch & Hack.</p><p>We hope you are also excited to participate in the ‘Hack’ portion of this ‘Lunch and Hack.’ This breakout track will include a variety of peer-programming exercises led by your fellow Kubernetes community members, with discussion leads working together with attendees hands-on to solve their Kubernetes-related problems in a welcoming, safe environment.</p><p>To make this all possible, we need you. Yes, you, to register. As much as we love having groups of diverse people all gather in the same room, we also need allies. If you’re a member of a privileged group or majority, you are welcome and encouraged to join us. Most importantly, we want you to take what you learn and experience at the Diversity Lunch back to both your companies and your open source communities, so that you can help us make positive changes not only within our industry, but beyond. No-one lives [or works] in a bubble. We hope that the things you learn here will carry over and bring about positive change in the world as a whole.</p><p>We look forward to seeing you!</p><p><em>Special thanks to <a href=https://www.linkedin.com/in/leahstunts/>Leah Petersen</a>, <a href=https://www.linkedin.com/in/sarah-conway-6166151/>Sarah Conway</a> and <a href=https://www.linkedin.com/in/parispittman/>Paris Pittman</a> for their help in editing this post.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d21acaf645c18e3c642d1cbc4482e575>How You Can Help Localize Kubernetes Docs</h1><div class="td-byline mb-4"><time datetime=2019-04-26 class=text-muted>Friday, April 26, 2019</time></div><p><strong>Author: Zach Corleissen (Linux Foundation)</strong></p><p>Last year we optimized the Kubernetes website for <a href=/blog/2018/11/08/kubernetes-docs-updates-international-edition/>hosting multilingual content</a>. Contributors responded by adding multiple new localizations: as of April 2019, Kubernetes docs are partially available in nine different languages, with six added in 2019 alone. You can see a list of available languages in the language selector at the top of each page.</p><p>By <em>partially available</em>, I mean that localizations are ongoing projects. They range from mostly complete (<a href=https://v1-12.docs.kubernetes.io/zh/>Chinese docs for 1.12</a>) to brand new (1.14 docs in <a href=https://kubernetes.io/pt/>Portuguese</a>). If you're interested in helping an existing localization, read on!</p><h2 id=what-is-a-localization>What is a localization?</h2><p>Translation is about words and meaning. Localization is about words, meaning, process, and design.</p><p>A localization is like a translation, but more thorough. Instead of just translating words, a localization optimizes the framework for writing and publishing words. For example, most site navigation features (button text) on kubernetes.io are strings contained in a <a href=https://github.com/kubernetes/website/tree/master/i18n>single file</a>. Part of creating a new localization involves adding a language-specific version of that file and translating the strings it contains.</p><p>Localization matters because it reduces barriers to adoption and support. When we can read Kubernetes docs in our own language, it's easier to get started using Kubernetes and contributing to its development.</p><h2 id=how-do-localizations-happen>How do localizations happen?</h2><p>The availability of docs in different languages is a feature—and like all Kubernetes features, contributors develop localized docs in a SIG, share them for review, and add them to the project.</p><p>Contributors work in teams to localize content. Because folks can't approve their own PRs, localization teams have a minimum size of two—for example, the Italian localization has two contributors. Teams can also be quite large: the Chinese team has several dozen contributors.</p><p>Each team has its own workflow. Some teams localize all content manually; others use editors with translation plugins and review machine output for accuracy. SIG Docs focuses on standards of output; this leaves teams free to adopt the workflow that works best for them. That said, teams frequently collaborate with each other on best practices, and sharing abounds in the best spirit of the Kubernetes community.</p><h2 id=helping-with-localizations>Helping with localizations</h2><p>If you're interested in starting a new localization for Kubernetes docs, the <a href=https://kubernetes.io/docs/contribute/localization/>Kubernetes contribution guide</a> shows you how.</p><p>Existing localizations also need help. If you'd like to contribute to an existing project, join the localization team's Slack channel and introduce yourself. Folks on that team can help you get started.</p><table><thead><tr><th>Localization</th><th>Slack channel</th></tr></thead><tbody><tr><td>Chinese (中文)</td><td><a href=https://kubernetes.slack.com/messages/CE3LNFYJ1/>#kubernetes-docs-zh</a></td></tr><tr><td>English</td><td><a href=https://kubernetes.slack.com/messages/C1J0BPD2M/>#sig-docs</a></td></tr><tr><td>French (Français)</td><td><a href=https://kubernetes.slack.com/messages/CG838BFT9/>#kubernetes-docs-fr</a></td></tr><tr><td>German (Deutsch)</td><td><a href=https://kubernetes.slack.com/messages/CH4UJ2BAL/>#kubernetes-docs-de</a></td></tr><tr><td>Hindi</td><td><a href=https://kubernetes.slack.com/messages/CJ14B9BDJ/>#kubernetes-docs-hi</a></td></tr><tr><td>Indonesian</td><td><a href=https://kubernetes.slack.com/messages/CJ1LUCUHM/>#kubernetes-docs-id</a></td></tr><tr><td>Italian</td><td><a href=https://kubernetes.slack.com/messages/CGB1MCK7X/>#kubernetes-docs-it</a></td></tr><tr><td>Japanese (日本語)</td><td><a href=https://kubernetes.slack.com/messages/CAG2M83S8/>#kubernetes-docs-ja</a></td></tr><tr><td>Korean (한국어)</td><td><a href=https://kubernetes.slack.com/messages/CA1MMR86S/>#kubernetes-docs-ko</a></td></tr><tr><td>Portuguese (Português)</td><td><a href=https://kubernetes.slack.com/messages/CJ21AS0NA/>#kubernetes-docs-pt</a></td></tr><tr><td>Spanish (Español)</td><td><a href=https://kubernetes.slack.com/messages/CH7GB2E3B/>#kubernetes-docs-es</a></td></tr></tbody></table><h2 id=what-s-next>What's next?</h2><p>There's a new <a href=https://kubernetes.slack.com/messages/CJ14B9BDJ/>Hindi localization</a> beginning. Why not add your language, too?</p><p>As a chair of SIG Docs, I'd love to see localization spread beyond the docs and into Kubernetes components. Is there a Kubernetes component you'd like to see supported in a different language? Consider making a <a href=https://github.com/kubernetes/enhancements/tree/master/keps>Kubernetes Enhancement Proposal</a> to support the change.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a761bb35081bc64e1f7247cf0de7cf0>Hardware Accelerated SSL/TLS Termination in Ingress Controllers using Kubernetes Device Plugins and RuntimeClass</h1><div class="td-byline mb-4"><time datetime=2019-04-24 class=text-muted>Wednesday, April 24, 2019</time></div><p><strong>Authors:</strong> Mikko Ylinen (Intel)</p><h2 id=abstract>Abstract</h2><p>A Kubernetes Ingress is a way to connect cluster services to the world outside the cluster. In order
to correctly route the traffic to service backends, the cluster needs an Ingress controller. The
Ingress controller is responsible for setting the right destinations to backends based on the
Ingress API objects’ information. The actual traffic is routed through a proxy server that
is responsible for tasks such as load balancing and SSL/TLS (later “SSL” refers to both SSL
or TLS ) termination. The SSL termination is a CPU heavy operation due to the crypto operations
involved. To offload some of the CPU intensive work away from the CPU, OpenSSL based proxy
servers can take the benefit of OpenSSL Engine API and dedicated crypto hardware. This frees
CPU cycles for other things and improves the overall throughput of the proxy server.</p><p>In this blog post, we will show how easy it is to make hardware accelerated crypto available
for containers running the Ingress controller proxy using some of the recently created Kubernetes
building blocks: Device plugin framework and RuntimeClass. At the end, a reference setup is given
using an HAproxy based Ingress controller accelerated using Intel® QuickAssist Technology cards.</p><h2 id=about-proxies-openssl-engine-and-crypto-hardware>About Proxies, OpenSSL Engine and Crypto Hardware</h2><p>The proxy server plays a vital role in a Kubernetes Ingress Controller function. It proxies
the traffic to the backends per Ingress objects routes. Under heavy traffic load, the performance
becomes critical especially if the proxying involves CPU intensive operations like SSL crypto.</p><p>The OpenSSL project provides the widely adopted library for implementing the SSL protocol. Of
the commonly known proxy servers used by Kubernetes Ingress controllers, Nginx and HAproxy use
OpenSSL. The CNCF graduated Envoy proxy uses BoringSSL but there seems to be <a href=https://github.com/envoyproxy/envoy/pull/5161#issuecomment-446374130>community interest
in having OpenSSL as the alternative</a> for it too.</p><p>The OpenSSL SSL protocol library relies on libcrypto that implements the cryptographic functions.
For quite some time now (first introduced in 0.9.6 release), OpenSSL has provided an <a href=https://github.com/openssl/openssl/blob/master/README.ENGINE>ENGINE
concept</a> that allows these cryptographic operations to be offloaded to a dedicated crypto
acceleration hardware. Later, a special <em>dynamic</em> ENGINE enabled the crypto hardware specific
pieces to be implemented in an independent loadable module that can be developed outside the
OpenSSL code base and distributed separately. From the application’s perspective, this is also
ideal because they don’t need to know the details of how to use the hardware, and the hardware
specific module can be loaded/used when the hardware is available.</p><p>Hardware based crypto can greatly improve Cloud applications’ performance due to hardware
accelerated processing in SSL operations as discussed, and can provide other crypto
services like key/random number generation. Clouds can make the hardware easily available
using the dynamic ENGINE and several loadable module implementations exist, for
example, <a href=https://docs.aws.amazon.com/cloudhsm/latest/userguide/openssl-library.html>CloudHSM</a>, <a href=https://github.com/opencryptoki/openssl-ibmca>IBMCA</a>, or <a href=https://github.com/intel/QAT_Engine/>QAT Engine</a>.</p><p>For Cloud deployments, the ideal scenario is for these modules to be shipped as part of
the container workload. The workload would get scheduled on a node that provides the
underlying hardware that the module needs to access. On the other hand, the workloads
should run the same way and without code modifications regardless of the crypto acceleration
hardware being available or not. The OpenSSL dynamic engine enables this. Figure 1 below
illustrates these two scenarios using a typical Ingress Controller container as an example.
The red colored boxes indicate the differences between a container with a crypto hardware
engine enabled container vs. a “standard” one. It’s worth pointing out that the configuration
changes shown do not necessarily require another version of the container since the configurations
could be managed, e.g., using ConfigMaps.</p><figure><img src=/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig1.png alt="Figure 1. Examples of Ingress controller containers" width=600><figcaption><p>Figure 1. Examples of Ingress controller containers</p></figcaption></figure><h2 id=hardware-resources-and-isolation>Hardware Resources and Isolation</h2><p>To be able to deploy workloads with hardware dependencies, Kubernetes provides excellent extension
and configurability mechanisms. Let’s take a closer look into Kubernetes the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>device plugin framework</a>
(beta in 1.14) and <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a> (beta in 1.14) and learn how they can be leveraged to expose crypto
hardware to workloads.</p><p>The device plugin framework, first introduced in Kubernetes 1.8, provides a way for hardware vendors
to register and allocate node hardware resources to Kubelets. The plugins implement the hardware
specific initialization logic and resource management. The pods can request hardware resources in
their PodSpec, which also guarantees the pod is scheduled on a node that can provide those resources.</p><p>The device resource allocation for containers is non-trivial. For applications dealing with security,
the hardware level isolation is critical. The PCIe based crypto acceleration device functions can
benefit from IO hardware virtualization, through an I/O Memory Management Unit (IOMMU), to provide
the isolation: an <em>IOMMU group</em> the device belongs to provides the isolated resource for a workload
(assuming the crypto cards do not share the IOMMU group with other devices). The number of isolated
resources can be further increased if the PCIe device supports the Single-Root I/O Virtualization
(SR-IOV) specification. SR-IOV allows the PCIe device to be split further to <em>virtual functions</em> (VF),
derived from <em>physical function</em> (PF) devices, and each belonging to their own IOMMU group. To expose
these IOMMU isolated device functions to user space and containers, the host kernel should bind
them to a specific device driver. In Linux, this driver is vfio-pci and it makes each device
available through a character device in user space. The kernel vfio-pci driver provides user space
applications with a direct, IOMMU backed access to PCIe devices and functions, using a mechanism
called <em>PCI passthrough</em>. The interface can be leveraged by user space frameworks, such as the
Data Plane Development Kit (DPDK). Additionally, virtual machine (VM) hypervisors can provide
these user space device nodes to VMs and expose them as PCI devices to the guest kernel.
Assuming support from the guest kernel, the VM gets close to native performant direct access to the
underlying host devices.</p><p>To advertise these device resources to Kubernetes, we can have a simple Kubernetes device plugin
that runs the initialization (i.e., binding), calls kubelet’s <code>Registration</code> gRPC service, and
implements the DevicePlugin gRPC service that kubelet calls to, e.g., to <code>Allocate</code> the resources
upon Pod creation.</p><h2 id=device-assignment-and-pod-deployment>Device Assignment and Pod Deployment</h2><p>At this point, you may ask what the container could do with a VFIO device node? The answer comes
after we first take a quick look into the Kubernetes RuntimeClass.</p><p>The Kubernetes RuntimeClass was created to provide better control and configurability
over a variety of <em>runtimes</em> (an earlier <a href=https://kubernetes.io/blog/2018/10/10/kubernetes-v1.12-introducing-runtimeclass/>blog post</a> goes into the details of the needs,
status and roadmap for it) that are available in the cluster. In essence, the RuntimeClass
provides cluster users better tools to pick and use the runtime that best suits for the pod use case.</p><p>The OCI compatible <a href=https://katacontainers.io/>Kata Containers runtime</a> provides workloads with a hardware virtualized
isolation layer. In addition to workload isolation, the Kata Containers VM has the added
side benefit that the VFIO devices, as <code>Allocate</code>’d by the device plugin, can be passed
through to the container as hardware isolated devices. The only requirement is that the
Kata Containers kernel has driver for the exposed device enabled.</p><p>That’s all it really takes to enable hardware accelerated crypto for container workloads. To summarize:</p><ol><li>Cluster needs a device plugin running on the node that provides the hardware</li><li>Device plugin exposes the hardware to user space using the VFIO driver</li><li>Pod requests the device resources and Kata Containers as the RuntimeClass in the PodSpec</li><li>The container has the hardware adaptation library and the OpenSSL engine module</li></ol><p>Figure 2 shows the overall setup using the Container A illustrated earlier.</p><figure><img src=/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig2.png alt="Figure 2. Deployment overview" width=600><figcaption><p>Figure 2. Deployment overview</p></figcaption></figure><h2 id=reference-setup>Reference Setup</h2><p>Finally, we describe the necessary building blocks and steps to build a functional
setup described in Figure 2 that enables hardware accelerated SSL termination in
an Ingress Controller using an Intel® QuickAssist Technology (QAT) PCIe device.
It should be noted that the use cases are not limited to Ingress controllers, but
any OpenSSL based workload can be accelerated.</p><h3 id=cluster-configuration>Cluster configuration:</h3><ul><li>Kubernetes 1.14 (<code>RuntimeClass</code> and <code>DevicePlugin</code> feature gates enabled (both are <code>true</code> in 1.14)</li><li>RuntimeClass ready runtime and Kata Containers configured</li></ul><h3 id=host-configuration>Host configuration:</h3><ul><li>Intel® QAT driver release with the kernel drivers installed for both host kernel and Kata Containers kernel (or on a rootfs as loadable modules)</li><li><a href=https://github.com/intel/intel-device-plugins-for-kubernetes/tree/master/cmd/qat_plugin>QAT device plugin</a> DaemonSet deployed</li></ul><h3 id=ingress-controller-configuration-and-deployment>Ingress controller configuration and deployment:</h3><ul><li><a href=https://github.com/jcmoraisjr/haproxy-ingress>HAproxy-ingress</a> ingress controller in a modified container that has<ul><li>the QAT HW HAL user space library (part of Intel® QAT SW release) and</li><li>the <a href=https://github.com/intel/QAT_Engine/>OpenSSL QAT Engine</a> built in</li></ul></li><li>Haproxy-ingress ConfigMap to enable QAT engine usage<ul><li><code>ssl-engine=”qat”</code></li><li><code>ssl-mode-async=true</code></li></ul></li><li>Haproxy-ingress deployment <code>.yaml</code> to<ul><li>Request <code>qat.intel.com: n</code> resources</li><li>Request <code>runtimeClassName: kata-containers</code> (name value depends on cluster config)</li></ul></li><li>(QAT device config file for each requested device resource with OpenSSL engine configured available in the container)</li></ul><p>Once the building blocks are available, the hardware accelerated SSL/TLS can be tested by following the <a href=https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/tls-termination>TLS termination
example</a> steps. In order to verify the hardware is used, you can check <code>/sys/kernel/debug/*/fw_counters</code> files on host as they
get updated by the Intel® QAT firmware.</p><p>Haproxy-ingress and HAproxy are used because HAproxy can be directly configured to use the OpenSSL engine using
<code>ssl-engine &lt;name> [algo ALGOs]</code> configuration flag without modifications to the global openssl configuration file.
Moreover, HAproxy can offload configured algorithms using asynchronous calls (with <code>ssl-mode-async</code>) to further improve performance.</p><h2 id=call-to-action>Call to Action</h2><p>In this blog post we have shown how Kubernetes Device Plugins and RuntimeClass can be used to provide isolated hardware
access for applications in pods to offload crypto operations to hardware accelerators. Hardware accelerators can be used
to speed up crypto operations and also save CPU cycles to other tasks. We demonstrated the setup using HAproxy that already
supports asynchronous crypto offload with OpenSSL.</p><p>The next steps for our team is to repeat the same for Envoy (with an OpenSSL based TLS transport socket built
as an extension). Furthermore, we are working to enhance Envoy to be able to <a href=https://github.com/envoyproxy/envoy/issues/6248>offload BoringSSL asynchronous
private key operations</a> to a crypto acceleration hardware. Any review feedback or help is appreciated!</p><p>How many CPU cycles can your crypto application save for other tasks when offloading crypto processing to a dedicated accelerator?</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9a73688c3a1583ce5dc650516cef2c37>Introducing kube-iptables-tailer: Better Networking Visibility in Kubernetes Clusters</h1><div class="td-byline mb-4"><time datetime=2019-04-19 class=text-muted>Friday, April 19, 2019</time></div><p><strong>Authors:</strong> Saifuding Diliyaer, Software Engineer, Box</p><p>At Box, we use Kubernetes to empower our engineers to own the whole lifecycle of their microservices. When it comes to networking, our engineers use Tigera’s <a href=https://www.tigera.io/tigera-calico/>Project Calico</a> to declaratively manage network policies for their apps running in our Kubernetes clusters. App owners define a Calico policy in order to enable their Pods to send/receive network traffic, which is instantiated as iptables rules.</p><p>There may be times, however, when such network policy is missing or declared incorrectly by app owners. In this situation, the iptables rules will cause network packet drops between the affected Pods, which get logged in a file that is inaccessible to app owners. We needed a mechanism to seamlessly deliver alerts about those iptables packet drops based on their network policies to help app owners quickly diagnose the corresponding issues. To solve this, we developed a service called <a href=https://github.com/box/kube-iptables-tailer>kube-iptables-tailer</a> to detect packet drops from iptables logs and report them as Kubernetes events. We are proud to open-source kube-iptables-tailer for you to utilize in your own cluster, regardless of whether you use Calico or other network policy tools.</p><h2 id=improved-experience-for-app-owners>Improved Experience for App Owners</h2><p>App owners do not have to apply any additional changes to utilize kube-iptables-tailer. They can simply run <code>kubectl describe pods</code> to check if any of their Pods' traffic has been dropped due to iptables rules. All the results sent from kube-iptables-tailer will be shown under the <em>Events</em> section, which is a much better experience for developers when compared to reading through raw iptables logs.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ kubectl describe pods --namespace<span style=color:#666>=</span>YOUR_NAMESPACE

...
Events:
 Type     Reason      Age    From                    Message
 ----     ------      ----   ----                    -------    
 Warning  PacketDrop  5s     kube-iptables-tailer    Packet dropped when receiving traffic from example-service-2 <span style=color:#666>(</span>IP: 22.222.22.222<span style=color:#666>)</span>.

 Warning  PacketDrop  10m    kube-iptables-tailer    Packet dropped when sending traffic to example-service-1 <span style=color:#666>(</span>IP: 11.111.11.111<span style=color:#666>)</span>.
</code></pre></div><p><em>* output of events sent from kube-iptables-tailer to Kubernetes Pods having networking issues</em></p><h2 id=process-behind-kube-iptables-tailer>Process behind kube-iptables-tailer</h2><p>Before we had kube-iptables-tailer, the only way for Box’s engineers to get information about packet drops related to their network policies was parsing through the raw iptables logs and matching their service IPs. This was a suboptimal experience because iptables logs only contain basic IP address information. Mapping these IPs to specific Pods could be painful, especially in the Kubernetes world where Pods and containers are ephemeral and IPs are frequently changing. This process involved a bunch of manual commands for our engineers. Additionally, iptables logs could be noisy due to a number of drops, and if IP addresses were being reused, the app owners might even have some stale data. With the help of kube-iptables-tailer, life now becomes much easier for our developers. As shown in the following diagram, the principle of this service can be divided into three steps:
<img src=https://i.imgur.com/fGAIVuS.png alt="sequence diagram for kube-iptables-tailer"></p><p><em>* sequence diagram for kube-iptables-tailer</em></p><h3 id=1-watch-changes-on-iptables-log-file>1. Watch changes on iptables log file</h3><p>Instead of requiring human engineers to manually decipher the raw iptables logs, we now use kube-iptables-tailer to help identify changes in that file. We run the service as a <strong>DaemonSet</strong> on every host node in our cluster, and it tails the iptables log file periodically. The service itself is written in Go, and it has multiple goroutines for the different service components running concurrently. We use channels to share information among those various components. In this step, for instance, the service will send out any changes it detected in iptables log file to a Go channel to be parsed later.</p><h3 id=2-parse-iptables-logs-based-on-log-prefix>2. Parse iptables logs based on log prefix</h3><p>Once the parser receives a new log message through a particular Go channel, it will first check whether the log message includes any network policy related packet drop information by parsing the log prefix. Packet drops based on our Calico policies will be logged containing “calico-drop:” as the log prefix in iptables log file. In this case, an object will be created by the parser with the data from the log message being stored as the object’s fields. These handy objects will be later used to locate the relevant Pods running in Kubernetes and post notifications directly to them. The parser is also able to identify duplicate logs and filter them to avoid causing confusion and consuming extra resources. After the parsing process, it will come to the final step for kube-iptables-tailer to send out the results.</p><h3 id=3-locate-pods-and-send-out-events>3. Locate pods and send out events</h3><p>Using the Kubernetes API, kube-iptables-tailer will try locating both senders and receivers in our cluster by matching the IPs stored in objects parsed from the previous step. As a result, an event will be posted to these affected Pods if they are located successfully. Kubernetes events are objects designed to provide information about what is happening inside a Kubernetes component. At Box, one of the use cases for Kubernetes events is to report errors directly to the corresponding applications (for more details, please refer to this <a href=https://kubernetes.io/blog/2018/01/reporting-errors-using-kubernetes-events/>blog post</a>). The event generated by kube-iptables-tailer includes useful information such as traffic direction, IPs and the namespace of Pods from the other side. We have added DNS lookup as well because our Pods also send and receive traffic from services running on bare-metal hosts and VMs. Besides, exponential backoff is implemented to avoid overwhelming the Kubernetes API server.</p><h2 id=summary>Summary</h2><p>At Box, kube-iptables-tailer has saved time as well as made life happier for many developers across various teams. Instead of flying blind with regards to packet drops based on network policies, the service is able to help detect changes in iptables log file and get the corresponding information delivered right to the Pods inside Kubernetes clusters. If you’re not using Calico, you can still apply any other log prefix (configured as an environment variable in the service) to match whatever is defined in your iptables rules and get notified about the network policy related packet drops. You may also find other cases where it is useful to make information from host systems available to Pods via the Kubernetes API. As an open-sourced project, every contribution is more than welcome to help improve the project together. You can find this project hosted on Github at <a href=https://github.com/box/kube-iptables-tailer>https://github.com/box/kube-iptables-tailer</a></p><p><em>Special thanks to <a href=https://www.linkedin.com/in/kunalparmar/>Kunal Parmar</a>, <a href=https://www.linkedin.com/in/greg-lyons-8277a188/>Greg Lyons</a> and <a href=https://www.linkedin.com/in/shrenikd/>Shrenik Dedhia</a> for contributing to this project.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-29a85b82a82ea398f1862b692cc0983e>The Future of Cloud Providers in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2019-04-17 class=text-muted>Wednesday, April 17, 2019</time></div><p><strong>Authors:</strong> Andrew Sy Kim (VMware), Mike Crute (AWS), Walter Fender (Google)</p><p>Approximately 9 months ago, the Kubernetes community agreed to form the Cloud Provider Special Interest Group (SIG). The justification was to have a single governing SIG to own and shape the integration points between Kubernetes and the many cloud providers it supported. A lot has been in motion since then and we’re here to share with you what has been accomplished so far and what we hope to see in the future.</p><h2 id=the-mission>The Mission</h2><p>First and foremost, I want to share what the mission of the SIG is, because we use it to guide our present & future work. Taken straight from our <a href=https://github.com/kubernetes/community/blob/master/sig-cloud-provider/CHARTER.md>charter</a>, the mission of the SIG is to simplify, develop and maintain cloud provider integrations as extensions, or add-ons, to Kubernetes clusters. The motivation behind this is two-fold: to ensure Kubernetes remains extensible and cloud agnostic.</p><h2 id=the-current-state-of-cloud-providers>The Current State of Cloud Providers</h2><p>In order to gain a forward looking perspective to our work, I think it’s important to take a step back to look at the current state of cloud providers. Today, each core Kubernetes component (except the scheduler and kube-proxy) has a --cloud-provider flag you can configure to enable a set of functionalities that integrate with the underlying infrastructure provider, a.k.a the cloud provider. Enabling this integration unlocks a wide set of features for your clusters such as: node address & zone discovery, cloud load balancers for Services with Type=LoadBalancer, IP address management, and cluster networking via VPC routing tables. Today, the cloud provider integrations can be done either in-tree or out-of-tree.</p><h2 id=in-tree-out-of-tree-providers>In-Tree & Out-of-Tree Providers</h2><p>In-tree cloud providers are the providers we develop & release in the <a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/cloudprovider/providers>main Kubernetes repository</a>. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.</p><center><figure><img src=/images/docs/pre-ccm-arch.png alt="In-Tree Cloud Provider Architecture (source: kubernetes.io)" width=600><figcaption><p>In-Tree Cloud Provider Architecture (source: kubernetes.io)</p></figcaption></figure></center><p>Out-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.</p><center><figure><img src=/images/docs/post-ccm-arch.png alt="Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)" width=600><figcaption><p>Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)</p></figcaption></figure></center><p>When cloud provider integrations were initially developed, they were developed natively (in-tree). We integrated each provider close to the core of Kubernetes and within the monolithic repository that is k8s.io/kubernetes today. As Kubernetes became more ubiquitous and more infrastructure providers wanted to support Kubernetes natively, we realized that this model was not going to scale. Each provider brings along a large set of dependencies which increases potential vulnerabilities in our code base and significantly increases the binary size of each component. In addition to this, more of the Kubernetes release notes started to focus on provider specific changes rather than core changes that impacted all Kubernetes users.</p><p>In late 2017, we developed a way for cloud providers to build integrations without adding them to the main Kubernetes tree (out-of-tree). This became the de-facto way for new infrastructure providers in the ecosystem to integrate with Kubernetes. Since then, we’ve been actively working towards migrating all cloud providers to use the out-of-tree architecture as most clusters today are still using the in-tree cloud providers.</p><h2 id=looking-ahead>Looking Ahead</h2><p>Looking ahead, the goal of the SIG is to remove all existing in-tree cloud providers in favor of their out-of-tree equivalents with minimal impact to users. In addition to the core cloud provider integration mentioned above, there are more extension points for cloud integrations like CSI and the image credential provider that are actively being worked on for v1.15. Getting to this point would mean that Kubernetes is truly cloud-agnostic with no native integrations for any cloud provider. By doing this work we empower each cloud provider to develop and release new versions at their own cadence independent of Kubernetes. We’ve learned by now that this is a large feat with a unique set of challenges. Migrating workloads is never easy, especially when it’s an essential part of the control plane. Providing a safe and easy migration path between in-tree and out-of-tree cloud providers is of the highest priority for our SIG in the upcoming releases. If any of this sounds interesting to you, I encourage you to check out of some of our <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider>KEPs</a> and get in touch with our SIG by joining the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cloud-provider>mailing list</a> or our slack channel (#sig-cloud-provider in Kubernetes slack).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-edde2cfeb109220abd1d26cb390060b0>Pod Priority and Preemption in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2019-04-16 class=text-muted>Tuesday, April 16, 2019</time></div><p><strong>Author</strong>: Bobby Salamat</p><p>Kubernetes is well-known for running scalable workloads. It scales your workloads based on their resource usage. When a workload is scaled up, more instances of the application get created. When the application is critical for your product, you want to make sure that these new instances are scheduled even when your cluster is under resource pressure. One obvious solution to this problem is to over-provision your cluster resources to have some amount of slack resources available for scale-up situations. This approach often works, but costs more as you would have to pay for the resources that are idle most of the time.</p><p><a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/>Pod priority and preemption</a> is a scheduler feature made generally available in Kubernetes 1.14 that allows you to achieve high levels of scheduling confidence for your critical workloads without overprovisioning your clusters. It also provides a way to improve resource utilization in your clusters without sacrificing the reliability of your essential workloads.</p><h2 id=guaranteed-scheduling-with-controlled-cost>Guaranteed scheduling with controlled cost</h2><p><a href=https://github.com/kubernetes/autoscaler/>Kubernetes Cluster Autoscaler</a> is an excellent tool in the ecosystem which adds more nodes to your cluster when your applications need them. However, cluster autoscaler has some limitations and may not work for all users:</p><ul><li>It does not work in physical clusters.</li><li>Adding more nodes to the cluster costs more.</li><li>Adding nodes is not instantaneous and could take minutes before those nodes become available for scheduling.</li></ul><p>An alternative is Pod Priority and Preemption. In this approach, you combine multiple workloads in a single cluster. For example, you may run your CI/CD pipeline, ML workloads, and your critical service in the same cluster. When multiple workloads run in the same cluster, the size of your cluster is larger than a cluster that you would use to run only your critical service. If you give your critical service the highest priority and your CI/CD and ML workloads lower priority, when your service needs more computing resources, the scheduler preempts (evicts) enough pods of your lower priority workloads, e.g., ML workload, to allow all your higher priority pods to schedule.</p><p>With pod priority and preemption you can set a maximum size for your cluster in the Autoscaler configuration to ensure your costs get controlled without sacrificing availability of your service. Moreover, preemption is much faster than adding new nodes to the cluster. Within seconds your high priority pods are scheduled, which is critical for latency sensitive services.</p><h2 id=improve-cluster-resource-utilization>Improve cluster resource utilization</h2><p>Cluster operators who run critical services learn over time a rough estimate of the number of nodes that they need in their clusters to achieve high service availability. The estimate is usually conservative. Such estimates take bursts of traffic into account to find the number of required nodes. Cluster autoscaler can be configured never to reduce the size of the cluster below this level. The only problem is that such estimates are often conservative and cluster resources may remain underutilized most of the time. Pod priority and preemption allows you to improve resource utilization significantly by running a non-critical workload in the cluster.</p><p>The non-critical workload may have many more pods that can fit in the cluster. If you give a negative priority to your non-critical workload, Cluster Autoscaler does not add more nodes to your cluster when the non-critical pods are pending. Therefore, you won’t incur higher expenses. When your critical workload requires more computing resources, the scheduler preempts non-critical pods and schedules critical ones.</p><p>The non-critical pods fill the “holes” in your cluster resources which improves resource utilization without raising your costs.</p><h2 id=get-involved>Get Involved</h2><p>If you have feedback for this feature or are interested in getting involved with the design and development, join the <a href=https://github.com/kubernetes/community/tree/master/sig-scheduling>Scheduling Special Interest Group</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d59cccde9a0772812b0e105b30f5b7c7>Process ID Limiting for Stability Improvements in Kubernetes 1.14</h1><div class="td-byline mb-4"><time datetime=2019-04-15 class=text-muted>Monday, April 15, 2019</time></div><p><strong>Author: Derek Carr</strong></p><p>Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming “Om nom nom nom.”</p><p>In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.</p><h2 id=can-you-spare-some-pids>Can You Spare Some PIDs?</h2><p>Here, we’re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.</p><p>In such a scenario, it’s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.</p><h2 id=fixing-the-problem>Fixing the Problem</h2><p>Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.</p><p>This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, we’ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs--similar to how one reserves CPU or memory today--and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, we’ll have protection against an easily starved Linux resource.</p><p>Get started with <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0>Kubernetes 1.14</a>.</p><h2 id=get-involved>Get Involved</h2><p>If you have feedback for this feature or are interested in getting involved with the design and development, join the <a href=https://github.com/kubernetes/community/tree/master/sig-node>Node Special Interest Group</a>.</p><h3 id=about-the-author>About the author:</h3><p>Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-710bd32d821afac28e34b616fa45b092>Kubernetes 1.14: Local Persistent Volumes GA</h1><div class="td-byline mb-4"><time datetime=2019-04-04 class=text-muted>Thursday, April 04, 2019</time></div><p><strong>Authors</strong>: Michelle Au (Google), Matt Schallert (Uber), Celina Ward (Uber)</p><p>The <a href=https://kubernetes.io/docs/concepts/storage/volumes/#local>Local Persistent Volumes</a>
feature has been promoted to GA in Kubernetes 1.14.
It was first introduced as alpha in Kubernetes 1.7, and then
<a href=https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/>beta</a> in Kubernetes
1.10. The GA milestone indicates that Kubernetes users may depend on the feature
and its API for production use. GA features are protected by the Kubernetes
<a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/>deprecation
policy</a>.</p><h2 id=what-is-a-local-persistent-volume>What is a Local Persistent Volume?</h2><p>A local persistent volume represents a local disk directly-attached to a single
Kubernetes Node.</p><p>Kubernetes provides a powerful volume plugin system that enables Kubernetes
workloads to use a <a href=https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes>wide
variety</a>
of block and file storage to persist data. Most
of these plugins enable remote storage -- these remote storage systems persist
data independent of the Kubernetes node where the data originated. Remote
storage usually can not offer the consistent high performance guarantees of
local directly-attached storage. With the Local Persistent Volume plugin,
Kubernetes workloads can now consume high performance local storage using the
same volume APIs that app developers have become accustomed to.</p><h2 id=how-is-it-different-from-a-hostpath-volume>How is it different from a HostPath Volume?</h2><p>To better understand the benefits of a Local Persistent Volume, it is useful to
compare it to a <a href=https://kubernetes.io/docs/concepts/storage/volumes/#hostpath>HostPath volume</a>.
HostPath volumes mount a file or directory from
the host node’s filesystem into a Pod. Similarly a Local Persistent Volume
mounts a local disk or partition into a Pod.</p><p>The biggest difference is that the Kubernetes scheduler understands which node a
Local Persistent Volume belongs to. With HostPath volumes, a pod referencing a
HostPath volume may be moved by the scheduler to a different node resulting in
data loss. But with Local Persistent Volumes, the Kubernetes scheduler ensures
that a pod using a Local Persistent Volume is always scheduled to the same node.</p><p>While HostPath volumes may be referenced via a Persistent Volume Claim (PVC) or
directly inline in a pod definition, Local Persistent Volumes can only be
referenced via a PVC. This provides additional security benefits since
Persistent Volume objects are managed by the administrator, preventing Pods from
being able to access any path on the host.</p><p>Additional benefits include support for formatting of block devices during
mount, and volume ownership using fsGroup.</p><h2 id=what-s-new-with-ga>What's New With GA?</h2><p>Since 1.10, we have mainly focused on improving stability and scalability of the
feature so that it is production ready.</p><p>The only major feature addition is the ability to specify a raw block device and
have Kubernetes automatically format and mount the filesystem. This reduces the
previous burden of having to format and mount devices before giving it to
Kubernetes.</p><h2 id=limitations-of-ga>Limitations of GA</h2><p>At GA, Local Persistent Volumes do not support <a href=https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/>dynamic volume
provisioning</a>.
However there is an <a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner>external
controller</a>
available to help manage the local
PersistentVolume lifecycle for individual disks on your nodes. This includes
creating the PersistentVolume objects, cleaning up and reusing disks once they
have been released by the application.</p><h2 id=how-to-use-a-local-persistent-volume>How to Use a Local Persistent Volume?</h2><p>Workloads can request a local persistent volume using the same
PersistentVolumeClaim interface as remote storage backends. This makes it easy
to swap out the storage backend across clusters, clouds, and on-prem
environments.</p><p>First, a StorageClass should be created that sets <code>volumeBindingMode: WaitForFirstConsumer</code> to enable <a href=https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode>volume topology-aware
scheduling</a>.
This mode instructs Kubernetes to wait to bind a PVC until a Pod using it is scheduled.</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
</code></pre><p>Then, the external static provisioner can be <a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#user-guide>configured and
run</a> to create PVs
for all the local disks on your nodes.</p><pre><code>$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM  STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Available          local-storage              8s
local-pv-3796b049   368Gi      RWO            Delete           Available          local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Available          local-storage              7s
</code></pre><p>Afterwards, workloads can start using the PVs by creating a PVC and Pod or a
StatefulSet with volumeClaimTemplates.</p><pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: local-test
spec:
  serviceName: &quot;local-service&quot;
  replicas: 3
  selector:
    matchLabels:
      app: local-test
  template:
    metadata:
      labels:
        app: local-test
    spec:
      containers:
      - name: test-container
        image: k8s.gcr.io/busybox
        command:
        - &quot;/bin/sh&quot;
        args:
        - &quot;-c&quot;
        - &quot;sleep 100000&quot;
        volumeMounts:
        - name: local-vol
          mountPath: /usr/test-pod
  volumeClaimTemplates:
  - metadata:
      name: local-vol
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: &quot;local-storage&quot;
      resources:
        requests:
          storage: 368Gi
</code></pre><p>Once the StatefulSet is up and running, the PVCs are all bound:</p><pre><code>$ kubectl get pvc
NAME                     STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS      AGE
local-vol-local-test-0   Bound    local-pv-27c0f084   368Gi      RWO            local-storage     3m45s
local-vol-local-test-1   Bound    local-pv-3ddecaea   368Gi      RWO            local-storage     3m40s
local-vol-local-test-2   Bound    local-pv-3796b049   368Gi      RWO            local-storage     3m36s
</code></pre><p>When the disk is no longer needed, the PVC can be deleted. The external static provisioner
will clean up the disk and make the PV available for use again.</p><pre><code>$ kubectl patch sts local-test -p '{&quot;spec&quot;:{&quot;replicas&quot;:2}}'
statefulset.apps/local-test patched

$ kubectl delete pvc local-vol-local-test-2
persistentvolumeclaim &quot;local-vol-local-test-2&quot; deleted

$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                            STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Bound       default/local-vol-local-test-0   local-storage              11m
local-pv-3796b049   368Gi      RWO            Delete           Available                                    local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Bound       default/local-vol-local-test-1   local-storage              19m
</code></pre><p>You can find full <a href=https://kubernetes.io/docs/concepts/storage/volumes/#local>documentation</a>
for the feature on the Kubernetes website.</p><h2 id=what-are-suitable-use-cases>What Are Suitable Use Cases?</h2><p>The primary benefit of Local Persistent Volumes over remote persistent storage
is performance: local disks usually offer higher IOPS and throughput and lower
latency compared to remote storage systems.</p><p>However, there are important limitations and caveats to consider when using
Local Persistent Volumes:</p><ul><li>Using local storage ties your application to a specific node, making your
application harder to schedule. Applications which use local storage should
specify a high priority so that lower priority pods, that don’t require local
storage, can be preempted if necessary.</li><li>If that node or local volume encounters a failure and becomes inaccessible, then
that pod also becomes inaccessible. Manual intervention, external controllers,
or operators may be needed to recover from these situations.</li><li>While most remote storage systems implement synchronous replication, most local
disk offerings do not provide data durability guarantees. Meaning loss of the
disk or node may result in loss of all the data on that disk</li></ul><p>For these reasons, local persistent storage should only be considered for
workloads that handle data replication and backup at the application layer, thus
making the applications resilient to node or data failures and unavailability
despite the lack of such guarantees at the individual disk level.</p><p>Examples of good workloads include software defined storage systems and
replicated databases. Other types of applications should continue to use highly
available, remotely accessible, durable storage.</p><h2 id=how-uber-uses-local-storage>How Uber Uses Local Storage</h2><p><a href=https://eng.uber.com/m3/>M3</a>, Uber’s in-house metrics platform,
piloted Local Persistent Volumes at scale
in an effort to evaluate <a href=https://m3db.io/>M3DB</a> —
an open-source, distributed timeseries database
created by Uber. One of M3DB’s notable features is its ability to shard its
metrics into partitions, replicate them by a factor of three, and then evenly
disperse the replicas across separate failure domains.</p><p>Prior to the pilot with local persistent volumes, M3DB ran exclusively in
Uber-managed environments. Over time, internal use cases arose that required the
ability to run M3DB in environments with fewer dependencies. So the team began
to explore options. As an open-source project, we wanted to provide the
community with a way to run M3DB as easily as possible, with an open-source
stack, while meeting M3DB’s requirements for high throughput, low-latency
storage, and the ability to scale itself out.</p><p>The Kubernetes Local Persistent Volume interface, with its high-performance,
low-latency guarantees, quickly emerged as the perfect abstraction to build on
top of. With Local Persistent Volumes, individual M3DB instances can comfortably
handle up to 600k writes per-second. This leaves plenty of headroom for spikes
on clusters that typically process a few million metrics per-second.</p><p>Because M3DB also gracefully handles losing a single node or volume, the limited
data durability guarantees of Local Persistent Volumes are not an issue. If a
node fails, M3DB finds a suitable replacement and the new node begins streaming
data from its two peers.</p><p>Thanks to the Kubernetes scheduler’s intelligent handling of volume topology,
M3DB is able to programmatically evenly disperse its replicas across multiple
local persistent volumes in all available cloud zones, or, in the case of
on-prem clusters, across all available server racks.</p><h2 id=uber-s-operational-experience>Uber's Operational Experience</h2><p>As mentioned above, while Local Persistent Volumes provide many benefits, they
also require careful planning and careful consideration of constraints before
committing to them in production. When thinking about our local volume strategy
for M3DB, there were a few things Uber had to consider.</p><p>For one, we had to take into account the hardware profiles of the nodes in our
Kubernetes cluster. For example, how many local disks would each node cluster
have? How would they be partitioned?</p><p>The local static provisioner provides
<a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/best-practices.md>guidance</a>
to help answer these questions. It’s best to be able to dedicate a full disk to each local volume
(for IO isolation) and a full partition per-volume (for capacity isolation).
This was easier in our cloud environments where we could mix and match local
disks. However, if using local volumes on-prem, hardware constraints may be a
limiting factor depending on the number of disks available and their
characteristics.</p><p>When first testing local volumes, we wanted to have a thorough understanding of
the effect
<a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>disruptions</a>
(voluntary and involuntary) would have on pods using
local storage, and so we began testing some failure scenarios. We found that
when a local volume becomes unavailable while the node remains available (such
as when performing maintenance on the disk), a pod using the local volume will
be stuck in a ContainerCreating state until it can mount the volume. If a node
becomes unavailable, for example if it is removed from the cluster or is
<a href=https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/>drained</a>,
then pods using local volumes on that node are stuck in an Unknown or
Pending state depending on whether or not the node was removed gracefully.</p><p>Recovering pods from these interim states means having to delete the PVC binding
the pod to its local volume and then delete the pod in order for it to be
rescheduled (or wait until the node and disk are available again). We took this
into account when building our <a href=https://github.com/m3db/m3db-operator>operator</a>
for M3DB, which makes changes to the
cluster topology when a pod is rescheduled such that the new one gracefully
streams data from the remaining two peers. Eventually we plan to automate the
deletion and rescheduling process entirely.</p><p>Alerts on pod states can help call attention to stuck local volumes, and
workload-specific controllers or operators can remediate them automatically.
Because of these constraints, it’s best to exclude nodes with local volumes from
automatic upgrades or repairs, and in fact some cloud providers explicitly
mention this as a best practice.</p><h2 id=portability-between-on-prem-and-cloud>Portability Between On-Prem and Cloud</h2><p>Local Volumes played a big role in Uber’s decision to build orchestration for
M3DB using Kubernetes, in part because it is a storage abstraction that works
the same across on-prem and cloud environments. Remote storage solutions have
different characteristics across cloud providers, and some users may prefer not
to use networked storage at all in their own data centers. On the other hand,
local disks are relatively ubiquitous and provide more predictable performance
characteristics.</p><p>By orchestrating M3DB using local disks in the cloud, where it was easier to get
up and running with Kubernetes, we gained confidence that we could still use our
operator to run M3DB in our on-prem environment without any modifications. As we
continue to work on how we’d run Kubernetes on-prem, having solved such an
important pending question is a big relief.</p><h2 id=what-s-next-for-local-persistent-volumes>What's Next for Local Persistent Volumes?</h2><p>As we’ve seen with Uber’s M3DB, local persistent volumes have successfully been
used in production environments. As adoption of local persistent volumes
continues to increase, SIG Storage continues to seek feedback for ways to
improve the feature.</p><p>One of the most frequent asks has been for a controller that can help with
recovery from failed nodes or disks, which is currently a manual process (or
something that has to be built into an operator). SIG Storage is investigating
creating a common controller that can be used by workloads with simple and
similar recovery processes.</p><p>Another popular ask has been to support dynamic provisioning using lvm. This can
simplify disk management, and improve disk utilization. SIG Storage is
evaluating the performance tradeoffs for the viability of this feature.</p><h2 id=getting-involved>Getting Involved</h2><p>If you have feedback for this feature or are interested in getting involved with
the design and development, join the <a href=https://github.com/kubernetes/community/blob/master/sig-storage/README.md>Kubernetes Storage
Special-Interest-Group</a>
(SIG). We’re rapidly growing and always welcome new contributors.</p><p>Special thanks to all the contributors that helped bring this feature to GA,
including Chuqiang Li (lichuqiang), Dhiraj Hedge (dhirajh), Ian Chakeres
(ianchakeres), Jan Šafránek (jsafrane), Michelle Au (msau42), Saad Ali
(saad-ali), Yecheng Fu (cofyc) and Yuquan Ren (nickrenren).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c1183bd13c423e576d983d923912b48f>Kubernetes v1.14 delivers production-level support for Windows nodes and Windows containers</h1><div class="td-byline mb-4"><time datetime=2019-04-01 class=text-muted>Monday, April 01, 2019</time></div><p><strong>Authors:</strong> Michael Michael (VMware), Patrick Lang (Microsoft)</p><p>The first release of Kubernetes in 2019 brings a highly anticipated feature - production-level support for Windows workloads. Up until now Windows node support in Kubernetes has been in beta, allowing many users to experiment and see the value of Kubernetes for Windows containers. While in beta, developers in the Kubernetes community and Windows Server team worked together to improve the container runtime, build a continuous testing process, and complete features needed for a good user experience. Kubernetes now officially supports adding Windows nodes as worker nodes and scheduling Windows containers, enabling a vast ecosystem of Windows applications to leverage the power of our platform.</p><p>As Windows developers and devops engineers have been adopting containers over the last few years, they've been looking for a way to manage all their workloads with a common interface. Kubernetes has taken the lead for container orchestration, and this gives users a consistent way to manage their container workloads whether they need to run on Linux or Windows.</p><p>The journey to a stable release of Windows in Kubernetes was not a walk in the park. The community has been working on Windows support for 3 years, delivering an alpha release with v1.5, a beta with v1.9, and now a stable release with v1.14. We would not be here today without rallying broad support and getting significant contributions from companies including Microsoft, Docker, VMware, Pivotal, Cloudbase Solutions, Google and Apprenda. During this journey, there were 3 critical points in time that significantly advanced our progress.</p><ol><li>Advancements in Windows Server container networking that provided the infrastructure to create CNI (Container Network Interface) plugins</li><li>Enhancements shipped in Windows Server semi-annual channel releases enabled Kubernetes development to move forward - culminating with Windows Server 2019 on the Long-Term Servicing Channel. This is the best release of Windows Server for running containers.</li><li>The adoption of the KEP (Kubernetes Enhancement Proposals) <a href=https://github.com/kubernetes/enhancements/blob/master/keps/README.md>process</a>. The Windows KEP outlined a clear and agreed upon set of goals, expectations, and deliverables based on review and feedback from stakeholders across multiple SIGs. This created a clear plan that SIG-Windows could follow, paving the path towards this stable release.</li></ol><p>With v1.14, we're declaring that Windows node support is stable, well-tested, and ready for adoption in production scenarios. This is a huge milestone for many reasons. For Kubernetes, it strengthens its position in the industry, enabling a vast ecosystem of Windows-based applications to be deployed on the platform. For Windows operators and developers, this means they can use the same tools and processes to manage their Windows and Linux workloads, taking full advantage of the efficiencies of the cloud-native ecosystem powered by Kubernetes. Let’s dig in a little bit into these.</p><h2 id=operator-advantages>Operator Advantages</h2><ul><li>Gain operational efficiencies by leveraging existing investments in solutions, tools, and technologies to manage Windows containers the same way as Linux containers</li><li>Knowledge, training and expertise on container orchestration transfers to Windows container support</li><li>IT can deliver a scalable self-service container platform to Linux and Windows developers</li></ul><h2 id=developer-advantages>Developer Advantages</h2><ul><li>Containers simplify packaging and deploying applications during development and test. Now you also get to take advantage of Kubernetes’ benefits in creating reliable, secure, and scalable distributed applications.</li><li>Windows developers can now take advantage of the growing ecosystem of cloud and container-native tools to build and deploy faster, resulting in a faster time to market for their applications</li><li>Taking advantage of Kubernetes as the leader in container orchestration, developers only need to learn how to use Kubernetes and that skillset will transfer across development environments and across clouds</li></ul><h2 id=cio-advantages>CIO Advantages</h2><ul><li>Leverage the operational and cost efficiencies that are introduced with Kubernetes</li><li>Containerize existing.NET applications or Windows-based workloads to eliminate old hardware or underutilized virtual machines, and streamline migration from end-of-support OS versions. You retain the benefit your application brings to the business, but decrease the cost of keeping it running</li></ul><p>“Using Kubernetes on Windows allows us to run our internal web applications as microservices. This provides quick scaling in response to load, smoother upgrades, and allows for different development groups to build without worry of other group's version dependencies. We save money because development times are shorter and operation's time is not spent maintaining multiple virtual machine environments,” said Jeremy, a lead devops engineer working for a top multinational legal firm, one of the early adopters of Windows on Kubernetes.</p><p>There are many features that are surfaced with this release. We want to turn your attention to a few key features and enablers of Windows support in Kubernetes. For a detailed list of supported functionality, you can read our <a href=https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#supported-functionality>documentation</a>.</p><ul><li>You can now add Windows Server 2019 worker nodes</li><li>You can now schedule Windows containers utilizing deployments, pods, services, and workload controllers</li><li>Out of tree CNI plugins are provided for Azure, OVN-Kubernetes, and Flannel</li><li>Containers can utilize a variety of in and out-of-tree storage plugins</li><li>Improved support for metrics/quotas closely matches the capabilities offered for Linux containers</li></ul><p>When looking at Windows support in Kubernetes, many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between <strong>operational limitations and differences between the Windows and Linux operating systems</strong>. From a container management standpoint, we must strike a balance between preserving OS-specific behaviors required for application compatibility, and reaching operational consistency in Kubernetes across multiple operating systems. For example, some Linux-specific file system features, user IDs and permissions exposed through Kubernetes will not work on Windows today, and users are familiar with these fundamental differences. We will also be adding support for Windows-specific configurations to meet the needs of Windows customers that may not exist on Linux. The alpha support for Windows Group Managed Service Accounts is one example. Other areas such as memory reservations for Windows pods and the Windows kubelet are a work in progress and highlight an operational limitation. We will continue working on operational limitations based on what’s important to our community in future releases.</p><p>Today, Kubernetes master components will continue to run on Linux. That way users can add Windows nodes without having to create a separate Kubernetes cluster. As always, our future direction is set by the community, so more components, features and deployment methods will come over time. Users should understand the differences between Windows and Linux and utilize the advantages of each platform. Our goal with this release is not to make Windows interchangeable with Linux or to answer the question of Windows vs Linux. We offer consistency in management. Managing workloads without automation is tedious and expensive. Rewriting or re-architecting workloads is even more expensive. Containers provide a clear path forward whether your app runs on Linux or Windows, and Kubernetes brings an IT organization operational consistency.</p><p>As a community, our work is not complete. As already mentioned , we still have a fair bit of <a href=https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#limitations>limitations</a> and a healthy <a href=https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#what-s-next>roadmap</a>. We will continue making progress and enhancing Windows container support in Kubernetes, with some notable upcoming features including:</p><ul><li>Support for CRI-ContainerD and Hyper-V isolation, bringing hypervisor-level isolation between pods for additional security and extending our container-to-node compatibility matrix</li><li>Additional network plugins, including the stable release of Flannel overlay support</li><li>Simple heterogeneous cluster creation using kubeadm on Windows</li></ul><p>We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.</p><ul><li>Read our getting started and contributor guides, which include links to the community meetings and past recordings, at <a href=https://github.com/kubernetes/community/tree/master/sig-windows>https://github.com/kubernetes/community/tree/master/sig-windows</a></li><li>Explore our documentation at <a href=https://kubernetes.io/docs/setup/production-environment/windows/>https://kubernetes.io/docs/setup/production-environment/windows/</a></li><li>Join us on <a href=https://kubernetes.slack.com/messages/sig-windows>Slack</a> or the <a href=https://discuss.kubernetes.io/c/general-discussions/windows>Kubernetes Community Forums</a> to chat about Windows containers on Kubernetes.</li></ul><p>Thank you and feel free to reach us individually if you have any questions.</p><p>Michael Michael<br>SIG-Windows Chair<br>Director of Product Management, VMware<br>@michmike77 on Twitter<br>@m2 on Slack</p><p>Patrick Lang<br>SIG-Windows Chair<br>Senior Software Engineer, Microsoft<br>@PatrickLang on Slack</p></div><div class=td-content style=page-break-before:always><h1 id=pg-13d6a556a0a6fc7a3c2f8be730f926db>kube-proxy Subtleties: Debugging an Intermittent Connection Reset</h1><div class="td-byline mb-4"><time datetime=2019-03-29 class=text-muted>Friday, March 29, 2019</time></div><p><strong>Author:</strong> <a href=mailto:ygui@google.com>Yongkun Gui</a>, Google</p><p>I recently came across a bug that causes intermittent connection resets. After
some digging, I found it was caused by a subtle combination of several different
network subsystems. It helped me understand Kubernetes networking better, and I
think it’s worthwhile to share with a wider audience who are interested in the same
topic.</p><h2 id=the-symptom>The symptom</h2><p>We received a user report claiming they were getting connection resets while using a
Kubernetes service of type ClusterIP to serve large files to pods running in the
same cluster. Initial debugging of the cluster did not yield anything
interesting: network connectivity was fine and downloading the files did not hit
any issues. However, when we ran the workload in parallel across many clients,
we were able to reproduce the problem. Adding to the mystery was the fact that
the problem could not be reproduced when the workload was run using VMs without
Kubernetes. The problem, which could be easily reproduced by <a href=https://github.com/tcarmet/k8s-connection-reset>a simple
app</a>, clearly has something to
do with Kubernetes networking, but what?</p><h2 id=kubernetes-networking-basics>Kubernetes networking basics</h2><p>Before digging into this problem, let’s talk a little bit about some basics of
Kubernetes networking, as Kubernetes handles network traffic from a pod
very differently depending on different destinations.</p><h3 id=pod-to-pod>Pod-to-Pod</h3><p>In Kubernetes, every pod has its own IP address. The benefit is that the
applications running inside pods could use their canonical port, instead of
remapping to a different random port. Pods have L3 connectivity between each
other. They can ping each other, and send TCP or UDP packets to each other.
<a href=https://github.com/containernetworking/cni>CNI</a> is the standard that solves
this problem for containers running on different hosts. There are tons of
different plugins that support CNI.</p><h3 id=pod-to-external>Pod-to-external</h3><p>For the traffic that goes from pod to external addresses, Kubernetes simply uses
<a href=https://en.wikipedia.org/wiki/Network_address_translation>SNAT</a>. What it does
is replace the pod’s internal source IP:port with the host’s IP:port. When
the return packet comes back to the host, it rewrites the pod’s IP:port as the
destination and sends it back to the original pod. The whole process is transparent
to the original pod, who doesn’t know the address translation at all.</p><h3 id=pod-to-service>Pod-to-Service</h3><p>Pods are mortal. Most likely, people want reliable service. Otherwise, it’s
pretty much useless. So Kubernetes has this concept called "service" which is
simply a L4 load balancer in front of pods. There are several different types of
services. The most basic type is called ClusterIP. For this type of service, it
has a unique VIP address that is only routable inside the cluster.</p><p>The component in Kubernetes that implements this feature is called kube-proxy.
It sits on every node, and programs complicated iptables rules to do all kinds
of filtering and NAT between pods and services. If you go to a Kubernetes node
and type <code>iptables-save</code>, you’ll see the rules that are inserted by Kubernetes
or other programs. The most important chains are <code>KUBE-SERVICES</code>, <code>KUBE-SVC-*</code>
and <code>KUBE-SEP-*</code>.</p><ul><li><code>KUBE-SERVICES</code> is the entry point for service packets. What it does is to
match the destination IP:port and dispatch the packet to the corresponding
<code>KUBE-SVC-*</code> chain.</li><li><code>KUBE-SVC-*</code> chain acts as a load balancer, and distributes the packet to
<code>KUBE-SEP-*</code> chain equally. Every <code>KUBE-SVC-*</code> has the same number of
<code>KUBE-SEP-*</code> chains as the number of endpoints behind it.</li><li><code>KUBE-SEP-*</code> chain represents a Service EndPoint. It simply does DNAT,
replacing service IP:port with pod's endpoint IP:Port.</li></ul><p>For DNAT, conntrack kicks in and tracks the connection state using a state
machine. The state is needed because it needs to remember the destination
address it changed to, and changed it back when the returning packet came back.
Iptables could also rely on the conntrack state (ctstate) to decide the destiny
of a packet. Those 4 conntrack states are especially important:</p><ul><li><em>NEW</em>: conntrack knows nothing about this packet, which happens when the SYN
packet is received.</li><li><em>ESTABLISHED</em>: conntrack knows the packet belongs to an established connection,
which happens after handshake is complete.</li><li><em>RELATED</em>: The packet doesn’t belong to any connection, but it is affiliated
to another connection, which is especially useful for protocols like FTP.</li><li><em>INVALID</em>: Something is wrong with the packet, and conntrack doesn’t know how
to deal with it. This state plays a centric role in this Kubernetes issue.</li></ul><p>Here is a diagram of how a TCP connection works between pod and service. The
sequence of events are:</p><ul><li>Client pod from left hand side sends a packet to a
service: 192.168.0.2:80</li><li>The packet is going through iptables rules in client
node and the destination is changed to pod IP, 10.0.1.2:80</li><li>Server pod handles the packet and sends back a packet with destination 10.0.0.2</li><li>The packet is going back to the client node, conntrack recognizes the packet and rewrites the source
address back to 192.169.0.2:80</li><li>Client pod receives the response packet</li></ul><figure><img src=/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/good-packet-flow.png alt="Good packet flow" width=100%><figcaption><p>Good packet flow</p></figcaption></figure><h2 id=what-caused-the-connection-reset>What caused the connection reset?</h2><p>Enough of the background, so what really went wrong and caused the unexpected
connection reset?</p><p>As the diagram below shows, the problem is packet 3. When conntrack cannot
recognize a returning packet, and mark it as <em>INVALID</em>. The most common
reasons include: conntrack cannot keep track of a connection because it is out
of capacity, the packet itself is out of a TCP window, etc. For those packets
that have been marked as <em>INVALID</em> state by conntrack, we don’t have the
iptables rule to drop it, so it will be forwarded to client pod, with source IP
address not rewritten (as shown in packet 4)! Client pod doesn’t recognize this
packet because it has a different source IP, which is pod IP, not service IP. As
a result, client pod says, "Wait a second, I don't recall this connection to
this IP ever existed, why does this dude keep sending this packet to me?" Basically,
what the client does is simply send a RST packet to the server pod IP, which
is packet 5. Unfortunately, this is a totally legit pod-to-pod packet, which can
be delivered to server pod. Server pod doesn’t know all the address translations
that happened on the client side. From its view, packet 5 is a totally legit
packet, like packet 2 and 3. All server pod knows is, "Well, client pod doesn’t
want to talk to me, so let’s close the connection!" Boom! Of course, in order
for all these to happen, the RST packet has to be legit too, with the right TCP
sequence number, etc. But when it happens, both parties agree to close the
connection.</p><figure><img src=/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/connection-reset-packet-flow.png alt="Connection reset packet flow" width=100%><figcaption><p>Connection reset packet flow</p></figcaption></figure><h2 id=how-to-address-it>How to address it?</h2><p>Once we understand the root cause, the fix is not hard. There are at least 2
ways to address it.</p><ul><li>Make conntrack more liberal on packets, and don’t mark the packets as
<em>INVALID</em>. In Linux, you can do this by <code>echo 1 > /proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal</code>.</li><li>Specifically add an iptables rule to drop the packets that are marked as
<em>INVALID</em>, so it won’t reach to client pod and cause harm.</li></ul><p>The fix is drafted (<a href=https://github.com/kubernetes/kubernetes/pull/74840),>https://github.com/kubernetes/kubernetes/pull/74840),</a> but
unfortunately it didn’t catch the v1.14 release window. However, for the users
that are affected by this bug, there is a way to mitigate the problem by applying
the following rule in your cluster.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>extensions/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>startup-script<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>startup-script<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>startup-script<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPID</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>startup-script<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-containers/startup-script:v1<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>privileged</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>STARTUP_SCRIPT<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span><span style=color:#b44;font-style:italic>            #! /bin/bash
</span><span style=color:#b44;font-style:italic>            echo 1 &gt; /proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal
</span><span style=color:#b44;font-style:italic>            echo done</span><span style=color:#bbb>            
</span></code></pre></div><h2 id=summary>Summary</h2><p>Obviously, the bug has existed almost forever. I am surprised that it
hasn’t been noticed until recently. I believe the reasons could be: (1) this
happens more in a congested server serving large payloads, which might not be a
common use case; (2) the application layer handles the retry to be tolerant of
this kind of reset. Anyways, regardless of how fast Kubernetes has been growing,
it’s still a young project. There are no other secrets than listening closely to
customers’ feedback, not taking anything for granted but digging deep, we can
make it the best platform to run applications.</p><p>Special thanks to <a href=https://github.com/bowei>bowei</a> for the consulting for both
debugging process and the blog, to <a href=https://github.com/tcarmet>tcarmet</a> for
reporting the issue and providing a reproduction.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-850bc36dd2ebd986000338492953967f>Running Kubernetes locally on Linux with Minikube - now with Kubernetes 1.14 support</h1><div class="td-byline mb-4"><time datetime=2019-03-28 class=text-muted>Thursday, March 28, 2019</time></div><p><strong>Author</strong>: <a href=https://twitter.com/idvoretskyi>Ihor Dvoretskyi</a>, Developer Advocate, Cloud Native Computing Foundation</p><center><figure><img src=/images/blog/2019-03-28-running-kubernetes-locally-on-linux-with-minikube/ihor-dvoretskyi-1470985-unsplash.jpg width=600></figure></center><p><em>A few days ago, the Kubernetes community announced <a href=https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/>Kubernetes 1.14</a>, the most recent version of Kubernetes. Alongside it, Minikube, a part of the Kubernetes project, recently hit the <a href=https://github.com/kubernetes/minikube/releases/tag/v1.0.0>1.0 milestone</a>, which supports <a href=https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/>Kubernetes 1.14</a> by default.</em></p><p>Kubernetes is a real winner (and a de facto standard) in the world of distributed Cloud Native computing. While it can handle up to <a href=https://kubernetes.io/blog/2017/03/scalability-updates-in-kubernetes-1-6/>5000 nodes</a> in a single cluster, local deployment on a single machine (e.g. a laptop, a developer workstation, etc.) is an increasingly common scenario for using Kubernetes.</p><p>A few weeks ago I ran a poll on Twitter asking the community to specify their preferred option for running Kubernetes locally on Linux:</p><center><blockquote class=twitter-tweet><p lang=en dir=ltr>Ok, Twitter ✋<br><br>Your local Kubernetes cluster on Linux is deployed by:</p>&mdash; ihor dvoretskyi (@idvoretskyi) <a href="https://twitter.com/idvoretskyi/status/1093154369040773120?ref_src=twsrc%5Etfw">February 6, 2019</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></center><p>This is post #1 in a series about the local deployment options on Linux, and it will cover Minikube, the most popular community-built solution for running Kubernetes on a local machine.</p><p><a href=https://github.com/kubernetes/minikube>Minikube</a> is a cross-platform, community-driven <a href=https://kubernetes.io/>Kubernetes</a> distribution, which is targeted to be used primarily in local environments. It deploys a single-node cluster, which is an excellent option for having a simple Kubernetes cluster up and running on localhost.</p><p>Minikube is designed to be used as a virtual machine (VM), and the default VM runtime is <a href=https://www.virtualbox.org/>VirtualBox</a>. At the same time, extensibility is one of the critical benefits of Minikube, so it's possible to use it with <a href=https://minikube.sigs.k8s.io/docs/drivers/>drivers</a> outside of VirtualBox.</p><p>By default, Minikube uses Virtualbox as a runtime for running the virtual machine. Virtualbox is a cross-platform solution, which can be used on a variety of operating systems, including GNU/Linux, Windows, and macOS.</p><p>At the same time, QEMU/KVM is a Linux-native virtualization solution, which may offer benefits compared to Virtualbox. For example, it's much easier to use KVM on a GNU/Linux server, so you can run a single-node Minikube cluster not only on a Linux workstation or laptop with GUI, but also on a remote headless server.</p><p>Unfortunately, Virtualbox and KVM can't be used simultaneously, so if you are already running KVM workloads on a machine and want to run Minikube there as well, using the KVM minikube driver is the preferred way to go.</p><p>In this guide, we'll focus on running Minikube with the KVM driver on Ubuntu 18.04 (I am using a bare metal machine running on <a href=https://www.packet.com>packet.com</a>.)</p><center><figure><img src=/images/blog/2019-03-28-running-kubernetes-locally-on-linux-with-minikube/module_01_cluster.png alt="Minikube architecture (source: kubernetes.io)" width=600><figcaption><p>Minikube architecture (source: kubernetes.io)</p></figcaption></figure></center><h2 id=disclaimer>Disclaimer</h2><p>This is not an official guide to Minikube. You may find detailed information on running and using Minikube on it's official <a href=https://github.com/kubernetes/minikube>webpage</a>, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this guide is to provide clear and easy guidelines for running Minikube with KVM on Linux.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Any Linux you like (in this tutorial we'll use Ubuntu 18.04 LTS, and all the instructions below are applicable to it. If you prefer using a different Linux distribution, please check out the relevant documentation)</li><li><code>libvirt</code> and QEMU-KVM installed and properly configured</li><li>The Kubernetes CLI (<code>kubectl</code>) for operating the Kubernetes cluster</li></ul><h3 id=qemu-kvm-and-libvirt-installation>QEMU/KVM and libvirt installation</h3><p><em>NOTE: skip if already installed</em></p><p>Before we proceed, we have to verify if our host can run KVM-based virtual machines. This can be easily checked using the <a href=https://manpages.ubuntu.com/manpages/bionic/man1/kvm-ok.1.html>kvm-ok</a> tool, available on Ubuntu.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt install cpu-checker <span style=color:#666>&amp;&amp;</span> sudo kvm-ok
</code></pre></div><p>If you receive the following output after running <code>kvm-ok</code>, you can use KVM on your machine (otherwise, please check out your configuration):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ sudo kvm-ok
INFO: /dev/kvm exists
KVM acceleration can be used
</code></pre></div><p>Now let's install KVM and libvirt and add our current user to the <code>libvirt</code> group to grant sufficient permissions:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt install libvirt-clients libvirt-daemon-system qemu-kvm <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#666>&amp;&amp;</span> sudo usermod -a -G libvirt <span style=color:#a2f;font-weight:700>$(</span>whoami<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#666>&amp;&amp;</span> newgrp libvirt
</code></pre></div><p>After installing libvirt, you may verify the host validity to run the virtual machines with <code>virt-host-validate</code> tool, which is a part of libvirt.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo virt-host-validate
</code></pre></div><h3 id=kubectl-kubernetes-cli-installation>kubectl (Kubernetes CLI) installation</h3><p><em>NOTE: skip if already installed</em></p><p>In order to manage the Kubernetes cluster, we need to install <a href=https://kubernetes.io/docs/reference/kubectl/overview/>kubectl</a>, the Kubernetes CLI tool.</p><p>The recommended way to install it on Linux is to download the pre-built binary and move it to a directory under the <code>$PATH</code>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://storage.googleapis.com/kubernetes-release/release/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt<span style=color:#a2f;font-weight:700>)</span>/bin/linux/amd64/kubectl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#666>&amp;&amp;</span> sudo install kubectl /usr/local/bin <span style=color:#666>&amp;&amp;</span> rm kubectl
</code></pre></div><p>Alternatively, kubectl can be installed with a big variety of different methods (eg. as a .deb or snap package - check out the <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>kubectl documentation</a> to find the best one for you).</p><h2 id=minikube-installation>Minikube installation</h2><h3 id=minikube-kvm-driver-installation>Minikube KVM driver installation</h3><p>A VM driver is an essential requirement for local deployment of Minikube. As we've chosen to use KVM as the Minikube driver in this tutorial, let's install the KVM driver with the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#666>&amp;&amp;</span> sudo install docker-machine-driver-kvm2 /usr/local/bin/ <span style=color:#666>&amp;&amp;</span> rm docker-machine-driver-kvm2
</code></pre></div><h3 id=minikube-installation-1>Minikube installation</h3><p>Now let's install Minikube itself:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#666>&amp;&amp;</span> sudo install minikube-linux-amd64 /usr/local/bin/minikube <span style=color:#666>&amp;&amp;</span> rm minikube-linux-amd64
</code></pre></div><h3 id=verify-the-minikube-installation>Verify the Minikube installation</h3><p>Before we proceed, we need to verify that Minikube is correctly installed. The simplest way to do this is to check Minikube’s status.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube version
</code></pre></div><h3 id=to-use-the-kvm2-driver>To use the KVM2 driver:</h3><p>Now let's run the local Kubernetes cluster with Minikube and KVM:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube start --vm-driver kvm2
</code></pre></div><h3 id=set-kvm2-as-a-default-vm-driver-for-minikube>Set KVM2 as a default VM driver for Minikube</h3><p>If KVM is used as the single driver for Minikube on our machine, it's more convenient to set it as a default driver and run Minikube with fewer command-line arguments. The following command sets the KVM driver as the default:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube config <span style=color:#a2f>set</span> vm-driver kvm2
</code></pre></div><p>So now let's run Minikube as usual:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>minikube start
</code></pre></div><h2 id=verify-the-kubernetes-installation>Verify the Kubernetes installation</h2><p>Let's check if the Kubernetes cluster is up and running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes
</code></pre></div><p>Now let's run a simple sample app (nginx in our case):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create deployment nginx --image<span style=color:#666>=</span>nginx
</code></pre></div><p>Let’s also check that the Kubernetes pods are correctly provisioned:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pods
</code></pre></div><h2 id=screencast>Screencast</h2><center>[![asciicast](https://asciinema.org/a/237106.svg)](https://asciinema.org/a/237106)</center><h2 id=next-steps>Next steps</h2><p>At this point, a Kubernetes cluster with Minikube and KVM is adequately set up and configured on your local machine.</p><p>To proceed, you may check out the Kubernetes tutorials on the project website:</p><ul><li><a href=https://kubernetes.io/docs/tutorials/hello-minikube/>Hello Minikube</a></li></ul><p>It’s also worth checking out the "Introduction to Kubernetes" course by The Linux Foundation/Cloud Native Computing Foundation, available for free on EDX:</p><ul><li><a href=https://www.edx.org/course/introduction-to-kubernetes#>Introduction to Kubernetes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e78429ce029dc0853cf8b0f87aab2d66>Kubernetes 1.14: Production-level support for Windows Nodes, Kubectl Updates, Persistent Local Volumes GA</h1><div class="td-byline mb-4"><time datetime=2019-03-25 class=text-muted>Monday, March 25, 2019</time></div><p><strong>Authors:</strong> The 1.14 <a href=https://bit.ly/k8s114-team>Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.14, our first release of 2019!</p><p>Kubernetes 1.14 consists of 31 enhancements: 10 moving to stable, 12 in beta, and 7 net new. The main themes of this release are extensibility and supporting more workloads on Kubernetes with three major features moving to general availability, and an important security feature moving to beta.</p><p>More enhancements graduated to stable in this release than any prior Kubernetes release. This represents an important milestone for users and operators in terms of setting support expectations. In addition, there are notable Pod and RBAC enhancements in this release, which are discussed in the “additional notable features” section below.</p><p>Let’s dive into the key features of this release:</p><h2 id=production-level-support-for-windows-nodes>Production-level Support for Windows Nodes</h2><p>Up until now Windows Node support in Kubernetes has been in beta, allowing many users to experiment and see the value of Kubernetes for Windows containers. Kubernetes now officially supports adding Windows nodes as worker nodes and scheduling Windows containers, enabling a vast ecosystem of Windows applications to leverage the power of our platform. Enterprises with investments in Windows-based applications and Linux-based applications don’t have to look for separate orchestrators to manage their workloads, leading to increased operational efficiencies across their deployments, regardless of operating system.</p><p>Some of the key features of enabling Windows containers in Kubernetes include:</p><ul><li>Support for Windows Server 2019 for worker nodes and containers</li><li>Support for out of tree networking with Azure-CNI, OVN-Kubernetes, and Flannel</li><li>Improved support for pods, service types, workload controllers, and metrics/quotas to closely match the capabilities offered for Linux containers</li></ul><h2 id=notable-kubectl-updates>Notable Kubectl Updates</h2><p><strong>New Kubectl Docs and Logo</strong></p><p>The documentation for kubectl has been rewritten from the ground up with a focus on managing Resources using declarative Resource Config. The documentation has been published as a standalone site with the format of a book, and it is linked from the main k8s.io documentation (available at <a href=https://kubectl.docs.kubernetes.io>https://kubectl.docs.kubernetes.io</a>).</p><p>The new kubectl logo and mascot (pronounced <em>kubee-cuddle</em>) are shown on the new docs site logo.</p><p><strong>Kustomize Integration</strong></p><p>The declarative Resource Config authoring capabilities of <a href=https://github.com/kubernetes-sigs/kustomize>kustomize</a> are now available in kubectl through the <code>-k</code> flag (e.g. for commands like <code>apply, get</code>) and the <code>kustomize</code> subcommand. Kustomize helps users author and reuse Resource Config using Kubernetes native concepts. Users can now apply directories with <code>kustomization.yaml</code> to a cluster using <code>kubectl apply -k dir/</code>. Users can also emit customized Resource Config to stdout without applying them via <code>kubectl kustomize dir/</code>. The new capabilities are documented in the new docs at <a href=https://kubectl.docs.kubernetes.io>https://kubectl.docs.kubernetes.io</a></p><p>The kustomize subcommand will continue to be developed in the Kubernetes owned <a href=https://github.com/kubernetes-sigs/kustomize>kustomize</a> repo. The latest kustomize features will be available from a standalone kustomize binary (published to the kustomize repo) at a frequent release cadence, and will be updated in kubectl prior to each Kubernetes releases.</p><p><strong>kubectl Plugin Mechanism Graduating to Stable</strong></p><p>The kubectl plugin mechanism allows developers to publish their own custom kubectl subcommands in the form of standalone binaries. This may be used to extend kubectl with new higher-level functionality and with additional porcelain (e.g. adding a <code>set-ns</code> command).</p><p>Plugins must have the <code>kubectl-</code> name prefix and exist on the user’s $PATH. The plugin mechanics have been simplified significantly for GA, and are similar to the git plugin system.</p><h2 id=persistent-local-volumes-are-now-ga>Persistent Local Volumes are Now GA</h2><p>This feature, graduating to stable, makes locally attached storage available as a persistent volume source. Distributed file systems and databases are the primary use cases for persistent local storage due performance and cost. On cloud providers, local SSDs give better performance than remote disks. On bare metal, in addition to performance, local storage is typically cheaper and using it is a necessity to provision distributed file systems.</p><h2 id=pid-limiting-is-moving-to-beta>PID Limiting is Moving to Beta</h2><p>Process IDs (PIDs) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine. Administrators require mechanisms to ensure that user pods cannot induce PID exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that PIDs are limited among pods in order to ensure they have limited impact to other workloads on the node.</p><p>Administrators are able to provide pod-to-pod PID isolation by defaulting the number of PIDs per pod as a beta feature. In addition, administrators can enable node-to-pod PID isolation as an alpha feature by reserving a number of allocatable PIDs to user pods via node allocatable. The community hopes to graduate this feature to beta in the next release.</p><h2 id=additional-notable-feature-updates>Additional Notable Feature Updates</h2><p><a href=https://github.com/kubernetes/enhancements/issues/564>Pod priority and preemption</a> enables Kubernetes scheduler to schedule more important Pods first and when cluster is out of resources, it removes less important pods to create room for more important ones. The importance is specified by priority.</p><p><a href=https://github.com/kubernetes/enhancements/issues/580>Pod Readiness Gates</a> introduce an extension point for external feedback on pod readiness.</p><p><a href=https://github.com/kubernetes/enhancements/issues/789>Harden the default RBAC discovery clusterrolebindings</a> removes discovery from the set of APIs which allow for unauthenticated access by default, improving privacy for CRDs and the default security posture of default clusters in general.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.14 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=https://kubernetes.io/docs/tutorials/>interactive tutorials</a>. You can also easily install 1.14 using <a href=https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h2 id=features-blog-series>Features Blog Series</h2><p>If you’re interested in exploring these features more in depth, check back next week for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:</p><ul><li>Day 1 - Windows Server Containers</li><li>Day 2 - Harden the default RBAC discovery clusterrolebindings</li><li>Day 3 - Pod Priority and Preemption in Kubernetes</li><li>Day 4 - PID Limiting</li><li>Day 5 - Persistent Local Volumes</li></ul><h2 id=release-team>Release Team</h2><p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://bit.ly/k8s114-team>release team</a> led by Aaron Crickenberger, Senior Test Engineer at Google. The 43 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over 28,000 individual contributors to date and an active community of more than 57,000 people.</p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, 381 different companies and over 2,458 individuals contribute to Kubernetes each month. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=user-highlights>User Highlights</h2><p>Established, global organizations are using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at massive scale. Recently published user stories from the community include:</p><ul><li><strong>NetEase</strong> moving to <a href=https://www.cncf.io/netease-case-study/>Kubernetes has increased their R&D efficiency</a> by more than 100% and deployment efficiency by 280%.</li><li><strong>VSCO</strong> found that moving to continuous integration, containerization, and Kubernetes, <a href=https://www.cncf.io/blog/2019/02/20/how-vsco-saved-with-kubernetes/>velocity was increased dramatically</a>. The time from code-complete to deployment in production on real infrastructure went from 1-2 weeks to 2-4 hours for a typical service.</li><li><strong>NAV</strong>’s move to <a href=https://www.cncf.io/blog/2019/03/21/nav-saved-the-company-50-in-infrastructure-costs-with-kubernetes/>Kubernetes saved the company 50% in infrastructure costs</a>, deployments increased 5x from 10 a day to 50 a day and resource utilization increased from 1% to 40%.</li></ul><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community.</p><h2 id=ecosystem-updates>Ecosystem Updates</h2><ul><li>Kubernetes is <a href=https://summerofcode.withgoogle.com/organizations/5638078861934592/>participating in GSoC 2019</a> under the CNCF. Check out the full list of project ideas for 2019 on <a href=https://github.com/cncf/soc#project-ideas>CNCF’s GitHub page</a>.</li><li>The <a href=https://www.cncf.io/cncf-annual-report-2018/>CNCF Annual report 2018</a> provides a look back at the growth of the Kubernetes and the foundation, community engagement, ecosystem tools, test conformance projects, KubeCon and more.</li><li>Out of 8,000 attendees at <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/>KubeCon + CloudNativeCon North America 2018</a>, 73% were first-time KubeCon-ers, highlighting massive growth and new interest in Kubernetes and cloud native technologies. Conference transparency <a href=https://events.linuxfoundation.org/wp-content/uploads/2019/02/KCCNC-NA-18-Report.pdf>report</a>.</li></ul><h2 id=kubecon>KubeCon</h2><p>The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/>Barcelona</a> from May 20-23, 2019 and <a href=https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/>Shanghai</a> (co-located with Open Source Summit) from June 24-26, 2019. These conferences will feature technical sessions, case studies, developer deep dives, salons, and more! <a href=https://www.cncf.io/community/kubecon-cloudnativecon-events/>Register today</a>!</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.14 release team on April 23rd at 10am PDT to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/WN_ViJ0aL4ARiCM15i6erX-pA>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Join the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0ec42c7422a7e045f9bdf5079f72cb85>Kubernetes End-to-end Testing for Everyone</h1><div class="td-byline mb-4"><time datetime=2019-03-22 class=text-muted>Friday, March 22, 2019</time></div><p><strong>Author:</strong> Patrick Ohly (Intel)</p><p>More and more components that used to be part of Kubernetes are now
being developed outside of Kubernetes. For example, storage drivers
used to be compiled into Kubernetes binaries, then were moved into
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md>stand-alone FlexVolume
binaries</a>
on the host, and now are delivered as <a href=https://github.com/container-storage-interface/spec>Container Storage Interface
(CSI) drivers</a>
that get deployed in pods inside the Kubernetes cluster itself.</p><p>This poses a challenge for developers who work on such components: how
can end-to-end (E2E) testing on a Kubernetes cluster be done for such
external components? The E2E framework that is used for testing
Kubernetes itself has all the necessary functionality. However, trying
to use it outside of Kubernetes was difficult and only possible by
carefully selecting the right versions of a large number of
dependencies. E2E testing has become a lot simpler in Kubernetes 1.13.</p><p>This blog post summarizes the changes that went into Kubernetes
1.13. For CSI driver developers, it will cover the ongoing effort to
also make the storage tests available for testing of third-party CSI
drivers. How to use them will be shown based on two Intel CSI drivers:</p><ul><li><a href=https://github.com/intel/oim/>Open Infrastructure Manager (OIM)</a></li><li><a href=https://github.com/intel/pmem-csi>PMEM-CSI</a></li></ul><p>Testing those drivers was the main motivation behind most of these
enhancements.</p><h2 id=e2e-overview>E2E overview</h2><p>E2E testing consists of several phases:</p><ul><li>Implementing a test suite. This is the main focus of this blog
post. The Kubernetes E2E framework is written in Go. It relies on
<a href=https://onsi.github.io/ginkgo/>Ginkgo</a> for managing tests and
<a href=http://onsi.github.io/gomega/>Gomega</a> for assertions. These tools
support “behavior driven development”, which describes expected
behavior in “specs”. In this blog post, “test” is used to reference
an individual <code>Ginkgo.It</code> spec. Tests interact with the Kubernetes
cluster using
<a href=https://godoc.org/k8s.io/client-go/kubernetes>client-go</a>.</li><li>Bringing up a test cluster. Tools like
<a href=https://github.com/kubernetes/test-infra/blob/master/kubetest/README.md>kubetest</a>
can help here.</li><li>Running an E2E test suite against that cluster. Ginkgo test suites
can be run with the <code>ginkgo</code> tool or as a normal Go test with <code>go test</code>. Without any parameters, a Kubernetes E2E test suite will
connect to the default cluster based on environment variables like
KUBECONFIG, exactly like kubectl. Kubetest also knows how to run the
Kubernetes E2E suite.</li></ul><h2 id=e2e-framework-enhancements-in-kubernetes-1-13>E2E framework enhancements in Kubernetes 1.13</h2><p>All of the following enhancements follow the same basic pattern: they
make the E2E framework more useful and easier to use outside of
Kubernetes, without changing the behavior of the original Kubernetes
e2e.test binary.</p><h3 id=splitting-out-provider-support>Splitting out provider support</h3><p>The main reason why using the E2E framework from Kubernetes &lt;= 1.12
was difficult were the dependencies on provider-specific SDKs, which
pulled in a large number of packages. Just getting it compiled was
non-trivial.</p><p>Many of these packages are only needed for certain tests. For example,
testing the mounting of a pre-provisioned volume must first provision
such a volume the same way as an administrator would, by talking
directly to a specific storage backend via some non-Kubernetes API.</p><p>There is an effort to <a href=https://github.com/kubernetes/kubernetes/issues/70194>remove cloud provider-specific
tests</a> from
core Kubernetes. The approach taken in <a href=https://github.com/kubernetes/kubernetes/pull/68483>PR
#68483</a> can be
seen as an incremental step towards that goal: instead of ripping out
the code immediately and breaking all tests that depend on it, all
cloud provider-specific code was moved into optional packages under
<a href=https://github.com/kubernetes/kubernetes/tree/release-1.13/test/e2e/framework/providers>test/e2e/framework/providers</a>. The
E2E framework then accesses it via <a href=https://github.com/kubernetes/kubernetes/blob/6c1e64b94a3e111199c934c39a0c25bc219ed5f9/test/e2e/framework/provider.go#L79-L99>an
interface</a>
that gets implemented separately by each vendor package.</p><p>The author of a E2E test suite decides which of these packages get
imported into the test suite. The vendor support is then activated via
the <code>--provider</code> command line flag. The Kubernetes e2e.test binary in
1.13 and 1.14 still contains support for the same providers as in
1.12. It is also okay to include no packages, which means that only
the generic providers will be available:</p><ul><li>“skeleton”: cluster is accessed via the Kubernetes API and nothing
else</li><li>“local”: like “skeleton”, but in addition the scripts in
kubernetes/kubernetes/cluster can retrieve logs via ssh after a test
suite is run</li></ul><h3 id=external-files>External files</h3><p>Tests may have to read additional files at runtime, like .yaml
manifests. But the Kubernetes e2e.test binary is supposed to be usable
and entirely stand-alone because that simplifies shipping and running
it. The solution in the Kubernetes build system is to link all files
under <code>test/e2e/testing-manifests</code> into the binary with
<a href=https://github.com/jteeuwen/go-bindata>go-bindata</a>. The
E2E framework used to have a hard dependency on the output of
<code>go-bindata</code>, now <a href=https://github.com/kubernetes/kubernetes/pull/69103>bindata support is
optional</a>. When
accessing a file via the <a href=https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/testfiles/testfiles.go>testfiles
package</a>,
files will be retrieved from different sources:</p><ul><li>relative to the directory specified with <code>--repo-root</code> parameter</li><li>zero or more bindata chunks</li></ul><h3 id=test-parameters>Test parameters</h3><p>The e2e.test binary takes additional parameters which control test
execution. In 2016, an effort was started to replace all E2E command
line parameters with a Viper configuration file. But that effort
<a href=https://github.com/kubernetes/kubernetes/blob/0ed33881dc4355495f623c6f22e7dd0b7632b7c0/test/e2e/framework/test_context.go#L318-L319>stalled</a>, which left developers without clear guidance how they should handle
test-specific parameters.</p><p>The approach in v1.12 was to add all flags to the central
<a href=https://github.com/kubernetes/kubernetes/blob/v1.12.0/test/e2e/framework/test_context.go>test/e2e/framework/test_context.go</a>,
which does not work for tests developed independently from the
framework. Since <a href=https://github.com/kubernetes/kubernetes/pull/69105>PR
#69105</a> the
recommendation has been to use the normal <code>flag</code> package to
define its parameters, in its own source code. Flag names must be
hierarchical with dots separating different levels, for example
<code>my.test.parameter</code>, and must be unique. Uniqueness is enforced by the
<code>flag</code> package which panics when registering a flag a second time. The
new
<a href=https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/config/config.go>config</a>
package simplifies the definition of multiple options, which are
stored in a single struct.</p><p>To summarize, this is how parameters are handled now:</p><ul><li>The init code in test packages defines tests and parameters. The
actual parameter <em>values</em> are not available yet, so test definitions
cannot use them.</li><li>The init code of the test suite parses parameters and (optionally)
the configuration file.</li><li>The tests run and now can use parameter values.</li></ul><p>However, recently it <a href=https://github.com/kubernetes/kubernetes/pull/69105#discussion_r267960062>was pointed
out</a>
that it is desirable and was possible to not expose test settings as
command line flags and only set them via a configuration file. There
is an <a href=https://github.com/kubernetes/kubernetes/issues/75590>open bug</a> and a
<a href=https://github.com/kubernetes/kubernetes/pull/75593>pending PR</a>
about this.</p><p>Viper support has been enhanced. Like the provider support, it is
completely optional. It gets pulled into a e2e.test binary by
importing the <code>viperconfig</code> package and <a href=https://github.com/kubernetes/kubernetes/blob/ddf47ac13c1a9483ea035a79cd7c10005ff21a6d/test/e2e/e2e_test.go#L49-L57>calling
it</a>
after parsing the normal command line flags. This has been implemented
so that all variables which can be set via command line flags are also
set when the flag appears in a Viper config file. For example, the
Kubernetes v1.13 <code>e2e.test</code> binary accepts
<code>--viper-config=/tmp/my-config.yaml</code> and that file will set the
<code>my.test.parameter</code> to <code>value</code> when it has this content: my: test:
parameter: value</p><p>In older Kubernetes releases, that option could only load a file from
the current directory, the suffix had to be left out, and only a few
parameters actually could be set this way. Beware that one limitation
of Viper still exists: it works by matching config file entries
against known flags, without warning about unknown config file entries
and thus leaving typos undetected. A <a href=https://github.com/kubernetes/kubeadm/issues/1040>better config file
parser</a> for
Kubernetes is still work in progress.</p><h3 id=creating-items-from-yaml-manifests>Creating items from .yaml manifests</h3><p>In Kubernetes 1.12, there was some support for loading individual
items from a .yaml file, but then creating that item had to be done by
hand-written code. Now the framework has <a href=https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/framework/create.go>new
methods</a>
for loading a .yaml file that has multiple items, patching those items
(for example, setting the namespace created for the current test), and
creating them. This is currently <a href=https://github.com/kubernetes/kubernetes/blob/ddf47ac13c1a9483ea035a79cd7c10005ff21a6d/test/e2e/storage/drivers/csi.go#L192-L209>used to deploy CSI
drivers</a> anew for each test from exactly the same .yaml files that are also
used for deployment via kubectl. If the CSI driver supports running
under different names, then tests are completely independent and can
run in parallel.</p><p>However, redeploying a driver slows down test execution and it does
not cover concurrent operations against the driver. A more realistic
test scenario is to deploy a driver once when bringing up the test
cluster, then run all tests against that deployment. Eventually the
Kubernetes E2E testing will move to that model, once it is clearer how
test cluster bringup can be extended such that it also includes
installing additional entities like CSI drivers.</p><h2 id=upcoming-enhancements-in-kubernetes-1-14>Upcoming enhancements in Kubernetes 1.14</h2><h3 id=reusing-storage-tests>Reusing storage tests</h3><p>Being able to use the framework outside of Kubernetes enables building
a custom test suite. But a test suite without tests is still
useless. Several of the existing tests, in particular for storage, can
also be applied to out-of-tree components. Thanks to the work done by
Masaki Kimura, <a href=https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/e2e/storage/testsuites>storage
tests</a>
in Kubernetes 1.13 are defined such that they can be instantiated
multiple times for different drivers.</p><p>But history has a habit of repeating itself. As with providers, the
package defining these tests also pulled in driver definitions for all
in-tree storage backends, which in turn pulled in more additional
packages than were needed. This has been
<a href=https://github.com/kubernetes/kubernetes/pull/70862>fixed</a> for the
upcoming Kubernetes 1.14.</p><h3 id=skipping-unsupported-tests>Skipping unsupported tests</h3><p>Some of the storage tests depend on features of the cluster (like
running on a host that supports XFS) or of the driver (like supporting
block volumes). These conditions are checked while the test runs,
leading to skipped tests when they are not satisfied. The good thing
is that this records an explanation why the test did not run.</p><p>Starting a test is slow, in particular when it must first deploy the
CSI driver, but also in other scenarios. Creating the namespace for a
test has been measured at 5 seconds on a fast cluster, and it produces
a lot of noisy test output. It would have been possible to address
that by <a href=https://github.com/kubernetes/kubernetes/pull/70992>skipping the definition of unsupported
tests</a>, but then
reporting why a test isn’t even part of the test suite becomes
tricky. This approach has been dropped in favor of reorganizing the
storage test suite such that it <a href=https://github.com/kubernetes/kubernetes/pull/72434>first checks
conditions</a>
before doing the more expensive test setup steps.</p><h3 id=more-readable-test-definitions>More readable test definitions</h3><p>The same PR also rewrites the tests to operate like conventional
Ginkgo tests, with test cases and their local variables in <a href=https://github.com/pohly/kubernetes/blob/ec3655a1d40ced6b1873e627b736aae1cf242477/test/e2e/storage/testsuites/provisioning.go#L82>a single
function</a>.</p><h3 id=testing-external-drivers>Testing external drivers</h3><p>Building a custom E2E test suite is still quite a bit of work. The
e2e.test binary that will get distributed in the <a href=https://dl.k8s.io/v1.14.0/kubernetes-test.tar.gz>Kubernetes 1.14 test
archive</a> will have
the <a href=https://github.com/kubernetes/kubernetes/pull/72836>ability to
test</a> already
installed storage drivers without rebuilding the test suite. See this
<a href=https://github.com/pohly/kubernetes/blob/6644db9914379a4a7b3d3487b41b2010f226e4dc/test/e2e/storage/external/README.md>README</a>
for further instructions.</p><h2 id=e2e-test-suite-howto>E2E test suite HOWTO</h2><h3 id=test-suite-initialization>Test suite initialization</h3><p>The first step is to set up the necessary boilerplate code that
defines the test suite. <a href=https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/e2e>In Kubernetes
E2E</a>,
this is done in the <code>e2e.go</code> and <code>e2e_test.go</code> files. It could also be
done in a single <code>e2e_test.go</code> file. Kubernetes imports all of the
various providers, in-tree tests, Viper configuration support, and
bindata file lookup in <code>e2e_test.go</code>. <code>e2e.go</code> controls the actual
execution, including some cluster preparations and metrics collection.</p><p>A simpler starting point are the <code>e2e_[test].go</code> files <a href=https://github.com/intel/pmem-csi/tree/586ae281ac2810cb4da6f1e160cf165c7daf0d80/test/e2e>from
PMEM-CSI</a>. It
doesn’t use any providers, no Viper, no bindata, and imports just the
storage tests.</p><p>Like PMEM-CSI, OIM drops all of the extra features, but is a bit more
complex because it integrates a custom cluster startup directly into
the <a href=https://github.com/intel/pmem-csi/blob/a7b0d66b59771bf615e07fcd3d4f0ba08cfdf90f/test/e2e/e2e.go>test
suite</a>,
which was useful in this case because some additional components have
to run on the host side. By running them directly in the E2E binary,
interactive debugging with <code>dlv</code> becomes easier.</p><p>Both CSI drivers follow the Kubernetes example and use the <code>test/e2e</code>
directory for their test suites, but any other directory and other
file names would also work.</p><h3 id=adding-e2e-storage-tests>Adding E2E storage tests</h3><p>Tests are defined by packages that get imported into a test suite. The
only thing specific to E2E tests is that they instantiate a
<code>framework.Framework</code> pointer (usually called <code>f</code>) with
<code>framework.NewDefaultFramework</code>. This variable gets initialized anew
in a <code>BeforeEach</code> for each test and freed in an <code>AfterEach</code>. It has a
<code>f.ClientSet</code> and <code>f.Namespace</code> at runtime (and only at runtime!)
which can be used by a test.</p><p>The <a href=https://github.com/intel/pmem-csi/blob/devel/test/e2e/storage/csi_volumes.go#L51>PMEM-CSI storage
test</a>
imports the Kubernetes storage test suite and sets up one instance of
the provisioning tests for a PMEM-CSI driver which must be already
installed in the test cluster. The storage test suite changes the
storage class to run tests with different filesystem types. Because of
this requirement, the storage class is created from a .yaml file.</p><p>Explaining all the various utility methods available in the framework
is out of scope for this blog post. Reading existing tests and the
source code of the framework is a good way to get started.</p><h3 id=vendoring>Vendoring</h3><p>Vendoring Kubernetes code is still not trivial, even after eliminating
many of the unnecessary dependencies. <code>k8s.io/kubernetes</code> is not meant
to be included in other projects and does not define its dependencies
in a way that is understood by tools like <code>dep</code>. The other <code>k8s.io</code>
packages are meant to be included, but <a href=https://github.com/kubernetes/kubernetes/issues/72638>don’t follow semantic
versioning
yet</a> or don’t
tag any releases (<code>k8s.io/kube-openapi</code>, <code>k8s.io/utils</code>).</p><p>PMEM-CSI uses <a href=https://golang.github.io/dep/>dep</a>. It’s
<a href=https://github.com/intel/pmem-csi/blob/0ad8251c064b1010c91e7fc1dd423b95d5594bba/Gopkg.toml>Gopkg.toml</a>
file is a good starting point. It enables pruning (not enabled in dep
by default) and locks certain projects onto versions that are
compatible with the Kubernetes version that is used. When <code>dep</code>
doesn’t pick a compatible version, then checking Kubernetes’
<a href=https://github.com/kubernetes/kubernetes/blob/master/Godeps/Godeps.json>Godeps.json</a>
helps to determine which revision might be the right one.</p><h3 id=compiling-and-running-the-test-suite>Compiling and running the test suite</h3><p><code>go test ./test/e2e -args -help</code> is the fastest way to test that the
test suite compiles.</p><p>Once it does compile and a cluster has been set up, the command <code>go test -timeout=0 -v ./test/e2e -ginkgo.v</code> runs all tests. In order to
run tests in parallel, use the <code>ginkgo -p ./test/e2e</code> command instead.</p><h2 id=getting-involved>Getting involved</h2><p>The Kubernetes E2E framework is owned by the testing-commons
sub-project in
<a href=https://github.com/kubernetes/community/tree/master/sig-testing>SIG-testing</a>. See
that page for contact information.</p><p>There are various tasks that could be worked on, including but not
limited to:</p><ul><li>Moving test/e2e/framework into a staging repo and restructuring it
so that it is more modular
(<a href=https://github.com/kubernetes/kubernetes/issues/74352>#74352</a>).</li><li>Simplifying <code>e2e.go</code> by moving more of its code into
<code>test/e2e/framework</code>
(<a href=https://github.com/kubernetes/kubernetes/issues/74353>#74353</a>).</li><li>Removing provider-specific code from the Kubernetes E2E test suite
(<a href=https://github.com/kubernetes/kubernetes/issues/70194>#70194</a>).</li></ul><p>Special thanks to the reviewers of this article:</p><ul><li>Olev Kartau (<a href=https://github.com/okartau>https://github.com/okartau</a>)</li><li>Mary Camp (<a href=https://github.com/MCamp859>https://github.com/MCamp859</a>)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-09e07ea24cc309e3d8a5fcd4c22bafe4>A Guide to Kubernetes Admission Controllers</h1><div class="td-byline mb-4"><time datetime=2019-03-21 class=text-muted>Thursday, March 21, 2019</time></div><p><strong>Author:</strong> Malte Isberner (StackRox)</p><p>Kubernetes has greatly improved the speed and manageability of backend clusters in production today. Kubernetes has emerged as the de facto standard in container orchestrators thanks to its flexibility, scalability, and ease of use. Kubernetes also provides a range of features that secure production workloads. A more recent introduction in security features is a set of plugins called “<a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>admission controllers</a>.” Admission controllers must be enabled to use some of the more advanced security features of Kubernetes, such as <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/>pod security policies</a> that enforce a security configuration baseline across an entire namespace. The following must-know tips and tricks will help you leverage admission controllers to make the most of these security capabilities in Kubernetes.</p><h2 id=what-are-kubernetes-admission-controllers>What are Kubernetes admission controllers?</h2><p>In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercept (authenticated) API requests and may change the request object or deny the request altogether. The admission control process has two phases: the <em>mutating</em> phase is executed first, followed by the <em>validating</em> phase. Consequently, admission controllers can act as mutating or validating controllers or as a combination of both. For example, the <strong>LimitRanger</strong> admission controller can augment pods with default resource requests and limits (mutating phase), as well as verify that pods with explicitly set resource requirements do not exceed the per-namespace limits specified in the <strong>LimitRange object</strong> (validating phase).</p><center><figure><img src=/images/blog/2019-03-21-a-guide-to-kubernetes-admission-controllers/admission-controller-phases.png alt="Admission Controller Phases" width=800><figcaption><p>Admission Controller Phases</p></figcaption></figure></center><p>It is worth noting that some aspects of Kubernetes’ operation that many users would consider built-in are in fact governed by admission controllers. For example, when a namespace is deleted and subsequently enters the <code>Terminating</code> state, the <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespacelifecycle><code>NamespaceLifecycle</code></a> admission controller is what prevents any new objects from being created in this namespace.</p><p>Among the more than 30 admission controllers shipped with Kubernetes, two take a special role because of their nearly limitless flexibility - <code>ValidatingAdmissionWebhooks</code> and <code>MutatingAdmissionWebhooks</code>, both of which are in beta status as of Kubernetes 1.13. We will examine these two admission controllers closely, as they do not implement any policy decision logic themselves. Instead, the respective action is obtained from a REST endpoint (a <em>webhook</em>) of a service running inside the cluster. This approach decouples the admission controller logic from the Kubernetes API server, thus allowing users to implement custom logic to be executed whenever resources are created, updated, or deleted in a Kubernetes cluster.</p><p>The difference between the two kinds of admission controller webhooks is pretty much self-explanatory: mutating admission webhooks may mutate the objects, while validating admission webhooks may not. However, even a mutating admission webhook can reject requests and thus act in a validating fashion. Validating admission webhooks have two main advantages over mutating ones: first, for security reasons it might be desirable to disable the <code>MutatingAdmissionWebhook</code> admission controller (or apply stricter RBAC restrictions as to who may create <code>MutatingWebhookConfiguration</code> objects) because of its potentially confusing or even dangerous side effects. Second, as shown in the previous diagram, validating admission controllers (and thus webhooks) are run after any mutating ones. As a result, whatever request object a validating webhook sees is the final version that would be persisted to <code>etcd</code>.</p><p>The set of enabled admission controllers is configured by passing a flag to the Kubernetes API server. Note that the old <strong>--admission-control</strong> flag was deprecated in 1.10 and replaced with <strong>--enable-admission-plugins</strong>.</p><pre><code>--enable-admission-plugins=ValidatingAdmissionWebhook,MutatingAdmissionWebhook
</code></pre><p>Kubernetes recommends the following admission controllers to be enabled by default.</p><pre><code>--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota,PodSecurityPolicy
</code></pre><p>The complete list of admission controllers with their descriptions can be found <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do>in the official Kubernetes reference</a>. This discussion will focus only on the webhook-based admission controllers.</p><h2 id=why-do-i-need-admission-controllers>Why do I need admission controllers?</h2><ul><li><strong>Security:</strong> Admission controllers can increase security by mandating a reasonable security baseline across an entire namespace or cluster. The built-in <code>PodSecurityPolicy</code> admission controller is perhaps the most prominent example; it can be used for disallowing containers from running as root or making sure the container’s root filesystem is always mounted read-only, for example. Further use cases that can be realized by custom, webhook-based admission controllers include:</li><li>Allow pulling images only from specific registries known to the enterprise, while denying unknown image registries.</li><li>Reject deployments that do not meet security standards. For example, containers using the <code>privileged</code> flag can circumvent a lot of security checks. This risk could be mitigated by a webhook-based admission controller that either rejects such deployments (validating) or overrides the <code>privileged</code> flag, setting it to <code>false</code>.</li><li><strong>Governance:</strong> Admission controllers allow you to enforce the adherence to certain practices such as having good labels, annotations, resource limits, or other settings. Some of the common scenarios include:</li><li>Enforce label validation on different objects to ensure proper labels are being used for various objects, such as every object being assigned to a team or project, or every deployment specifying an app label.</li><li>Automatically add annotations to objects, such as attributing the correct cost center for a “dev” deployment resource.</li><li><strong>Configuration management:</strong> Admission controllers allow you to validate the configuration of the objects running in the cluster and prevent any obvious misconfigurations from hitting your cluster. Admission controllers can be useful in detecting and fixing images deployed without semantic tags, such as by:</li><li>automatically adding resource limits or validating resource limits,</li><li>ensuring reasonable labels are added to pods, or</li><li>ensuring image references used in production deployments are not using the <code>latest</code> tags, or tags with a <code>-dev</code> suffix.</li></ul><p>In this way, admission controllers and policy management help make sure that applications stay in compliance within an ever-changing landscape of controls.</p><h2 id=example-writing-and-deploying-an-admission-controller-webhook>Example: Writing and Deploying an Admission Controller Webhook</h2><p>To illustrate how admission controller webhooks can be leveraged to establish custom security policies, let’s consider an example that addresses one of the shortcomings of Kubernetes: a lot of its defaults are optimized for ease of use and reducing friction, sometimes at the expense of security. One of these settings is that containers are by default allowed to run as root (and, without further configuration and no <code>USER</code> directive in the Dockerfile, will also do so). Even though containers are isolated from the underlying host to a certain extent, running containers as root does increase the risk profile of your deployment— and should be avoided as one of many <a href=https://www.stackrox.com/post/2018/12/6-container-security-best-practices-you-should-be-following/>security best practices</a>. The <a href=https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/>recently exposed runC vulnerability</a> (<a href=https://nvd.nist.gov/vuln/detail/CVE-2019-5736>CVE-2019-5736</a>), for example, could be exploited only if the container ran as root.</p><p>You can use a custom mutating admission controller webhook to apply more secure defaults: unless explicitly requested, our webhook will ensure that pods run as a non-root user (we assign the user ID 1234 if no explicit assignment has been made). Note that this setup does not prevent you from deploying any workloads in your cluster, including those that legitimately require running as root. It only requires you to explicitly enable this riskier mode of operation in the deployment configuration, while defaulting to non-root mode for all other workloads.</p><p>The full code along with deployment instructions can be found in our accompanying <a href=https://github.com/stackrox/admission-controller-webhook-demo>GitHub repository</a>. Here, we will highlight a few of the more subtle aspects about how webhooks work.</p><h2 id=mutating-webhook-configuration>Mutating Webhook Configuration</h2><p>A mutating admission controller webhook is defined by creating a <code>MutatingWebhookConfiguration</code> object in Kubernetes. In our example, we use the following configuration:</p><pre><code>apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: &quot;/mutate&quot;
      caBundle: ${CA_PEM_B64}
    rules:
      - operations: [ &quot;CREATE&quot; ]
        apiGroups: [&quot;&quot;]
        apiVersions: [&quot;v1&quot;]
        resources: [&quot;pods&quot;]
</code></pre><p>This configuration defines a <code>webhook webhook-server.webhook-demo.svc</code>, and instructs the Kubernetes API server to consult the service <code>webhook-server</code> in <code>namespace webhook-demo</code> whenever a pod is created by making a HTTP POST request to the <code>/mutate</code> URL. For this configuration to work, several prerequisites have to be met.</p><h2 id=webhook-rest-api>Webhook REST API</h2><p>The Kubernetes API server makes an HTTPS POST request to the given service and URL path, with a JSON-encoded <a href=https://github.com/kubernetes/api/blob/master/admission/v1beta1/types.go#L29><code>AdmissionReview</code></a> (with the <code>Request</code> field set) in the request body. The response should in turn be a JSON-encoded <code>AdmissionReview</code>, this time with the Response field set.</p><p>Our demo repository contains a <a href=https://github.com/stackrox/admission-controller-webhook-demo/blob/master/cmd/webhook-server/admission_controller.go#L132>function</a> that takes care of the serialization/deserialization boilerplate code and allows you to focus on implementing the logic operating on Kubernetes API objects. In our example, the function implementing the admission controller logic is called <code>applySecurityDefaults</code>, and an HTTPS server serving this function under the /mutate URL can be set up as follows:</p><pre><code>mux := http.NewServeMux()
mux.Handle(&quot;/mutate&quot;, admitFuncHandler(applySecurityDefaults))
server := &amp;http.Server{
  Addr:    &quot;:8443&quot;,
  Handler: mux,
}
log.Fatal(server.ListenAndServeTLS(certPath, keyPath))
</code></pre><p>Note that for the server to run without elevated privileges, we have the HTTP server listen on port 8443. Kubernetes does not allow specifying a port in the webhook configuration; it always assumes the HTTPS port 443. However, since a service object is required anyway, we can easily map port 443 of the service to port 8443 on the container:</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server  # specified by the deployment/pod
  ports:
    - port: 443
      targetPort: webhook-api  # name of port 8443 of the container
</code></pre><h2 id=object-modification-logic>Object Modification Logic</h2><p>In a mutating admission controller webhook, mutations are performed via <a href=https://tools.ietf.org/html/rfc6902>JSON patches</a>. While the JSON patch standard includes a lot of intricacies that go well beyond the scope of this discussion, the Go data structure in our example as well as its usage should give the user a good initial overview of how JSON patches work:</p><pre><code>type patchOperation struct {
  Op    string      `json:&quot;op&quot;`
  Path  string      `json:&quot;path&quot;`
  Value interface{} `json:&quot;value,omitempty&quot;`
}
</code></pre><p>For setting the field <code>.spec.securityContext.runAsNonRoot</code> of a pod to true, we construct the following <code>patchOperation</code> object:</p><pre><code>patches = append(patches, patchOperation{
  Op:    &quot;add&quot;,
  Path:  &quot;/spec/securityContext/runAsNonRoot&quot;,
  Value: true,
})
</code></pre><h2 id=tls-certificates>TLS Certificates</h2><p>Since a webhook must be served via HTTPS, we need proper certificates for the server. These certificates can be self-signed (rather: signed by a self-signed CA), but we need Kubernetes to instruct the respective CA certificate when talking to the webhook server. In addition, the common name (CN) of the certificate must match the server name used by the Kubernetes API server, which for internal services is <code>&lt;service-name></code>.<code>&lt;namespace>.svc</code>, i.e., <code>webhook-server.webhook-demo.svc</code> in our case. Since the generation of self-signed TLS certificates is well documented across the Internet, we simply refer to the respective <a href=https://github.com/stackrox/admission-controller-webhook-demo/blob/master/deployment/generate-keys.sh>shell script</a> in our example.</p><p>The webhook configuration shown previously contains a placeholder <code>${CA_PEM_B64}</code>. Before we can create this configuration, we need to replace this portion with the Base64-encoded PEM certificate of the CA. The <code>openssl base64 -A</code> command can be used for this purpose.</p><h2 id=testing-the-webhook>Testing the Webhook</h2><p>After deploying the webhook server and configuring it, which can be done by invoking the ./deploy.sh script from the repository, it is time to test and verify that the webhook indeed does its job. The repository contains <a href=https://github.com/stackrox/admission-controller-webhook-demo/tree/master/examples>three examples</a>:</p><ul><li>A pod that does not specify a security context (<code>pod-with-defaults</code>). We expect this pod to be run as non-root with user id 1234.</li><li>A pod that does specify a security context, explicitly allowing it to run as root (<code>pod-with-override</code>).</li><li>A pod with a conflicting configuration, specifying it must run as non-root but with a user id of 0 (<code>pod-with-conflict</code>). To showcase the rejection of object creation requests, we have augmented our admission controller logic to reject such obvious misconfigurations.</li></ul><p>Create one of these pods by running <code>kubectl create -f examples/&lt;name>.yaml</code>. In the first two examples, you can verify the user id under which the pod ran by inspecting the logs, for example:</p><pre><code>$ kubectl create -f examples/pod-with-defaults.yaml
$ kubectl logs pod-with-defaults
I am running as user 1234
</code></pre><p>In the third example, the object creation should be rejected with an appropriate error message:</p><pre><code>$ kubectl create -f examples/pod-with-conflict.yaml
Error from server (InternalError): error when creating &quot;examples/pod-with-conflict.yaml&quot;: Internal error occurred: admission webhook &quot;webhook-server.webhook-demo.svc&quot; denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)
</code></pre><p>Feel free to test this with your own workloads as well. Of course, you can also experiment a little bit further by changing the logic of the webhook and see how the changes affect object creation. More information on how to do experiment with such changes can be found in the <a href=https://github.com/stackrox/admission-controller-webhook-demo/blob/master/README.md>repository’s readme</a>.</p><h2 id=summary>Summary</h2><p>Kubernetes admission controllers offer significant advantages for security. Digging into two powerful examples, with accompanying available code, will help you get started on leveraging these powerful capabilities.</p><h2 id=references>References:</h2><ul><li><a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></li><li><a href=https://docs.okd.io/latest/architecture/additional_concepts/dynamic_admission_controllers.html>https://docs.okd.io/latest/architecture/additional_concepts/dynamic_admission_controllers.html</a></li><li><a href=https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/>https://kubernetes.io/blog/2018/01/extensible-admission-is-beta/</a></li><li><a href=https://medium.com/ibm-cloud/diving-into-kubernetes-mutatingadmissionwebhook-6ef3c5695f74>https://medium.com/ibm-cloud/diving-into-kubernetes-mutatingadmissionwebhook-6ef3c5695f74</a></li><li><a href=https://github.com/kubernetes/kubernetes/blob/v1.10.0-beta.1/test/images/webhook/main.go>https://github.com/kubernetes/kubernetes/blob/v1.10.0-beta.1/test/images/webhook/main.go</a></li><li><a href=https://github.com/istio/istio>https://github.com/istio/istio</a></li><li><a href=https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/>https://www.stackrox.com/post/2019/02/the-runc-vulnerability-a-deep-dive-on-protecting-yourself/</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-34045ffb28ad186738e3c43508ded599>A Look Back and What's in Store for Kubernetes Contributor Summits</h1><div class="td-byline mb-4"><time datetime=2019-03-20 class=text-muted>Wednesday, March 20, 2019</time></div><p><strong>Authors:</strong>
Paris Pittman (Google), Jonas Rosland (VMware)</p><p><strong>tl;dr</strong> - <a href=https://events.linuxfoundation.org/events/contributor-summit-europe-2019/>click here</a> for Barcelona Contributor Summit information.</p><figure><img src=/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/celebrationsig.jpg alt="Seattle Contributor Summit" width=600><figcaption><p>Seattle Contributor Summit</p></figcaption></figure><p>As our contributing community grows in great numbers, with more than 16,000 contributors this year across 150+ GitHub repositories, it’s important to provide face to face connections for our large distributed teams to have opportunities for collaboration and learning. In <a href=https://github.com/kubernetes/community/tree/master/sig-contributor-experience>Contributor Experience</a>, our methodology with planning events is a lot like our documentation; we build from personas -- interests, skills, and motivators to name a few. This way we ensure there is valuable content and learning for everyone.</p><p>We build the contributor summits around you:</p><ul><li>New Contributor</li><li>Current Contributor<ul><li>docs</li><li>code</li><li>community management</li></ul></li><li><a href=https://github.com/kubernetes/community/blob/master/community-membership.md>Subproject OWNERs</a> - aka maintainers in other OSS circles.</li><li>Special Interest Group (SIG) / Working Group (WG) <a href=https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md>Chair or Tech Lead</a></li><li>Active Contributors</li><li>Casual Contributors</li></ul><figure><img src=/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/newcontrib.jpg alt="New Contributor Workshop" width=600><figcaption><p>New Contributor Workshop</p></figcaption></figure><p>These personas combined with ample feedback from previous events, produce the altogether experience that welcomed over 600 contributors in Copenhagen (May), Shanghai(November), and Seattle(December) in 2018. Seattle's event drew over 300+ contributors, equal to Shanghai and Copenhagen combined, for the 6th contributor event in Kubernetes history. In true Kubernetes fashion, we expect another record breaking year of attendance. We've pre-ordered 900+ <a href=https://store.cncf.io/collections/kubernetes/products/copy-of-kubernetes-decal>contributor patches</a>, a tradition, and we are looking forward to giving them to you!</p><p>With that said...<br><strong>Save the Dates:</strong><br>Barcelona: May 19th (evening) and 20th (all day)<br>Shanghai: June 24th (all day)<br>San Diego: November 18th, 19th, and activities in KubeCon/CloudNativeCon week</p><p>In an effort of continual improvement, here's what to expect from us this year:</p><ul><li>Large new contributor workshops and contributor socials at all three events expected to break previous attendance records</li><li>A multiple track event in San Diego for all contributor types including workshops, birds of a feather, lightning talks and more</li><li>Addition of a “201” / “Intermediate” edition of the new contributor workshop in San Diego</li><li><a href=https://events.linuxfoundation.org/events/contributor-summit-europe-2019/>An event website</a>!</li><li>Follow along with updates: <a href=mailto:kubernetes-dev@googlegroups.com>kubernetes-dev@googlegroups.com</a> is our main communication hub as always; however, we will also blog here, our <a href=https://github.com/kubernetes/community/blob/master/events/community-meeting.md>Thursday Kubernetes Community Meeting</a>, <a href=https://twitter.com/kubernetesio>twitter</a>, SIG meetings, event site, discuss.kubernetes.io, and #contributor-summit on Slack.</li><li>Opportunities to get involved: We still have 2019 roles available!
Reach out to Contributor Experience via <a href=mailto:community@kubernetes.io>community@kubernetes.io</a>, stop by a Wednesday SIG update meeting, or catch us on Slack (#sig-contribex).</li></ul><figure><img src=/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/unconference.jpg alt="Unconference voting" width=600><figcaption><p>Unconference voting</p></figcaption></figure><h2 id=thanks>Thanks!</h2><p>Our 2018 crew 🥁<br>Jorge Castro, Paris Pittman, Bob Killen, Jeff Sica, Megan Lehn, Guinevere Saenger, Josh Berkus, Noah Abrahams, Yang Li, Xiangpeng Zhao, Puja Abbassi, Lindsey Tulloch, Zach Corleissen, Tim Pepper, Ihor Dvoretskyi, Nancy Mohamed, Chris Short, Mario Loria, Jason DeTiberus, Sahdev Zala, Mithra Raja</p><p>And an introduction to our 2019 crew (a thanks in advance ;) )...<br>Jonas Rosland, Josh Berkus, Paris Pittman, Jorge Castro, Bob Killen, Deb Giles, Guinevere Saenger, Noah Abrahams, Yang Li, Xiangpeng Zhao, Puja Abbassi, Rui Chen, Tim Pepper, Ihor Dvoretskyi, Dawn Foster</p><h2 id=relive-seattle-contributor-summit>Relive Seattle Contributor Summit</h2><p>📈 80% growth rate since the Austin 2017 December event</p><p>📜 Event waiting list: 103</p><p>🎓 76 contributors were on-boarded through the New Contributor Workshop</p><p>🎉 92% of the current contributors RSVPs attended and of those:</p><p>👩🏻‍🚒 25% were <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Group</a> or Working Group Chairs or Tech Leads</p><p>🗳 70% were eligible to vote in the last <a href=https://github.com/kubernetes/steering/blob/master/elections.md>steering committee election</a> - more than 50 contributions in 2018</p><p>📹 20+ <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP0kaZWKZc9KizriafE4pzh0">Sessions</a></p><p>👀 Most watched to date: Technical Vision, Security, API Code Base Tour</p><p>🌟 Top 3 according to survey: Live API Code Review, Deflaking Unconference, Technical Vision</p><p>🎱 🎳 160 attendees for the social at <a href=https://www.garagebilliards.com/>Garage</a> on Sunday night where we sunk eight balls and recorded strikes (out in some cases)</p><p>🏆 Special recognition: SIG Storage, @dims, and @jordan</p><p>📸 Pictures (special thanks to <a href=https://github.com/rdodev>rdodev</a>)</p><p>Garage Pic
Reg Desk</p><figure><img src=/images/blog/2019-03-14-A-Look-Back-And-Whats-In-Store-For-Kubernetes-Contributor-Summits/grouppicseatle.JPG alt="Some of the group in Seattle" width=600><figcaption><p>Some of the group in Seattle</p></figcaption></figure><p>“I love Contrib Summit! The intros and deep dives during KubeCon were a great extension of Contrib Summit. Y'all did an excellent job in the morning to level set expectations and prime everyone.” -- julianv<br>“great work! really useful and fun!” - coffeepac</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fdffb224c52f5dc23b58f61b3bacc2a7>KubeEdge, a Kubernetes Native Edge Computing Framework</h1><div class="td-byline mb-4"><time datetime=2019-03-19 class=text-muted>Tuesday, March 19, 2019</time></div><p><strong>Author:</strong> Sanil Kumar D (Huawei), Jun Du(Huawei)</p><h2 id=kubeedge-becomes-the-first-kubernetes-native-edge-computing-platform-with-both-edge-and-cloud-components-open-sourced>KubeEdge becomes the first Kubernetes Native Edge Computing Platform with both Edge and Cloud components open sourced!</h2><p>Open source edge computing is going through its most dynamic phase of development in the industry. So many open source platforms, so many consolidations and so many initiatives for standardization! This shows the strong drive to build better platforms to bring cloud computing to the edges to meet ever increasing demand. <a href=https://github.com/kubeedge/kubeedge>KubeEdge</a>, which was announced last year, now brings great news for cloud native computing! It provides a complete edge computing solution based on Kubernetes with separate cloud and edge core modules. Currently, both the cloud and edge modules are open sourced.</p><p>Unlike certain light weight kubernetes platforms available around, KubeEdge is made to build edge computing solutions extending the cloud. The control plane resides in cloud, though scalable and extendable. At the same time, the edge can work in offline mode. Also it is lightweight and containerized, and can support heterogeneous hardware at the edge. With the optimization in edge resource utlization, KubeEdge positions to save significant setup and operation cost for edge solutions. This makes it the most compelling edge computing platform in the world currently, based on Kubernetes!</p><h2 id=kube-rnetes-edge-opening-up-a-new-kubernetes-based-ecosystem-for-edge-computing>Kube(rnetes)Edge! - Opening up a new Kubernetes-based ecosystem for Edge Computing</h2><p>The key goal for KubeEdge is extending Kubernetes ecosystem from cloud to edge. From the time it was announced to the public at KubeCon in Shanghai in November 2018, the architecture direction for KubeEdge was aligned to Kubernetes, as its name!</p><p>It started with its v0.1 providing the basic edge computing features. Now, with its latest release v0.2, it brings the cloud components to connect and complete the loop. With consistent and scalable Kubernetes-based interfaces, KubeEdge enables the orchestration and management of edge clusters similar to how Kubernetes manages in the cloud. This opens up seamless possibilities of bringing cloud computing capabilities to the edge, quickly and efficiently.</p><p align=center><img src=/images/blog/2019-03-12-kubeedge-k8s-based-edge-intro/kubeedge-logo.png></p><p><strong>KubeEdge Links:</strong></p><ul><li><a href=https://kubeedge.io>Website</a></li><li><a href=https://github.com/kubeedge/kubeedge>SourceCode</a></li><li><a href=https://docs.kubeedge.io>Documentation</a></li></ul><p>Based on its roadmap and architecture, KubeEdge tries to support all edge nodes, applications, devices and even the cluster management consistent with the Kuberenetes interface. This will help the edge cloud act exactly like a cloud cluster. This can save a lot of time and cost on the edge cloud development deployment based on KubeEdge.</p><p>KubeEdge provides a containerized edge computing platform, which is inherently scalable. As it's modular and optimized, it is lightweight (66MB foot print and ~30MB running memory) and could be deployed on low resource devices. Similarly, the edge node can be of different hardware architecture and with different hardware configurations. For the device connectivity, it can support multiple protocols and it uses a standard MQTT-based communication. This helps in scaling the edge clusters with new nodes and devices efficiently.</p><p><strong><p align=center>You heard it right!</p></strong><strong><p align=center>KubeEdge Cloud Core modules are open sourced!</p></strong></p><p>By open sourcing both the edge and cloud modules, KubeEdge brings a complete cloud vendor agnostic lightweight heterogeneous edge computing platform. It is now ready to support building a complete Kubernetes ecosystem for edge computing, exploiting most of the existing cloud native projects or software modules. This can enable a mini-cloud at the edge to support demanding use cases like data analytics, video analytics, machine learning and more.</p><h2 id=kubeedge-architecture-building-kuberenetes-native-edge-computing>KubeEdge Architecture: Building Kuberenetes Native Edge computing!</h2><p>The core architecture tenet for KubeEdge is to build interfaces that are consistent with Kubernetes, be it on the cloud side or edge side.</p><p align=center><img src=/images/blog/2019-03-12-kubeedge-k8s-based-edge-intro/kubeedge-highlevel-arch.png></p><p><strong>Edged</strong>: Manages containerized Applications at the Edge.</p><p><strong>EdgeHub</strong>: Communication interface module at the Edge. It is a web socket client responsible for interacting with Cloud Service for edge computing.</p><p><strong>CloudHub</strong>: Communication interface module at the Cloud. A web socket server responsible for watching changes on the cloud side, caching and sending messages to EdgeHub.</p><p><strong>EdgeController</strong>: Manages the Edge nodes. It is an extended Kubernetes controller which manages edge nodes and pods metadata so that the data can be targeted to a specific edge node.</p><p><strong>EventBus</strong>: Handles the internal edge communications using MQTT. It is an MQTT client to interact with MQTT servers (mosquitto), offering publish and subscribe capabilities to other components.</p><p><strong>DeviceTwin</strong>: It is software mirror for devices that handles the device metadata. This module helps in handling device status and syncing the same to cloud. It also provides query interfaces for applications, as it interfaces to a lightweight database (SQLite).</p><p><strong>MetaManager</strong>: It manages the metadata at the edge node. This is the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata to/from a lightweight database (SQLite).</p><p>Even if you want to add more control plane modules based on the architecture refinement and improvement (for example enhanced security), it is simple as it uses consistent registration and modular communication within these modules.</p><p><strong><p align=center>KubeEdge provides scalable lightweight Kubernetes Native Edge Computing Platform which can work in offline mode.</p></strong></p><p><strong><p align=center>It helps simplify edge application development and deployment.</p></strong></p><p><strong><p align=center>Cloud vendor agnostic and can run the cloud core modules on any compute node.</p></strong></p><h2 id=release-0-1-to-0-2-game-changer>Release 0.1 to 0.2 -- game changer!</h2><p>KubeEdge v0.1 was released at the end of December 2018 with very basic edge features to manage edge applications along with Kubernetes API primitives for node, pod, config etc. In ~2 months, KubeEdge v0.2 was release on March 5th, 2019. This release provides the cloud core modules and enables the end to end open source edge computing solution. The cloud core modules can be deployed to any compute node from any cloud vendors or on-prem.</p><p>Now, the complete edge solution can be installed and tested very easily, also with a laptop.</p><h2 id=run-anywhere-simple-and-light>Run Anywhere - Simple and Light</h2><p>As described, the KubeEdge Edge and Cloud core components can be deployed easily and can run the user applications. The edge core has a foot print of 66MB and just needs 30MB memory to run. Similarly the cloud core can run on any cloud nodes. (User can experience by running it on a laptop as well)</p><p>The installation is simple and can be done in few steps:</p><ol><li>Setup the pre-requisites Docker, Kubernetes, MQTT and openssl</li><li>Clone and Build KubeEdge Cloud and Edge</li><li>Run Cloud</li><li>Run Edge</li></ol><p>The detailed steps for each are available at <a href=https://github.com/kubeedge/kubeedge>KubeEdge/kubeedge</a></p><h2 id=future-taking-off-with-competent-features-and-community-collaboration>Future: Taking off with competent features and community collaboration</h2><p>KubeEdge has been developed by members from the community who are active contributors to Kubernetes/CNCF and doing research in edge computing. The KubeEdge team is also actively collaborating with Kubernetes IOT/EDGE WORKING GROUP. Within a few months of the KubeEdge announcement it has attracted members from different organizations including JingDong, Zhejiang University, SEL Lab, Eclipse, China Mobile, ARM, Intel to collaborate in building the platform and ecosystem.</p><p>KubeEdge has a clear roadmap for its upcoming major releases in 2019. vc1.0 targets to provide a complete edge cluster and device management solution with standard edge to edge communication, while v2.0 targets to have advanced features like service mesh, function service , data analytics etc at edge. Also, for all the features, KubeEdge architecture would attempt to utilize the existing CNCF projects/software.</p><p>The KubeEdge community needs varied organizations, their requirements, use cases and support to build it. Please join to make a kubernetes native edge computing platform which can extend the cloud native computing paradigm to edge cloud.</p><h2 id=how-to-get-involved>How to Get Involved?</h2><p>We welcome more collaboration to build the Kubernetes native edge computing ecosystem. Please join us!</p><ul><li>Twitter: <a href=https://twitter.com/kubeedge>https://twitter.com/kubeedge</a></li><li>Slack: <a href=mailto:kubeedge.slack.com>kubeedge.slack.com</a></li><li>Website: <a href=https://kubeedge.io>https://kubeedge.io</a></li><li>GitHub: <a href=https://github.com/kubeedge/kubeedge>https://github.com/kubeedge/kubeedge</a></li><li>Email: <a href=mailto:kubeedge@gmail.com>kubeedge@gmail.com</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-adc41261346fc01af487b093b7dac8c7>Kubernetes Setup Using Ansible and Vagrant</h1><div class="td-byline mb-4"><time datetime=2019-03-15 class=text-muted>Friday, March 15, 2019</time></div><p><strong>Author:</strong> Naresh L J (Infosys)</p><h2 id=objective>Objective</h2><p>This blog post describes the steps required to setup a multi node Kubernetes cluster for development purposes. This setup provides a production-like cluster that can be setup on your local machine.</p><h2 id=why-do-we-require-multi-node-cluster-setup>Why do we require multi node cluster setup?</h2><p>Multi node Kubernetes clusters offer a production-like environment which has various advantages. Even though Minikube provides an excellent platform for getting started, it doesn't provide the opportunity to work with multi node clusters which can help solve problems or bugs that are related to application design and architecture. For instance, Ops can reproduce an issue in a multi node cluster environment, Testers can deploy multiple versions of an application for executing test cases and verifying changes. These benefits enable teams to resolve issues faster which make the more agile.</p><h2 id=why-use-vagrant-and-ansible>Why use Vagrant and Ansible?</h2><p>Vagrant is a tool that will allow us to create a virtual environment easily and it eliminates pitfalls that cause the works-on-my-machine phenomenon. It can be used with multiple providers such as Oracle VirtualBox, VMware, Docker, and so on. It allows us to create a disposable environment by making use of configuration files.</p><p>Ansible is an infrastructure automation engine that automates software configuration management. It is agentless and allows us to use SSH keys for connecting to remote machines. Ansible playbooks are written in yaml and offer inventory management in simple text files.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>Vagrant should be installed on your machine. Installation binaries can be found <a href=https://www.vagrantup.com/downloads.html>here</a>.</li><li>Oracle VirtualBox can be used as a Vagrant provider or make use of similar providers as described in Vagrant's official <a href=https://www.vagrantup.com/docs/providers/>documentation</a>.</li><li>Ansible should be installed in your machine. Refer to the <a href=https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html>Ansible installation guide</a> for platform specific installation.</li></ul><h2 id=setup-overview>Setup overview</h2><p>We will be setting up a Kubernetes cluster that will consist of one master and two worker nodes. All the nodes will run Ubuntu Xenial 64-bit OS and Ansible playbooks will be used for provisioning.</p><h4 id=step-1-creating-a-vagrantfile>Step 1: Creating a Vagrantfile</h4><p>Use the text editor of your choice and create a file with named <code>Vagrantfile</code>, inserting the code below. The value of N denotes the number of nodes present in the cluster, it can be modified accordingly. In the below example, we are setting the value of N as 2.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ruby data-lang=ruby><span style=color:#800>IMAGE_NAME</span> <span style=color:#666>=</span> <span style=color:#b44>&#34;bento/ubuntu-16.04&#34;</span>
N <span style=color:#666>=</span> <span style=color:#666>2</span>

<span style=color:#800>Vagrant</span><span style=color:#666>.</span>configure(<span style=color:#b44>&#34;2&#34;</span>) <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>config<span style=color:#666>|</span>
    config<span style=color:#666>.</span>ssh<span style=color:#666>.</span>insert_key <span style=color:#666>=</span> <span style=color:#a2f>false</span>

    config<span style=color:#666>.</span>vm<span style=color:#666>.</span>provider <span style=color:#b44>&#34;virtualbox&#34;</span> <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>v<span style=color:#666>|</span>
        v<span style=color:#666>.</span>memory <span style=color:#666>=</span> <span style=color:#666>1024</span>
        v<span style=color:#666>.</span>cpus <span style=color:#666>=</span> <span style=color:#666>2</span>
    <span style=color:#a2f;font-weight:700>end</span>
      
    config<span style=color:#666>.</span>vm<span style=color:#666>.</span>define <span style=color:#b44>&#34;k8s-master&#34;</span> <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>master<span style=color:#666>|</span>
        master<span style=color:#666>.</span>vm<span style=color:#666>.</span>box <span style=color:#666>=</span> <span style=color:#800>IMAGE_NAME</span>
        master<span style=color:#666>.</span>vm<span style=color:#666>.</span>network <span style=color:#b44>&#34;private_network&#34;</span>, <span style=color:#b8860b>ip</span>: <span style=color:#b44>&#34;192.168.50.10&#34;</span>
        master<span style=color:#666>.</span>vm<span style=color:#666>.</span>hostname <span style=color:#666>=</span> <span style=color:#b44>&#34;k8s-master&#34;</span>
        master<span style=color:#666>.</span>vm<span style=color:#666>.</span>provision <span style=color:#b44>&#34;ansible&#34;</span> <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>ansible<span style=color:#666>|</span>
            ansible<span style=color:#666>.</span>playbook <span style=color:#666>=</span> <span style=color:#b44>&#34;kubernetes-setup/master-playbook.yml&#34;</span>
            ansible<span style=color:#666>.</span>extra_vars <span style=color:#666>=</span> {
                <span style=color:#b8860b>node_ip</span>: <span style=color:#b44>&#34;192.168.50.10&#34;</span>,
            }
        <span style=color:#a2f;font-weight:700>end</span>
    <span style=color:#a2f;font-weight:700>end</span>

    (<span style=color:#666>1</span><span style=color:#666>..</span>N)<span style=color:#666>.</span>each <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>i<span style=color:#666>|</span>
        config<span style=color:#666>.</span>vm<span style=color:#666>.</span>define <span style=color:#b44>&#34;node-</span><span style=color:#b68;font-weight:700>#{</span>i<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span> <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>node<span style=color:#666>|</span>
            node<span style=color:#666>.</span>vm<span style=color:#666>.</span>box <span style=color:#666>=</span> <span style=color:#800>IMAGE_NAME</span>
            node<span style=color:#666>.</span>vm<span style=color:#666>.</span>network <span style=color:#b44>&#34;private_network&#34;</span>, <span style=color:#b8860b>ip</span>: <span style=color:#b44>&#34;192.168.50.</span><span style=color:#b68;font-weight:700>#{</span>i <span style=color:#666>+</span> <span style=color:#666>10</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>
            node<span style=color:#666>.</span>vm<span style=color:#666>.</span>hostname <span style=color:#666>=</span> <span style=color:#b44>&#34;node-</span><span style=color:#b68;font-weight:700>#{</span>i<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>
            node<span style=color:#666>.</span>vm<span style=color:#666>.</span>provision <span style=color:#b44>&#34;ansible&#34;</span> <span style=color:#a2f;font-weight:700>do</span> <span style=color:#666>|</span>ansible<span style=color:#666>|</span>
                ansible<span style=color:#666>.</span>playbook <span style=color:#666>=</span> <span style=color:#b44>&#34;kubernetes-setup/node-playbook.yml&#34;</span>
                ansible<span style=color:#666>.</span>extra_vars <span style=color:#666>=</span> {
                    <span style=color:#b8860b>node_ip</span>: <span style=color:#b44>&#34;192.168.50.</span><span style=color:#b68;font-weight:700>#{</span>i <span style=color:#666>+</span> <span style=color:#666>10</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>,
                }
            <span style=color:#a2f;font-weight:700>end</span>
        <span style=color:#a2f;font-weight:700>end</span>
    <span style=color:#a2f;font-weight:700>end</span>
<span style=color:#a2f;font-weight:700>end</span>
</code></pre></div><h3 id=step-2-create-an-ansible-playbook-for-kubernetes-master>Step 2: Create an Ansible playbook for Kubernetes master.</h3><p>Create a directory named <code>kubernetes-setup</code> in the same directory as the <code>Vagrantfile</code>. Create two files named <code>master-playbook.yml</code> and <code>node-playbook.yml</code> in the directory <code>kubernetes-setup</code>.</p><p>In the file <code>master-playbook.yml</code>, add the code below.</p><h4 id=step-2-1-install-docker-and-its-dependent-components>Step 2.1: Install Docker and its dependent components.</h4><p>We will be installing the following packages, and then adding a user named “vagrant” to the “docker” group.</p><ul><li>docker-ce</li><li>docker-ce-cli</li><li>containerd.io</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>hosts</span>:<span style=color:#bbb> </span>all<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>become</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tasks</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Install packages that allow apt to be used over HTTPS<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;{{ packages }}&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>update_cache</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>yes</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>vars</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>packages</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- apt-transport-https<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- ca-certificates<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- curl<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- gnupg-agent<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- software-properties-common<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Add an apt signing key for Docker<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt_key</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>url</span>:<span style=color:#bbb> </span>https://download.docker.com/linux/ubuntu/gpg<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Add apt repository for stable version<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt_repository</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>repo</span>:<span style=color:#bbb> </span>deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Install docker and its dependecies<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt</span>:<span style=color:#bbb> 
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;{{ packages }}&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>update_cache</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>yes</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>vars</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>packages</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- docker-ce <span style=color:#bbb>
</span><span style=color:#bbb>      </span>- docker-ce-cli <span style=color:#bbb>
</span><span style=color:#bbb>      </span>- containerd.io<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>notify</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- docker status<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Add vagrant user to docker group<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vagrant<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb> </span>docker<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-2-kubelet-will-not-start-if-the-system-has-swap-enabled-so-we-are-disabling-swap-using-the-below-code>Step 2.2: Kubelet will not start if the system has swap enabled, so we are disabling swap using the below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Remove swapfile from /etc/fstab<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mount</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;{{ item }}&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>fstype</span>:<span style=color:#bbb> </span>swap<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>absent<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>with_items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- swap<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- none<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Disable swap<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>swapoff -a<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>when</span>:<span style=color:#bbb> </span>ansible_swaptotal_mb &gt; 0<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-3-installing-kubelet-kubeadm-and-kubectl-using-the-below-code>Step 2.3: Installing kubelet, kubeadm and kubectl using the below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Add an apt signing key for Kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt_key</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>url</span>:<span style=color:#bbb> </span>https://packages.cloud.google.com/apt/doc/apt-key.gpg<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Adding apt repository for Kubernetes<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt_repository</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>repo</span>:<span style=color:#bbb> </span>deb https://apt.kubernetes.io/ kubernetes-xenial main<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>filename</span>:<span style=color:#bbb> </span>kubernetes.list<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Install Kubernetes binaries<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apt</span>:<span style=color:#bbb> 
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;{{ packages }}&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>present<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>update_cache</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>yes</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>vars</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>packages</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- kubelet <span style=color:#bbb>
</span><span style=color:#bbb>        </span>- kubeadm <span style=color:#bbb>
</span><span style=color:#bbb>        </span>- kubectl<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Configure node ip<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lineinfile</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/etc/default/kubelet<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>line</span>:<span style=color:#bbb> </span>KUBELET_EXTRA_ARGS=--node-ip={{ node_ip }}<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Restart kubelet<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubelet<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>daemon_reload</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>yes</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>state</span>:<span style=color:#bbb> </span>restarted<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-3-initialize-the-kubernetes-cluster-with-kubeadm-using-the-below-code-applicable-only-on-master-node>Step 2.3: Initialize the Kubernetes cluster with kubeadm using the below code (applicable only on master node).</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Initialize the Kubernetes cluster using kubeadm<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>kubeadm init --apiserver-advertise-address=&#34;192.168.50.10&#34; --apiserver-cert-extra-sans=&#34;192.168.50.10&#34;  --node-name k8s-master --pod-network-cidr=192.168.0.0/16<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-4-setup-the-kube-config-file-for-the-vagrant-user-to-access-the-kubernetes-cluster-using-the-below-code>Step 2.4: Setup the kube config file for the vagrant user to access the Kubernetes cluster using the below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Setup kubeconfig for vagrant user<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;{{ item }}&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>with_items</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span>- mkdir -p /home/vagrant/.kube<span style=color:#bbb>
</span><span style=color:#bbb>     </span>- cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config<span style=color:#bbb>
</span><span style=color:#bbb>     </span>- chown vagrant:vagrant /home/vagrant/.kube/config<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-5-setup-the-container-networking-provider-and-the-network-policy-engine-using-the-below-code>Step 2.5: Setup the container networking provider and the network policy engine using the below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Install calico pod network<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>become</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>kubectl create -f https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/calico.yaml<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-6-generate-kube-join-command-for-joining-the-node-to-the-kubernetes-cluster-and-store-the-command-in-the-file-named-join-command>Step 2.6: Generate kube join command for joining the node to the Kubernetes cluster and store the command in the file named <code>join-command</code>.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Generate join command<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>kubeadm token create --print-join-command<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>register</span>:<span style=color:#bbb> </span>join_command<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Copy join command to local file<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>local_action</span>:<span style=color:#bbb> </span>copy content=&#34;{{ join_command.stdout_lines[0] }}&#34; dest=&#34;./join-command&#34;<span style=color:#bbb>
</span></code></pre></div><h4 id=step-2-7-setup-a-handler-for-checking-docker-daemon-using-the-below-code>Step 2.7: Setup a handler for checking Docker daemon using the below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>handlers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>docker status<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb> </span>name=docker state=started<span style=color:#bbb>
</span></code></pre></div><h4 id=step-3-create-the-ansible-playbook-for-kubernetes-node>Step 3: Create the Ansible playbook for Kubernetes node.</h4><p>Create a file named <code>node-playbook.yml</code> in the directory <code>kubernetes-setup</code>.</p><p>Add the code below into <code>node-playbook.yml</code></p><h4 id=step-3-1-start-adding-the-code-from-steps-2-1-till-2-3>Step 3.1: Start adding the code from Steps 2.1 till 2.3.</h4><h4 id=step-3-2-join-the-nodes-to-the-kubernetes-cluster-using-below-code>Step 3.2: Join the nodes to the Kubernetes cluster using below code.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Copy the join command to server location<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>copy</span>:<span style=color:#bbb> </span>src=join-command dest=/tmp/join-command.sh mode=0777<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>Join the node to cluster<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>sh /tmp/join-command.sh<span style=color:#bbb>
</span></code></pre></div><h4 id=step-3-3-add-the-code-from-step-2-7-to-finish-this-playbook>Step 3.3: Add the code from step 2.7 to finish this playbook.</h4><h4 id=step-4-upon-completing-the-vagrantfile-and-playbooks-follow-the-below-steps>Step 4: Upon completing the Vagrantfile and playbooks follow the below steps.</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ <span style=color:#a2f>cd</span> /path/to/Vagrantfile
$ vagrant up
</code></pre></div><p>Upon completion of all the above steps, the Kubernetes cluster should be up and running.
We can login to the master or worker nodes using Vagrant as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ <span style=color:#080;font-style:italic>## Accessing master</span>
$ vagrant ssh k8s-master
vagrant@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   18m     v1.13.3
node-1       Ready    &lt;none&gt;   12m     v1.13.3
node-2       Ready    &lt;none&gt;   6m22s   v1.13.3

$ <span style=color:#080;font-style:italic>## Accessing nodes</span>
$ vagrant ssh node-1
$ vagrant ssh node-2
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-19cd0c4556be5dfd65e562e60778ef83>Raw Block Volume support to Beta</h1><div class="td-byline mb-4"><time datetime=2019-03-07 class=text-muted>Thursday, March 07, 2019</time></div><p><strong>Authors:</strong>
Ben Swartzlander (NetApp), Saad Ali (Google)</p><p>Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.</p><h2 id=what-are-block-devices>What are block devices?</h2><p>Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.</p><p>Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.</p><p>It's worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.</p><h2 id=why-add-raw-block-volumes-to-kubernetes>Why add raw block volumes to kubernetes?</h2><p>There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).</p><p>From a programmer's perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.</p><p>As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.</p><h2 id=which-volume-plugins-support-raw-blocks>Which volume plugins support raw blocks?</h2><p>As of the publishing of this blog, the following in-tree volumes types support raw blocks:</p><ul><li>AWS EBS</li><li>Azure Disk</li><li>Cinder</li><li>Fibre Channel</li><li>GCE PD</li><li>iSCSI</li><li>Local volumes</li><li>RBD (Ceph)</li><li>Vsphere</li></ul><p>Out-of-tree <a href=https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/>CSI volume drivers</a> may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation <a href=https://kubernetes-csi.github.io/docs/raw-block.html>here</a>.</p><h2 id=kubernetes-raw-block-volume-api>Kubernetes raw block volume API</h2><p>Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating <code>PersistentVolumeClaim</code> objects which bind to <code>PersistentVolume</code> objects, and are attached to Pods in Kubernetes by including them in the volumes array of the <code>PodSpec</code>.</p><p>There are 2 important differences however. First, to request a raw block <code>PersistentVolumeClaim</code>, you must set <code>volumeMode = "Block"</code> in the <code>PersistentVolumeClaimSpec</code>. Leaving <code>volumeMode</code> blank is the same as specifying <code>volumeMode = "Filesystem"</code> which results in the traditional behavior. <code>PersistentVolumes</code> also have a <code>volumeMode</code> field in their <code>PersistentVolumeSpec</code>, and <code>"Block"</code> type PVCs can only bind to <code>"Block"</code> type PVs and <code>"Filesystem"</code> PVCs can only bind to <code>"Filesystem"</code> PVs.</p><p>Secondly, when using a raw block volume in your Pods, you must specify a <code>VolumeDevice</code> in the Container portion of the <code>PodSpec</code> rather than a <code>VolumeMount</code>. <code>VolumeDevices</code> have <code>devicePaths</code> instead of <code>mountPaths</code>, and inside the container, applications will see a device at that path instead of a mounted file system.</p><p>Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.</p><h2 id=creating-a-new-raw-block-pvc>Creating a new raw block PVC</h2><p>First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.</p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Block
  storageClassName: my-sc
  resources:
    requests:
    storage: 1Gi
</code></pre><h2 id=using-a-raw-block-pvc>Using a raw block PVC</h2><p>When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: busybox
      command:
        - sleep
        - “3600”
      volumeDevices:
        - devicePath: /dev/block
          name: my-volume
      imagePullPolicy: IfNotPresent
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc
</code></pre><h2 id=as-a-storage-vendor-how-do-i-add-support-for-raw-block-devices-to-my-csi-plugin>As a storage vendor, how do I add support for raw block devices to my CSI plugin?</h2><p>Raw block support for CSI plugins is still alpha, but support can be added today. The <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>CSI specification</a> details how to handle requests for volume that have the <code>BlockVolume</code> capability instead of the <code>MountVolume</code> capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see <a href=https://kubernetes-csi.github.io/docs/raw-block.html>documentation here</a>.</p><h2 id=issues-gotchas>Issues/gotchas</h2><p>Because block devices are actually devices, it’s possible to do low-level actions on them from inside containers that wouldn’t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.</p><p>By default, Linux won’t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the <code>SYS_RAWIO</code> capability to the container security context to allow this. See documentation <a href=/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container>here</a>.</p><p>Also, while Kubernetes is guaranteed to deliver a block device to the container, there’s no guarantee that it’s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>Check out additional documentation on the snapshot feature here: <a href=/docs/concepts/storage/persistent-volumes/#raw-block-volume-support>Raw Block Volume Support</a></p><p>How do I get involved?</p><p>Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!</p><p><a href=https://github.com/kubernetes/community/tree/master/sig-storage>https://github.com/kubernetes/community/tree/master/sig-storage</a>
<a href=https://github.com/container-storage-interface/community/blob/master/README.md>https://github.com/container-storage-interface/community/blob/master/README.md</a></p><p>Special thanks to all the contributors who helped add block volume support to Kubernetes including:</p><ul><li>Ben Swartzlander (<a href=https://github.com/bswartz>https://github.com/bswartz</a>)</li><li>Brad Childs (<a href=https://github.com/childsb>https://github.com/childsb</a>)</li><li>Erin Boyd (<a href=https://github.com/erinboyd>https://github.com/erinboyd</a>)</li><li>Masaki Kimura (<a href=https://github.com/mkimuram>https://github.com/mkimuram</a>)</li><li>Matthew Wong (<a href=https://github.com/wongma7>https://github.com/wongma7</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>https://github.com/msau42</a>)</li><li>Mitsuhiro Tanino (<a href=https://github.com/mtanino>https://github.com/mtanino</a>)</li><li>Saad Ali (<a href=https://github.com/saad-ali>https://github.com/saad-ali</a>)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-01e11a71b0c44482439c19248d7559c5>Automate Operations on your Cluster with OperatorHub.io</h1><div class="td-byline mb-4"><time datetime=2019-02-28 class=text-muted>Thursday, February 28, 2019</time></div><p><strong>Author:</strong>
Diane Mueller, Director of Community Development, Cloud Platforms, Red Hat</p><p>One of the important challenges facing developers and Kubernetes administrators has been a lack of ability to quickly find common services that are operationally ready for Kubernetes. Typically, the presence of an Operator for a specific service - a pattern that was introduced in 2016 and has gained momentum - is a good signal for the operational readiness of the service on Kubernetes. However, there has to date not existed a registry of Operators to simplify the discovery of such services.</p><p>To help address this challenge, today Red Hat is launching OperatorHub.io in collaboration with AWS, Google Cloud and Microsoft. OperatorHub.io enables developers and Kubernetes administrators to find and install curated Operator-backed services with a base level of documentation, active maintainership by communities or vendors, basic testing, and packaging for optimized life-cycle management on Kubernetes.</p><p>The Operators currently in OperatorHub.io are just the start. We invite the Kubernetes community to join us in building a vibrant community for Operators by developing, packaging, and publishing Operators on OperatorHub.io.</p><h2 id=what-does-operatorhub-io-provide>What does OperatorHub.io provide?</h2><p>OperatorHub.io is designed to address the needs of both Kubernetes developers and users. For the former it provides a common registry where they can publish their Operators alongside with descriptions, relevant details like version, image, code repository and have them be readily packaged for installation. They can also update already published Operators to new versions when they are released.</p><p>Users get the ability to discover and download Operators at a central location, that has content which has been screened for the previously mentioned criteria and scanned for known vulnerabilities. In addition, developers can guide users of their Operators with prescriptive examples of the <code>CustomResources</code> that they introduce to interact with the application.</p><h2 id=what-is-an-operator>What is an Operator?</h2><p>Operators were first introduced in 2016 by CoreOS and have been used by Red Hat and the Kubernetes community as a way to package, deploy and manage a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and well-known tooling, like kubectl.</p><p>An Operator is implemented as a custom controller that watches for certain Kubernetes resources to appear, be modified or deleted. These are typically <code>CustomResourceDefinitions</code> that the Operator “owns.” In the spec properties of these objects the user declares the desired state of the application or the operation. The Operator’s reconciliation loop will pick these up and perform the required actions to achieve the desired state. For example, the intent to create a highly available etcd cluster could be expressed by creating an new resource of type <code>EtcdCluster</code>:</p><pre><code>apiVersion: &quot;etcd.database.coreos.com/v1beta2&quot;
kind: &quot;EtcdCluster&quot;
metadata:
  name: &quot;my-etcd-cluster&quot;
spec:
  size: 3
  version: &quot;3.3.12&quot;
</code></pre><p>The <code>EtcdOperator</code> would be responsible for creating a 3-node etcd cluster running version v3.3.12 as a result. Similarly, an object of type <code>EtcdBackup</code> could be defined to express the intent to create a consistent backup of the etcd database to an S3 bucket.</p><h2 id=how-do-i-create-and-run-an-operator>How do I create and run an Operator?</h2><p>One way to get started is with the <a href=https://github.com/operator-framework>Operator Framework</a>, an open source toolkit that provides an SDK, lifecycle management, metering and monitoring capabilities. It enables developers to build, test, and package Operators. Operators can be implemented in several programming and automation languages, including Go, Helm, and Ansible, all three of which are supported directly by the SDK.</p><p>If you are interested in creating your own Operator, we recommend checking out the Operator Framework to <a href=https://github.com/operator-framework/getting-started>get started</a>.</p><p>Operators vary in where they fall along <a href=https://github.com/operator-framework/operator-sdk/blob/master/doc/images/operator-capability-level.png>the capability spectrum</a> ranging from basic functionality to having specific operational logic for an application to automate advanced scenarios like backup, restore or tuning. Beyond basic installation, advanced Operators are designed to handle upgrades more seamlessly and react to failures automatically. Currently, Operators on OperatorHub.io span the maturity spectrum, but we anticipate their continuing maturation over time.</p><p>While Operators on OperatorHub.io don’t need to be implemented using the SDK, they are packaged for deployment through the <a href=https://github.com/operator-framework/operator-lifecycle-manager>Operator Lifecycle Manager</a> (OLM). The format mainly consists of a YAML manifest referred to as <code>[ClusterServiceVersion]</code>(<a href=https://github.com/operator-framework/operator-lifecycle-manager/blob/master/doc/design/building-your-csv.md>https://github.com/operator-framework/operator-lifecycle-manager/blob/master/doc/design/building-your-csv.md</a>) which provides information about the <code>CustomResourceDefinitions</code> the Operator owns or requires, which RBAC definition it needs, where the image is stored, etc. This file is usually accompanied by additional YAML files which define the Operators’ own CRDs. This information is processed by OLM at the time a user requests to install an Operator to provide dependency resolution and automation.</p><h2 id=what-does-listing-of-an-operator-on-operatorhub-io-mean>What does listing of an Operator on OperatorHub.io mean?</h2><p>To be listed, Operators must successfully show cluster lifecycle features, be packaged as a CSV to be maintained through OLM, and have acceptable documentation for its intended users.</p><p>Some examples of Operators that are currently listed on OperatorHub.io include: Amazon Web Services Operator, Couchbase Autonomous Operator, CrunchyData’s PostgreSQL, etcd Operator, Jaeger Operator for Kubernetes, Kubernetes Federation Operator, MongoDB Enterprise Operator, Percona MySQL Operator, PlanetScale’s Vitess Operator, Prometheus Operator, and Redis Operator.</p><h2 id=want-to-add-your-operator-to-operatorhub-io-follow-these-steps>Want to add your Operator to OperatorHub.io? Follow these steps</h2><p>If you have an existing Operator, follow the <a href=https://www.operatorhub.io/contribute>contribution guide</a> using a fork of the <a href=https://github.com/operator-framework/community-operators/>community-operators</a> repository. Each contribution contains the CSV, all of the <code>CustomResourceDefinitions</code>, access control rules and references to the container image needed to install and run your Operator, plus other info like a description of its features and supported Kubernetes versions. A complete example, including multiple versions of the Operator, can be found with the <a href=https://github.com/operator-framework/community-operators/tree/master/community-operators/etcd>EtcdOperator</a>.</p><p>After testing out your Operator on your own cluster, submit a PR to the <a href=https://github.com/operator-framework/community-operators>community repository</a> with all of YAML files following <a href=https://github.com/operator-framework/community-operators#adding-your-operator>this directory structure</a>. Subsequent versions of the Operator can be published in the same way. At first this will be reviewed manually, but automation is on the way. After it’s merged by the maintainers, it will show up on OperatorHub.io along with its documentation and a convenient installation method.</p><h2 id=want-to-learn-more>Want to learn more?</h2><ul><li>Attend one of the upcoming Kubernetes Operator Framework hands-on workshops at <a href=https://www.socallinuxexpo.org/scale/17x/presentations/workshop-kubernetes-operator-framework>ScaleX</a> in Pasadena on March 7 and at the <a href=https://commons.openshift.org/gatherings/Santa_Clara_2019.html>OpenShift Commons Gathering on Operating at Scale in Santa Clara on March 11</a></li><li>Listen to this <a href="https://www.youtube.com/watch?v=GgEKEYH9MMM&feature=youtu.be">OpenShift Commons Briefing on “The State of Operators” with Daniel Messer and Diane Mueller</a></li><li>Join in on the online conversations in the community <a href=https://kubernetes.slack.com/messages/CAW0GV7A5>Kubernetes-Operator Slack Channel</a> and the <a href=https://groups.google.com/forum/#!forum/operator-framework>Operator Framework Google Group</a></li><li>Finally, read up on how to add your Operator to OperatorHub.io: <a href=https://operatorhub.io/contribute>https://operatorhub.io/contribute</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4b1a370b544d5c6b8d88292e9f3b691b>Building a Kubernetes Edge (Ingress) Control Plane for Envoy v2</h1><div class="td-byline mb-4"><time datetime=2019-02-12 class=text-muted>Tuesday, February 12, 2019</time></div><p><strong>Author:</strong>
Daniel Bryant, Product Architect, Datawire;
Flynn, Ambassador Lead Developer, Datawire;
Richard Li, CEO and Co-founder, Datawire</p><p>Kubernetes has become the de facto runtime for container-based microservice applications, but this orchestration framework alone does not provide all of the infrastructure necessary for running a distributed system. Microservices typically communicate through Layer 7 protocols such as HTTP, gRPC, or WebSockets, and therefore having the ability to make routing decisions, manipulate protocol metadata, and observe at this layer is vital. However, traditional load balancers and edge proxies have predominantly focused on L3/4 traffic. This is where the <a href=https://www.envoyproxy.io/>Envoy Proxy</a> comes into play.</p><p>Envoy proxy was designed as a <a href=https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a>universal data plane</a> from the ground-up by the Lyft Engineering team for today's distributed, L7-centric world, with broad support for L7 protocols, a real-time API for managing its configuration, first-class observability, and high performance within a small memory footprint. However, Envoy's vast feature set and flexibility of operation also makes its configuration highly complicated -- this is evident from looking at its rich but verbose <a href=https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc>control plane</a> syntax.</p><p>With the open source <a href=https://www.getambassador.io>Ambassador API Gateway</a>, we wanted to tackle the challenge of creating a new control plane that focuses on the use case of deploying Envoy as an forward-facing edge proxy within a Kubernetes cluster, in a way that is idiomatic to Kubernetes operators. In this article, we'll walk through two major iterations of the Ambassador design, and how we integrated Ambassador with Kubernetes.</p><h2 id=ambassador-pre-2019-envoy-v1-apis-jinja-template-files-and-hot-restarts>Ambassador pre-2019: Envoy v1 APIs, Jinja Template Files, and Hot Restarts</h2><p>Ambassador itself is deployed within a container as a Kubernetes service, and uses annotations added to Kubernetes Services as its <a href=https://www.getambassador.io/reference/configuration>core configuration model</a>. This approach <a href=https://www.getambassador.io/concepts/developers>enables application developers to manage routing</a> as part of the Kubernetes service definition. We explicitly decided to go down this route because of <a href=https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d>limitations</a> in the current <a href=/docs/concepts/services-networking/ingress/>Ingress API spec</a>, and we liked the simplicity of extending Kubernetes services, rather than introducing another custom resource type. An example of an Ambassador annotation can be seen here:</p><pre><code>kind: Service
apiVersion: v1
metadata:
  name: my-service
  annotations:
    getambassador.io/config: |
      ---
        apiVersion: ambassador/v0
        kind:  Mapping
        name:  my_service_mapping
        prefix: /my-service/
        service: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
</code></pre><p>Translating this simple Ambassador annotation config into valid <a href=https://www.envoyproxy.io/docs/envoy/v1.6.0/configuration/overview/v1_overview>Envoy v1</a> config was not a trivial task. By design, Ambassador's configuration isn't based on the same conceptual model as Envoy's configuration -- we deliberately wanted to aggregate and simplify operations and config. Therefore, translating between one set of concepts to the other involves a fair amount of logic within Ambassador.</p><p>In this first iteration of Ambassador we created a Python-based service that watched the Kubernetes API for changes to Service objects. When new or updated Ambassador annotations were detected, these were translated from the Ambassador syntax into an intermediate representation (IR) which embodied our core configuration model and concepts. Next, Ambassador translated this IR into a representative Envoy configuration which was saved as a file within pods associated with the running Ambassador k8s Service. Ambassador then "hot-restarted" the Envoy process running within the Ambassador pods, which triggered the loading of the new configuration.</p><p>There were many benefits with this initial implementation. The mechanics involved were fundamentally simple, the transformation of Ambassador config into Envoy config was reliable, and the file-based hot restart integration with Envoy was dependable.</p><p>However, there were also notable challenges with this version of Ambassador. First, although the hot restart was effective for the majority of our customers' use cases, it was not very fast, and some customers (particularly those with huge application deployments) found it was limiting the frequency with which they could change their configuration. Hot restart can also drop connections, especially long-lived connections like WebSockets or gRPC streams.</p><p>More crucially, though, the first implementation of the IR allowed rapid prototyping but was primitive enough that it proved very difficult to make substantial changes. While this was a pain point from the beginning, it became a critical issue as Envoy shifted to the <a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/overview/v2_overview>Envoy v2 API</a>. It was clear that the v2 API would offer Ambassador many benefits -- as Matt Klein outlined in his blog post, "<a href=https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a>The universal data plane API</a>" -- including access to new features and a solution to the connection-drop problem noted above, but it was also clear that the existing IR implementation was not capable of making the leap.</p><h2 id=ambassador-v0-50-envoy-v2-apis-ads-testing-with-kat-and-golang>Ambassador >= v0.50: Envoy v2 APIs (ADS), Testing with KAT, and Golang</h2><p>In consultation with the <a href=http://d6e.co/slack>Ambassador community</a>, the <a href=https://www.datawire.io>Datawire</a> team undertook a redesign of the internals of Ambassador in 2018. This was driven by two key goals. First, we wanted to integrate Envoy's v2 configuration format, which would enable the support of features such as <a href=https://www.getambassador.io/user-guide/sni/>SNI</a>, <a href=https://www.getambassador.io/user-guide/rate-limiting>rate limiting</a> and <a href=https://www.getambassador.io/user-guide/auth-tutorial>gRPC authentication APIs</a>. Second, we also wanted to do much more robust semantic validation of Envoy configuration due to its increasing complexity (particularly when operating with large-scale application deployments).</p><h3 id=initial-stages>Initial stages</h3><p>We started by restructuring the Ambassador internals more along the lines of a multipass compiler. The class hierarchy was made to more closely mirror the separation of concerns between the Ambassador configuration resources, the IR, and the Envoy configuration resources. Core parts of Ambassador were also redesigned to facilitate contributions from the community outside Datawire. We decided to take this approach for several reasons. First, Envoy Proxy is a very fast moving project, and we realized that we needed an approach where a seemingly minor Envoy configuration change didn't result in days of reengineering within Ambassador. In addition, we wanted to be able to provide semantic verification of configuration.</p><p>As we started working more closely with Envoy v2, a testing challenge was quickly identified. As more and more features were being supported in Ambassador, more and more bugs appeared in Ambassador's handling of less common but completely valid combinations of features. This drove to creation of a new testing requirement that meant Ambassador's test suite needed to be reworked to automatically manage many combinations of features, rather than relying on humans to write each test individually. Moreover, we wanted the test suite to be fast in order to maximize engineering productivity.</p><p>Thus, as part of the Ambassador rearchitecture, we introduced the <a href=https://github.com/datawire/ambassador/tree/master/python/kat>Kubernetes Acceptance Test (KAT)</a> framework. KAT is an extensible test framework that:</p><ol><li>Deploys a bunch of services (along with Ambassador) to a Kubernetes cluster</li><li>Run a series of verification queries against the spun up APIs</li><li>Perform a bunch of assertions on those query results</li></ol><p>KAT is designed for performance -- it batches test setup upfront, and then runs all the queries in step 3 asynchronously with a high performance client. The traffic driver in KAT runs locally using <a href=https://www.telepresence.io>Telepresence</a>, which makes it easier to debug issues.</p><h3 id=introducing-golang-to-the-ambassador-stack>Introducing Golang to the Ambassador Stack</h3><p>With the KAT test framework in place, we quickly ran into some issues with Envoy v2 configuration and hot restart, which presented the opportunity to switch to use Envoy’s Aggregated Discovery Service (ADS) APIs instead of hot restart. This completely eliminated the requirement for restart on configuration changes, which we found could lead to dropped connection under high loads or long-lived connections.</p><p>However, we faced an interesting question as we considered the move to the ADS. The ADS is not as simple as one might expect: there are explicit ordering dependencies when sending updates to Envoy. The Envoy project has reference implementations of the ordering logic, but only in Go and Java, where Ambassador was primarily in Python. We agonized a bit, and decided that the simplest way forward was to accept the polyglot nature of our world, and do our ADS implementation in Go.</p><p>We also found, with KAT, that our testing had reached the point where Python’s performance with many network connections was a limitation, so we took advantage of Go here, as well, writing KAT’s querying and backend services primarily in Go. After all, what’s another Golang dependency when you’ve already taken the plunge?</p><p>With a new test framework, new IR generating valid Envoy v2 configuration, and the ADS, we thought we were done with the major architectural changes in Ambassador 0.50. Alas, we hit one more issue. On the Azure Kubernetes Service, Ambassador annotation changes were no longer being detected.</p><p>Working with the highly-responsive AKS engineering team, we were able to identify the issue -- namely, the Kubernetes API server in AKS is exposed through a chain of proxies, requiring clients to be updating to understand how to connect using the FQDN of the API server, which is provided through a mutating webhook in AKS. Unfortunately, support for this feature was not available in the official Kubernetes Python client, so this was the third spot where we chose to switch to Go instead of Python.</p><p>This raises the interesting question of, “why not ditch all the Python code, and just rewrite Ambassador entirely in Go?” It’s a valid question. The main concern with a rewrite is that Ambassador and Envoy operate at different conceptual levels rather than simply expressing the same concepts with different syntax. Being certain that we’ve expressed the conceptual bridges in a new language is not a trivial challenge, and not something to undertake without already having really excellent test coverage in place</p><p>At this point, we use Go to coverage very specific, well-contained functions that can be verified for correctness much more easily that we could verify a complete Golang rewrite. In the future, who knows? But for 0.50.0, this functional split let us both take advantage of Golang’s strengths, while letting us retain more confidence about all the changes already in 0.50.</p><h2 id=lessons-learned>Lessons Learned</h2><p>We've learned a lot in the process of building <a href=https://blog.getambassador.io/ambassador-0-50-ga-release-notes-sni-new-authservice-and-envoy-v2-support-3b30a4d04c81>Ambassador 0.50</a>. Some of our key takeaways:</p><ul><li>Kubernetes and Envoy are very powerful frameworks, but they are also extremely fast moving targets -- there is sometimes no substitute for reading the source code and talking to the maintainers (who are fortunately all quite accessible!)</li><li>The best supported libraries in the Kubernetes / Envoy ecosystem are written in Go. While we love Python, we have had to adopt Go so that we're not forced to maintain too many components ourselves.</li><li>Redesigning a test harness is sometimes necessary to move your software forward.</li><li>The real cost in redesigning a test harness is often in porting your old tests to the new harness implementation.</li><li>Designing (and implementing) an effective control plane for the edge proxy use case has been challenging, and the feedback from the open source community around Kubernetes, Envoy and Ambassador has been extremely useful.</li></ul><p>Migrating Ambassador to the Envoy v2 configuration and ADS APIs was a long and difficult journey that required lots of architecture and design discussions and plenty of coding, but early feedback from results have been positive. <a href=https://blog.getambassador.io/announcing-ambassador-0-50-8dffab5b05e0>Ambassador 0.50 is available now</a>, so you can take it for a test run and share your feedback with the community on our <a href=http://d6e.co/slack>Slack channel</a> or on <a href=https://www.twitter.com/getambassadorio>Twitter</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-83e0e3e4554a758f903a88d1bfb65118>Runc and CVE-2019-5736</h1><div class="td-byline mb-4"><time datetime=2019-02-11 class=text-muted>Monday, February 11, 2019</time></div><p>This morning <a href=https://www.openwall.com/lists/oss-security/2019/02/11/2>a container escape vulnerability in runc was announced</a>. We wanted to provide some guidance to Kubernetes users to ensure everyone is safe and secure.</p><h2 id=what-is-runc>What Is Runc?</h2><p>Very briefly, runc is the low-level tool which does the heavy lifting of spawning a Linux container. Other tools like Docker, Containerd, and CRI-O sit on top of runc to deal with things like data formatting and serialization, but runc is at the heart of all of these systems.</p><p>Kubernetes in turn sits on top of those tools, and so while no part of Kubernetes itself is vulnerable, most Kubernetes installations are using runc under the hood.</p><h3 id=what-is-the-vulnerability>What Is The Vulnerability?</h3><p>While full details are still embargoed to give people time to patch, the rough version is that when running a process as root (UID 0) inside a container, that process can exploit a bug in runc to gain root privileges on the host running the container. This then allows them unlimited access to the server as well as any other containers on that server.</p><p>If the process inside the container is either trusted (something you know is not hostile) or is not running as UID 0, then the vulnerability does not apply. It can also be prevented by SELinux, if an appropriate policy has been applied. RedHat Enterprise Linux and CentOS both include appropriate SELinux permissions with their packages and so are believed to be unaffected if SELinux is enabled.</p><p>The most common source of risk is attacker-controller container images, such as unvetted images from public repositories.</p><h3 id=what-should-i-do>What Should I Do?</h3><p>As with all security issues, the two main options are to mitigate the vulnerability or upgrade your version of runc to one that includes the fix.</p><p>As the exploit requires UID 0 within the container, a direct mitigation is to ensure all your containers are running as a non-0 user. This can be set within the container image, or via your pod specification:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>run-as-uid-1000<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>runAsUser</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></code></pre></div><p>This can also be enforced globally using a PodSecurityPolicy:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>policy/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PodSecurityPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>non-root<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>privileged</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>allowPrivilegeEscalation</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runAsUser</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Require the container to run without root privileges.</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>rule</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;MustRunAsNonRoot&#39;</span><span style=color:#bbb>
</span></code></pre></div><p>Setting a policy like this is highly encouraged given the overall risks of running as UID 0 inside a container.</p><p>Another potential mitigation is to ensure all your container images are vetted and trusted. This can be accomplished by building all your images yourself, or by vetting the contents of an image and then pinning to the image version hash (<code>image: external/someimage@sha256:7832659873hacdef</code>).</p><p>Upgrading runc can generally be accomplished by upgrading the package <code>runc</code> for your distribution or by upgrading your OS image if using immutable images. This is a list of known safe versions for various distributions and platforms:</p><ul><li>Ubuntu - <a href=https://people.canonical.com/~ubuntu-security/cve/2019/CVE-2019-5736.html><code>runc 1.0.0~rc4+dfsg1-6ubuntu0.18.10.1</code></a></li><li>Debian - <a href=https://security-tracker.debian.org/tracker/CVE-2019-5736><code>runc 1.0.0~rc6+dfsg1-2</code></a></li><li>RedHat Enterprise Linux - <a href=https://access.redhat.com/security/vulnerabilities/runcescape><code>docker 1.13.1-91.git07f3374.el7</code></a> (if SELinux is disabled)</li><li>Amazon Linux - <a href=https://alas.aws.amazon.com/ALAS-2019-1156.html><code>docker 18.06.1ce-7.25.amzn1.x86_64</code></a></li><li>CoreOS - Stable: <a href=https://coreos.com/releases/#1967.5.0><code>1967.5.0</code></a> / Beta: <a href=https://coreos.com/releases/#2023.2.0><code>2023.2.0</code></a> / Alpha: <a href=https://coreos.com/releases/#2051.0.0><code>2051.0.0</code></a></li><li>Kops Debian - <a href=https://github.com/kubernetes/kops/pull/6460>in progress</a> (see <a href=https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md>advisory</a> for how to address until Kops Debian is patched)</li><li>Docker - <a href=https://github.com/docker/docker-ce/releases/tag/v18.09.2><code>18.09.2</code></a></li></ul><p>Some platforms have also posted more specific instructions:</p><h4 id=google-container-engine-gke>Google Container Engine (GKE)</h4><p>Google has issued a <a href=https://cloud.google.com/kubernetes-engine/docs/security-bulletins#february-11-2019-runc>security bulletin</a> with more detailed information but in short, if you are using the default GKE node image then you are safe. If you are using an Ubuntu node image then you will need to mitigate or upgrade to an image with a fixed version of runc.</p><h4 id=amazon-elastic-container-service-for-kubernetes-eks>Amazon Elastic Container Service for Kubernetes (EKS)</h4><p>Amazon has also issued a <a href=https://aws.amazon.com/security/security-bulletins/AWS-2019-002/>security bulletin</a> with more detailed information. All EKS users should mitigate the issue or upgrade to a new node image.</p><h4 id=azure-kubernetes-service-aks>Azure Kubernetes Service (AKS)</h4><p>Microsoft has issued a <a href=https://azure.microsoft.com/en-us/updates/cve-2019-5736-and-runc-vulnerability/>security bulletin</a> with detailed information on mitigating the issue. Microsoft recommends all AKS users to upgrade their cluster to mitigate the issue.</p><h4 id=kops>Kops</h4><p>Kops has issued an <a href=https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md>advisory</a> with detailed information on mitigating this issue.</p><h3 id=docker>Docker</h3><p>We don't have specific confirmation that Docker for Mac and Docker for Windows are vulnerable, however it seems likely. Docker has released a fix in <a href=https://github.com/docker/docker-ce/releases/tag/v18.09.2>version 18.09.2</a> and it is recommended you upgrade to it. This also applies to other deploy systems using Docker under the hood.</p><p>If you are unable to upgrade Docker, the Rancher team has provided backports of the fix for many older versions at <a href=https://github.com/rancher/runc-cve>github.com/rancher/runc-cve</a>.</p><h2 id=getting-more-information>Getting More Information</h2><p>If you have any further questions about how this vulnerability impacts Kubernetes, please join us at <a href=https://discuss.kubernetes.io/>discuss.kubernetes.io</a>.</p><p>If you would like to get in contact with the <a href=https://github.com/opencontainers/org/blob/master/README.md#communications>runc team</a>, you can reach them on <a href=https://groups.google.com/a/opencontainers.org/forum/#!forum/dev>Google Groups</a> or <code>#opencontainers</code> on Freenode IRC.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7d9b9c3a596900b88b644a72f10d2d3>Poseidon-Firmament Scheduler – Flow Network Graph Based Scheduler</h1><div class="td-byline mb-4"><time datetime=2019-02-06 class=text-muted>Wednesday, February 06, 2019</time></div><p><strong>Authors:</strong> Deepak Vij (Huawei), Shivram Shrivastava (Huawei)</p><h2 id=introduction>Introduction</h2><p>Cluster Management systems such as Mesos, Google Borg, Kubernetes etc. in a cloud scale datacenter environment (also termed as <em><strong>Datacenter-as-a-Computer</strong></em> or <em><strong>Warehouse-Scale Computing - WSC</strong></em>) typically manage application workloads by performing tasks such as tracking machine live-ness, starting, monitoring, terminating workloads and more importantly using a <strong>Cluster Scheduler</strong> to decide on workload placements.</p><p>A <strong>Cluster Scheduler</strong> essentially performs the scheduling of workloads to compute resources – combining the global placement of work across the WSC environment makes the “warehouse-scale computer” more efficient, increases utilization, and saves energy. <strong>Cluster Scheduler</strong> examples are Google Borg, Kubernetes, Firmament, Mesos, Tarcil, Quasar, Quincy, Swarm, YARN, Nomad, Sparrow, Apollo etc.</p><p>In this blog post, we briefly describe the novel Firmament flow network graph based scheduling approach (<a href=https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gog>OSDI paper</a>) in Kubernetes. We specifically describe the Firmament Scheduler and how it integrates with the Kubernetes cluster manager using Poseidon as the integration glue. We have seen extremely impressive scheduling throughput performance benchmarking numbers with this novel scheduling approach. Originally, Firmament Scheduler was conceptualized, designed and implemented by University of Cambridge researchers, <a href=http://www.malteschwarzkopf.de/>Malte Schwarzkopf</a> & <a href=http://ionelgog.org/>Ionel Gog</a>.</p><h2 id=poseidon-firmament-scheduler-how-it-works>Poseidon-Firmament Scheduler – How It Works</h2><p>At a very high level, <a href=/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/>Poseidon-Firmament scheduler</a> augments the current Kubernetes scheduling capabilities by incorporating novel flow network graph based scheduling capabilities alongside the default Kubernetes Scheduler. It models the scheduling problem as a constraint-based optimization over a flow network graph – by reducing scheduling to a min-cost max-flow optimization problem. Due to the inherent rescheduling capabilities, the new scheduler enables a globally optimal scheduling environment that constantly keeps refining the workloads placements dynamically.</p><h2 id=key-advantages>Key Advantages</h2><p>Flow graph scheduling based <a href=/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/>Poseidon-Firmament scheduler</a> provides the following key advantages:</p><ul><li><p>Workloads (pods) are bulk scheduled to enable scheduling decisions at massive scale.</p></li><li><p>Based on the extensive performance test results, Poseidon-Firmament scales much better than Kubernetes default scheduler as the number of nodes increase in a cluster. This is due to the fact that Poseidon-Firmament is able to amortize more and more work across workloads.</p></li><li><p>Poseidon-Firmament Scheduler outperforms the Kubernetes default scheduler by a wide margin when it comes to throughput performance numbers for scenarios where compute resource requirements are somewhat uniform across jobs (Replicasets/Deployments/Jobs). Poseidon-Firmament scheduler end-to-end throughput performance numbers, including bind time, consistently get better as the number of nodes in a cluster increase. For example, for a 2,700 node cluster (shown in the graphs <a href=https://github.com/kubernetes-sigs/poseidon/blob/master/docs/benchmark/README.md>here</a>), Poseidon-Firmament scheduler achieves a 7X or greater end-to-end throughput than the Kubernetes default scheduler, which includes bind time.</p></li><li><p>Availability of complex rule constraints.</p></li><li><p>Scheduling in Poseidon-Firmament is very dynamic; it keeps cluster resources in a global optimal state during every scheduling run.</p></li><li><p>Highly efficient resource utilizations.</p></li></ul><h2 id=firmament-flow-network-graph-an-overview>Firmament Flow Network Graph – An Overview</h2><p>Firmament scheduler runs a min-cost flow algorithm over the flow network to find an optimal flow, from which it extracts the implied workload (pod placements). A flow network is a directed graph whose arcs carry flow from source nodes (i.e. pod nodes) to a sink node. A cost and capacity associated with each arc constrain the flow, and specify preferential routes for it.</p><p>Figure 1 below shows an example of a flow network for a cluster with two tasks (workloads or pods) and four machines (nodes) – each workload on the left hand side, is a source of one unit of flow. All such flow must be drained into the sink node (S) for a feasible solution to the optimization problem.</p><figure><img src=/images/blog/2019-02-03-poseidon-firmament-scheduler/example-of-a-flow-network.png alt="Figure 1. Example of a Flow Network" width=600><figcaption><p>Figure 1. Example of a Flow Network</p></figcaption></figure><h2 id=poseidon-mediation-layer-an-overview>Poseidon Mediation Layer – An Overview</h2><p>Poseidon is a service that acts as the integration glue for the Firmament scheduler with Kubernetes. It augments the current Kubernetes scheduling capabilities by incorporating new flow network graph based Firmament scheduling capabilities alongside the default Kubernetes Scheduler; multiple schedulers running simultaneously. Figure 2 below describes the high level overall design as far as how Poseidon integration glue works in conjunction with the underlying Firmament flow network graph based scheduler.</p><figure><img src=/images/blog/2019-02-03-poseidon-firmament-scheduler/firmament-kubernetes-integration-overview.png alt="Figure 2. Firmament Kubernetes Integration Overview" width=600><figcaption><p>Figure 2. Firmament Kubernetes Integration Overview</p></figcaption></figure><p>As part of the Kubernetes multiple schedulers support, each new pod is typically scheduled by the default scheduler, but Kubernetes can be instructed to use another scheduler by specifying the name of another custom scheduler (in our case, <a href=/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/>Poseidon-Firmament</a>) at the time of pod deployment. In this case, the default scheduler will ignore that Pod and allow Poseidon scheduler to schedule the Pod to a relevant node.</p><blockquote class="note callout"><div><strong>Note:</strong> For details about the design of this project see the <a href=https://github.com/kubernetes-sigs/poseidon/blob/master/docs/design/README.md>design document</a>.</div></blockquote><h2 id=possible-use-case-scenarios-when-to-use-it>Possible Use Case Scenarios – When To Use It</h2><p><a href=/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/>Poseidon-Firmament scheduler</a> enables extremely high throughput scheduling environment at scale due to its bulk scheduling approach superiority versus K8s pod-at-a-time approach. In our extensive tests, we have observed substantial throughput benefits as long as resource requirements (CPU/Memory) for incoming Pods is uniform across jobs (Replicasets/Deployments/Jobs), mainly due to efficient amortization of work across jobs.</p><p>Although, <a href=/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/>Poseidon-Firmament scheduler</a> is capable of scheduling various types of workloads (service, batch, etc.), following are the few use cases where it excels the most:</p><ol><li><p>For “Big Data/AI” jobs consisting of a large number of tasks, throughput benefits are tremendous.</p></li><li><p>Substantial throughput benefits also for service or batch job scenarios where workload resource requirements are uniform across jobs (Replicasets/Deplyments/Jobs).</p></li></ol><h2 id=current-project-stage>Current Project Stage</h2><p>Currently Poseidon-Firmament project is an incubation project. Alpha Release is available at <a href=https://github.com/kubernetes-sigs/poseidon>https://github.com/kubernetes-sigs/poseidon</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cbbb564a6c5a16bbf59a355b79dfee03>Update on Volume Snapshot Alpha for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2019-01-17 class=text-muted>Thursday, January 17, 2019</time></div><p><strong>Authors:</strong> Jing Xu (Google), Xing Yang (Huawei), Saad Ali (Google)</p><p>Volume snapshotting support was introduced in Kubernetes v1.12 as an alpha feature. In Kubernetes v1.13, it remains an alpha feature, but a few enhancements were added and some breaking changes were made. This post summarizes the changes.</p><h2 id=breaking-changes>Breaking Changes</h2><p><a href=https://github.com/container-storage-interface/spec/releases/tag/v1.0.0>CSI spec v1.0</a> introduced a few breaking changes to the volume snapshot feature. CSI driver maintainers should be aware of these changes as they upgrade their drivers to support v1.0.</p><h2 id=snapshotstatus-replaced-with-boolean-readytouse>SnapshotStatus replaced with Boolean ReadyToUse</h2><p>CSI v0.3.0, defined a <code>SnapshotStatus</code> enum in <code>CreateSnapshotResponse</code> which indicates whether the snapshot is <code>READY</code>, <code>UPLOADING</code>, or <code>ERROR_UPLOADING</code>. In CSI v1.0, <code>SnapshotStatus</code> has been removed from <code>CreateSnapshotResponse</code> and replaced with a <code>boolean ReadyToUse</code>. A <code>ReadyToUse</code> value of <code>true</code> indicates that post snapshot processing (such as uploading) is complete and the snapshot is ready to be used as a source to create a volume.</p><p>Storage systems that need to do post snapshot processing (such as uploading after the snapshot is cut) should return a successful <code>CreateSnapshotResponse</code> with the <code>ReadyToUse</code> field set to <code>false</code> as soon as the snapshot has been taken. This indicates that the Container Orchestration System (CO) can resume any workload that was quiesced for the snapshot to be taken. The CO can then repeatedly call <code>CreateSnapshot</code> until the <code>ReadyToUse</code> field is set to <code>true</code> or the call returns an error indicating a problem in processing. The CSI <code>ListSnapshot</code> call could be used along with <code>snapshot_id</code> filtering to determine if the snapshot is ready to use, but is not recommended because it provides no way to detect errors during processing (the <code>ReadyToUse</code> field simply remains <code>false</code> indefinitely).</p><p>The <a href=https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1>v1.x.x releases</a> of the CSI external-snapshotter sidecar container already handle this change by calling <code>CreateSnapshot</code> instead of <code>ListSnapshots</code> to check if a snapshot is ready to use. When upgrading their drivers to CSI 1.0, driver maintainers should use the appropriate 1.0 compatible sidecar container.</p><p>To be consistent with the change in the CSI spec, the <code>Ready</code> field in the <code>VolumeSnapshot</code> API object has been renamed to <code>ReadyToUse</code>. This change is visible to the user when running <code>kubectl describe volumesnapshot</code> to view the details of a snapshot.</p><h2 id=timestamp-data-type>Timestamp Data Type</h2><p>The creation time of a snapshot is available to Kubernetes admins as part of the <code>VolumeSnapshotContent</code> API object. This field is populated using the <code>creation_time</code> field in the CSI <code>CreateSnapshotResponse</code>. In CSI v1.0, this <code>creation_time</code> field type was changed to <a href=https://godoc.org/github.com/golang/protobuf/ptypes/timestamp><code>.google.protobuf.Timestamp</code></a> instead of <code>int64</code>. When upgrading drivers to CSI 1.0, driver maintainers must make changes accordingly. The <a href=https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1>v1.x.x releases</a> of the CSI external-snapshotter sidecar container has been updated to handle this change.</p><h2 id=deprecations>Deprecations</h2><p>The following <code>VolumeSnapshotClass</code> parameters are deprecated and will be removed in a future release. They will be replaced with parameters listed in the <code>Replacement</code> section below.</p><p>Deprecated
Replacement
csiSnapshotterSecretName
csi.storage.k8s.io/snapshotter-secret-name
csiSnapshotterSecretNameSpace
csi.storage.k8s.io/snapshotter-secret-namespace</p><h2 id=new-features>New Features</h2><h3 id=snapshotcontent-deletion-retain-policy>SnapshotContent Deletion/Retain Policy</h3><p>As described in the <a href=https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/>initial blog post announcing the snapshot alpha</a>, the Kubernetes snapshot APIs are similar to the PV/PVC APIs: just like a volume is represented by a bound PVC and PV pair, a snapshot is represented by a bound <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> pair.</p><p>With PV/PVC pairs, when a user is done with a volume, they can delete the PVC. And the reclaim policy on the PV determines what happens to the PV (whether it is also deleted or retained).</p><p>In the initial alpha release, snapshots did not support the ability to specify a reclaim policy. Instead when a snapshot object was deleted it always resulted in the snapshot being deleted. In Kubernetes v1.13, a snapshot content <code>DeletionPolicy</code> was added. It enables an admin to configure what what happens to a <code>VolumeSnapshotContent</code> after the <code>VolumeSnapshot</code> object it is bound to is deleted. The <code>DeletionPolicy</code> of a volume snapshot can either be <code>Retain</code> or <code>Delete</code>. If the value is not specified, the default depends on whether the <code>SnapshotContent</code> object was created via static binding or dynamic provisioning.</p><h3 id=retain>Retain</h3><p>The <code>Retain</code> policy allows for manual reclamation of the resource. If a <code>VolumeSnapshotContent</code> is statically created and bound, the default <code>DeletionPolicy</code> is <code>Retain</code>. When the <code>VolumeSnapshot</code> is deleted, the <code>VolumeSnapshotContent</code> continues to exist and the <code>VolumeSnapshotContent</code> is considered “released”. But it is not available for binding to other <code>VolumeSnapshot</code> objects because it contains data. It is up to an administrator to decide how to handle the remaining API object and resource cleanup.</p><h3 id=delete>Delete</h3><p>A <code>Delete</code> policy enables automatic deletion of the bound <code>VolumeSnapshotContent</code> object from Kubernetes and the associated storage asset in the external infrastructure (such as an AWS EBS snapshot or GCE PD snapshot, etc.). Snapshots that are dynamically provisioned inherit the deletion policy of their <a href=/docs/concepts/storage/volume-snapshot-classes/><code>VolumeSnapshotClass</code></a>, which defaults to <code>Delete</code>. The administrator should configure the <code>VolumeSnapshotClass</code> with the desired retention policy. The policy may be changed for individual <code>VolumeSnapshotContent</code> after it is created by patching the object.</p><p>The following example demonstrates how to check the deletion policy of a dynamically provisioned <code>VolumeSnapshotContent</code>.</p><pre><code>$ kubectl create -f ./examples/kubernetes/demo-defaultsnapshotclass.yaml
$ kubectl create -f ./examples/kubernetes/demo-snapshot.yaml
$ kubectl get volumesnapshots demo-snapshot-podpvc -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  creationTimestamp: &quot;2018-11-27T23:57:09Z&quot;
...
spec:
  snapshotClassName: default-snapshot-class
  snapshotContentName: snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002
  source:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: podpvc
status:
…
$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
…
spec:
  csiVolumeSnapshotSource:
    creationTime: 1546469777852000000
    driver: pd.csi.storage.gke.io
    restoreSize: 6442450944
    snapshotHandle: projects/jing-k8s-dev/global/snapshots/snapshot-26cd0db3-f2a0-11e8-8be6-42010a800002
  deletionPolicy: Delete
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
    resourceVersion: &quot;21117&quot;
    uid: ae400e9f-f28b-11e8-8be6-42010a800002
  snapshotClassName: default-snapshot-class
  volumeSnapshotRef:
    apiVersion: snapshot.storage.k8s.io/v1alpha1
    kind: VolumeSnapshot
    name: demo-snapshot-podpvc
    namespace: default
    resourceVersion: &quot;6948065&quot;
    uid: 26cd0db3-f2a0-11e8-8be6-42010a800002
</code></pre><p>User can change the deletion policy by using patch:</p><pre><code>$ kubectl patch volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -p '{&quot;spec&quot;:{&quot;deletionPolicy&quot;:&quot;Retain&quot;}}' --type=merge

$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
...
spec:
  csiVolumeSnapshotSource:
...
  deletionPolicy: Retain
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
...
</code></pre><h2 id=snapshot-object-in-use-protection>Snapshot Object in Use Protection</h2><p>The purpose of the Snapshot Object in Use Protection feature is to ensure that in-use snapshot API objects are not removed from the system (as this may result in data loss). There are two cases that require “in-use” protection:</p><ol><li>If a volume snapshot is in active use by a persistent volume claim as a source to create a volume.</li><li>If a <code>VolumeSnapshotContent</code> API object is bound to a VolumeSnapshot API object, the content object is considered in use.</li></ol><p>If a user deletes a <code>VolumeSnapshot</code> API object in active use by a PVC, the <code>VolumeSnapshot</code> object is not removed immediately. Instead, removal of the <code>VolumeSnapshot</code> object is postponed until the <code>VolumeSnapshot</code> is no longer actively used by any PVCs. Similarly, if an admin deletes a <code>VolumeSnapshotContent</code> that is bound to a <code>VolumeSnapshot</code>, the <code>VolumeSnapshotContent</code> is not removed immediately. Instead, the <code>VolumeSnapshotContent</code> removal is postponed until the <code>VolumeSnapshotContent</code> is not bound to the <code>VolumeSnapshot</code> object.</p><h2 id=which-volume-plugins-support-kubernetes-snapshots>Which volume plugins support Kubernetes Snapshots?</h2><p>Snapshots are only supported for CSI drivers (not for in-tree or FlexVolume). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.</p><p>As of the publishing of this blog post, the following CSI drivers support snapshots:</p><ul><li><a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>GCE Persistent Disk CSI Driver</a></li><li><a href=https://github.com/opensds/nbp/tree/master/csi/server>OpenSDS CSI Driver</a></li><li><a href=https://github.com/ceph/ceph-csi/tree/master/pkg/rbd>Ceph RBD CSI Driver</a></li><li><a href=https://github.com/libopenstorage/openstorage/tree/master/csi>Portworx CSI Driver</a></li><li><a href=https://github.com/gluster/gluster-csi-driver>GlusterFS CSI Driver</a></li><li><a href=https://github.com/digitalocean/csi-digitalocean>DigitalOcean CSI Driver</a></li><li><a href=https://github.com/embercsi/ember-csi>Ember CSI Driver</a></li><li><a href=https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder>Cinder CSI Driver</a></li><li><a href=https://github.com/Datera/datera-csi>Datera CSI Driver</a></li><li><a href=https://github.com/Nexenta/nexentastor-csi-driver>NexentaStor CSI Driver</a></li></ul><p>Snapshot support for other <a href=https://kubernetes-csi.github.io/docs/drivers.html>drivers</a> is pending, and should be available soon. Read the “Container Storage Interface (CSI) for Kubernetes GA” blog post to learn more about CSI and how to deploy CSI drivers.</p><h2 id=what-s-next>What’s next?</h2><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to beta in either 1.15 or 1.16. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, and more.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>The code repository for snapshot APIs and controller is here: <a href=https://github.com/kubernetes-csi/external-snapshotter>https://github.com/kubernetes-csi/external-snapshotter</a></p><p>Check out additional documentation on the snapshot feature here: <a href=http://k8s.io/docs/concepts/storage/volume-snapshots>http://k8s.io/docs/concepts/storage/volume-snapshots</a> and <a href=https://kubernetes-csi.github.io/docs/>https://kubernetes-csi.github.io/docs/</a></p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>Special thanks to all the contributors that helped add CSI v1.0 support and improve the snapshot feature in this release, including Saad Ali (<a href=https://github.com/saadali>saadali</a>), Michelle Au (<a href=https://github.com/msau42>msau42</a>), Deep Debroy (<a href=https://github.com/ddebroy>ddebroy</a>), James DeFelice (<a href=https://github.com/jdef>jdef</a>), John Griffith (<a href=https://github.com/j-griffith>j-griffith</a>), Julian Hjortshoj (<a href=https://github.com/julian-hj>julian-hj</a>), Tim Hockin (<a href=https://github.com/thockin>thockin</a>), Patrick Ohly (<a href=https://github.com/pohly>pohly</a>), Luis Pabon (<a href=https://github.com/lpabon>lpabon</a>), Cheng Xing (<a href=https://github.com/verult>verult</a>), Jing Xu (<a href=https://github.com/jingxu97>jingxu97</a>), Shiwei Xu (<a href=https://github.com/wackxu>wackxu</a>), Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>), Jie Yu (<a href=https://github.com/jieyu>jieyu</a>), David Zhu (<a href=https://github.com/davidz627>davidz627</a>).</p><p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p>We also hold regular <a href="https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing">SIG-Storage Snapshot Working Group meetings</a>. New attendees are welcome to join for design and development discussions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a9a9fec669d1f677c64e58cb5feb3b43>Container Storage Interface (CSI) for Kubernetes GA</h1><div class="td-byline mb-4"><time datetime=2019-01-15 class=text-muted>Tuesday, January 15, 2019</time></div><p><img src=/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-kubernetes.png alt="Kubernetes Logo">
<img src=/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-logo.png alt="CSI Logo"></p><p><strong>Author:</strong> Saad Ali, Senior Software Engineer, Google</p><p>The Kubernetes implementation of the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>Container Storage Interface</a> (CSI) has been promoted to GA in the Kubernetes v1.13 release. Support for CSI was <a href=http://blog.kubernetes.io/2018/01/introducing-container-storage-interface.html>introduced as alpha</a> in Kubernetes v1.9 release, and <a href=https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/>promoted to beta</a> in the Kubernetes v1.10 release.</p><p>The GA milestone indicates that Kubernetes users may depend on the feature and its API without fear of backwards incompatible changes in future causing regressions. GA features are protected by the <a href=/docs/reference/using-api/deprecation-policy/>Kubernetes deprecation policy</a>.</p><h2 id=why-csi>Why CSI?</h2><p>Although prior to CSI Kubernetes provided a powerful volume plugin system, it was challenging to add support for new volume plugins to Kubernetes: volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain.</p><p>CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable.</p><h2 id=what-s-new>What’s new?</h2><p>With the promotion to GA, the Kubernetes implementation of CSI introduces the following changes:</p><ul><li>Kubernetes is now compatible with CSI spec <a href=https://github.com/container-storage-interface/spec/releases/tag/v1.0.0>v1.0</a> and <a href=https://github.com/container-storage-interface/spec/releases/tag/v0.3.0>v0.3</a> (instead of CSI spec <a href=https://github.com/container-storage-interface/spec/releases/tag/v0.2.0>v0.2</a>).<ul><li>There were breaking changes between CSI spec v0.3.0 and v1.0.0, but Kubernetes v1.13 supports both versions so either version will work with Kubernetes v1.13.</li><li>Please note that with the release of the CSI 1.0 API, support for CSI drivers using 0.3 and older releases of the CSI API is deprecated, and is planned to be removed in Kubernetes v1.15.</li><li>There were no breaking changes between CSI spec v0.2 and v0.3, so v0.2 drivers should also work with Kubernetes v1.10.0+.</li><li>There were breaking changes between the CSI spec v0.1 and v0.2, so very old drivers implementing CSI 0.1 must be updated to be at least 0.2 compatible before use with Kubernetes v1.10.0+.</li></ul></li><li>The Kubernetes <code>VolumeAttachment</code> object (introduced in v1.9 in the storage v1alpha1 group, and added to the v1beta1 group in v1.10) has been added to the storage v1 group in v1.13.</li><li>The Kubernetes <code>CSIPersistentVolumeSource</code> volume type has been promoted to GA.</li><li>The <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration>Kubelet device plugin registration mechanism</a>, which is the means by which kubelet discovers new CSI drivers, has been promoted to GA in Kubernetes v1.13.</li></ul><h2 id=how-to-deploy-a-csi-driver>How to deploy a CSI driver?</h2><p>Kubernetes users interested in how to deploy or manage an existing CSI driver on Kubernetes should look at the documentation provided by the author of the CSI driver.</p><h2 id=how-to-use-a-csi-volume>How to use a CSI volume?</h2><p>Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects: <code>PersistentVolumeClaims</code>, <code>PersistentVolumes</code>, and <code>StorageClasses</code>. Documented <a href=/docs/concepts/storage/volumes/#csi>here</a>.</p><p>Although the Kubernetes implementation of CSI is a GA feature in Kubernetes v1.13, it may require the following flag:</p><ul><li>API server binary and kubelet binaries:<ul><li><code>--allow-privileged=true</code><ul><li>Most CSI plugins will require bidirectional mount propagation, which can only be enabled for privileged pods. Privileged pods are only permitted on clusters where this flag has been set to true (this is the default in some environments like GCE, GKE, and kubeadm).</li></ul></li></ul></li></ul><h3 id=dynamic-provisioning>Dynamic Provisioning</h3><p>You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a <code>StorageClass</code> pointing to the CSI plugin.</p><p>The following StorageClass, for example, enables dynamic creation of “<code>fast-storage</code>” volumes by a CSI volume plugin called “<code>csi-driver.example.com</code>”.</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-storage
provisioner: csi-driver.example.com
parameters:
  type: pd-ssd
  csi.storage.k8s.io/provisioner-secret-name: mysecret
  csi.storage.k8s.io/provisioner-secret-namespace: mynamespace
</code></pre><p>New for GA, the <a href=https://github.com/kubernetes-csi/external-provisioner>CSI external-provisioner</a> (v1.0.1+) reserves the parameter keys prefixed with <code>csi.storage.k8s.io/</code>. If the keys do not correspond to a set of known keys the values are simply ignored (and not passed to the CSI driver). The older secret parameter keys (<code>csiProvisionerSecretName</code>, <code>csiProvisionerSecretNamespace</code>, etc.) are also supported by CSI external-provisioner v1.0.1 but are deprecated and may be removed in future releases of the CSI external-provisioner.</p><p>Dynamic provisioning is triggered by the creation of a <code>PersistentVolumeClaim</code> object. The following <code>PersistentVolumeClaim</code>, for example, triggers dynamic provisioning using the <code>StorageClass</code> above.</p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-request-for-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
</code></pre><p>When volume provisioning is invoked, the parameter type: <code>pd-ssd</code> and the secret any referenced secret(s) are passed to the CSI plugin <code>csi-driver.example.com</code> via a <code>CreateVolume</code> call. In response, the external volume plugin provisions a new volume and then automatically create a <code>PersistentVolume</code> object to represent the new volume. Kubernetes then binds the new <code>PersistentVolume</code> object to the <code>PersistentVolumeClaim</code>, making it ready to use.</p><p>If the <code>fast-storage StorageClass</code> is marked as “default”, there is no need to include the <code>storageClassName</code> in the <code>PersistentVolumeClaim</code>, it will be used by default.</p><h3 id=pre-provisioned-volumes>Pre-Provisioned Volumes</h3><p>You can always expose a pre-existing volume in Kubernetes by manually creating a PersistentVolume object to represent the existing volume. The following <code>PersistentVolume</code>, for example, exposes a volume with the name “<code>existingVolumeName</code>” belonging to a CSI storage plugin called “<code>csi-driver.example.com</code>”.</p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manually-created-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: csi-driver.example.com
    volumeHandle: existingVolumeName
    readOnly: false
    fsType: ext4
    volumeAttributes:
      foo: bar
    controllerPublishSecretRef:
      name: mysecret1
      namespace: mynamespace
    nodeStageSecretRef:
      name: mysecret2
      namespace: mynamespace
    nodePublishSecretRef
      name: mysecret3
      namespace: mynamespace
</code></pre><h3 id=attaching-and-mounting>Attaching and Mounting</h3><p>You can reference a <code>PersistentVolumeClaim</code> that is bound to a CSI volume in any pod or pod template.</p><pre><code>kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
      - mountPath: &quot;/var/www/html&quot;
        name: my-csi-volume
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: my-request-for-storage
</code></pre><p>When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (<code>ControllerPublishVolume</code>, <code>NodeStageVolume</code>, <code>NodePublishVolume</code>, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.</p><p>For more details please see the CSI implementation <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md>design doc</a> and <a href=/docs/concepts/storage/volumes/#csi>documentation</a>.</p><h2 id=how-to-write-a-csi-driver>How to write a CSI Driver?</h2><p>The <a href=https://kubernetes-csi.github.io/>kubernetes-csi</a> site details how to develop, deploy, and test a CSI driver on Kubernetes. In general, CSI Drivers should be deployed on Kubernetes along with the following sidecar (helper) containers:</p><ul><li><a href=https://github.com/kubernetes-csi/external-attacher>external-attacher</a><ul><li>Watches Kubernetes <code>VolumeAttachment</code> objects and triggers <code>ControllerPublish</code> and <code>ControllerUnpublish</code> operations against a CSI endpoint.</li></ul></li><li><a href=https://github.com/kubernetes-csi/external-provisioner>external-provisioner</a><ul><li>Watches Kubernetes <code>PersistentVolumeClaim</code> objects and triggers <code>CreateVolume</code> and <code>DeleteVolume</code> operations against a CSI endpoint.</li></ul></li><li><a href=https://github.com/kubernetes-csi/node-driver-registrar>node-driver-registrar</a><ul><li>Registers the CSI driver with kubelet using the <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration>Kubelet device plugin mechanism</a>.</li></ul></li><li><a href=https://github.com/kubernetes-csi/cluster-driver-registrar>cluster-driver-registrar</a> (Alpha)<ul><li>Registers a CSI Driver with the Kubernetes cluster by creating a <code>CSIDriver</code> object which enables the driver to customize how Kubernetes interacts with it.</li></ul></li><li><a href=https://github.com/kubernetes-csi/external-snapshotter>external-snapshotter</a> (Alpha)<ul><li>Watches Kubernetes <code>VolumeSnapshot</code> CRD objects and triggers <code>CreateSnapshot</code> and <code>DeleteSnapshot</code> operations against a CSI endpoint.</li></ul></li><li><a href=https://github.com/kubernetes-csi/livenessprobe>livenessprobe</a><ul><li>May be included in a CSI plugin pod to enable the <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/>Kubernetes Liveness Probe</a> mechanism.</li></ul></li></ul><p>Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.</p><h2 id=list-of-csi-drivers>List of CSI Drivers</h2><p>CSI drivers are developed and maintained by third parties. You can find a non-definitive list of CSI drivers <a href=https://kubernetes-csi.github.io/docs/drivers.html>here</a>.</p><h2 id=what-about-in-tree-volume-plugins>What about in-tree volume plugins?</h2><p>There is a plan to migrate most of the persistent, remote in-tree volume plugins to CSI. For more details see <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md>design doc</a>.</p><h2 id=limitations-of-ga>Limitations of GA</h2><p>The GA implementation of CSI has the following limitations:</p><ul><li>Ephemeral local volumes must create a PVC (pod inline referencing of CSI volumes is not supported).</li></ul><h2 id=what-s-next>What’s next?</h2><ul><li>Work on moving Kubernetes CSI features that are still alpha to beta:<ul><li>Raw block volumes</li><li>Topology awareness (the ability for Kubernetes to understand and influence where a CSI volume is provisioned (zone, regions, etc.).</li><li>Features depending on CSI CRDs (e.g. “Skip attach” and “Pod info on mount”).</li><li>Volume Snapshots</li></ul></li><li>Work on completing support for local ephemeral volumes.</li><li>Work on migrating remote persistent in-tree volume plugins to CSI.</li></ul><h2 id=how-to-get-involved>How to get involved?</h2><p>The Kubernetes Slack channel <a href=https://kubernetes.slack.com/messages/C8EJ01Z46/details/>wg-csi</a> and the Google group <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-storage-wg-csi>kubernetes-sig-storage-wg-csi</a> along with any of the standard <a href=https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact>SIG storage communication channels</a> are all great mediums to reach out to the SIG Storage team.</p><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the new contributors who stepped up this quarter to help the project reach GA:</p><ul><li>Saad Ali (<a href=https://github.com/saad-ali>saad-ali</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>msau42</a>)</li><li>Serguei Bezverkhi (<a href=https://github.com/sbezverk>sbezverk</a>)</li><li>Masaki Kimura (<a href=https://github.com/mkimuram>mkimuram</a>)</li><li>Patrick Ohly (<a href=https://github.com/pohly>pohly</a>)</li><li>Luis Pabón (<a href=https://github.com/lpabon>lpabon</a>)</li><li>Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>)</li><li>Vladimir Vivien (<a href=https://github.com/vladimirvivien>vladimirvivien</a>)</li><li>Cheng Xing (<a href=https://github.com/verult>verult</a>)</li><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li><li>David Zhu (<a href=https://github.com/davidz627>davidz627</a>)</li></ul><p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bab3cd81db1bb9b4d144a957121355a9>APIServer dry-run and kubectl diff</h1><div class="td-byline mb-4"><time datetime=2019-01-14 class=text-muted>Monday, January 14, 2019</time></div><p><strong>Author</strong>: Antoine Pelisse (Google Cloud, @apelisse)</p><p>Declarative configuration management, also known as configuration-as-code, is
one of the key strengths of Kubernetes. It allows users to commit the desired state of
the cluster, and to keep track of the different versions, improve auditing and
automation through CI/CD pipelines. The <a href=https://groups.google.com/forum/#!forum/kubernetes-wg-apply>Apply working-group</a>
is working on fixing some of the gaps, and is happy to announce that Kubernetes
1.13 promoted server-side dry-run and <code>kubectl diff</code> to beta. These
two features are big improvements for the Kubernetes declarative model.</p><h2 id=challenges>Challenges</h2><p>A few pieces are still missing in order to have a seamless declarative
experience with Kubernetes, and we tried to address some of these:</p><ul><li>While compilers and linters do a good job to detect errors in pull-requests
for code, a good validation is missing for Kubernetes configuration files.
The existing solution is to run <code>kubectl apply --dry-run</code>, but this runs a
<em>local</em> dry-run that doesn't talk to the server: it doesn't have server
validation and doesn't go through validating admission controllers. As an
example, Custom resource names are only validated on the server so a local
dry-run won't help.</li><li>It can be difficult to know how your object is going to be applied by the
server for multiple reasons:<ul><li>Defaulting will set some fields to potentially unexpected values,</li><li>Mutating webhooks might set fields or clobber/change some values.</li><li>Patch and merges can have surprising effects and result in unexpected
objects. For example, it can be hard to know how lists are going to be
ordered once merged.</li></ul></li></ul><p>The working group has tried to address these problems.</p><h2 id=apiserver-dry-run>APIServer dry-run</h2><p><a href=/docs/reference/using-api/api-concepts/#dry-run>APIServer dry-run</a> was implemented to address these two problems:</p><ul><li>it allows individual requests to the apiserver to be marked as "dry-run",</li><li>the apiserver guarantees that dry-run requests won't be persisted to storage,</li><li>the request is still processed as typical request: the fields are
defaulted, the object is validated, it goes through the validation admission
chain, and through the mutating admission chain, and then the final object is
returned to the user as it normally would, without being persisted.</li></ul><p>While dynamic admission controllers are not supposed to have side-effects on
each request, dry-run requests are only processed if all admission controllers
explicitly announce that they don't have any dry-run side-effects.</p><h3 id=how-to-enable-it>How to enable it</h3><p>Server-side dry-run is enabled through a feature-gate. Now that the feature is
Beta in 1.13, it should be enabled by default, but still can be enabled/disabled
using <code>kube-apiserver --feature-gates DryRun=true</code>.</p><p>If you have dynamic admission controllers, you might have to fix them to:</p><ul><li>Remove any side-effects when the dry-run parameter is specified on the webhook request,</li><li>Specify in the <a href=https://v1-13.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#webhook-v1beta1-admissionregistration><code>sideEffects</code></a> field of the <code>admissionregistration.k8s.io/v1beta1.Webhook</code> object to indicate that the object doesn't have side-effects on dry-run (or at all).</li></ul><h3 id=how-to-use-it>How to use it</h3><p>You can trigger the feature from kubectl by using <code>kubectl apply --server-dry-run</code>, which will decorate the request with the dryRun flag
and return the object as it would have been applied, or an error if it would
have failed.</p><h2 id=kubectl-diff>Kubectl diff</h2><p>APIServer dry-run is convenient because it lets you see how the object would be
processed, but it can be hard to identify exactly what changed if the object is
big. <code>kubectl diff</code> does exactly what you want by showing the differences between
the current "live" object and the new "dry-run" object. It makes it very
convenient to focus on only the changes that are made to the object, how the
server has merged these and how the mutating webhooks affects the output.</p><h3 id=how-to-use-it-1>How to use it</h3><p><code>kubectl diff</code> is meant to be as similar as possible to <code>kubectl apply</code>:
<code>kubectl diff -f some-resources.yaml</code> will show a diff for the resources in the yaml file. One can even use the diff program of their choice by using the KUBECTL_EXTERNAL_DIFF environment variable, for example:</p><pre><code>KUBECTL_EXTERNAL_DIFF=meld kubectl diff -f some-resources.yaml
</code></pre><h2 id=what-s-next>What's next</h2><p>The working group is still busy trying to improve some of these things:</p><ul><li>Server-side apply is trying to improve the apply scenario, by adding owner
semantics to fields! It's also going to improve support for CRDs and unions!</li><li>Some kubectl apply features are missing from diff and could be useful, like the ability
to filter by label, or to display pruned resources.</li><li>Eventually, kubectl diff will use server-side apply!</li></ul><blockquote class="note callout"><div><strong>Note:</strong> The flag <code>kubectl apply --server-dry-run</code> is deprecated in v1.18.
Use the flag <code>--dry-run=server</code> for using server-side dry-run in
<code>kubectl apply</code> and other subcommands.</div></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-c6d29bd07e6e325a3eea49e1cf2cfefd>Kubernetes Federation Evolution</h1><div class="td-byline mb-4"><time datetime=2018-12-12 class=text-muted>Wednesday, December 12, 2018</time></div><p><strong>Authors</strong>: Irfan Ur Rehman (Huawei), Paul Morie (RedHat) and Shashidhara T D (Huawei)</p><p>Kubernetes provides great primitives for deploying applications to a cluster: it can be as simple as <code>kubectl create -f app.yaml</code>. Deploy apps across multiple clusters has never been that simple. How should app workloads be distributed? Should the app resources be replicated into all clusters, replicated into selected clusters, or partitioned into clusters? How is access to the clusters managed? What happens if some of the resources that a user wants to distribute pre-exist, in some or all of the clusters, in some form?</p><p>In SIG Multicluster, our journey has revealed that there are multiple possible models to solve these problems and there probably is no single best-fit, all-scenario solution. <a href=/docs/concepts/cluster-administration/federation/>Federation</a>, however, is the single biggest Kubernetes open source sub-project, and has seen the maximum interest and contribution from the community in this problem space. The project initially reused the Kubernetes API to do away with any added usage complexity for an existing Kubernetes user. This approach was not viable, because of the problems summarised below:</p><ul><li>Difficulties in re-implementing the Kubernetes API at the cluster level, as federation-specific extensions were stored in annotations.</li><li>Limited flexibility in federated types, placement and reconciliation, due to 1:1 emulation of the Kubernetes API.</li><li>No settled path to GA, and general confusion on API maturity; for example, Deployments are GA in Kubernetes but not even Beta in Federation v1.</li></ul><p>The ideas have evolved further with a federation-specific API architecture and a community effort which now continues as Federation v2.</p><h1 id=conceptual-overview>Conceptual Overview</h1><p>Because Federation attempts to address a complex set of problems, it pays to break the different parts of those problems down. Let’s take a look at the different high-level areas involved:<figure><img src=/images/blog/2018-12-11-Kubernetes-Federation-Evolution/concepts.png alt="Kubernetes Federation v2 Concepts"><figcaption><p>Kubernetes Federation v2 Concepts</p></figcaption></figure></p><h2 id=federating-arbitrary-resources>Federating arbitrary resources</h2><p>One of the main goals of Federation is to be able to define the APIs and API groups which encompass basic tenets needed to federate any given Kubernetes resource. This is crucial, due to the popularity of CustomResourceDefinitions as a way to extend Kubernetes with new APIs.</p><p>The workgroup arrived at a common definition of the federation API and API groups as <em>'a mechanism that distributes “normal” Kubernetes API resources into different clusters'</em>. The distribution in its most simple form could be imagined as <em><strong>simple propagation</strong></em> of this <em>'normal Kubernetes API resource'</em> across the federated clusters. A thoughtful reader can certainly discern more complicated mechanisms, other than this simple propagation of the Kubernetes resources.</p><p>During the journey of defining building blocks of the federation APIs, one of the near term goals also evolved as <em>'to be able to create a simple federation a.k.a. simple propagation of any Kubernetes resource or a CRD, writing almost zero code'</em>. What ensued further was a core API group defining the building blocks as a <code>Template</code> resource, a <code>Placement</code> resource and an <code>Override</code> resource per given Kubernetes resource, a <code>TypeConfig</code> to specify sync or no sync for the given resource and associated controller(s) to carry out the sync. More details follow <a href=#federating-resources-the-details>in the next section</a>. Further sections will also talk about being able to follow a layered behaviour with higher-level federation APIs consuming the behaviour of these core building blocks, and users being able to consume whole or part of the API and associated controllers. Lastly, this architecture also allows the users to write additional controllers or replace the available reference controllers with their own, to carry out desired behaviour.</p><p>The ability to <em>'easily federate arbitrary Kubernetes resources'</em>, and a decoupled API, divided into building blocks APIs, higher level APIs and possible user intended types, presented such that different users can consume parts and write controllers composing solutions specific to them, makes a compelling case for Federation v2.</p><h2 id=federating-resources-the-details>Federating resources: the details</h2><p>Fundamentally, federation must be configured with two types of information:</p><ul><li>Which API types federation should handle</li><li>Which clusters federation should target for distributing those resources.</li></ul><p>For each API type that federation handles, different parts of the declared state live in different API resources:</p><ul><li>A <code>Template</code> type holds the base specification of the resource - for example, a type called <code>FederatedReplicaSet</code> holds the base specification of a <code>ReplicaSet</code> that should be distributed to the targeted clusters</li><li>A <code>Placement</code> type holds the specification of the clusters the resource should be distributed to - for example, a type called <code>FederatedReplicaSetPlacement</code> holds information about which clusters <code>FederatedReplicaSets</code> should be distributed to</li><li>An optional <code>Overrides</code> type holds the specification of how the <code>Template</code> resource should be varied in some clusters - for example, a type called <code>FederatedReplicaSetOverrides</code> holds information about how a <code>FederatedReplicaSet</code> should be varied in certain clusters.</li></ul><p>These types are all associated by name - meaning that for a particular Template resource with name <code>foo</code>, the Placement and Override information for that resource are contained by the Override and Placement resources with the name <code>foo</code> and in the same namespace as the Template.</p><h2 id=higher-level-behaviour>Higher-level behaviour</h2><p>The architecture of the v2 API allows higher-level APIs to be constructed using the mechanics provided by the core API types (<code>Template</code>, <code>Placement</code> and <code>Override</code>), and associated controllers, for a given resource. In the community we uncovered a few use cases and implemented the higher-level APIs and associated controllers useful for those cases. Some of these types described in further sections also provide an useful reference to anybody interested in solving more complex use cases, building on top of the mechanics already available with the v2 API.</p><h3 id=replicaschedulingpreference>ReplicaSchedulingPreference</h3><p><code>ReplicaSchedulingPreference</code> provides an automated mechanism of distributing and maintaining total number of replicas for Deployment or ReplicaSet-based federated workloads into federated clusters. This is based on high-level user preferences given by the user. These preferences include the semantics of <em>weighted distribution</em> and <em>limits</em> (min and max) for distributing the replicas. These also include semantics to allow redistribution of replicas dynamically in case some replica Pods remain unscheduled in some clusters, for example due to insufficient resources in that cluster.
More details can be found at the <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#replicaschedulingpreference>user guide for ReplicaSchedulingPreferences</a>.</p><h3 id=federated-services-cross-cluster-service-discovery>Federated services & cross-cluster service discovery</h3><p>Kubernetes Services are very useful in constructing a microservices architecture. There is a clear desire to deploy services across cluster, zone, region and cloud boundaries. Services that span clusters provide geographic distribution, enable hybrid and multi-cloud scenarios and improve the level of high availability beyond single cluster deployments. Customers who want their services to span one or more (possibly remote) clusters, need them to be reachable in a consistent manner from both within and outside their clusters.</p><p>Federated <code>Service</code>, at its core, contains a <code>Template</code> (a definition of a Kubernetes Service), a <code>Placement</code> (which clusters to be deployed into), an <code>Override</code> (optional variation in particular clusters) and a <code>ServiceDNSRecord</code> (specifying details on how to discover it).</p><p>Note: The federated service has to be of type <code>LoadBalancer</code> in order for it to be discoverable across clusters.</p><h4 id=discovering-a-federated-service-from-pods-inside-your-federated-clusters>Discovering a federated service from Pods inside your federated clusters</h4><p>By default, Kubernetes clusters come preconfigured with a cluster-local DNS server, as well as an intelligently constructed DNS search path, which together ensure that DNS queries like <code>myservice</code>, <code>myservice.mynamespace</code>, or <code>some-other-service.other-namespace</code>, issued by software running inside Pods, are automatically expanded and resolved correctly to the appropriate IP of Services running in the local cluster.</p><p>With the introduction of federated services and cross-cluster service discovery, this concept is extended to cover Kubernetes Services running in any other cluster across your cluster federation, globally. To take advantage of this extended range, you use a slightly different DNS name (e.g. <code>myservice.mynamespace.myfederation</code>) to resolve federated services. Using a different DNS name also avoids having your existing applications accidentally traversing cross-zone or cross-region networks and you incurring perhaps unwanted network charges or latency, without you explicitly opting in to this behavior.</p><p>Lets consider an example, using a service named <code>nginx</code>.</p><p>A Pod in a cluster in the <code>us-central1-a</code> availability zone needs to contact our <code>nginx</code> service. Rather than use the service’s traditional cluster-local DNS name (<code>nginx.mynamespace</code>, which is automatically expanded to <code>nginx.mynamespace.svc.cluster.local</code>) it can now use the service’s federated DNS name, which is <code>nginx.mynamespace.myfederation</code>. This will be automatically expanded and resolved to the closest healthy shard of my <code>nginx</code> service, wherever in the world that may be. If a healthy shard exists in the local cluster, that service’s cluster-local IP address will be returned (by the cluster-local DNS). This is exactly equivalent to non-federated service resolution.</p><p>If the Service does not exist in the local cluster (or it exists but has no healthy backend pods), the DNS query is automatically expanded to <code>nginx.mynamespace.myfederation.svc.us-central1-a.us-central1.example.com</code>. Behind the scenes, this finds the external IP of one of the shards closest to my availability zone. This expansion is performed automatically by the cluster-local DNS server, which returns the associated CNAME record. This results in a traversal of the hierarchy of DNS records, and ends up at one of the external IP’s of the federated service nearby.</p><p>It is also possible to target service shards in availability zones and regions other than the ones local to a Pod by specifying the appropriate DNS names explicitly, and not relying on automatic DNS expansion. For example, <code>nginx.mynamespace.myfederation.svc.europe-west1.example.com</code>will resolve to all of the currently healthy service shards in Europe, even if the Pod issuing the lookup is located in the U.S., and irrespective of whether or not there are healthy shards of the service in the U.S. This is useful for remote monitoring and other similar applications.</p><h4 id=discovering-a-federated-service-from-other-clients-outside-your-federated-clusters>Discovering a federated service from other clients outside your federated clusters</h4><p>For external clients, automatic DNS expansion described is not currently possible. External clients need to specify one of the fully qualified DNS names of the federated service, be that a zonal, regional or global name. For convenience reasons, it is often a good idea to manually configure additional static CNAME records in your service, for example:</p><table><thead><tr><th>SHORT NAME</th><th>CNAME</th></tr></thead><tbody><tr><td>eu.nginx.acme.com</td><td>nginx.mynamespace.myfederation.svc.europe-west1.example.com</td></tr><tr><td>us.nginx.acme.com</td><td>nginx.mynamespace.myfederation.svc.us-central1.example.com</td></tr><tr><td>nginx.acme.com</td><td>nginx.mynamespace.myfederation.svc.example.com</td></tr></tbody></table><p>That way, your clients can always use the short form on the left, and always be automatically routed to the closest healthy shard on their home continent. All of the required failover is handled for you automatically by Kubernetes cluster federation.</p><p>As further reading, a more elaborate example for users is available in the <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/servicedns-with-externaldns.md>Multi-Cluster Service DNS with ExternalDNS guide</a>.</p><h1 id=try-it-yourself>Try it yourself</h1><p>To get started with Federation v2, please refer to the <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md>user guide</a>. Deployment can be accomplished with a <a href=https://github.com/kubernetes-sigs/kubefed/blob/master/charts/kubefed/README.md>Helm chart</a>, and once the control plane is available, the <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#example>user guide’s example</a> can be used to get some hands-on experience with using Federation V2.</p><p>Federation v2 can be deployed in both <em>cluster-scoped</em> and <em>namespace-scoped</em> configurations. A cluster-scoped deployment will require cluster-admin privileges to both host and member clusters, and may be a good fit for evaluating federation on clusters that are not running critical workloads. Namespace-scoped deployment requires access to only a single namespace on host and member clusters, and is a better fit for evaluating federation on clusters running workloads. Most of the user guide refers to cluster-scoped deployment, with the <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#namespaced-federation>namespaced federation</a> section documenting how use of a namespaced deployment differs. The same cluster can host multiple federations, and clusters can be part of multiple federations when using namespaced federation.</p><h1 id=next-steps>Next Steps</h1><p>As we noted in the beginning of this post, the multicluster problem space is extremely broad. It can be difficult to know exactly how to handle such broad problem spaces without concrete pieces of software to frame those conversations around. Our hope in the Federation working group is that Federation v2 can be a concrete artifact to frame discussions around. We would love to know experiences that folks have had in this problem space, how they feel about Federation v2, and what use-cases they’re interested in exploring in the future.</p><p>Please feel welcome to join us at the <a href=https://kubernetes.slack.com/messages/C09R1PJR3>sig-multicluster slack channel</a> or at <a href=https://docs.google.com/document/d/1FQx0BPlkkl1Bn0c9ocVBxYIKojpmrS1CFP5h0DI68AE/edit>Federation working group meetings</a> on Wednesdays at 07:30 PST.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f9a43c5c77029d8b5d1f024a2c34e056>etcd: Current status and future roadmap</h1><div class="td-byline mb-4"><time datetime=2018-12-11 class=text-muted>Tuesday, December 11, 2018</time></div><p><strong>Author</strong>: Gyuho Lee (Amazon Container OSS Team, @gyuho), Joe Betz (Google Cloud, @jpbetz)</p><p>etcd is a distributed key value store that provides a reliable way to manage the coordination state of distributed systems. etcd was first announced in June 2013 by CoreOS (part of Red Hat as of 2018). Since its adoption in Kubernetes in 2014, etcd has become a fundamental part of the Kubernetes cluster management software design, and the etcd community has grown exponentially. etcd is now being used in production environments of multiple companies, including large cloud provider environments such as AWS, Google Cloud Platform, Azure, and other on-premises Kubernetes implementations. CNCF currently has <a href=https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/>32 conformant Kubernetes platforms and distributions</a>, all of which use etcd as the datastore.</p><p>In this blog post, we’ll review some of the milestones achieved in latest etcd releases, and go over the future roadmap for etcd. Share your thoughts and feedback on features you consider important on the mailing list: <a href=mailto:etcd-dev@googlegroups.com>etcd-dev@googlegroups.com</a>.</p><h2 id=etcd-2013>etcd, 2013</h2><p>In June 2014, Kubernetes was released with etcd as a backing storage for all master states. Kubernetes v0.4 used etcd v0.2 API, which was in an alpha stage at the time. As Kubernetes reached the v1.0 milestone in 2015, etcd stabilized its v2.0 API. The widespread adoption of Kubernetes led to a dramatic increase in the scalability requirements for etcd. To handle large number of workloads and the growing requirements on scale, etcd released v3.0 API in June 2016. Kubernetes v1.13 finally <a href=https://github.com/kubernetes/enhancements/issues/622>dropped support for etcd v2.0 API</a> and adopted the etcd v3.0 API. The table below gives a visual snapshot of the release cycles of etcd and Kubernetes.</p><table><thead><tr><th></th><th>etcd</th><th>Kubernetes</th></tr></thead><tbody><tr><td>Initial Commit</td><td>June 2, 2013</td><td>June 1, 2014</td></tr><tr><td>First Stable Release</td><td>January 28, 2015 (v2.0.0)</td><td>July 13, 2015 (v1.0.0)</td></tr><tr><td>Latest Release</td><td>October 10, 2018 (v3.3.10)</td><td>December 3, 2018 (v1.13.0)</td></tr></tbody></table><h2 id=etcd-v3-1-early-2017>etcd v3.1, early 2017</h2><p>etcd v3.1 features provide better read performance and better availability during version upgrades. Given the high use of etcd in production even to this day, these features were very useful for users. It implements Raft read index, which bypasses <a href=https://godoc.org/github.com/etcd-io/etcd/wal>Raft WAL</a> disk writes for linearizable reads. The follower requests read index from the leader. Responses from the leader indicate whether a follower has advanced as much as the leader. When the follower's logs are up-to-date, quorum read is served locally without going through the full Raft protocol. Thus, no disk write is required for read requests. etcd v3.1 introduces automatic leadership transfer. When etcd leader receives an interrupt signal, it automatically transfers its leadership to a follower. This provides higher availability when the cluster adds or loses a member.</p><h2 id=etcd-v3-2-summer-2017>etcd v3.2 (summer 2017)</h2><p>etcd v3.2 focuses on stability. Its client was shipped in Kubernetes v1.10, v1.11, and v1.12. The etcd team still actively maintains the branch by backporting all the bug fixes. This release introduces gRPC proxy to support, watch, and coalesce all watch event broadcasts into one gRPC stream. These event broadcasts can go up to one million events per second.</p><p>etcd v3.2 also introduces changes such as <code>“snapshot-count”</code> default value from 10,000 to 100,000. With higher snapshot count, etcd server holds Raft entries in-memory for longer periods before compacting the old ones. etcd v3.2 default configuration shows higher memory usage, while giving more time for slow followers to catch up. It is a trade-off between less frequent snapshot sends and higher memory usage. Users can employ lower <code>etcd --snapshot-count</code> value to reduce the memory usage or higher <code>“snapshot-count”</code> value to increase the availability of slow followers.</p><p>Another new feature backported to etcd v3.2.19 was <code>etcd --initial-election-tick-advance</code> flag. By default, a rejoining follower fast-forwards election ticks to speed up its initial cluster bootstrap. For example, the starting follower node only waits 200ms instead of full election timeout 1-second before starting an election. Ideally, within the 200ms, it receives a leader heartbeat and immediately joins the cluster as a follower. However, if network partition happens, heartbeat may drop and thus leadership election will be triggered. A vote request from a partitioned node is quite disruptive. If it contains a higher Raft term, current leader is forced to step down. With “initial-election-tick-advance” set to false, a rejoining node has <a href=https://github.com/etcd-io/etcd/pull/9591>more chance to receive leader heartbeats</a> before disrupting the cluster.</p><h2 id=etcd-v3-3-early-2018>etcd v3.3 (early 2018)</h2><p>etcd v3.3 continues the theme of stability. Its client is included in <a href=https://github.com/kubernetes/kubernetes/pull/69322>Kubernetes v1.13</a>. Previously, etcd client carelessly retried on network disconnects without any backoff or failover logic. The client was often stuck with a partitioned node, <a href=https://github.com/etcd-io/etcd/issues/7321>affecting several production users</a>. v3.3 client balancer now maintains a list of unhealthy endpoints using gRPC health checking protocol, making more efficient retries and failover in the face of transient disconnects and <a href=https://github.com/etcd-io/etcd/issues/8711>network partitions</a>. This was backported to etcd v3.2 and also <a href=https://github.com/kubernetes/kubernetes/pull/57480>included in Kubernetes v1.10 API server</a>. etcd v3.3 also provides more predictable database size. etcd used to maintain a separate freelist DB to track pages that were no longer in use and freed after transactions, so that following transactions can reuse them. However, it turns out persisting freelist demands high disk space and introduces high latency for Kubernetes workloads. Especially when there were frequent snapshots with lots of read transactions, etcd database size quickly grew from 16 MB to 4 GB. etcd v3.3 disables freelist sync and rebuilds the freelist on restart. The overhead is so small that it is unnoticeable to most users. See <a href=https://github.com/etcd-io/etcd/issues/8009>"database space exceeded" issue</a> for more information on this.</p><h2 id=etcd-v3-4-and-beyond>etcd v3.4 and beyond</h2><p>etcd v3.4 focuses on improving the operational experience. It adds <a href=https://github.com/etcd-io/etcd/pull/9352>Raft pre-vote feature</a> to improve the robustness of leadership election. When a node becomes isolated (e.g. network partition), this member will start an election requesting votes with increased Raft terms. When a leader receives a vote request with a higher term, it steps down to a follower. With pre-vote, Raft runs an additional election phase to check if the candidate can get enough votes to win an election. The isolated follower's vote request is rejected because it does not contain the latest log entries.</p><p>etcd v3.4 adds a <a href=https://etcd.io/docs/v3.4.0/learning/design-learner/#Raft%20Learner>Raft learner</a> that joins the cluster as a non-voting member that still receives all the updates from leader. Adding a learner node does not increase the size of quorum and hence improves the cluster availability during membership reconfiguration. It only serves as a standby node until it gets promoted to a voting member. Moreover, to handle unexpected upgrade failures, v3.4 introduces <a href="https://groups.google.com/forum/?hl=en#!topic/etcd-dev/Hq6zru44L74">etcd downgrade</a> feature.</p><p>etcd v3 storage uses multi-version concurrency control model to preserve key updates as event history. Kubernetes runs compaction to discard the event history that is no longer needed, and reclaims the storage space. etcd v3.4 will improve this storage compact operation, boost backend <a href=https://github.com/etcd-io/etcd/pull/9384>concurrency for large read transactions</a>, and <a href=https://github.com/etcd-io/etcd/pull/10283>optimize storage commit interval</a> for Kubernetes use-case.</p><p>To further improve etcd client load balancer, the v3.4 balancer was rewritten to leverage the newly introduced gRPC load balancing API. By leveraging gPRC, the etcd client load balancer codebase was substantially simplified while retaining feature parity with the v3.3 implementation and improving overall load balancing by round-robining requests across healthy endpoints. See <a href=https://etcd.io/docs/v3.4.0/learning/design-client/>Client Architecture</a> for more details.</p><p>Additionally, etcd maintainers will continue to make improvements to Kubernetes test frameworks: kubemark integration for scalability tests, Kubernetes API server conformance tests with etcd to provide release recommends and version skew policy, specifying conformance testing requirements for each cloud provider, etc.</p><h2 id=etcd-joins-cncf>etcd Joins CNCF</h2><p>etcd now has a new home at <a href=https://github.com/etcd-io>etcd-io</a> and <a href=https://www.cncf.io/blog/2018/12/11/cncf-to-host-etcd/>joined CNCF as an incubating project</a>.</p><p>The synergistic efforts with Kubernetes have driven the evolution of etcd. Without community feedback and contribution, etcd could not have achieved its maturity and reliability. We’re looking forward to continuing the growth of etcd as an open source project and are excited to work with the Kubernetes and the wider CNCF community.</p><p>Finally, we’d like to thank all contributors with special thanks to <a href=https://github.com/xiang90>Xiang Li</a> for his leadership in etcd and Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-015d4916d80a01d1f89156ab0583fd6c>New Contributor Workshop Shanghai</h1><div class="td-byline mb-4"><time datetime=2018-12-05 class=text-muted>Wednesday, December 05, 2018</time></div><p><strong>Authors</strong>: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)</p><figure><img src=/images/blog/2018-12-05-new-contributor-shanghai/attendees.png alt="Kubecon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang"><figcaption><p>Kubecon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang</p></figcaption></figure><p>We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.</p><p>This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.</p><figure><img src=/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png alt="Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang"><figcaption><p>Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang</p></figcaption></figure><p>The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have "guest speakers" from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.</p><p>Those hands-on exercises use a repository known as <a href=https://github.com/kubernetes-sigs/contributor-playground>the contributor playground</a>, created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.</p><figure><img src=/images/blog/2018-12-05-new-contributor-shanghai/yangli.png alt="Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus"><figcaption><p>Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus</p></figcaption></figure><p>Both the "Great Firewall" and the language barrier prevent contributing Kubernetes from China from being straightforward. What's more, because open source business models are not mature in China, the time for employees work on open source projects is limited.</p><p>Chinese engineers are eager to participate in the development of Kubernetes, but many of them don't know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.</p><p>"I have been participating in the Kubernetes community for about three years," said XiangPeng Zhao. "In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it's not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago."</p><figure><img src=/images/blog/2018-12-05-new-contributor-shanghai/panel.png alt="Panel of contributors. Photo by Jerry Zhang"><figcaption><p>Panel of contributors. Photo by Jerry Zhang</p></figcaption></figure><p>The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor's journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.</p><p>After the workshop, Xiang Peng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, "I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor." Another attendee, Jie Jia, said, "The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!"</p><p>SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming Kubecon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future Kubecon. And, when you meet an NCW attendee, make sure to welcome them to the community.</p><p>Links:</p><ul><li>English versions of the slides: <a href=https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf>PDF</a> or <a href="https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing">Google Docs with speaker notes</a></li><li>Chinese version of the slides: <a href=https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf>PDF</a></li><li><a href=https://github.com/kubernetes-sigs/contributor-playground>Contributor playground</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5277ea0d4854d7761ef876f9ed4a151b>Production-Ready Kubernetes Cluster Creation with kubeadm</h1><div class="td-byline mb-4"><time datetime=2018-12-04 class=text-muted>Tuesday, December 04, 2018</time></div><p><strong>Authors</strong>: Lucas Käldström (CNCF Ambassador) and Luc Perkins (CNCF Developer Advocate)</p><p><a href=/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a> is a tool that enables Kubernetes administrators to quickly and easily bootstrap minimum viable clusters that are fully compliant with <a href=https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md>Certified Kubernetes</a> guidelines. It's been under active development by <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG Cluster Lifecycle</a> since 2016 and we're excited to announce that it has now graduated from beta to stable and generally available (GA)!</p><p>This GA release of kubeadm is an important event in the progression of the Kubernetes ecosystem, bringing stability to an area where stability is paramount.</p><p>The goal of kubeadm is to provide a foundational implementation for Kubernetes cluster setup and administration. kubeadm ships with best-practice defaults but can also be customized to support other ecosystem requirements or vendor-specific approaches. kubeadm is designed to be easy to integrate into larger deployment systems and tools.</p><h3 id=the-scope-of-kubeadm>The scope of kubeadm</h3><p>kubeadm is focused on bootstrapping Kubernetes clusters on existing infrastructure and performing an essential set of maintenance tasks. The core of the kubeadm interface is quite simple: new control plane nodes are created by running <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/><code>kubeadm init</code></a> and worker nodes are joined to the control plane by running <a href=/docs/reference/setup-tools/kubeadm/kubeadm-join/><code>kubeadm join</code></a>. Also included are utilities for managing already bootstrapped clusters, such as control plane upgrades and token and certificate renewal.</p><p>To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of its scope:</p><ul><li>Infrastructure provisioning</li><li>Third-party networking</li><li>Non-critical add-ons, e.g. for monitoring, logging, and visualization</li><li>Specific cloud provider integrations</li></ul><p>Infrastructure provisioning, for example, is left to other SIG Cluster Lifecycle projects, such as the <a href=https://github.com/kubernetes-sigs/cluster-api>Cluster API</a>. Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the <a href=/docs/concepts/overview/components/#control-plane-components>control plane</a>. The user may install their preferred networking solution and other add-ons on top of Kubernetes <em>after</em> cluster creation.</p><h3 id=what-kubeadm-s-ga-release-means>What kubeadm's GA release means</h3><p>General Availability means different things for different projects. For kubeadm, going GA means not only that the process of creating a conformant Kubernetes cluster is now stable, but also that kubeadm is flexible enough to support a wide variety of deployment options.</p><p>We now consider kubeadm to have achieved GA-level maturity in each of these important domains:</p><ul><li><strong>Stable command-line UX</strong> --- The kubeadm CLI conforms to <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-flag-or-cli>#5a GA rule of the Kubernetes Deprecation Policy</a>, which states that a command or flag that exists in a GA version must be kept for at least 12 months after deprecation.</li><li><strong>Stable underlying implementation</strong> --- kubeadm now creates a new Kubernetes cluster using methods that shouldn't change any time soon. The control plane, for example, is run as a set of static Pods, bootstrap tokens are used for the <a href=/docs/reference/setup-tools/kubeadm/kubeadm-join/><code>kubeadm join</code></a> flow, and <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/wgs/0014-20180707-componentconfig-api-types-to-staging.md>ComponentConfig</a> is used for configuring the <a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a>.</li><li><strong>Configuration file schema</strong> --- With the new <strong>v1beta1</strong> API version, you can now tune almost every part of the cluster declaratively and thus build a "GitOps" flow around kubeadm-built clusters. In future versions, we plan to graduate the API to version <strong>v1</strong> with minimal changes (and perhaps none).</li><li><strong>The "toolbox" interface of kubeadm</strong> --- Also known as <strong>phases</strong>. If you don't want to perform all <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/><code>kubeadm init</code></a> tasks, you can instead apply more fine-grained actions using the <code>kubeadm init phase</code> command (for example generating certificates or control plane <a href=/docs/tasks/administer-cluster/static-pod/>Static Pod</a> manifests).</li><li><strong>Upgrades between minor versions</strong> --- The <a href=/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/><code>kubeadm upgrade</code></a> command is now fully GA. It handles control plane upgrades for you, which includes upgrades to <a href=https://etcd.io>etcd</a>, the <a href=/docs/reference/using-api/api-overview/>API Server</a>, the <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>Controller Manager</a>, and the <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>Scheduler</a>. You can seamlessly upgrade your cluster between minor or patch versions (e.g. v1.12.2 -> v1.13.1 or v1.13.1 -> v1.13.3).</li><li><strong>etcd setup</strong> --- <a href=https://etcd.io>etcd</a> is now set up in a way that is secure by default, with TLS communication everywhere, and allows for expanding to a highly available cluster when needed.</li></ul><h3 id=who-will-benefit-from-a-stable-kubeadm>Who will benefit from a stable kubeadm</h3><p>SIG Cluster Lifecycle has identified a handful of likely kubeadm user profiles, although we expect that kubeadm at GA can satisfy many other scenarios as well.</p><p>Here's our list:</p><ul><li>You're a <strong>new user</strong> who wants to take Kubernetes for a spin. kubeadm is the fastest way to get up and running on <a href=/docs/setup/independent/create-cluster-kubeadm/>Linux machines</a>. If you're using <a href=https://github.com/kubernetes/minikube>Minikube</a> on a Mac or Windows workstation, you're actually already running kubeadm inside the Minikube VM!</li><li>You're a <strong>system administrator</strong> responsible for setting up Kubernetes on bare metal machines and you want to quickly create Kubernetes clusters that are secure and in conformance with best practices but also highly configurable.</li><li>You're a <strong>cloud provider</strong> who wants to add a Kubernetes offering to your suite of cloud services. kubeadm is the go-to tool for creating clusters at a low level.</li><li>You're an <strong>organization that requires highly customized Kubernetes clusters</strong>. Existing public cloud offerings like <a href=https://aws.amazon.com/eks/>Amazon EKS</a> and <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a> won't cut it for you; you need customized Kubernetes clusters tailored to your hardware, security, policy, and other needs.</li><li>You're creating a <strong>higher-level cluster creation tool</strong> than kubeadm, building the cluster experience from the ground up, but you don't want to reinvent the wheel. You can "rebase" on top of kubeadm and utilize the common bootstrapping tools kubeadm provides for you. Several community tools have adopted kubeadm, and it's a perfect match for <a href=https://github.com/kubernetes-sigs/cluster-api>Cluster API</a> implementations.</li></ul><p>All these users can benefit from kubeadm graduating to a stable GA state.</p><h3 id=kubeadm-survey>kubeadm survey</h3><p>Although kubeadm is GA, the SIG Cluster Lifecycle will continue to be committed to improving the user experience in managing Kubernetes clusters. We're launching a survey to collect community feedback about kubeadm for the sake of future improvement.</p><p>The survey is available at <a href=https://bit.ly/2FPfRiZ>https://bit.ly/2FPfRiZ</a>. Your participation would be highly valued!</p><h3 id=thanks-to-the-community>Thanks to the community!</h3><p>This release wouldn't have been possible without the help of the great people that have been contributing to the SIG. SIG Cluster Lifecycle would like to thank a few key kubeadm contributors:</p><table><thead><tr><th><strong>Name</strong></th><th><strong>Organization</strong></th><th><strong>Role</strong></th></tr></thead><tbody><tr><td><a href=https://github.com/timothysc>Tim St. Clair</a></td><td>Heptio</td><td>SIG co-chair</td></tr><tr><td><a href=https://github.com/roberthbailey>Robert Bailey</a></td><td>Google</td><td>SIG co-chair</td></tr><tr><td><a href=https://github.com/fabriziopandini>Fabrizio Pandini</a></td><td>Independent</td><td>Approver</td></tr><tr><td><a href=https://github.com/neolit123>Lubomir Ivanov</a></td><td>VMware</td><td>Approver</td></tr><tr><td><a href=https://github.com/mikedanese>Mike Danese</a></td><td>Google</td><td>Emeritus approver</td></tr><tr><td><a href=https://github.com/errordeveloper>Ilya Dmitrichenko</a></td><td>Weaveworks</td><td>Emeritus approver</td></tr><tr><td><a href=https://github.com/xiangpengzhao>Peter Zhao</a></td><td>ZTE</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/dixudx>Di Xu</a></td><td>Ant Financial</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/chuckha>Chuck Ha</a></td><td>Heptio</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/liztio>Liz Frost</a></td><td>Heptio</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/detiber>Jason DeTiberus</a></td><td>Heptio</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/kad>Alexander Kanievsky</a></td><td>Intel</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/rosti>Ross Georgiev</a></td><td>VMware</td><td>Reviewer</td></tr><tr><td><a href=https://github.com/yagonobre>Yago Nobre</a></td><td>Nubank</td><td>Reviewer</td></tr></tbody></table><p>We also want to thank all the companies making it possible for their developers to work on Kubernetes, and all the other people that have contributed in various ways towards making kubeadm as stable as it is today!</p><h3 id=about-the-authors>About the authors</h3><h4 id=lucas-käldström>Lucas Käldström</h4><ul><li>kubeadm subproject owner and SIG Cluster Lifecycle co-chair</li><li>Kubernetes upstream contractor, last two years contracting for <a href=https://weave.works>Weaveworks</a></li><li>CNCF Ambassador</li><li>GitHub: <a href=https://github.com/luxas>luxas</a></li></ul><h4 id=luc-perkins>Luc Perkins</h4><ul><li><a href=https://cncf.io>CNCF</a> Developer Advocate</li><li>Kubernetes SIG Docs contributor and SIG Docs tooling WG chair</li><li>GitHub: <a href=https://github.com/lucperkins>lucperkins</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-02d3dfa7ff4d2d5a8f59e09d88b9217e>Kubernetes 1.13: Simplified Cluster Management with Kubeadm, Container Storage Interface (CSI), and CoreDNS as Default DNS are Now Generally Available</h1><div class="td-byline mb-4"><time datetime=2018-12-03 class=text-muted>Monday, December 03, 2018</time></div><p><strong>Author</strong>: The 1.13 <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.13/release_team.md>Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.13, our fourth and final release of 2018!</p><p>Kubernetes 1.13 has been one of the shortest releases to date at 10 weeks. This release continues to focus on stability and extensibility of Kubernetes with three major features graduating to general availability this cycle in the areas of Storage and Cluster Lifecycle. Notable features graduating in this release include: simplified cluster management with kubeadm, Container Storage Interface (CSI), and CoreDNS as the default DNS.</p><p>These stable graduations are an important milestone for users and operators in terms of setting support expectations. In addition, there’s a continual and steady stream of internal improvements and new alpha features that are made available to the community in this release. These features are discussed in the “additional notable features” section below.</p><p>Let’s dive into the key features of this release:</p><h2 id=simplified-kubernetes-cluster-management-with-kubeadm-in-ga>Simplified Kubernetes Cluster Management with kubeadm in GA</h2><p>Most people who have gotten hands-on with Kubernetes have at some point been hands-on with kubeadm. It's an essential tool for managing the cluster lifecycle, from creation to configuration to upgrade; and now kubeadm is officially GA. <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a> handles the bootstrapping of production clusters on existing hardware and configuring the core Kubernetes components in a best-practice-manner to providing a secure yet easy joining flow for new nodes and supporting easy upgrades. What’s notable about this GA release are the now graduated advanced features, specifically around pluggability and configurability. The scope of kubeadm is to be a toolbox for both admins and automated, higher-level system and this release is a significant step in that direction.</p><h2 id=container-storage-interface-csi-goes-ga>Container Storage Interface (CSI) Goes GA</h2><p>The Container Storage Interface (<a href=https://github.com/container-storage-interface>CSI</a>) is now GA after being introduced as alpha in v1.9 and beta in v1.10. With CSI, the Kubernetes volume layer becomes truly extensible. This provides an opportunity for third party storage providers to write plugins that interoperate with Kubernetes without having to touch the core code. The <a href=https://github.com/container-storage-interface/spec>specification itself</a> has also reached a 1.0 status.</p><p>With CSI now stable, plugin authors are developing storage plugins out of core, at their own pace. You can find a list of sample and production drivers in the <a href=https://kubernetes-csi.github.io/docs/drivers.html>CSI Documentation</a>.</p><h2 id=coredns-is-now-the-default-dns-server-for-kubernetes>CoreDNS is Now the Default DNS Server for Kubernetes</h2><p>In 1.11, we announced CoreDNS had reached General Availability for DNS-based service discovery. In 1.13, <a href=https://github.com/kubernetes/features/issues/566>CoreDNS is now replacing kube-dns as the default DNS server</a> for Kubernetes. CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. CoreDNS has fewer moving parts than the previous DNS server, since it’s a single executable and a single process, and supports flexible use cases by creating custom DNS entries. It’s also written in Go making it memory-safe.</p><p>CoreDNS is now the recommended DNS solution for Kubernetes 1.13+. The project has switched the common test infrastructure to use CoreDNS by default and we recommend users switching as well. KubeDNS will still be supported for at least one more release, but it's time to start planning your migration. Many OSS installer tools have already made the switch, including <a href=https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/>Kubeadm in 1.11</a>. If you use a hosted solution, please work with your vendor to understand how this will impact you.</p><h2 id=additional-notable-feature-updates>Additional Notable Feature Updates</h2><p><a href=https://github.com/kubernetes/features/issues/606>Support for 3rd party device monitoring plugins</a> has been introduced as an alpha feature. This removes current device-specific knowledge from the kubelet to enable future use-cases requiring device-specific knowledge to be out-of-tree.</p><p><a href=https://github.com/kubernetes/features/issues/595>Kubelet Device Plugin Registration</a> is graduating to stable. This creates a common Kubelet plugin discovery model that can be used by different types of node-level plugins, such as device plugins, CSI and CNI, to establish communication channels with Kubelet.</p><p><a href=https://github.com/kubernetes/enhancements/issues/490>Topology Aware Volume Scheduling</a> is now stable. This make the scheduler aware of a Pod's volume's topology constraints, such as zone or node.</p><p><a href=https://github.com/kubernetes/features/issues/576>APIServer DryRun</a> is graduating to beta. This moves "apply" and declarative object management from <code>kubectl</code> to the <code>apiserver</code> in order to fix many of the existing bugs that can't be fixed today.</p><p><a href=https://github.com/kubernetes/features/issues/491>Kubectl Diff</a> is graduating to beta. This allows users to run a <code>kubectl</code> command to view the difference between a locally declared object configuration and the current state of a live object.</p><p><a href=https://github.com/kubernetes/features/issues/351>Raw block device using persistent volume source</a> is graduating to beta. This makes raw block devices (non-networked) available for consumption via a Persistent Volume Source.</p><p>Each Special Interest Group (SIG) within the community continues to deliver the most-requested enhancements, fixes, and functionality for their respective specialty areas. For a complete list of inclusions by SIG, please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.13.md#113-release-notes>release notes</a>.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.13 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.13.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=/docs/tutorials/>interactive tutorials</a>. You can also easily install 1.13 using <a href=/docs/setup/independent/create-cluster-kubeadm/>kubeadm</a>.</p><h2 id=features-blog-series>Features Blog Series</h2><p>If you’re interested in exploring these features more in depth, check back tomorrow for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:</p><ul><li>Day 1 - Simplified Kubernetes Cluster Creation with Kubeadm</li><li>Day 2 - Out-of-tree CSI Volume Plugins</li><li>Day 3 - Switch default DNS plugin to CoreDNS</li><li>Day 4 - New CLI Tips and Tricks (Kubectl Diff and APIServer Dry run)</li><li>Day 5 - Raw Block Volume</li></ul><h2 id=release-team>Release team</h2><p>This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.13/release_team.md>release team</a> led by Aishwarya Sundar, Software Engineer at Google. The 39 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 25,000 individual contributors to date and an active community of more than 51,000 people.</p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, 347 different companies and over 2,372 individuals contribute to Kubernetes each month. <a href=https://devstats.k8s.io>Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=user-highlights>User Highlights</h2><p>Established, global organizations are using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at massive scale. Recently published user stories from the community include:</p><ul><li><strong>IBM Cloud</strong>, a leading provider of public, private, and hybrid cloud functionality, is using <a href=https://kubernetes.io/case-studies/ibm/>cloud native technology for high-availability deployments</a> with three instances across two zones in each of the five regions, load balanced with failover support.</li><li><strong>The National Association of Insurance Commissioners (NAIC)</strong>, the U.S. standard-setting and regulatory support organization, leverages Kubernetes to <a href=https://kubernetes.io/case-studies/naic/>create rapid prototypes in two days</a> that would have previously taken at least a month.</li><li><strong>Ocado</strong>, the world’s largest online-only grocery retailer, <a href=https://kubernetes.io/case-studies/ocado/>use 15-25% less hardware resources</a> to host the same applications in Kubernetes in their test environments.</li><li><strong>Adform</strong>, a provider of advertising technology to enable digital ads across devices, <a href=https://kubernetes.io/case-studies/adform/>uses Kubernetes to reduce utilization of hardware resources</a>, with containers notching 2-3 times more efficiency over virtual machines.</li></ul><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community.</p><h2 id=ecosystem-updates>Ecosystem Updates</h2><ul><li>CNCF recently released the findings of their <a href=https://www.cncf.io/blog/2018/11/13/cncf-survey-china-november-2018/>bi-annual CNCF survey</a> in Mandarin, finding that cloud usage in Asia has grown 135% since March 2018.</li><li>CNCF expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual's ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found <a href=https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/>here</a>.</li><li>CNCF added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more <a href=https://www.cncf.io/certification/training/>here</a>.</li><li>CNCF also offers <a href=https://www.cncf.io/certification/training/>online training</a> that teaches the skills needed to create and configure a real-world Kubernetes cluster.</li><li>Kubernetes documentation now features <a href=https://k8s.io/docs/home/>user journeys</a>: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.</li></ul><h2 id=kubecon>KubeCon</h2><p>The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/>Seattle</a> from December 10-13, 2018 and <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/>Barcelona</a> from May 20-23, 2019. This conference will feature technical sessions, case studies, developer deep dives, salons, and more. <a href=https://www.cncf.io/community/kubecon-cloudnativecon-events/>Registration</a> will open up in early 2019.</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.13 release team on January 10th at 9am PDT to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/WN_A2FZovz-TIWn_Xvrb5uERQ>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/tree/master/communication>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community discussion on <a href=https://discuss.kubernetes.io/>Discuss Kubernetes</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5193292ae0d55535d8029ff9a7566fca>Kubernetes Docs Updates, International Edition</h1><div class="td-byline mb-4"><time datetime=2018-11-08 class=text-muted>Thursday, November 08, 2018</time></div><p><strong>Author</strong>: Zach Corleissen (Linux Foundation)</p><p>As a co-chair of SIG Docs, I'm excited to share that Kubernetes docs have a fully mature workflow for localization (l10n).</p><h2 id=abbreviations-galore>Abbreviations galore</h2><p>L10n is an abbreviation for <em>localization</em>.</p><p>I18n is an abbreviation for <em>internationalization</em>.</p><p>I18n is <a href=https://www.w3.org/International/questions/qa-i18n>what you do</a> to make l10n easier. L10n is a fuller, more comprehensive process than translation (<em>t9n</em>).</p><h2 id=why-localization-matters>Why localization matters</h2><p>The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible.</p><p>One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), <a href=https://kubernetes.io/blog/2018/05/05/hugo-migration/>much transformation</a>, and <a href=https://github.com/kubernetes/website/pull/10485>renewed commitment to easier localization</a>, we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what's possible.</p><p>Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we've paid off a significant amount of technical debt and streamlined l10n in a single workflow. That's great for the future as well as the present.</p><h2 id=consolidated-workflow>Consolidated workflow</h2><p>Localization is now consolidated in the <a href=https://github.com/kubernetes/website>kubernetes/website</a> repository. We've configured the Kubernetes CI/CD system, <a href=https://github.com/kubernetes/test-infra/tree/master/prow>Prow</a>, to handle automatic language label assignment as well as team-level PR review and approval.</p><h3 id=language-labels>Language labels</h3><p>Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor <a href=https://github.com/kubernetes/test-infra/pull/9835>June Yi</a>, folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label <code>language/ko</code> (Korean).</p><pre><code>/language ko
</code></pre><p>These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for <a href="https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh">PRs with Chinese content</a>.</p><h3 id=team-review>Team review</h3><p>L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are <a href=https://github.com/kubernetes/website/blob/master/content/en/OWNERS>assigned in an OWNERS file</a> in the top subfolder for English content.</p><p>Adding <code>OWNERS</code> files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency.</p><h2 id=what-s-next>What's next</h2><p>We're looking forward to the <a href=https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required>doc sprint in Shanghai</a> to serve as a resource for the Chinese l10n team.</p><p>We're excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress.</p><p>If you're interested in localizing Kubernetes for your own language or region, check out our <a href=/docs/contribute/localization/>guide to localizing Kubernetes docs</a> and reach out to a <a href=https://github.com/kubernetes/community/tree/master/sig-docs#leadership>SIG Docs chair</a> for support.</p><h3 id=get-involved-with-sig-docs>Get involved with SIG Docs</h3><p>If you're interested in Kubernetes documentation, come to a SIG Docs <a href=https://github.com/kubernetes/community/tree/master/sig-docs#meetings>weekly meeting</a>, or join <a href=https://kubernetes.slack.com/messages/C1J0BPD2M/details/>#sig-docs in Kubernetes Slack</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-15ef0489e073bddfa8d3cd35e3b189df>gRPC Load Balancing on Kubernetes without Tears</h1><div class="td-byline mb-4"><time datetime=2018-11-07 class=text-muted>Wednesday, November 07, 2018</time></div><p><strong>Author</strong>: William Morgan (Buoyant)</p><p>Many new gRPC users are surprised to find that Kubernetes's default load
balancing often doesn't work out of the box with gRPC. For example, here's what
happens when you take a <a href=https://github.com/sourishkrout/nodevoto>simple gRPC Node.js microservices
app</a> and deploy it on Kubernetes:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0116-c4d86100-afc1-4a08-a01c-16da391756dd.34.36.png alt></p><p>While the <code>voting</code> service displayed here has several pods, it's clear from
Kubernetes's CPU graphs that only one of the pods is actually doing any
work—because only one of the pods is receiving any traffic. Why?</p><p>In this blog post, we describe why this happens, and how you can easily fix it
by adding gRPC load balancing to any Kubernetes app with
<a href=https://linkerd.io>Linkerd</a>, a <a href=https://cncf.io>CNCF</a> service mesh and service sidecar.</p><h1 id=why-does-grpc-need-special-load-balancing>Why does gRPC need special load balancing?</h1><p>First, let's understand why we need to do something special for gRPC.</p><p>gRPC is an increasingly common choice for application developers. Compared to
alternative protocols such as JSON-over-HTTP, gRPC can provide some significant
benefits, including dramatically lower (de)serialization costs, automatic type
checking, formalized APIs, and less TCP management overhead.</p><p>However, gRPC also breaks the standard connection-level load balancing,
including what's provided by Kubernetes. This is because gRPC is built on
HTTP/2, and HTTP/2 is designed to have a single long-lived TCP connection,
across which all requests are <em>multiplexed</em>—meaning multiple requests can be
active on the same connection at any point in time. Normally, this is great, as
it reduces the overhead of connection management. However, it also means that
(as you might imagine) connection-level balancing isn't very useful. Once the
connection is established, there's no more balancing to be done. All requests
will get pinned to a single destination pod, as shown below:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Mono-8d2e53ef-b133-4aa0-9551-7e36a880c553.png alt></p><h1 id=why-doesn-t-this-affect-http-1-1>Why doesn't this affect HTTP/1.1?</h1><p>The reason why this problem doesn't occur in HTTP/1.1, which also has the
concept of long-lived connections, is because HTTP/1.1 has several features
that naturally result in cycling of TCP connections. Because of this,
connection-level balancing is "good enough", and for most HTTP/1.1 apps we
don't need to do anything more.</p><p>To understand why, let's take a deeper look at HTTP/1.1. In contrast to HTTP/2,
HTTP/1.1 cannot multiplex requests. Only one HTTP request can be active at a
time per TCP connection. The client makes a request, e.g. <code>GET /foo</code>, and then
waits until the server responds. While that request-response cycle is
happening, no other requests can be issued on that connection.</p><p>Usually, we want lots of requests happening in parallel. Therefore, to have
concurrent HTTP/1.1 requests, we need to make multiple HTTP/1.1 connections,
and issue our requests across all of them. Additionally, long-lived HTTP/1.1
connections typically expire after some time, and are torn down by the client
(or server). These two factors combined mean that HTTP/1.1 requests typically
cycle across multiple TCP connections, and so connection-level balancing works.</p><h1 id=so-how-do-we-load-balance-grpc>So how do we load balance gRPC?</h1><p>Now back to gRPC. Since we can't balance at the connection level, in order to
do gRPC load balancing, we need to shift from connection balancing to <em>request</em>
balancing. In other words, we need to open an HTTP/2 connection to each
destination, and balance <em>requests</em> across these connections, as shown below:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Stereo-09aff9d7-1c98-4a0a-9184-9998ed83a531.png alt></p><p>In network terms, this means we need to make decisions at L5/L7 rather than
L3/L4, i.e. we need to understand the protocol sent over the TCP connections.</p><p>How do we accomplish this? There are a couple options. First, our application
code could manually maintain its own load balancing pool of destinations, and
we could configure our gRPC client to <a href=https://godoc.org/google.golang.org/grpc/balancer>use this load balancing
pool</a>. This approach gives
us the most control, but it can be very complex in environments like Kubernetes
where the pool changes over time as Kubernetes reschedules pods. Our
application would have to watch the Kubernetes API and keep itself up to date
with the pods.</p><p>Alternatively, in Kubernetes, we could deploy our app as <a href=/docs/concepts/services-networking/service/#headless-services>headless
services</a>.
In this case, Kubernetes <a href=/docs/concepts/services-networking/service/#headless-services>will create multiple A
records</a>
in the DNS entry for the service. If our gRPC client is sufficiently advanced,
it can automatically maintain the load balancing pool from those DNS entries.
But this approach restricts us to certain gRPC clients, and it's rarely
possible to only use headless services.</p><p>Finally, we can take a third approach: use a lightweight proxy.</p><h1 id=grpc-load-balancing-on-kubernetes-with-linkerd>gRPC load balancing on Kubernetes with Linkerd</h1><p><a href=https://linkerd.io>Linkerd</a> is a <a href=https://cncf.io>CNCF</a>-hosted <em>service
mesh</em> for Kubernetes. Most relevant to our purposes, Linkerd also functions as
a <em>service sidecar</em>, where it can be applied to a single service—even without
cluster-wide permissions. What this means is that when we add Linkerd to our
service, it adds a tiny, ultra-fast proxy to each pod, and these proxies watch
the Kubernetes API and do gRPC load balancing automatically. Our deployment
then looks like this:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Linkerd-8df1031c-cdd1-4164-8e91-00f2d941e93f.io.png alt></p><p>Using Linkerd has a couple advantages. First, it works with services written in
any language, with any gRPC client, and any deployment model (headless or not).
Because Linkerd's proxies are completely transparent, they auto-detect HTTP/2
and HTTP/1.x and do L7 load balancing, and they pass through all other traffic
as pure TCP. This means that everything will <em>just work.</em></p><p>Second, Linkerd's load balancing is very sophisticated. Not only does Linkerd
maintain a watch on the Kubernetes API and automatically update the load
balancing pool as pods get rescheduled, Linkerd uses an <em>exponentially-weighted
moving average</em> of response latencies to automatically send requests to the
fastest pods. If one pod is slowing down, even momentarily, Linkerd will shift
traffic away from it. This can reduce end-to-end tail latencies.</p><p>Finally, Linkerd's Rust-based proxies are incredibly fast and small. They
introduce &lt;1ms of p99 latency and require &lt;10mb of RSS per pod, meaning that
the impact on system performance will be negligible.</p><h1 id=grpc-load-balancing-in-60-seconds>gRPC Load Balancing in 60 seconds</h1><p>Linkerd is very easy to try. Just follow the steps in the <a href=https://linkerd.io/2/getting-started/>Linkerd Getting
Started Instructions</a>—install the
CLI on your laptop, install the control plane on your cluster, and "mesh" your
service (inject the proxies into each pod). You'll have Linkerd running on your
service in no time, and should see proper gRPC balancing immediately.</p><p>Let's take a look at our sample <code>voting</code> service again, this time after
installing Linkerd:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0116-24b8ee81-144c-4eac-b73d-871bbf0ea22e.57.42.png alt></p><p>As we can see, the CPU graphs for all pods are active, indicating that all pods
are now taking traffic—without having to change a line of code. Voila,
gRPC load balancing as if by magic!</p><p>Linkerd also gives us built-in traffic-level dashboards, so we don't even need
to guess what's happening from CPU charts any more. Here's a Linkerd graph
that's showing the success rate, request volume, and latency percentiles of
each pod:</p><p><img src=/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0212-15ed0448-5424-4e47-9828-20032de868b5.08.38.png alt></p><p>We can see that each pod is getting around 5 RPS. We can also see that, while
we've solved our load balancing problem, we still have some work to do on our
success rate for this service. (The demo app is built with an intentional
failure—as an exercise to the reader, see if you can figure it out by
using the Linkerd dashboard!)</p><h1 id=wrapping-it-up>Wrapping it up</h1><p>If you're interested in a dead simple way to add gRPC load balancing to your
Kubernetes services, regardless of what language it's written in, what gRPC
client you're using, or how it's deployed, you can use Linkerd to add gRPC load
balancing in a few commands.</p><p>There's a lot more to Linkerd, including security, reliability, and debugging
and diagnostics features, but those are topics for future blog posts.</p><p>Want to learn more? We’d love to have you join our rapidly-growing community!
Linkerd is a <a href=https://cncf.io>CNCF</a> project, <a href=https://github.com/linkerd/linkerd2>hosted on
GitHub</a>, and has a thriving community
on <a href=https://slack.linkerd.io>Slack</a>, <a href=https://twitter.com/linkerd>Twitter</a>,
and the <a href=https://lists.cncf.io/g/cncf-linkerd-users>mailing lists</a>. Come and
join the fun!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e07d63b1f3e1dff24501d92717c4ce52>Tips for Your First Kubecon Presentation - Part 2</h1><div class="td-byline mb-4"><time datetime=2018-10-26 class=text-muted>Friday, October 26, 2018</time></div><p><strong>Author</strong>: Michael Gasch (VMware)</p><p>Hello and welcome back to the second and final part about tips for KubeCon first-time speakers. If you missed the last post, please give it a read <a href=https://kubernetes.io/blog/2018/10/18/tips-for-your-first-kubecon-presentation-part-1/>here</a>.</p><h2 id=the-day-before-the-show>The Day before the Show</h2><p><strong>Tip #13 - Get enough sleep</strong>. I don't know about you, but when I don't get enough sleep (especially when beer is in the game), the next day my brain power is around 80% at best. It's very easy to get distracted at KubeCon (in a positive sense). "Let's have dinner tonight and chat about XYZ". Get some food, beer or wine because you're so excited and all the good resolutions you had set for the day before your presentation are forgotten :)</p><p>OK, I'm slightly exaggerating here. But don't underestimate the dynamics of this conference, the amazing people you meet, the inspiring talks and of course the conference party. Be disciplined, at least that one day. There's enough time to party after your great presentation!</p><p><strong>Tip #14 - A final dry-run</strong>. Usually, I do a final dry-run of my presentation the day before the talk. This helps me to recall the first few sentences I want to say so I keep the flow no matter what happens when the red recording light goes on. Especially when your talk is later during the conference, there's so much new stuff your brain has to digest which could "overwrite" the very important parts of your presentation. I think you know what I mean. So, if you're like me, a final dry-run is never a bad idea (also to check equipment, demos, etc.).</p><p><strong>Tip #15 - Promote your session, again</strong>. Send out a final reminder on your social media channels so your followers (and KubeCon attendees) will recall to attend your session (again, KubeCon is busy and it's hard to keep up with all the talks you wanted to attend). I was surprised to see my attendee list jumping from ~80 at the beginning of the week to >300 the day before the talk. The number kept rising even an hour before going on stage. So don't worry about the stats too early.</p><p><strong>Tip #16 - Ask your idols to attend</strong>. <a href=https://twitter.com/cantbewong>Steve Wong</a>, a colleague of mine who I really admire for his knowledge and passion, gave me a great advise. Reach out to the people you always wanted to attend your talk and kindly ask them to come along.</p><p>So I texted the one and only <a href="https://twitter.com/thockin?lang=de">Tim Hockin</a>. Even though these well-respected community leaders are super busy and thus usually cannot attend many talks during the conference, the worst thing that can happen is that they cannot show up and will let you know. (see the end of this post to find out whether or not I was lucky :))</p><h2 id=the-show-is-on>The show is on!</h2><p>Your day has come and it doesn't make <strong>any</strong> sense to make big changes to your presentation now! Actually, that's a very bad idea unless you're an expert and your heartbeat at rest is around 40 BPM. (But even then many things can go horribly wrong).</p><p>So, without further ado, here are my final tips for you.</p><p><strong>Tip #17 - Arrive ahead of time</strong>. Set an alert (or two) to not miss your presentation, e.g. because somebody caught you on the way to the room or you got a call/have been pulled in a meeting. It's a good idea to find out were your room is at least some hours before your talk. These conference buildings can be very large. Also look for last minute schedule (time/room) changes, just because you never know...</p><p><strong>Tip #18 - Ask a friend to take photos</strong>. My dear colleague <a href=https://twitter.com/bbrundert>Bjoern</a>, without me asking for it, took a lot of pictures and watched the audience during the talk. This was really helpful, not just because I now have some nice shots that will always remind me of this great day. He also gave me honest feedback, e.g. what people said, whether they liked it or what I could have done better.</p><p><strong>Tip #19 - Restroom</strong>. If you're like me, when I'm nervous I could run every 15 minutes. The last thing you want is that you are fully cabled (microphone), everything is set up and two minutes before your presentation you feel like "oh oh"...nothing more to say here ;)</p><p><strong>Tip #20 - The audience</strong>. I had many examples and references from other Kubernetes users (and their postmortem stories) in my talk. So I tried to give them credit and actually some of them were in the room and really liked that I did so. It gave them (and hopefully the rest of the audience as well) the feeling that I did not invent the wheel and we are all in the same boat. Also feel free to ask some questions in the beginning, e.g. to get a better feeling about who is attending your talk, or who would consider himself an expert in the area of what you are talking about, etc.</p><p><strong>Tip #21 - Repeat questions. Always</strong>. Because of the time constraints, questions should be asked at the end of your presentation (unless you are giving a community meeting or panel of course). Always (always!) repeat the questions at the end. Sometimes people will not use the microphone. This is not only hard for the people in the back, but also it won't be captured on the recording. I am sure you also had that moment watching a recording and not getting what is being asked/discussed because the question was not captured.</p><p><strong>Tip #22 - Feedback</strong>. Don't forget to ask the audience to fill out the survey. They're not always enforced/mandatory during conferences (especially not at KubeCon), so it's easy to forget to give the speaker feedback. Feedback is super critical (also for the committee) as sometimes people won't directly tell you but rather write their thoughts. Also, you might want to block your calendar to leave some time after the presentation for follow-up questions, so you are not in the hurry to catch your next meeting/session.</p><p><strong>Tip #23 - Invite your audience</strong>. No, I don't mean to throw a round of beer for everyone attending your talk (I mean, you could). But you might let them know, at the end of your presentation, that you would like to hang out, have dinner, etc. A great opportunity to reflect and geek out with like-minded friends.</p><p><strong>Final Tip - Your Voice matters</strong>. Don't underestimate the power of giving a talk at a conference. In my case I was lucky that the Zalando crew was in the room and took this talk as an opportunity for an ad hoc meeting after the conference. This drove an important performance fix forward, which eventually was <a href=https://github.com/kubernetes/kubernetes/pull/63437>merged</a> (kudos to the Zalando team again!).</p><p>Embrace the opportunity to give a talk at a conference, take it serious, be professional and make the best use of your time. But I'm sure I don't have to tell you that ;)</p><h2 id=now-it-s-on-you>Now it's on you :)</h2><p>I hope some of these tips are useful for you as well. And I wish you all the best for your upcoming talk!!! Believing in and being yourself is key to success. And perhaps your Kubernetes idol is in the room and has some nice words for you after your presentation!</p><p>Besides my fantastic reviewers and the speaker support team already mentioned above, I also would like to thank the people who supported me along this KubeCon journey: Bjoern, Timo, Emad and Steve!</p><blockquote class=twitter-tweet><p lang=en dir=ltr><a href="https://twitter.com/hashtag/KubeCon?src=hash&ref_src=twsrc%5Etfw">#KubeCon</a> <a href="https://twitter.com/embano1?ref_src=twsrc%5Etfw">@embano1</a> is laying down an homage to an earlier <a href="https://twitter.com/thockin?ref_src=twsrc%5Etfw">@thockin</a> presentation in his "Inside K8S Resource Management" session, if you didn't make it in the door, check out the deck here <a href=https://t.co/4xQq5CQ3aZ>https://t.co/4xQq5CQ3aZ</a> <a href=https://t.co/ObFEuKskhB>pic.twitter.com/ObFEuKskhB</a></p>&mdash; Steve Wong (@cantbewong) <a href="https://twitter.com/cantbewong/status/992400932968288256?ref_src=twsrc%5Etfw">May 4, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></div><div class=td-content style=page-break-before:always><h1 id=pg-5defc40e42ae29676f37e15b364e2948>Tips for Your First Kubecon Presentation - Part 1</h1><div class="td-byline mb-4"><time datetime=2018-10-18 class=text-muted>Thursday, October 18, 2018</time></div><p><strong>Author</strong>: Michael Gasch (VMware)</p><p>First of all, let me congratulate you to this outstanding achievement. Speaking at KubeCon, especially if it's your first time, is a tremendous honor and experience. Well done!</p><center><blockquote class=twitter-tweet><p lang=en dir=ltr>Congrats to everyone who got Kubecon talks accepted! 👏👏👏<br><br>To everyone who got a rejection don't feel bad. Only 13% could be accepted. Keep trying. There will be other opportunities.</p>&mdash; Justin Garrison (@rothgar) <a href="https://twitter.com/rothgar/status/1044345018490662912?ref_src=twsrc%5Etfw">September 24, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></center><p>When I was informed that my <a href="https://www.youtube.com/watch?v=8-apJyr2gi0">KubeCon talk about Kubernetes Resource Management</a> was accepted for KubeCon EU in Denmark (2018), I really could not believe it. By then, the chances to get your talk accepted were around 10% (or less, don't really remember the exact number). There were over a 1,000 submissions just for that KubeCon (recall that we now have <strong>three KubeCon events during the year</strong> - US, EU and Asia region). The popularity of Kubernetes is ever increasing and so is the number of people trying to get a talk accepted. Once again, <strong>outstanding achievement to get your talk in</strong>!</p><p>But now comes the tough part - research, write, practice, repeat, go on stage, perform :) Let me tell you that I went through several sleepless nights preparing for my first KubeCon talk. The day of the presentation, until I got on stage, was a mixture of every emotion I could have possibly gone through. Even though I had presented uncountable times before, including large industry conferences, KubeCon was very different. Different because it was the first time everything was recorded (including the presenter on stage) and I did not really know the audience, or more precisely: I was assuming everyone in the room is a Kubernetes expert and that my presentation not only had to be entertaining but also technically deep and 100% accurate. It's not seldom that maintainers and SIG (Special Interest Group) leads are in the room as well.</p><p>Another challenge for me was squeezing a topic, that can easily fill a full-day workshop, into a 35min presentation (including Q&A). Before KubeCon, I was used to giving breakouts which were typically 60 minutes long. This doesn't say anything about the quality of the presentation but I knew how many slides I can squeeze into 60 minutes, covering important details but not killing people with "Death by PowerPoint".</p><p>So I learned a lot going through this endless cycle of preparation, practicing, these doubts of failing and time pressure finishing your deck, and of course giving the talk. When I left Copenhagen, I took some notes based on my speaker experience during the flight, which my friend <a href=https://twitter.com/bbrundert>Bjoern</a> encouraged me to share. Not all of them might apply to you, but I still hope some of them are useful for your first KubeCon talk.</p><h2 id=submitting-a-good-talk>Submitting a Good Talk</h2><p>Some of you might read these lines even though you did not submit a talk or it wasn't accepted. I found these resources (not specifically targeted at KubeCon) for writing a good proposal very useful:</p><ul><li><a href=http://www.novelr.com/2008/08/16/vonnegut-how-to-write-with-style>How to write with Style</a></li><li><a href=https://docs.google.com/document/d/16llwMgq38wIt19Oj-TrunrPsfczrCNgvIqioslcdb6Q/edit>Talk Framework</a> by the incredible <a href=https://twitter.com/goinggodotnet>goinggodotnet</a></li><li><a href=https://medium.com/@LachlanEvenson/lachies-7-step-guide-to-writing-a-winning-tech-conference-cfp-4fa36a0d2672>Lachie’s 7 step guide to writing a winning tech conference CFP</a></li></ul><p>Believe it or not, mine went through several reviews by Justin Garrison, Liz Rice, Bill Kennedy, Emad Benjamin and Kelsey Hightower (yes, THE Kelsey Hightower)! Some of them didn't know me before, they just live by our community values to grow newcomers and thus drive this great community forward every day.</p><p>I think, without their feedback my proposal wouldn't have been on point to be selected. Their feedback was often direct and required me to completely change the first revisions. But they were right and their feedback was useful to stay within the character limit while still standing out with the proposal.</p><p>Feel free to reach out for professional and/or experienced speakers. They know this stuff. And I was surprised by the support and help offered. Many had their DMs in Twitter open, so ask for help and you will be helped :) Besides Twitter, related forums to ask for assistance might be <a href=https://discuss.kubernetes.io/>Discuss</a> and <a href=https://www.reddit.com/r/kubernetes/>Reddit</a>.</p><h2 id=preparing-for-your-presentation>Preparing for your Presentation</h2><p><strong>Tip #1 - Appreciate that you were selected</strong> and don't be upset about the slot your presentation was scheduled in. For example, my talk was put as second last presentation of final KubeCon day (Friday). I was like, who's going to stay there and not catch his/her flight or hang out and relax after this crazy week? The session stats, where speakers can see who signed up, were slowly increasing until the week of KubeCon. I think at the beginning of the week it showed ~80 people interested in my session (there is no mandatory sign-up). I was so happy, especially since there were some interesting talks running at the same time as my presentation.</p><p>Without spoiling (see below), the KubeCon community and attendees fully leverage the time and effort they've put into traveling to KubeCon. Rest assured that even if you have the last presentation at KubeCon, people will show up!</p><p><strong>Tip #2 - Study the Masters on <a href=https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA/playlists>Youtube</a></strong>. Get some inspirations from great speakers (some of them already mentioned above) and top rated sessions of previous KubeCons. Observe how they present and interact with the audience, while still keeping the tough timing throughout the presentation.</p><p><strong>Tip #3 - Find reviewers</strong>. Having experienced or professional speakers review your slides is super critical. Not only to check for language/translation (see below) but also to improve the flow of your presentation and get feedback on whether the content is logically structured and not too dense (too many slides, timing). They will help you to leave out less important information while also making the presentation fit for your audience (not everyone has the level of knowledge as you in your specific area).</p><p><strong>Tip #4 - Language barriers</strong>. Nobody is perfect and the community encourages diversity. This makes us all better and is what I probably like the most about the Kubernetes community. However, make sure that the audience understands the message of your talk. For non-native speakers, it can be really hard to present in English (e.g. at the US/EU conferences), especially if you're not used to it. Add to that the tension during the talk and it can become really hard for the audience to follow.</p><p>I am not saying that everyone has to present in perfect business English. Nobody expects that, let me be very clear. But if you feel that this could be an issue for you, reach out for help. Reviewers can help fix grammar and wording in your slide deck. Practicing and recording yourself (see below) are helpful to reflect yourself. The slides should reflect your message so people can read along if they lost you. Simple, less busy slides are definitely recommended. Make sure to add speaker notes to your presentation. Not only does this help with getting better every time you run through your presentation (memory effect and the flow). It also serves as a safety net when you think language will definitely be an issue, or when you're suddenly completely lost during the presentation on stage.</p><p><strong>Tip #5 - Study the Speaker Guidelines</strong>. Nothing to add here, take them seriously and reach out to the (fantastic) speaker support if you have questions. Also submit your presentation in time (plan ahead accordingly) to not risk any trouble with the committee.</p><p><strong>Tip #6 - Practice like never before</strong>. Practicing is probably the most important tip I can give you. I don't know how many times I practiced my talk, e.g. at home but also at some local Meetups to get some early feedback. First I was shocked with timing. Even though I had my deck down to 40min in my dry runs at home, at the Meetup I completely run out of time (50min). I was shocked, as I didn't know what to leave out.</p><p>The feedback from these sessions helped me to trim down content as it helped me to understand what to leave out/shorten. Keep in mind to also leave room for questions as a best practice (requirement?) by the speaker guidelines.</p><p><strong>Tip #7 - The Demo Gods are not always with you</strong>. Demos can and will go wrong. Not just because of the typical suspect like slow WiFi, etc. I heard horror stories about expired certificates, daylight saving times (for those traveling through time zones on their way to KubeCon) affecting deployments, the content of a variable in your BASH script changing (e.g. when curling stuff from the web), keyboards breaking (Mac lovers, can you believe that?), hard disks crashing (even the backup disk not working), and so on.</p><p>Never ever rely on the demo gods, especially when you're not Kelsey Hightower :) Take <a href=https://asciinema.org/>video recordings</a> of your demos so you not only have a backup when the live demo breaks. But also in case you're afraid of running out of time. In order to avoid the primary and backup disks crashing (yes, it happened at that KubeCon I was told), store a copy at your trusted cloud provider.</p><p><strong>Tip #8 - The right Tools for the job</strong>. Sometimes you want to highlight something on your slide. This has two potentially issues. First, you have to constantly turn away from the audience which does not necessarily look good (if you can avoid it). Second, it might not always work depending on the (laser) pointer and room equipment (light, background).</p><p><a href=https://www.logitech.com/en-us/product/spotlight-presentation-remote>This presenter</a> from Logitech has really served me well. It has several useful features, the "Spotlight" (that's why the name) being my favorite feature. You'll never want to go back.</p><p><strong>Tip #9 - Being recorded</strong>. I am not sure if you can opt-out from being recorded (please check with speaker support on the latest guidelines here) if you don't want to appear on Youtube for the rest of your life. But audio definitely will be recorded so chose your words (jokes) wisely. Again, practicing and reviewing helps. If you're ok with being recorded, at least think about which shirt (logos and "art") you want to be remembered by the internet ;)</p><p><strong>Tip #10 - Social media</strong>. Social media is great for promoting your session and you should definitely send out reminders on various channels for your presentation. Something that is missing almost every time in the presentation templates (if you want to use them) is placeholders for your social media account and, more importantly, for your session ID. Even if the conference does not use session IDs externally (in the schedule builder), you might still want to add a handle to every slide so people can refer to your presentation (or particular slide) on social media with a hashtag that you then can search for feedback, questions, etc.</p><p><strong>Tip #11 - Be yourself</strong>. Be authentic and don't try to sound super smart or funny (unless you are ;)). Seriously. Just be yourself and people will love you. Authenticity is key for a great presentation and the audience will smell when you try to fool them. It also makes practicing and the live performance easier as you don't have to pay attention on acting like somebody else.</p><p>From a content perspective make sure that <strong>you</strong> own and develop the content and you did not copy and paste like crazy from the Kubernetes docs or other presentations. It's absolutely ok to reference other sources, but please give them credit. Again, the audience will smell if you make things up. You should know what you're speaking about (not saying that you have to be an expert, but experience is what makes your talk unique). The final proof is during Q&A and KubeCon is a sharp audience ;)</p><p><strong>Tip #12 - Changes to the proposal</strong>. The committee, based on your proposal description and details, might change the audience level. For example, I put my talk in as intermediate, but it was changed to all skill levels. This is not bad per se. Just watch out for changes and adapt your presentation accordingly or reach out to speaker support. If you were not expecting beginners or architects in your talk (because you had chosen another skill level and target group), you might lose parts of your audience. This could also negatively affect your session feedback/scores.</p><h2 id=wrapping-up>Wrapping up</h2><p>I hope some of these tips are already useful and will help you getting started to work on your presentation. In the next post we are going to cover speaker tips when you are finally at the KubeCon event.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5acf5044e610fc3bc7a65c6616cdf99d>Kubernetes 2018 North American Contributor Summit</h1><div class="td-byline mb-4"><time datetime=2018-10-16 class=text-muted>Tuesday, October 16, 2018</time></div><p><strong>Authors:</strong>
<a href=https://twitter.com/mrbobbytables>Bob Killen</a> (University of Michigan)
<a href=https://twitter.com/sp_zala>Sahdev Zala</a> (IBM),
<a href=https://twitter.com/idvoretskyi>Ihor Dvoretskyi</a> (CNCF)</p><p>The 2018 North American Kubernetes Contributor Summit to be hosted right before
<a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/>KubeCon + CloudNativeCon</a> Seattle is shaping up to be the largest yet.
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.</p><p>Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed ‘hallway’ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the <a href=https://www.garagebilliards.com/>Garage Lounge and Gaming Hall</a>, just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.</p><p>Things pick up the following day, Monday the 10th with three separate tracks:</p><h3 id=new-contributor-workshop>New Contributor Workshop:</h3><p>A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.</p><h3 id=current-contributor-track>Current Contributor Track:</h3><p>Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the <a href=https://git.k8s.io/community/events/2018/12-contributor-summit#agenda>schedule in GitHub</a> as content is frequently being updated.</p><h3 id=docs-sprint>Docs Sprint:</h3><p>SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.</p><h2 id=to-register>To Register:</h2><p>To register for the Contributor Summit, see the <a href=https://git.k8s.io/community/events/2018/12-contributor-summit#registration>Registration section of the
Event Details in GitHub</a>. Please note that registrations are being
reviewed. If you select the “Current Contributor Track” and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.</p><p>If you have any questions or concerns, please don’t hesitate to reach out to
the Contributor Summit Events Team at <a href=mailto:community@kubernetes.io>community@kubernetes.io</a>.</p><p>Look forward to seeing everyone there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-241e2cb43c3584096fd288edfdbbdebb>2018 Steering Committee Election Results</h1><div class="td-byline mb-4"><time datetime=2018-10-15 class=text-muted>Monday, October 15, 2018</time></div><p><strong>Authors</strong>: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)</p><h2 id=results>Results</h2><p>The <a href=https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/>Kubernetes Steering Committee Election</a> is now complete and the following candidates came ahead to secure two year terms that start immediately:</p><ul><li>Aaron Crickenberger, Google, <a href=https://github.com/spiffxp>@spiffxp</a></li><li>Davanum Srinivas, Huawei, <a href=https://github.com/dims>@dims</a></li><li>Tim St. Clair, Heptio, <a href=https://github.com/timothysc>@timothysc</a></li></ul><h2 id=big-thanks>Big Thanks!</h2><ul><li>Steering Committee Member Emeritus <a href=https://github.com/quinton-hoole>Quinton Hoole</a> for his service to the community over the past year. We look forward to</li><li>The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.</li><li>All 307 voters who cast a ballot.</li><li>And last but not least...Cornell University for hosting <a href=https://civs.cs.cornell.edu/>CIVS</a>!</li></ul><h2 id=get-involved-with-the-steering-committee>Get Involved with the Steering Committee</h2><p>You can follow along to Steering Committee <a href=https://git.k8s.io/steering/backlog.md>backlog items</a> and weigh in by filing an issue or creating a PR against their <a href=https://github.com/kubernetes/steering>repo</a>. They meet bi-weekly on <a href=https://github.com/kubernetes/steering>Wednesdays at 8pm UTC</a> and regularly attend Meet Our Contributors.</p><p>Steering Committee Meetings:</p><ul><li><a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist</a></li></ul><p>Meet Our Contributors Steering AMA’s:</p><ul><li><a href=https://youtu.be/x6Jm8p0K-IQ>Oct 3 2018</a></li><li><a href=https://youtu.be/UbxWV12Or58>Sept 5 2018</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f460d202f8a5ec9dfaefd832c33cfa7a>Topology-Aware Volume Provisioning in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-10-11 class=text-muted>Thursday, October 11, 2018</time></div><p><strong>Author</strong>: Michelle Au (Google)</p><p>The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.</p><h2 id=previous-challenges>Previous challenges</h2><p>Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.</p><p>This resulted in unschedulable pods because volumes were provisioned in zones that:</p><ul><li>did not have enough CPU or memory resources to run the pod</li><li>conflicted with node selectors, pod affinity or anti-affinity policies</li><li>could not run the pod due to taints</li></ul><p>Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.</p><p>Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.</p><p>The topology-aware dynamic provisioning feature addresses all of the above issues.</p><h2 id=supported-volume-types>Supported Volume Types</h2><p>In 1.12, the following drivers support topology-aware dynamic provisioning:</p><ul><li>AWS EBS</li><li>Azure Disk</li><li>GCE PD (including Regional PD)</li><li>CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support</li></ul><h2 id=design-principles>Design Principles</h2><p>While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.</p><p>In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage system’s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.</p><h2 id=getting-started>Getting Started</h2><p>To enable this feature, all you need to do is to create a StorageClass with <code>volumeBindingMode</code> set to <code>WaitForFirstConsumer</code>:</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: pd-standard
</code></pre><p>This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass <code>zone</code> and <code>zones</code> parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.</p><p>Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:</p><ul><li>multiple PVCs in a pod</li><li>nodeAffinity across a subset of zones</li><li>pod anti-affinity on zones</li></ul><pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:   
  serviceName: &quot;nginx&quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: failure-domain.beta.kubernetes.io/zone
                operator: In
                values:
                - us-central1-a
                - us-central1-f
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: failure-domain.beta.kubernetes.io/zone
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
        - name: logs
          mountPath: /logs
 volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: topology-aware-standard
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: logs
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: topology-aware-standard
      resources:
        requests:
          storage: 1Gi
</code></pre><p>Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:</p><pre><code>$ kubectl get pv -o=jsonpath='{range .items[*]}{.spec.claimRef.name}{&quot;\t&quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&quot;\n&quot;}{end}'
www-web-0       us-central1-f
logs-web-0      us-central1-f
www-web-1       us-central1-a
logs-web-1      us-central1-a
</code></pre><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>Official documentation on the topology-aware dynamic provisioning feature is available <a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>here</a></p><p>Documentation for CSI drivers is available at <a href=https://kubernetes-csi.github.io/docs/>https://kubernetes-csi.github.io/docs/</a></p><h2 id=what-s-next>What’s next?</h2><p>We are actively working on improving this feature to support:</p><ul><li>more volume types, including dynamic provisioning for local volumes</li><li>dynamic volume attachable count and capacity limits per node</li></ul><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>If you have feedback for this feature or are interested in getting involved with the design and development, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special-Interest-Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p>Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing (<a href=https://github.com/verult>verult</a>), Chuqiang Li (<a href=https://github.com/lichuqiang>lichuqiang</a>), David Zhu (<a href=https://github.com/davidz627>davidz627</a>), Deep Debroy (<a href=https://github.com/ddebroy>ddebroy</a>), Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>), Jordan Liggitt (<a href=https://github.com/liggitt>liggitt</a>), Michelle Au (<a href=https://github.com/msau42>msau42</a>), Pengfei Ni (<a href=https://github.com/feiskyer>feiskyer</a>), Saad Ali (<a href=https://github.com/saad-ali>saad-ali</a>), Tim Hockin (<a href=https://github.com/thockin>thockin</a>), and Yecheng Fu (<a href=https://github.com/cofyc>cofyc</a>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-744fea7e150e5cd9616530e5a9339c47>Kubernetes v1.12: Introducing RuntimeClass</h1><div class="td-byline mb-4"><time datetime=2018-10-10 class=text-muted>Wednesday, October 10, 2018</time></div><p><strong>Author</strong>: Tim Allclair (Google)</p><p>Kubernetes originally launched with support for Docker containers running native applications on a Linux host. Starting with <a href=https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/>rkt</a> in Kubernetes 1.3 more runtimes were coming, which lead to the development of the <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>Container Runtime Interface</a> (CRI). Since then, the set of alternative runtimes has only expanded: projects like <a href=https://katacontainers.io/>Kata Containers</a> and <a href=https://github.com/google/gvisor>gVisor</a> were announced for stronger workload isolation, and Kubernetes' Windows support has been <a href=https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/>steadily progressing</a>.</p><p>With runtimes targeting so many different use cases, a clear need for mixed runtimes in a cluster arose. But all these different ways of running containers have brought a new set of problems to deal with:</p><ul><li>How do users know which runtimes are available, and select the runtime for their workloads?</li><li>How do we ensure pods are scheduled to the nodes that support the desired runtime?</li><li>Which runtimes support which features, and how can we surface incompatibilities to the user?</li><li>How do we account for the varying resource overheads of the runtimes?</li></ul><p><strong>RuntimeClass</strong> aims to solve these issues.</p><h2 id=runtimeclass-in-kubernetes-1-12>RuntimeClass in Kubernetes 1.12</h2><p>RuntimeClass was recently introduced as an alpha feature in Kubernetes 1.12. The initial implementation focuses on providing a runtime selection API, and paves the way to address the other open problems.</p><p>The RuntimeClass resource represents a container runtime supported in a Kubernetes cluster. The cluster provisioner sets up, configures, and defines the concrete runtimes backing the RuntimeClass. In its current form, a RuntimeClassSpec holds a single field, the <strong>RuntimeHandler</strong>. The RuntimeHandler is interpreted by the CRI implementation running on a node, and mapped to the actual runtime configuration. Meanwhile the PodSpec has been expanded with a new field, <strong>RuntimeClassName</strong>, which names the RuntimeClass that should be used to run the pod.</p><p>Why is RuntimeClass a pod level concept? The Kubernetes resource model expects certain resources to be shareable between containers in the pod. If the pod is made up of different containers with potentially different resource models, supporting the necessary level of resource sharing becomes very challenging. For example, it is extremely difficult to support a loopback (localhost) interface across a VM boundary, but this is a common model for communication between two containers in a pod.</p><h2 id=what-s-next>What's next?</h2><p>The RuntimeClass resource is an important foundation for surfacing runtime properties to the control plane. For example, to implement scheduler support for clusters with heterogeneous nodes supporting different runtimes, we might add <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>NodeAffinity</a> terms to the RuntimeClass definition. Another area to address is managing the variable resource requirements to run pods of different runtimes. The <a href=https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview>Pod Overhead proposal</a> was an early take on this that aligns nicely with the RuntimeClass design, and may be pursued further.</p><p>Many other RuntimeClass extensions have also been proposed, and will be revisited as the feature continues to develop and mature. A few more extensions that are being considered include:</p><ul><li>Surfacing optional features supported by runtimes, and better visibility into errors caused by incompatible features.</li><li>Automatic runtime or feature discovery, to support scheduling decisions without manual configuration.</li><li>Standardized or conformant RuntimeClass names that define a set of properties that should be supported across clusters with RuntimeClasses of the same name.</li><li>Dynamic registration of additional runtimes, so users can install new runtimes on existing clusters with no downtime.</li><li>"Fitting" a RuntimeClass to a pod's requirements. For instance, specifying runtime properties and letting the system match an appropriate RuntimeClass, rather than explicitly assigning a RuntimeClass by name.</li></ul><p>RuntimeClass will be under active development at least through 2019, and we’re excited to see the feature take shape, starting with the RuntimeClass alpha in Kubernetes 1.12.</p><h2 id=learn-more>Learn More</h2><ul><li>Take it for a spin! As an alpha feature, there are some additional setup steps to use RuntimeClass. Refer to the <a href=/docs/concepts/containers/runtime-class/#runtime-class>RuntimeClass documentation</a> for how to get it running.</li><li>Check out the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md>RuntimeClass Kubernetes Enhancement Proposal</a> for more nitty-gritty design details.</li><li>The <a href=https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview>Sandbox Isolation Level Decision</a> documents the thought process that initially went into making RuntimeClass a pod-level choice.</li><li>Join the discussions and help shape the future of RuntimeClass with the <a href=https://github.com/kubernetes/community/tree/master/sig-node>SIG-Node community</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-893d8df6ae10a8e8631db99cae979e87>Introducing Volume Snapshot Alpha for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-10-09 class=text-muted>Tuesday, October 09, 2018</time></div><p><strong>Author</strong>: Jing Xu (Google) Xing Yang (Huawei), Saad Ali (Google)</p><p>Kubernetes v1.12 introduces alpha support for volume snapshotting. This feature allows creating/deleting volume snapshots, and the ability to create new volumes from a snapshot natively using the Kubernetes API.</p><h2 id=what-is-a-snapshot>What is a Snapshot?</h2><p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a "snapshot" of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore the existing volume to a previous state (represented by the snapshot).</p><h2 id=why-add-snapshots-to-kubernetes>Why add Snapshots to Kubernetes?</h2><p>The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.</p><p>Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.</p><p>The <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage SIG</a> identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.</p><p>By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).</p><p>Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.</p><p>Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: such as data protection, data replication, and data migration.</p><h2 id=which-volume-plugins-support-kubernetes-snapshots>Which volume plugins support Kubernetes Snapshots?</h2><p>Kubernetes supports three types of volume plugins: in-tree, Flex, and CSI. See <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Kubernetes Volume Plugin FAQ</a> for details.</p><p>Snapshots are only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.</p><p>As of the publishing of this blog, the following CSI drivers support snapshots:</p><ul><li><a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>GCE Persistent Disk CSI Driver</a></li><li><a href=https://github.com/opensds/nbp/tree/master/csi/server>OpenSDS CSI Driver</a></li><li><a href=https://github.com/ceph/ceph-csi/tree/master/pkg/rbd>Ceph RBD CSI Driver</a></li><li><a href=https://github.com/libopenstorage/openstorage/tree/master/csi>Portworx CSI Driver</a></li></ul><p>Snapshot support for other <a href=https://kubernetes-csi.github.io/docs/drivers.html>drivers</a> is pending, and should be available soon. Read the “<a href=https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/>Container Storage Interface (CSI) for Kubernetes Goes Beta</a>” blog post to learn more about CSI and how to deploy CSI drivers.</p><h2 id=kubernetes-snapshots-api>Kubernetes Snapshots API</h2><p>Similar to the API for managing Kubernetes Persistent Volumes, Kubernetes Volume Snapshots introduce three new API objects for managing snapshots:</p><ul><li><code>VolumeSnapshot</code><ul><li>Created by a Kubernetes user to request creation of a snapshot for a specified volume. It contains information about the snapshot operation such as the timestamp when the snapshot was taken and whether the snapshot is ready to use.</li><li>Similar to the <code>PersistentVolumeClaim</code> object, the creation and deletion of this object represents a user desire to create or delete a cluster resource (a snapshot).</li></ul></li><li><code>VolumeSnapshotContent</code><ul><li>Created by the CSI volume driver once a snapshot has been successfully created. It contains information about the snapshot including snapshot ID.</li><li>Similar to the <code>PersistentVolume</code> object, this object represents a provisioned resource on the cluster (a snapshot).</li><li>Like <code>PersistentVolumeClaim</code> and <code>PersistentVolume</code> objects, once a snapshot is created, the <code>VolumeSnapshotContent</code> object binds to the VolumeSnapshot for which it was created (with a one-to-one mapping).</li></ul></li><li><code>VolumeSnapshotClass</code><ul><li>Created by cluster administrators to describe how snapshots should be created. including the driver information, the secrets to access the snapshot, etc.</li></ul></li></ul><p>It is important to note that unlike the core Kubernetes Persistent Volume objects, these Snapshot objects are defined as <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions>CustomResourceDefinitions (CRDs)</a>. The Kubernetes project is moving away from having resource types pre-defined in the API server, and is moving towards a model where the API server is independent of the API objects. This allows the API server to be reused for projects other than Kubernetes, and consumers (like Kubernetes) can simply install the resource types they require as CRDs.</p><p><a href=https://kubernetes-csi.github.io/docs/drivers.html>CSI Drivers</a> that support snapshots will automatically install the required CRDs. Kubernetes end users only need to verify that a CSI driver that supports snapshots is deployed on their Kubernetes cluster.</p><p>In addition to these new objects, a new, DataSource field has been added to the <code>PersistentVolumeClaim</code> object:</p><pre><code>type PersistentVolumeClaimSpec struct {
	AccessModes []PersistentVolumeAccessMode
	Selector *metav1.LabelSelector
	Resources ResourceRequirements
	VolumeName string
	StorageClassName *string
	VolumeMode *PersistentVolumeMode
	DataSource *TypedLocalObjectReference
}
</code></pre><p>This new alpha field enables a new volume to be created and automatically pre-populated with data from an existing snapshot.</p><h2 id=kubernetes-snapshots-requirements>Kubernetes Snapshots Requirements</h2><p>Before using Kubernetes Volume Snapshotting, you must:</p><ul><li>Ensure a CSI driver implementing snapshots is deployed and running on your Kubernetes cluster.</li><li>Enable the Kubernetes Volume Snapshotting feature via new Kubernetes feature gate (disabled by default for alpha):<ul><li>Set the following flag on the API server binary: <code>--feature-gates=VolumeSnapshotDataSource=true</code></li></ul></li></ul><p>Before creating a snapshot, you also need to specify CSI driver information for snapshots by creating a <code>VolumeSnapshotClass</code> object and setting the <code>snapshotter</code> field to point to your CSI driver. In the example of <code>VolumeSnapshotClass</code> below, the CSI driver is <code>com.example.csi-driver</code>. You need at least one <code>VolumeSnapshotClass</code> object per snapshot provisioner. You can also set a default <code>VolumeSnapshotClass</code> for each individual CSI driver by putting an annotation <code>snapshot.storage.kubernetes.io/is-default-class: "true"</code> in the class definition.</p><pre><code>apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotClass
metadata:
  name: default-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot;
snapshotter: com.example.csi-driver


apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapclass
snapshotter: com.example.csi-driver
parameters:
  fakeSnapshotOption: foo
  csiSnapshotterSecretName: csi-secret
  csiSnapshotterSecretNamespace: csi-namespace
</code></pre><p>You must set any required opaque parameters based on the documentation for your CSI driver. As the example above shows, the parameter <code>fakeSnapshotOption: foo</code> and any referenced secret(s) will be passed to CSI driver during snapshot creation and deletion. The <a href=https://github.com/kubernetes-csi/external-snapshotter>default CSI external-snapshotter</a> reserves the parameter keys <code>csiSnapshotterSecretName</code> and <code>csiSnapshotterSecretNamespace</code>. If specified, it fetches the secret and passes it to the CSI driver when creating and deleting a snapshot.</p><p>And finally, before creating a snapshot, you must provision a volume using your CSI driver and populate it with some data that you want to snapshot (see the <a href=https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/>CSI blog post</a> on how to create and use CSI volumes).</p><h2 id=creating-a-new-snapshot-with-kubernetes>Creating a new Snapshot with Kubernetes</h2><p>Once a <code>VolumeSnapshotClass</code> object is defined and you have a volume you want to snapshot, you may create a new snapshot by creating a <code>VolumeSnapshot</code> object.</p><p>The source of the snapshot specifies the volume to create a snapshot from. It has two parameters:</p><ul><li><code>kind</code> - must be <code>PersistentVolumeClaim</code></li><li><code>name</code> - the PVC API object name</li></ul><p>The namespace of the volume to snapshot is assumed to be the same as the namespace of the <code>VolumeSnapshot</code> object.</p><pre><code>apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-demo
  namespace: demo-namespace
spec:
  snapshotClassName: csi-snapclass
  source:
    name: mypvc
    kind: PersistentVolumeClaim
</code></pre><p>In the <code>VolumeSnapshot</code> spec, user can specify the <code>VolumeSnapshotClass</code> which has the information about which CSI driver should be used for creating the snapshot . When the <code>VolumeSnapshot</code> object is created, the parameter <code>fakeSnapshotOption: foo</code> and any referenced secret(s) from the <code>VolumeSnapshotClass</code> are passed to the CSI plugin <code>com.example.csi-driver</code> via a <code>CreateSnapshot</code> call.</p><p>In response, the CSI driver triggers a snapshot of the volume and then automatically creates a <code>VolumeSnapshotContent</code> object to represent the new snapshot, and binds the new <code>VolumeSnapshotContent</code> object to the <code>VolumeSnapshot</code>, making it ready to use. If the CSI driver fails to create the snapshot and returns error, the snapshot controller reports the error in the status of <code>VolumeSnapshot</code> object and does not retry (this is different from other controllers in Kubernetes, and is to prevent snapshots from being taken at an unexpected time).</p><p>If a snapshot class is not specified, the external snapshotter will try to find and set a default snapshot class for the snapshot. The <code>CSI driver</code> specified by <code>snapshotter</code> in the default snapshot class must match the <code>CSI driver</code> specified by the <code>provisioner</code> in the storage class of the PVC.</p><p>Please note that the alpha release of Kubernetes Snapshot does not provide any consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency.</p><p>You can verify that the <code>VolumeSnapshot</code> object is created and bound with <code>VolumeSnapshotContent</code> by running <code>kubectl describe volumesnapshot</code>:</p><ul><li><code>Ready</code> should be set to true under <code>Status</code> to indicate this volume snapshot is ready for use.</li><li><code>Creation Time</code> field indicates when the snapshot is actually created (cut).</li><li><code>Restore Size</code> field indicates the minimum volume size when restoring a volume from the snapshot.</li><li><code>Snapshot Content Name</code> field in the <code>spec</code> points to the <code>VolumeSnapshotContent</code> object created for this snapshot.</li></ul><h2 id=importing-an-existing-snapshot-with-kubernetes>Importing an existing snapshot with Kubernetes</h2><p>You can always import an existing snapshot to Kubernetes by manually creating a <code>VolumeSnapshotContent</code> object to represent the existing snapshot. Because <code>VolumeSnapshotContent</code> is a non-namespace API object, only a system admin may have the permission to create it. Once a <code>VolumeSnapshotContent</code> object is created, the user can create a <code>VolumeSnapshot</code> object pointing to the <code>VolumeSnapshotContent</code> object. The external-snapshotter controller will mark snapshot as ready after verifying the snapshot exists and the binding between <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> objects is correct. Once bound, the snapshot is ready to use in Kubernetes.</p><p>A <code>VolumeSnapshotContent</code> object should be created with the following fields to represent a pre-provisioned snapshot:</p><ul><li><code>csiVolumeSnapshotSource</code> - Snapshot identifying information.<ul><li><code>snapshotHandle</code> - name/identifier of the snapshot. This field is required.</li><li><code>driver</code> - CSI driver used to handle this volume. This field is required. It must match the snapshotter name in the snapshot controller.</li><li><code>creationTime</code> and <code>restoreSize</code> - these fields are not required for pre-provisioned volumes. The external-snapshotter controller will automatically update them after creation.</li></ul></li><li><code>volumeSnapshotRef</code> - Pointer to the <code>VolumeSnapshot</code> object this object should bind to.<ul><li><code>name</code> and <code>namespace</code> - It specifies the name and namespace of the <code>VolumeSnapshot</code> object which the content is bound to.</li><li><code>UID</code> - these fields are not required for pre-provisioned volumes.The external-snapshotter controller will update the field automatically after binding. If user specifies UID field, he/she must make sure that it matches with the binding snapshot’s UID. If the specified UID does not match the binding snapshot’s UID, the content is considered an orphan object and the controller will delete it and its associated snapshot.</li></ul></li><li><code>snapshotClassName</code> - This field is optional. The external-snapshotter controller will update the field automatically after binding.</li></ul><pre><code>apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
metadata:
  name: static-snapshot-content
spec:
  csiVolumeSnapshotSource:
    driver: com.example.csi-driver
    snapshotHandle: snapshotcontent-example-id
  volumeSnapshotRef:
    kind: VolumeSnapshot
    name: static-snapshot-demo
    namespace: demo-namespace
</code></pre><p>A <code>VolumeSnapshot</code> object should be created to allow a user to use the snapshot:</p><ul><li><code>snapshotClassName</code> - name of the volume snapshot class. This field is optional. If set, the snapshotter field in the snapshot class must match the snapshotter name of the snapshot controller. If not set, the snapshot controller will try to find a default snapshot class.</li><li><code>snapshotContentName</code> - name of the volume snapshot content. This field is required for pre-provisioned volumes.</li></ul><pre><code>apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: static-snapshot-demo
  namespace: demo-namespace
spec:
  snapshotClassName: csi-snapclass
  snapshotContentName: static-snapshot-content
</code></pre><p>Once these objects are created, the snapshot controller will bind them together, and set the field Ready (under <code>Status</code>) to True to indicate the snapshot is ready to use.</p><h2 id=provision-a-new-volume-from-a-snapshot-with-kubernetes>Provision a new volume from a snapshot with Kubernetes</h2><p>To provision a new volume pre-populated with data from a snapshot object, use the new dataSource field in the <code>PersistentVolumeClaim</code>. It has three parameters:</p><ul><li>name - name of the <code>VolumeSnapshot</code> object representing the snapshot to use as source</li><li>kind - must be <code>VolumeSnapshot</code></li><li>apiGroup - must be <code>snapshot.storage.k8s.io</code></li></ul><p>The namespace of the source <code>VolumeSnapshot</code> object is assumed to be the same as the namespace of the <code>PersistentVolumeClaim</code> object.</p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-restore
  Namespace: demo-namespace
spec:
  storageClassName: csi-storageclass
  dataSource:
    name: new-snapshot-demo
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
</code></pre><p>When the <code>PersistentVolumeClaim</code> object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified snapshot.</p><h2 id=as-a-storage-vendor-how-do-i-add-support-for-snapshots-to-my-csi-driver>As a storage vendor, how do I add support for snapshots to my CSI driver?</h2><p>To implement the snapshot feature, a CSI driver MUST add support for additional controller capabilities <code>CREATE_DELETE_SNAPSHOT</code> and <code>LIST_SNAPSHOTS</code>, and implement additional controller RPCs: <code>CreateSnapshot</code>, <code>DeleteSnapshot</code>, and <code>ListSnapshots</code>. For details, see <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>the CSI spec</a>.</p><p>Although Kubernetes is as <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers>minimally prescriptive</a> on the packaging and deployment of a CSI Volume Driver as possible, it provides a <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes>suggested mechanism</a> for deploying an arbitrary containerized CSI driver on Kubernetes to simplify deployment of containerized CSI compatible volume drivers.</p><p>As part of this recommended deployment process, the Kubernetes team provides a number of sidecar (helper) containers, including a new <a href=https://github.com/kubernetes-csi/external-snapshotter>external-snapshotter</a> sidecar container.</p><p>The external-snapshotter watches the Kubernetes API server for <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> objects and triggers CreateSnapshot and DeleteSnapshot operations against a CSI endpoint. The CSI <a href=https://github.com/kubernetes-csi/external-provisioner>external-provisioner</a> sidecar container has also been updated to support restoring volume from snapshot using the new <code>dataSource</code> PVC field.</p><p>In order to support snapshot feature, it is recommended that storage vendors deploy the external-snapshotter sidecar containers in addition to the external provisioner the external attacher, along with their CSI driver in a statefulset as shown in the following diagram.</p><p><img src=/images/blog/2018-10-09-volume-snapshot-alpha/snapshot.png alt></p><p>In this <a href=https://github.com/kubernetes-csi/external-snapshotter/blob/e011fe31df548813d2eb6dacb278c0ca58533b34/deploy/kubernetes/setup-csi-snapshotter.yaml>example deployment yaml</a> file, two sidecar containers, the external provisioner and the external snapshotter, and CSI drivers are deployed together with the hostpath CSI plugin in the statefulset pod. Hostpath CSI plugin is a sample plugin, not for production.</p><h2 id=what-are-the-limitations-of-alpha>What are the limitations of alpha?</h2><p>The alpha implementation of snapshots for Kubernetes has the following limitations:</p><ul><li>Does not support reverting an existing volume to an earlier state represented by a snapshot (alpha only supports provisioning a new volume from a snapshot).</li><li>Does not support “in-place restore” of an existing PersistentVolumeClaim from a snapshot: i.e. provisioning a new volume from a snapshot, but updating an existing PersistentVolumeClaim to point to the new volume and effectively making the PVC appear to revert to an earlier state (alpha only supports using a new volume provisioned from a snapshot via a new PV/PVC).</li><li>No snapshot consistency guarantees beyond any guarantees provided by storage system (e.g. crash consistency).</li></ul><h2 id=what-s-next>What’s next?</h2><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to beta in either 1.13 or 1.14.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>Check out additional documentation on the snapshot feature here: <a href=http://k8s.io/docs/concepts/storage/volume-snapshots>http://k8s.io/docs/concepts/storage/volume-snapshots</a> and <a href=https://kubernetes-csi.github.io/docs/>https://kubernetes-csi.github.io/docs/</a></p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>In addition to the contributors who have been working on the Snapshot feature:</p><ul><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li><li>Jing Xu (<a href=https://github.com/jingxu97>jingxu97</a>)</li><li>Huamin Chen (<a href=https://github.com/rootfs>rootfs</a>)</li><li>Tomas Smetana (<a href=https://github.com/tsmetana>tsmetana</a>)</li><li>Shiwei Xu (<a href=https://github.com/wackxu>wackxu</a>)</li></ul><p>We offer a huge thank you to all the contributors in Kubernetes Storage SIG and CSI community who helped review the design and implementation of the project, including but not limited to the following:</p><ul><li>Saad Ali (<a href=https://github.com/saadali>saadali</a>)</li><li>Tim Hockin (<a href=https://github.com/thockin>thockin</a>)</li><li>Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>)</li><li>Luis Pabon (<a href=https://github.com/lpabon>lpabon</a>)</li><li>Jordan Liggitt (<a href=https://github.com/liggitt>liggitt</a>)</li><li>David Zhu (<a href=https://github.com/davidz627>davidz627</a>)</li><li>Garth Bushell (<a href=https://github.com/garthy>garthy</a>)</li><li>Ardalan Kangarlou (<a href=https://github.com/kangarlou>kangarlou</a>)</li><li>Seungcheol Ko (<a href=https://github.com/sngchlko>sngchlko</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>msau42</a>)</li><li>Humble Devassy Chirammal (<a href=https://github.com/humblec>humblec</a>)</li><li>Vladimir Vivien (<a href=https://github.com/vladimirvivien>vladimirvivien</a>)</li><li>John Griffith (<a href=https://github.com/j-griffith>j-griffith</a>)</li><li>Bradley Childs (<a href=https://github.com/childsb>childsb</a>)</li><li>Ben Swartzlander (<a href=https://github.com/bswartz>bswartz</a>)</li></ul><p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-af9abd5ba8bd4dd15dc8503f6dc2b1ff>Support for Azure VMSS, Cluster-Autoscaler and User Assigned Identity</h1><div class="td-byline mb-4"><time datetime=2018-10-08 class=text-muted>Monday, October 08, 2018</time></div><p><strong>Author</strong>: <a href=https://twitter.com/kkwriting>Krishnakumar R (KK)</a> (Microsoft), <a href=https://twitter.com/feisky>Pengfei Ni</a> (Microsoft)</p><h2 id=introduction>Introduction</h2><p>With Kubernetes v1.12, Azure virtual machine scale sets (VMSS) and cluster-autoscaler have reached their General Availability (GA) and User Assigned Identity is available as a preview feature.</p><p><em>Azure VMSS allow you to create and manage identical, load balanced VMs that automatically increase or decrease based on demand or a set schedule. This enables you to easily manage and scale multiple VMs to provide high availability and application resiliency, ideal for large-scale applications like container workloads <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview>[1]</a>.</em></p><p>Cluster autoscaler allows you to adjust the size of the Kubernetes clusters based on the load conditions automatically.</p><p>Another exciting feature which v1.12 brings to the table is the ability to use User Assigned Identities with Kubernetes clusters <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview>[12]</a>.</p><p>In this article, we will do a brief overview of VMSS, cluster autoscaler and user assigned identity features on Azure.</p><h2 id=vmss>VMSS</h2><p>Azure’s Virtual Machine Scale sets (VMSS) feature offers users an ability to automatically create VMs from a single central configuration, provide load balancing via L4 and L7 load balancing, provide a path to use availability zones for high availability, provides large-scale VM instances et. al.</p><p>VMSS consists of a group of virtual machines, which are identical and can be managed and configured at a group level. More details of this feature in Azure itself can be found at the following link <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview>[1]</a>.</p><p>With Kubernetes v1.12 customers can create k8s cluster out of VMSS instances and utilize VMSS features.</p><h2 id=cluster-components-on-azure>Cluster components on Azure</h2><p>Generally, standalone Kubernetes cluster in Azure consists of the following parts</p><ul><li>Compute - the VM itself and its properties.</li><li>Networking - this includes the IPs and load balancers.</li><li>Storage - the disks which are associated with the VMs.</li></ul><h2 id=compute>Compute</h2><p>Compute in cloud k8s cluster consists of the VMs. These VMs are created by provisioning tools such as acs-engine or AKS (in case of managed service). Eventually, they run various system daemons such as kubelet, kube-api server etc. either as a process (in some versions) or as a docker container.</p><p><img src=/images/blog/2018-10-08-support-for-azure-vmss/sample-azure-cluster.png alt></p><h2 id=networking>Networking</h2><p>In Azure Kubernetes cluster various networking components are brought together to provide features required for users. Typically they consist of the network interfaces, network security groups, public IP resource, VNET (virtual networks), load balancers etc.</p><h2 id=storage>Storage</h2><p>Kubernetes clusters are built on top of disks created in Azure. In a typical configuration, we have managed disks which are used to hold the regular OS images and a separate disk is used for etcd.</p><h2 id=cloud-provider-components>Cloud provider components</h2><p><img src=/images/blog/2018-10-08-support-for-azure-vmss/cloud-provider-components.png alt></p><p>Kubernetes cloud provider interface provides interactions with clouds for managing cloud-specific resources, e.g. public IPs and routes. A good overview of these components is given in <a href=/docs/concepts/architecture/cloud-controller/>[2]</a>. In case of Azure Kubernetes cluster, the Kubernetes interactions go through the Azure cloud provider layer and contact the various services running in the cloud.</p><p>The cloud provider implementation of K8s can be largely divided into the following component interfaces which we need to implement:</p><ol><li>Load Balancer</li><li>Instances</li><li>Zones</li><li>Routes</li></ol><p>In addition to the above interfaces, the storage services from the cloud provider is linked via the volume plugin layer.</p><h2 id=azure-cloud-provider-implementation-and-vmss>Azure cloud provider implementation and VMSS</h2><p>In the Azure cloud provider, for every type of cluster we implement, there is a VMType option which we specify. In case of VMSS, the VM type is “vmss”. The provisioning software (acs-engine, in future AKS etc.) would setup these values in /etc/kubernetes/azure.json file. Based on this type, various implementations would get instantiated <a href=https://github.com/kubernetes/kubernetes/blob/release-1.17/staging/src/k8s.io/legacy-cloud-providers/azure/azure_vmss.go>[3]</a></p><p>The load balancer interface provides access to the underlying cloud provider load balancer service. The information about the load balancers and the control operations on them are required for Kubernetes to handle the services which gets hosted on the Kubernetes cluster. For VMSS support the changes ensure that the VMSS instances are part of the load balancer pool as required.</p><p>The instances interfaces help the cloud controller to get various details about a node from the cloud provider layer. For example, the details of a node like the IP address, the instance id etc, is obtained by the controller by means of the instances interfaces which the cloud provider layer registers with it. In case of VMSS support, we talk to VMSS service to gather information regarding the instances.</p><p>The zones interfaces help the cloud controller to get zone information for each node. Scheduler could spread pods to different availability zones with such information. It is also required for supporting topology aware dynamic provisioning features, e.g. AzureDisk. Each VMSS instances will be labeled with its current zone and region.</p><p>The routes interfaces help the cloud controller to setup advanced routes for Pod network. For example, a route with prefix node’s podCIDR and next hop node’s internal IP will be set for each node. In case of VMSS support, the next hops are VMSS virtual machines’ internal IP address.</p><p>The Azure volume plugin interfaces have been modified for VMSS to work properly. For example, the attach/detach to the AzureDisk have been modified to perform these operations at VMSS instance level.</p><h2 id=setting-up-a-vmss-cluster-on-azure>Setting up a VMSS cluster on Azure</h2><p>The following link <a href=https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md>[4]</a> provides an example of acs-engine to create a Kubernetes cluster.</p><pre><code>acs-engine deploy --subscription-id &lt;subscription id&gt; \
    --dns-prefix &lt;dns&gt; --location &lt;location&gt; \
    --api-model examples/kubernetes.json
</code></pre><p>API model file provides various configurations which acs-engine uses to create a cluster. The API model here <a href=https://github.com/Azure/acs-engine/blob/master/examples/kubernetes-vmss/kubernetes.json>[5]</a> gives a good starting configuration to setup the VMSS cluster.</p><p>Once a VMSS cluster is created, here are some of the steps you can run to understand more about the cluster setup. Here is the output of kubectl get nodes from a cluster created using the above command:</p><pre><code>$ kubectl get nodes
NAME                                 STATUS    ROLES     AGE       VERSION
k8s-agentpool1-92998111-vmss000000   Ready     agent     1h        v1.12.0-rc.2
k8s-agentpool1-92998111-vmss000001   Ready     agent     1h        v1.12.0-rc.2
k8s-master-92998111-0                Ready     master    1h        v1.12.0-rc.2
</code></pre><p>This cluster consists of two worker nodes and one master. Now how do we check which node is which in Azure parlance? In VMSS listing, we can see a single VMSS:</p><pre><code>$ az vmss list -o table -g k8sblogkk1
Name                          ResourceGroup    Location    Zones      Capacity  Overprovision    UpgradePolicy
----------------------------  ---------------  ----------  -------  ----------  ---------------  ---------------
k8s-agentpool1-92998111-vmss  k8sblogkk1       westus2                       2  False            Manual
</code></pre><p>The nodes which we see as agents (in the kubectl get nodes command) are part of this vmss. We can use the following command to list the instances which are part of the VM scale set:</p><pre><code>$ az vmss list-instances -g k8sblogkk1 -n k8s-agentpool1-92998111-vmss -o table
  InstanceId  LatestModelApplied    Location    Name                            ProvisioningState    ResourceGroup    VmId
------------  --------------------  ----------  ------------------------------  -------------------  ---------------  ------------------------------------
           0  True                  westus2     k8s-agentpool1-92998111-vmss_0  Succeeded            K8SBLOGKK1       21c57d6c-9c8f-4a62-970f-63ed0fcba53f
           1  True                  westus2     k8s-agentpool1-92998111-vmss_1  Succeeded            K8SBLOGKK1       840743b9-0076-4a2e-920e-5ba9da296665
</code></pre><p>The node name does not match the name in the vm scale set, but if we run the following command to list the providerID we can find the matching node which resembles the instance name:</p><pre><code>$  kubectl describe nodes k8s-agentpool1-92998111-vmss000000| grep ProviderID
ProviderID:                  azure:///subscriptions/&lt;subscription id&gt;/resourceGroups/k8sblogkk1/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agentpool1-92998111-vmss/virtualMachines/0
</code></pre><h2 id=current-status-and-future>Current Status and Future</h2><p>Currently the following is supported:</p><ol><li>VMSS master nodes and worker nodes</li><li>VMSS on worker nodes and Availability set on master nodes combination.</li><li>Per vm disk attach</li><li>Azure Disk & Azure File support</li><li>Availability zones (Alpha)</li></ol><p>In future there will be support for the following:</p><ol><li>AKS with VMSS support</li><li>Per VM instance public IP</li></ol><h2 id=cluster-autoscaler>Cluster Autoscaler</h2><p>A Kubernetes cluster consists of nodes. These nodes can be virtual machines, bare metal servers or could be even virtual node (virtual kubelet). To avoid getting lost in permutations and combinations of Kubernetes ecosystem ;-), let's consider that the cluster we are discussing consists of virtual machines, which are hosted in a cloud (eg: Azure, Google or AWS). What this effectively means is that you have access to virtual machines which run Kubernetes agents and a master node which runs k8s services like API server. A detailed version of k8s architecture can be found here <a href=/docs/concepts/architecture/>[11]</a>.</p><p>The number of nodes which are required on a cluster depends on the workload on the cluster. When the load goes up there is a need to increase the nodes and when it subsides, there is a need to reduce the nodes and clean up the resources which are no longer in use. One way this can be taken care of is to manually scale up the nodes which are part of the Kubernetes cluster and manually scale down when the demand reduces. But shouldn’t this be done automatically ? Answer to this question is the Cluster Autoscaler (CA).</p><p>The cluster autoscaler itself runs as a pod within the kubernetes cluster. The following figure illustrates the high level view of the setup with respect to the k8s cluster:</p><p><img src=/images/blog/2018-10-08-support-for-azure-vmss/cluster-autoscaler.png alt></p><p>Since Cluster Autoscaler is a pod within the k8s cluster, it can use the in-cluster config and the Kubernetes go client <a href=https://github.com/kubernetes/client-go>[10]</a> to contact the API server.</p><h2 id=internals>Internals</h2><p>The API server is the central service which manages the state of the k8s cluster utilizing a backing store (an etcd database), runs on the management node or runs within the cloud (in case of managed service such as AKS). For any component within the Kubernetes cluster to figure out the state of the cluster, like for example the nodes registered in the cluster, contacting the API server is the way to go.</p><p>In order to simplify our discussion let’s divide the CA functionality into 3 parts as given below:</p><p><img src=/images/blog/2018-10-08-support-for-azure-vmss/ca-functionality.png alt></p><p>The main portion of the CA is a control loop which keeps running at every scan interval. This loop is responsible for updating the autoscaler metrics and health probes. Before this loop is entered auto scaler performs various operations such as claiming the leader state after performing a Kubernetes leader election. The main loop initializes static autoscaler component. This component initializes the underlying cloud provider based on the parameters passed onto the CA.</p><p>Various operations performed by the CA to manage the state of the cluster is passed onto the cloud provider component. Some examples like - increase target size, decrease target size etc, results in the cloud provider component talking to the cloud services internally and performing operations such as adding a node or deleting a node. These operations are performed on group of nodes in the cluster. The static autoscaler also keeps tab on the state of the system by querying the API server - operations such as list pods and list nodes are used to get hold of such information.</p><p>The decision to make a scale up is based on pods which remain unscheduled and a variety of checks and balances. The nodes which are free to be scaled down are deleted from the cluster and deleted from the cloud itself. The cluster autoscaler applies checks and balances before scaling up and scaling down - for example the nodes which have been recently added are given special consideration. During the deletion the nodes are drained to ensure that no disruption happens to the running pods.</p><h2 id=setting-up-ca-on-azure>Setting up CA on Azure:</h2><p>Cluster Autoscaler is available as an add-on with acs-engine. The following link <a href=https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler>[15]</a> has an example configuration file used to deploy autoscaler with acs-engine. The following link <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md>[8]</a> provides details on manual step by step way to do the same.</p><p>In acs-engine case we use the regular command line to deploy:</p><pre><code>acs-engine deploy --subscription-id &lt;subscription id&gt; \
    --dns-prefix &lt;dns&gt; --location &lt;location&gt; \
    --api-model examples/kubernetes.json
</code></pre><p>The main difference are the following lines in the config file at <a href=https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler>[15]</a> makes sure that CA is deployed as an addon:</p><pre><code>&quot;addons&quot;: [
          {
            &quot;name&quot;: &quot;cluster-autoscaler&quot;,
            &quot;enabled&quot;: true,
            &quot;config&quot;: {
              &quot;minNodes&quot;: &quot;1&quot;,
              &quot;maxNodes&quot;: &quot;5&quot;
            }
          }
        ]
</code></pre><p>The config section in the json above can be used to provide the configuration to the cluster autoscaler pod, eg: min and max nodes as above.</p><p>Once the setup completes we can see that the cluster-autoscaler pod is deployed in the system namespace:</p><pre><code>$kubectl get pods -n kube-system  | grep autoscaler
cluster-autoscaler-7bdc74d54c-qvbjs             1/1       Running             1          6m
</code></pre><p>Here is the output from the CA configmap and events from a sample cluster:</p><pre><code>$kubectl -n kube-system describe configmap cluster-autoscaler-status
Name:         cluster-autoscaler-status
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  cluster-autoscaler.kubernetes.io/last-updated=2018-10-02 01:21:17.850010508 +0000 UTC

Data
====
status:
----
Cluster-autoscaler status at 2018-10-02 01:21:17.850010508 +0000 UTC:
Cluster-wide:
  Health:      Healthy (ready=3 unready=0 notStarted=0 longNotStarted=0 registered=3 longUnregistered=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleUp:     NoActivity (ready=3 registered=3)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:39:50.493307405 +0000 UTC m=+674.133759650

NodeGroups:
  Name:        k8s-agentpool1-92998111-vmss
  Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=5))
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:39:50.493307405 +0000 UTC m=+674.133759650


Events:
  Type    Reason          Age   From                Message
  ----    ------          ----  ----                -------
  Normal  ScaleDownEmpty  42m   cluster-autoscaler  Scale-down: removing empty node k8s-agentpool1-92998111-vmss000002
</code></pre><p>As can be seen the events, the cluster autoscaler scaled down and deleted a node as there was no load on this cluster. The rest of the configmap in this case indicates that there are no further actions which the autoscaler is taking at this moment.</p><h2 id=current-status-and-future-1>Current status and future:</h2><p>Cluster Autoscaler currently supports four VM types: standard (VMAS), VMSS, ACS and AKS. In the future, Cluster Autoscaler will be integrated within AKS product, so that users can enable it by one-click.</p><h2 id=user-assigned-identity>User Assigned Identity</h2><p>Inorder for the Kubernetes cluster components to securely talk to the cloud services, it needs to authenticate with the cloud provider. In Azure Kubernetes clusters, up until now this was done using two ways - Service Principals or Managed Identities. In case of service principal the credentials are stored within the cluster and there are password rotation and other challenges which user needs to incur to accommodate this model. Managed service identities takes out this burden from the user and manages the service instances directly <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview>[12]</a>.</p><p>There are two kinds of managed identities possible - one is system assigned and another is user assigned. In case of system assigned identity each vm in the Kubernetes cluster is assigned a managed identity during creation. This identity is used by various Kubernetes components needing access to Azure resources. Examples to these operations are getting/updating load balancer configuration, getting/updating vm information etc. With the system assigned managed identity, user has no control over the identity which is assigned to the underlying vm. The system automatically assigns it and this reduces the flexibility for the user.</p><p>With v1.12 we bring user assigned managed identity support for Kubernetes. With this support user does not have to manage any passwords but at the same time has the flexibility to manage the identity which is used by the cluster. For example if the user needs to allow access to a cluster for a specific storage account or a Azure key vault, the user assigned identity can be created in advance and key vault access provided.</p><h2 id=internals-1>Internals</h2><p>To understand the internals, we will focus on a cluster created using acs-engine. This can be configured in other ways, but the basic interactions are of the same pattern.</p><p>The acs-engine sets up the cluster with the required configuration. The /etc/kubernetes/azure.json file provides a way for the cluster components (eg: kube-apiserver) to gather configuration on how to access the cloud resources. In a user managed identity cluster there is a value filled with the key as <code>UserAssignedIdentityID</code>. This value is filled with the client id of the user assigned identity created by acs-engine or provided by the user, however the case may be. The code which does the authentication for Kubernetes on azure can be found here <a href=https://github.com/kubernetes/kubernetes/blob/release-1.17/staging/src/k8s.io/legacy-cloud-providers/azure/auth/azure_auth.go>[14]</a>. This code uses Azure adal packages to get authenticated to access various resources in the cloud. In case of user assigned identity the following API call is made to get new token:</p><pre><code>adal.NewServicePrincipalTokenFromMSIWithUserAssignedID(msiEndpoint,
env.ServiceManagementEndpoint,
config.UserAssignedIdentityID)
</code></pre><p>This calls hits either the instance metadata service or the vm extension <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview>[12]</a> to gather the token which is then used to access various resources.</p><h2 id=setting-up-a-cluster-with-user-assigned-identity>Setting up a cluster with user assigned identity</h2><p>With the upstream support for user assigned identity in v1.12, it is now supported in the acs-engine to create a cluster with the user assigned identity. The json config files present here <a href=https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned>[13]</a> can be used to create a cluster with user assigned identity. The same step used to create a vmss cluster can be used to create a cluster which has user assigned identity assigned.</p><pre><code>acs-engine deploy --subscription-id &lt;subscription id&gt; \
    --dns-prefix &lt;dns&gt; --location &lt;location&gt; \
    --api-model examples/kubernetes-msi-userassigned/kube-vmss.json
</code></pre><p>The main config values here are the following:</p><pre><code>&quot;useManagedIdentity&quot;: true
&quot;userAssignedID&quot;: &quot;acsenginetestid&quot;
</code></pre><p>The first one <code>useManagedIdentity</code> indicates to acs-engine that we are going to use the managed identity extension. This sets up the necessary packages and extensions required for the managed identities to work. The next one <code>userAssignedID</code> provides the information on the user identity which is to be used with the cluster.</p><h2 id=current-status-and-future-2>Current status and future</h2><p>Currently we support the user assigned identity creation with the cluster using deploy of the acs-engine. In future this will become part of AKS.</p><h2 id=get-involved>Get involved</h2><p>For azure specific discussions - please checkout the Azure SIG page at <a href=https://github.com/kubernetes/community/tree/master/sig-azure>[6]</a> and come and join the <a href=https://kubernetes.slack.com/messages/sig-azure>#sig-azure</a> slack channel for more.</p><p>For CA, please checkout the Autoscaler project here <a href=http://www.github.com/kubernetes/autoscaler>[7]</a> and join the <a href=https://kubernetes.slack.com/messages/sig-autoscaling>#sig-autoscaling</a> Slack for more discussions.</p><p>For the acs-engine (the unmanaged variety) on Azure docs can be found here: <a href=https://github.com/Azure/acs-engine>[9]</a>. More details about the managed service from Azure Kubernetes Service (AKS) here <a href=https://docs.microsoft.com/en-us/azure/aks/>[5]</a>.</p><h2 id=references>References</h2><ol><li><p><a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview>https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview</a></p></li><li><p>/docs/concepts/architecture/cloud-controller/</p></li><li><p><a href=https://github.com/kubernetes/kubernetes/blob/release-1.17/staging/src/k8s.io/legacy-cloud-providers/azure/azure_vmss.go>https://github.com/kubernetes/kubernetes/blob/release-1.17/staging/src/k8s.io/legacy-cloud-providers/azure/azure_vmss.go</a></p></li><li><p><a href=https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md>https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md</a></p></li><li><p><a href=https://docs.microsoft.com/en-us/azure/aks/>https://docs.microsoft.com/en-us/azure/aks/</a></p></li><li><p><a href=https://github.com/kubernetes/community/tree/master/sig-azure>https://github.com/kubernetes/community/tree/master/sig-azure</a></p></li><li><p><a href=https://github.com/kubernetes/autoscaler>https://github.com/kubernetes/autoscaler</a></p></li><li><p><a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md>https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md</a></p></li><li><p><a href=https://github.com/Azure/acs-engine>https://github.com/Azure/acs-engine</a></p></li><li><p><a href=https://github.com/kubernetes/client-go>https://github.com/kubernetes/client-go</a></p></li><li><p>/docs/concepts/architecture/</p></li><li><p><a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview>https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview</a></p></li><li><p><a href=https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned>https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned</a></p></li></ol><p>14)https://github.com/kubernetes/kubernetes/blob/release-1.17/staging/src/k8s.io/legacy-cloud-providers/azure/auth/azure_auth.go</p><ol start=15><li><a href=https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler>https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler</a></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-2323347af026c34a08d65e44879345ed>Introducing the Non-Code Contributor’s Guide</h1><div class="td-byline mb-4"><time datetime=2018-10-04 class=text-muted>Thursday, October 04, 2018</time></div><p><strong>Author</strong>: <a href=https://twitter.com/noah_abrahams>Noah Abrahams</a> (InfoSiftr), <a href=https://twitter.com/jonasrosland>Jonas Rosland</a> (VMware), <a href=https://twitter.com/idvoretskyi>Ihor Dvoretskyi</a> (CNCF)</p><p>It was May 2018 in Copenhagen, and the Kubernetes community was enjoying the contributor summit at KubeCon/CloudNativeCon, complete with the first run of the New Contributor Workshop. As a time of tremendous collaboration between contributors, the topics covered ranged from signing the CLA to deep technical conversations. Along with the vast exchange of information and ideas, however, came continued scrutiny of the topics at hand to ensure that the community was being as inclusive and accommodating as possible. Over that spring week, some of the pieces under the microscope included the many themes being covered, and how they were being presented, but also the overarching characteristics of the people contributing and the skill sets involved. From the discussions and analysis that followed grew the idea that the community was not benefiting as much as it could from the many people who wanted to contribute, but whose strengths were in areas other than writing code.</p><p><strong>This all led to an effort called the <a href=https://github.com/kubernetes/community/blob/master/contributors/guide/non-code-contributions.md>Non-Code Contributor’s Guide</a>.</strong></p><p>Now, it’s important to note that Kubernetes is rare, if not unique, in the open source world, in that it was defined very early on as both a project and a community. While the project itself is focused on the codebase, it is the community of people driving it forward that makes the project successful. The community works together with an explicit set of <a href=https://git.k8s.io/community/values.md>community values</a>, guiding the day-to-day behavior of contributors whether on GitHub, Slack, Discourse, or sitting together over tea or coffee.</p><p>By having a community that values people first, and explicitly values a diversity of people, the Kubernetes project is building a product to serve people with diverse needs. The different backgrounds of the contributors bring different approaches to the problem solving, with different methods of collaboration, and all those different viewpoints ultimately create a better project.</p><p>The Non-Code Contributor’s Guide aims to make it easy for anyone to contribute to the Kubernetes project in a way that makes sense for them. This can be in many forms, technical and non-technical, based on the person's knowledge of the project and their available time. Most individuals are not developers, and most of the world’s developers are not paid to fully work on open source projects. Based on this we have started an ever-growing list of possible ways to contribute to the Kubernetes project in a Non-Code way!</p><h2 id=get-involved>Get Involved</h2><p>Some of the ways that you can contribute to the Kubernetes community without writing a single line of code include:</p><ul><li>Community education, answering questions on <a href=https://discuss.kubernetes.io/>Discuss</a>, <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>, and <a href=http://slack.k8s.io/>Slack</a></li><li>Outward facing community work such as <a href=https://www.meetup.com/pro/cncf/>hosting meetups</a> and events</li><li>Writing <a href=https://github.com/kubernetes/community/tree/master/sig-docs>project documentation</a></li><li>Writing operational manuals, helping users understand how to run Kubernetes</li><li>Helping deliver Kubernetes, as a part of the <a href=https://github.com/kubernetes/sig-release/blob/master/release-team/README.md>release team</a></li><li>Project, program, and <a href=https://github.com/kubernetes/community/blob/master/sig-pm/README.md>product management</a></li><li>And many more!</li></ul><p>The guide to get started with Kubernetes project contribution is <a href=https://github.com/kubernetes/community/tree/master/contributors/guide>documented on GitHub</a>, and as the Non-Code Contributors Guide is a part of that Kubernetes Contributors Guide, it can be found <a href=https://github.com/kubernetes/community/blob/master/contributors/guide/non-code-contributions.md>here</a>. As stated earlier, this list is not exhaustive and will continue to be a work in progress.</p><p>To date, the typical Non-Code contributions fall into the following categories:</p><ul><li>Roles that are based on skill sets other than “software developer”</li><li>Non-Code contributions in primarily code-based roles</li><li>“Post-Code” roles, that are not code-based, but require knowledge of either the code base or management of the code base</li></ul><p>If you, dear reader, have any additional ideas for a Non-Code way to contribute, whether or not it fits in an existing category, the team will always appreciate if you could help us expand the list.</p><p>If a contribution of the Non-Code nature appeals to you, please read the Non-Code Contributions document, and then check the <a href=https://discuss.kubernetes.io/c/contributors/role-board>Contributor Role Board</a> to see if there are any open positions where your expertise could be best used! If there are no listed open positions that match your skill set, drop on by the <a href=https://kubernetes.slack.com/messages/sig-contribex>#sig-contribex</a> channel on Slack, and we’ll point you in the right direction.</p><p>We hope to see you contributing to the Kubernetes community soon!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cfe43151cb19b506570e75c8f0a61259>KubeDirector: The easy way to run complex stateful applications on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-10-03 class=text-muted>Wednesday, October 03, 2018</time></div><p><strong>Author</strong>: Thomas Phelan (BlueData)</p><p>KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.</p><p>We recently <a href=https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/>introduced the KubeDirector project</a>, as part of a broader open source Kubernetes initiative we call BlueK8s. I’m happy to announce that the pre-alpha
code for <a href=https://github.com/bluek8s/kubedirector/>KubeDirector</a> is now available. And in this blog post, I’ll show how it works.</p><p>KubeDirector provides the following capabilities:</p><ul><li>The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, it’s not necessary to decompose these existing applications to fit a microservices design pattern.</li><li>Native support for preserving application-specific configuration and state.</li><li>An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.</li></ul><p>KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts. The application metadata is referred to as a KubeDirectorApp resource.</p><p>To understand the components of KubeDirector, clone the repository on <a href=https://github.com/bluek8s/kubedirector/>GitHub</a> using a command similar to:</p><pre><code>git clone http://&lt;userid&gt;@github.com/bluek8s/kubedirector.
</code></pre><p>The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file <code>kubedirector/deploy/example_catalog/cr-app-spark221e2.json</code>.</p><pre><code>~&gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
{
   &quot;apiVersion&quot;: &quot;kubedirector.bluedata.io/v1alpha1&quot;,
   &quot;kind&quot;: &quot;KubeDirectorApp&quot;,
   &quot;metadata&quot;: {
       &quot;name&quot; : &quot;spark221e2&quot;
   },
   &quot;spec&quot; : {
       &quot;systemctlMounts&quot;: true,
       &quot;config&quot;: {
           &quot;node_services&quot;: [
               {
                   &quot;service_ids&quot;: [
                       &quot;ssh&quot;,
                       &quot;spark&quot;,
                       &quot;spark_master&quot;,
                       &quot;spark_worker&quot;
                   ],
…
</code></pre><p>The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
<code>kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml</code>.</p><pre><code>~&gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &quot;kubedirector.bluedata.io/v1alpha1&quot;
kind: &quot;KubeDirectorCluster&quot;
metadata:
  name: &quot;spark221e2&quot;
spec:
  app: spark221e2
  roles:
  - name: controller
    replicas: 1
    resources:
      requests:
        memory: &quot;4Gi&quot;
        cpu: &quot;2&quot;
      limits:
        memory: &quot;4Gi&quot;
        cpu: &quot;2&quot;
  - name: worker
    replicas: 2
    resources:
      requests:
        memory: &quot;4Gi&quot;
        cpu: &quot;2&quot;
      limits:
        memory: &quot;4Gi&quot;
        cpu: &quot;2&quot;
  - name: jupyter
…
</code></pre><h2 id=running-spark-on-kubernetes-with-kubedirector>Running Spark on Kubernetes with KubeDirector</h2><p>With KubeDirector, it’s easy to run Spark clusters on Kubernetes.</p><p>First, verify that Kubernetes (version 1.9 or later) is running, using the command <code>kubectl version</code></p><pre><code>~&gt; kubectl version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T18:02:47Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;11&quot;, GitVersion:&quot;v1.11.3&quot;, GitCommit:&quot;a4529464e4629c21224b3d52edfe0ea91b072862&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-09-09T17:53:03Z&quot;, GoVersion:&quot;go1.10.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}                                    
</code></pre><p>Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:</p><pre><code>cd kubedirector
make deploy
</code></pre><p>These will start the KubeDirector pod:</p><pre><code>~&gt; kubectl get pods
NAME                           READY     STATUS     RESTARTS     AGE
kubedirector-58cf59869-qd9hb   1/1       Running    0            1m     
</code></pre><p>List the installed KubeDirector applications with <code>kubectl get KubeDirectorApp</code></p><pre><code>~&gt; kubectl get KubeDirectorApp
NAME           AGE
cassandra311   30m
spark211up     30m
spark221e2     30m
</code></pre><p>Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
<code>kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml</code> command.
Verify that the Spark cluster has been started:</p><pre><code>~&gt; kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
kubedirector-58cf59869-djdwl     1/1       Running   0          19m
spark221e2-controller-zbg4d-0    1/1       Running   0          23m
spark221e2-jupyter-2km7q-0       1/1       Running   0          23m
spark221e2-worker-4gzbz-0        1/1       Running   0          23m
spark221e2-worker-4gzbz-1        1/1       Running   0          23m
</code></pre><p>The running services now include the Spark services:</p><pre><code>~&gt; kubectl get service
NAME                                TYPE         CLUSTER-IP        EXTERNAL-IP    PORT(S)                                                    AGE
kubedirector                        ClusterIP    10.98.234.194     &lt;none&gt;         60000/TCP                                                  1d
kubernetes                          ClusterIP    10.96.0.1         &lt;none&gt;         443/TCP                                                    1d
svc-spark221e2-5tg48                ClusterIP    None              &lt;none&gt;         8888/TCP                                                   21s
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123    &lt;none&gt;         22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP  20s
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249    &lt;none&gt;         22:30632/TCP,8888:30355/TCP                                20s
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165    &lt;none&gt;         22:30358/TCP,8081:32144/TCP                                20s
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221     &lt;none&gt;         22:30294/TCP,8081:31436/TCP                                20s
</code></pre><p>Pointing the browser at port 31533 connects to the Spark Master UI:</p><p><img src=/images/blog/2018-10-03-kubedirector/kubedirector.png alt=kubedirector></p><p>That’s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.</p><p>To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:</p><pre><code>kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
</code></pre><p>See the running Cassandra cluster:</p><pre><code>~&gt; kubectl get pods
NAME                              READY     STATUS    RESTARTS   AGE
cassandra311-seed-v24r6-0         1/1       Running   0          1m
cassandra311-seed-v24r6-1         1/1       Running   0          1m
cassandra311-worker-rqrhl-0       1/1       Running   0          1m
cassandra311-worker-rqrhl-1       1/1       Running   0          1m
kubedirector-58cf59869-djdwl      1/1       Running   0          1d
spark221e2-controller-tq8d6-0     1/1       Running   0          22m
spark221e2-jupyter-6989v-0        1/1       Running   0          22m
spark221e2-worker-d9892-0         1/1       Running   0          22m
spark221e2-worker-d9892-1         1/1       Running   0          22m
</code></pre><p>Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use <code>kubectl get service</code> to see the set of services.</p><pre><code>~&gt; kubectl get service
NAME                                TYPE         CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE
kubedirector                        ClusterIP    10.98.234.194    &lt;none&gt;        60000/TCP                                                 1d
kubernetes                          ClusterIP    10.96.0.1        &lt;none&gt;        443/TCP                                                   1d
svc-cassandra311-seed-v24r6-0       NodePort     10.96.94.204     &lt;none&gt;        22:31131/TCP,9042:30739/TCP                               3m
svc-cassandra311-seed-v24r6-1       NodePort     10.106.144.52    &lt;none&gt;        22:30373/TCP,9042:32662/TCP                               3m
svc-cassandra311-vhh29              ClusterIP    None             &lt;none&gt;        8888/TCP                                                  3m
svc-cassandra311-worker-rqrhl-0     NodePort     10.109.61.194    &lt;none&gt;        22:31832/TCP,9042:31962/TCP                               3m
svc-cassandra311-worker-rqrhl-1     NodePort     10.97.147.131    &lt;none&gt;        22:31454/TCP,9042:31170/TCP                               3m
svc-spark221e2-5tg48                ClusterIP    None             &lt;none&gt;        8888/TCP                                                  24m
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123   &lt;none&gt;        22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249   &lt;none&gt;        22:30632/TCP,8888:30355/TCP                               24m
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165   &lt;none&gt;        22:30358/TCP,8081:32144/TCP                               24m
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221    &lt;none&gt;        22:30294/TCP,8081:31436/TCP                               24m
</code></pre><h2 id=get-involved>Get Involved</h2><p>KubeDirector is a fully open source, Apache v2 licensed, project – the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow <a href=https://twitter.com/BlueK8s/>@BlueK8s</a> on Twitter and get involved through these channels:</p><ul><li>KubeDirector <a href=https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/>chat room on Slack</a></li><li>KubeDirector <a href=https://github.com/bluek8s/kubedirector/>GitHub repo</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0fc9e8fed3020770724c866512e3bc08>Building a Network Bootable Server Farm for Kubernetes with LTSP</h1><div class="td-byline mb-4"><time datetime=2018-10-02 class=text-muted>Tuesday, October 02, 2018</time></div><p><strong>Author</strong>: Andrei Kvapil (WEDOS)</p><p><img src=/images/blog/2018-10-01-network-bootable-farm-with-ltsp/k8s+ltsp.svg alt=k8s+ltsp></p><p>In this post, I'm going to introduce you to a cool technology for Kubernetes, LTSP. It is useful for large baremetal Kubernetes deployments.</p><p>You don't need to think about installing an OS and binaries on each node anymore. Why? You can do that automatically through Dockerfile!</p><p>You can buy and put 100 new servers into a production environment and get them working immediately - it's really amazing!</p><p>Intrigued? Let me walk you through how it works.</p><h1 id=summary>Summary</h1><p><em><strong>Please note:</strong> this is a cool hack, but is not officially supported in Kubernetes.</em></p><p>First, we need to understand how exactly it works.</p><p>In short, for all nodes we have prepared the image with the OS, Docker, Kubelet and everything else that you need there. This image with the kernel is building automatically by CI using Dockerfile. End nodes are booting the kernel and OS from this image via the network.</p><p>Nodes are using overlays as the root filesystem and after reboot any changes will be lost (like in Docker containers). You have a config-file where you can describe mounts and some initial commands which should be executed during node boot (Example: set root user ssh-key and kubeadm join commands)</p><h2 id=image-preparation-process>Image Preparation Process</h2><p>We will use LTSP project because it's gives us everything we need to organize the network booting environment. Basically, LTSP is a pack of shell-scripts which makes our life much easier.</p><p>LTSP provides a initramfs module, a few helper-scripts, and the configuration system which prepare the system during the early state of boot, before the main init process call.</p><p><strong>This is what the image preparation procedure looks like:</strong></p><ul><li>You're deploying the basesystem in the chroot environment.</li><li>Make any needed changes there, install software.</li><li>Run the <code>ltsp-build-image</code> command</li></ul><p>After that, you will get the squashed image from the chroot with all the software inside. Each node will download this image during the boot and use it as the rootfs. For the update node, you can just reboot it. The new squashed image will be downloaded and mounted into the rootfs.</p><h2 id=server-components>Server Components</h2><p><strong>The server part of LTSP includes two components in our case:</strong></p><ul><li><strong>TFTP-server</strong> - TFTP is the initial protocol, it is used the download the kernel, initramfs and main config - lts.conf.</li><li><strong>NBD-server</strong> - NBD protocol is used to distribute the squashed rootfs image to the clients. It is the fastest way, but if you want, it can be replaced by the NFS or AoE protocol.</li></ul><p>You should also have:</p><ul><li><strong>DHCP-server</strong> - it will distribute the IP-settings and a few specific options to the clients to make it possible for them to boot from our LTSP-server.</li></ul><h2 id=node-booting-process>Node Booting Process</h2><p><strong>This is how the node is booting up</strong></p><ul><li>The first time, the node will ask DHCP for IP-settings and <code>next-server</code>, <code>filename</code> options.</li><li>Next, the node will apply settings and download bootloader (pxelinux or grub)</li><li>Bootloader will download and read config with the kernel and initramfs image.</li><li>Then bootloader will download the kernel and initramfs and execute it with specific cmdline options.</li><li>During the boot, initramfs modules will handle options from cmdline and do some actions like connect NBD-device, prepare overlay rootfs, etc.</li><li>Afterwards it will call the ltsp-init system instead of the normal init.</li><li>ltsp-init scripts will prepare the system on the earlier stage, before the main init will be called. Basically it applies the setting from lts.conf (main config): write fstab and rc.local entries etc.</li><li>Call the main init (systemd) which is booting the configured system as usual, mounts shares from fstab, start targets and services, executes commands from rc.local file.</li><li>In the end you have a fully configured and booted system ready for further operations.</li></ul><h1 id=preparing-the-server>Preparing the Server</h1><p>As I said before, I'm preparing the LTSP-server with the squashed image automatically using Dockerfile. This method is quite good because you have all steps described in your git repository.
You have versioning, branches, CI and everything that you used to use for preparing your usual Docker projects.</p><p>Otherwise, you can deploy the LTSP server manually by executing all steps by hand. This is a good practice for learning and understanding the basic principles.</p><p>Just repeat all the steps listed here by hand, just to try to install LTSP without Dockerfile.</p><h2 id=used-patches-list>Used Patches List</h2><p>LTSP still has some issues which authors don’t want to apply, yet. However LTSP is easy customizable so I prepared a few patches for myself and will share them here.</p><p>I’ll create a fork if the community will warmly accept my solution.</p><ul><li><a href=https://github.com/kvaps/ltsp/compare/feature-grub.diff>feature-grub.diff</a>
LTSP does not support EFI by default, so I've prepared a patch which adds GRUB2 with EFI support.</li><li><a href=https://github.com/kvaps/ltsp/compare/feature_preinit.diff>feature_preinit.diff</a>
This patch adds a PREINIT option to lts.conf, which allows you to run custom commands before the main init call. It may be useful to modify the systemd units and configure the network. It's remarkable that all environment variables from the boot environment are saved and you can use them in your scripts.</li><li><a href=https://github.com/kvaps/ltsp/compare/feature_initramfs_params_from_lts_conf.diff>feature_initramfs_params_from_lts_conf.diff</a>
Solves a problem with NBD_TO_RAM option, after this patch you can specify it on lts.conf inside chroot. (not in tftp directory)</li><li><a href=https://gist.githubusercontent.com/kvaps/1a6a7d8b73bf7444f0f99b22379c9e4e/raw/eb0d60c638ef72b7e28438b7f4d2beda89c41f75/nbd-server-wrapper.sh>nbd-server-wrapper.sh</a>
This is not a patch but a special wrapper script which allows you to run NBD-server in the foreground. It is useful if you want to run it inside a Docker container.</li></ul><h2 id=dockerfile-stages>Dockerfile Stages</h2><p>We will use <a href=https://docs.docker.com/develop/develop-images/multistage-build/>stage building</a> in our Dockerfile to leave only the needed parts in our Docker image. The unused parts will be removed from the final image.</p><pre><code>ltsp-base
(install basic LTSP server software)
   |
   |---basesystem
   |   (prepare chroot with main software and kernel)
   |     |
   |     |---builder
   |     |   (build additional software from sources, if needed)
   |     |
   |     '---ltsp-image
   |         (install additional software, docker, kubelet and build squashed image)
   |
   '---final-stage
       (copy squashed image, kernel and initramfs into first stage)
</code></pre><h3 id=stage-1-ltsp-base>Stage 1: ltsp-base</h3><p>Let's start writing our Dockerfile. This is the first part:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#a2f;font-weight:700>FROM</span><span style=color:#b44> ubuntu:16.04 as ltsp-base</span><span>
</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>ADD</span> nbd-server-wrapper.sh /bin/<span>
</span><span></span><span style=color:#a2f;font-weight:700>ADD</span> /patches/feature-grub.diff /patches/feature-grub.diff<span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> apt-get -y update <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> apt-get -y install <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      ltsp-server <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      tftpd-hpa <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      nbd-server <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      grub-common <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      grub-pc-bin <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      grub-efi-amd64-bin <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      curl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      patch <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> sed -i <span style=color:#b44>&#39;s|in_target mount|in_target_nofail mount|&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      /usr/share/debootstrap/functions <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  <span style=color:#080;font-style:italic># Add EFI support and Grub bootloader (#1745251)</span><span>
</span><span></span> <span style=color:#666>&amp;&amp;</span> patch -p2 -d /usr/sbin &lt; /patches/feature-grub.diff <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> rm -rf /var/lib/apt/lists <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> apt-get clean<span>
</span></code></pre></div><p>At this stage our Docker image has already been installed:</p><ul><li>NBD-server</li><li>TFTP-server</li><li>LTSP-scripts with grub bootloader support (for EFI)</li></ul><h3 id=stage-2-basesystem>Stage 2: basesystem</h3><p>In this stage we will prepare a chroot environment with basesystem, and install basic software with the kernel.</p><p>We will use the classic <strong>debootstrap</strong> instead of <strong>ltsp-build-client</strong> to prepare the base image, because <strong>ltsp-build-client</strong> will install GUI and few other things which we don't need for the server deployment.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#a2f;font-weight:700>FROM</span><span style=color:#b44> ltsp-base as basesystem</span><span>
</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>ARG</span> <span style=color:#b8860b>DEBIAN_FRONTEND</span><span style=color:#666>=</span>noninteractive

<span style=color:#080;font-style:italic># Prepare base system</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> debootstrap --arch amd64 xenial /opt/ltsp/amd64<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Install updates</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;\
</span><span style=color:#b44>      deb http://archive.ubuntu.com/ubuntu xenial main restricted universe multiverse\n\
</span><span style=color:#b44>      deb http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiverse\n\
</span><span style=color:#b44>      deb http://archive.ubuntu.com/ubuntu xenial-security main restricted universe multiverse&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      &gt; /opt/ltsp/amd64/etc/apt/sources.list <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> ltsp-chroot apt-get -y update <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> ltsp-chroot apt-get -y upgrade<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Installing LTSP-packages</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot apt-get -y install ltsp-client-core<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Apply initramfs patches</span><span>
</span><span></span><span style=color:#080;font-style:italic># 1: Read params from /etc/lts.conf during the boot (#1680490)</span><span>
</span><span></span><span style=color:#080;font-style:italic># 2: Add support for PREINIT variables in lts.conf</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>ADD</span> /patches /patches<span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> patch -p4 -d /opt/ltsp/amd64/usr/share &lt; /patches/feature_initramfs_params_from_lts_conf.diff <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> patch -p3 -d /opt/ltsp/amd64/usr/share &lt; /patches/feature_preinit.diff<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Write new local client config for boot NBD image to ram:</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;[Default]\nLTSP_NBD_TO_RAM = true&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      &gt; /opt/ltsp/amd64/etc/lts.conf<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Install packages</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> <span style=color:#a2f>echo</span> <span style=color:#b44>&#39;APT::Install-Recommends &#34;0&#34;;\nAPT::Install-Suggests &#34;0&#34;;&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      &gt;&gt; /opt/ltsp/amd64/etc/apt/apt.conf.d/01norecommend <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> ltsp-chroot apt-get -y install <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      software-properties-common <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      apt-transport-https <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      ca-certificates <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      ssh <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      bridge-utils <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      pv <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      jq <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      vlan <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      bash-completion <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      screen <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      vim <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      mc <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      lm-sensors <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      htop <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      jnettop <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      rsync <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      curl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      wget <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      tcpdump <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      arping <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      apparmor-utils <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      nfs-common <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      telnet <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      sysstat <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      ipvsadm <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      ipset <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      make<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Install kernel</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot apt-get -y install linux-generic-hwe-16.04<span>
</span></code></pre></div><p>Note that you may encounter problems with some packages, such as <code>lvm2</code>.
They have not fully optimized for installing in an unprivileged chroot.
Their postinstall scripts try to call some privileged commands which can fail with errors and block the package installation.</p><p>Solution:</p><ul><li>Some of them can be installed before the kernel without any problems (like <code>lvm2</code>)</li><li>But for some of them you will need to use <a href=https://askubuntu.com/a/482936/327437>this workaround</a> to install without the postinstall script.</li></ul><h3 id=stage-3-builder>Stage 3: builder</h3><p>Now we can build all the necessary software and kernel modules. It's really cool that you can do that automatically in this stage.
You can skip this stage if you have nothing to do here.</p><p>Here is example for install latest MLNX_EN driver:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#a2f;font-weight:700>FROM</span><span style=color:#b44> basesystem as builder</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Set cpuinfo (for building from sources)</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> cp /proc/cpuinfo /opt/ltsp/amd64/proc/cpuinfo<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Compile Mellanox driver</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot sh -cx <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   <span style=color:#b44>&#39;  VERSION=4.3-1.0.1.0-ubuntu16.04-x86_64 \
</span><span style=color:#b44>   &amp;&amp; curl -L http://www.mellanox.com/downloads/ofed/MLNX_EN-${VERSION%%-ubuntu*}/mlnx-en-${VERSION}.tgz \
</span><span style=color:#b44>      | tar xzf - \
</span><span style=color:#b44>   &amp;&amp; export \
</span><span style=color:#b44>        DRIVER_DIR=&#34;$(ls -1 | grep &#34;MLNX_OFED_LINUX-\|mlnx-en-&#34;)&#34; \
</span><span style=color:#b44>        KERNEL=&#34;$(ls -1t /lib/modules/ | head -n1)&#34; \
</span><span style=color:#b44>   &amp;&amp; cd &#34;$DRIVER_DIR&#34; \
</span><span style=color:#b44>   &amp;&amp; ./*install --kernel &#34;$KERNEL&#34; --without-dkms --add-kernel-support \
</span><span style=color:#b44>   &amp;&amp; cd - \
</span><span style=color:#b44>   &amp;&amp; rm -rf &#34;$DRIVER_DIR&#34; /tmp/mlnx-en* /tmp/ofed*&#39;</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Save kernel modules</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot sh -c <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#b44>&#39; export KERNEL=&#34;$(ls -1t /usr/src/ | grep -m1 &#34;^linux-headers&#34; | sed &#34;s/^linux-headers-//g&#34;)&#34; \
</span><span style=color:#b44>   &amp;&amp; tar cpzf /modules.tar.gz /lib/modules/${KERNEL}/updates&#39;</span><span>
</span><span>
</span></code></pre></div><h3 id=stage-4-ltsp-image>Stage 4: ltsp-image</h3><p>In this stage we will install what we built in the previous step:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#a2f;font-weight:700>FROM</span><span style=color:#b44> basesystem as ltsp-image</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Retrieve kernel modules</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>COPY</span> --from<span style=color:#666>=</span>builder /opt/ltsp/amd64/modules.tar.gz /opt/ltsp/amd64/modules.tar.gz<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Install kernel modules</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot sh -c <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#b44>&#39; export KERNEL=&#34;$(ls -1t /usr/src/ | grep -m1 &#34;^linux-headers&#34; | sed &#34;s/^linux-headers-//g&#34;)&#34; \
</span><span style=color:#b44>   &amp;&amp; tar xpzf /modules.tar.gz \
</span><span style=color:#b44>   &amp;&amp; depmod -a &#34;${KERNEL}&#34; \
</span><span style=color:#b44>   &amp;&amp; rm -f /modules.tar.gz&#39;</span><span>
</span></code></pre></div><p>Then do some additional changes to finalize our ltsp-image:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#080;font-style:italic># Install docker</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot sh -c <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   <span style=color:#b44>&#39;  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
</span><span style=color:#b44>   &amp;&amp; echo &#34;deb https://download.docker.com/linux/ubuntu xenial stable&#34; \
</span><span style=color:#b44>        &gt; /etc/apt/sources.list.d/docker.list \
</span><span style=color:#b44>   &amp;&amp; apt-get -y update \
</span><span style=color:#b44>   &amp;&amp; apt-get -y install \
</span><span style=color:#b44>        docker-ce=$(apt-cache madison docker-ce | grep 18.06 | head -1 | awk &#34;{print $ 3}&#34;)&#39;</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Configure docker options</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> <span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span><span style=color:#a2f>echo</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --storage-driver<span style=color:#666>=</span>overlay2 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --iptables<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --ip-masq<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --log-driver<span style=color:#666>=</span>json-file <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --log-opt<span style=color:#666>=</span>max-size<span style=color:#666>=</span>10m <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      --log-opt<span style=color:#666>=</span>max-file<span style=color:#666>=</span><span style=color:#666>5</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> sed <span style=color:#b44>&#34;/^ExecStart=/ s|</span>$<span style=color:#b44>| </span><span style=color:#b8860b>$DOCKER_OPTS</span><span style=color:#b44>|g&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      /opt/ltsp/amd64/lib/systemd/system/docker.service <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      &gt; /opt/ltsp/amd64/etc/systemd/system/docker.service<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Install kubeadm, kubelet and kubectl</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot sh -c <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#b44>&#39;  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
</span><span style=color:#b44>      &amp;&amp; echo &#34;deb http://apt.kubernetes.io/ kubernetes-xenial main&#34; \
</span><span style=color:#b44>           &gt; /etc/apt/sources.list.d/kubernetes.list \
</span><span style=color:#b44>      &amp;&amp; apt-get -y update \
</span><span style=color:#b44>      &amp;&amp; apt-get -y install kubelet kubeadm kubectl cri-tools&#39;</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Disable automatic updates</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> rm -f /opt/ltsp/amd64/etc/apt/apt.conf.d/20auto-upgrades<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Disable apparmor profiles</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-chroot find /etc/apparmor.d <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      -maxdepth <span style=color:#666>1</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      -type f <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      -name <span style=color:#b44>&#34;sbin.*&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      -o -name <span style=color:#b44>&#34;usr.*&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      -exec ln -sf <span style=color:#b44>&#34;{}&#34;</span> /etc/apparmor.d/disable/ <span style=color:#b62;font-weight:700>\;</span><span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Write kernel cmdline options</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> <span style=color:#b8860b>KERNEL_OPTIONS</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span><span style=color:#a2f>echo</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#b8860b>init</span><span style=color:#666>=</span>/sbin/init-ltsp <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      forcepae <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#b8860b>console</span><span style=color:#666>=</span>tty1 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#b8860b>console</span><span style=color:#666>=</span>ttyS0,9600n8 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      nvme_core.default_ps_max_latency_us<span style=color:#666>=</span><span style=color:#666>0</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>    <span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> sed -i <span style=color:#b44>&#34;/^CMDLINE_LINUX_DEFAULT=/ s|=.*|=\&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>KERNEL_OPTIONS</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>\&#34;|&#34;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>      <span style=color:#b44>&#34;/opt/ltsp/amd64/etc/ltsp/update-kernels.conf&#34;</span><span>
</span></code></pre></div><p>Then we will make the squashed image from our chroot:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#080;font-style:italic># Cleanup caches</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> rm -rf /opt/ltsp/amd64/var/lib/apt/lists <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span> <span style=color:#666>&amp;&amp;</span> ltsp-chroot apt-get clean<span>
</span><span>
</span><span></span><span style=color:#080;font-style:italic># Build squashed image</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>RUN</span> ltsp-update-image<span>
</span></code></pre></div><h3 id=stage-5-final-stage>Stage 5: Final Stage</h3><p>In the final stage we will save only our squashed image and kernels with initramfs.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=color:#a2f;font-weight:700>FROM</span><span style=color:#b44> ltsp-base</span><span>
</span><span></span><span style=color:#a2f;font-weight:700>COPY</span> --from<span style=color:#666>=</span>ltsp-image /opt/ltsp/images /opt/ltsp/images<span>
</span><span></span><span style=color:#a2f;font-weight:700>COPY</span> --from<span style=color:#666>=</span>ltsp-image /etc/nbd-server/conf.d /etc/nbd-server/conf.d<span>
</span><span></span><span style=color:#a2f;font-weight:700>COPY</span> --from<span style=color:#666>=</span>ltsp-image /var/lib/tftpboot /var/lib/tftpboot<span>
</span></code></pre></div><p>Ok, now we have docker image which includes:</p><ul><li>TFTP-server</li><li>NBD-server</li><li>configured bootloader</li><li>kernel with initramfs</li><li>squashed rootfs image</li></ul><h1 id=usage>Usage</h1><p>OK, now when our docker-image with LTSP-server, kernel, initramfs and squashed rootfs fully prepared we can run the deployment with it.</p><p>We can do that as usual, but one more thing is networking.
Unfortunately, we can't use the standard Kubernetes service abstraction for our deployment, because TFTP can't work behind the NAT. During the boot, our nodes are not part of Kubernetes cluster and they requires ExternalIP, but Kubernetes always enables NAT for ExternalIPs, and there is no way to override this behavior.</p><p>For now I have two ways for avoid this: use <code>hostNetwork: true</code> or use <a href=https://github.com/dreamcat4/docker-images/blob/master/pipework/3.%20Examples.md#kubernetes>pipework</a>. The second option will also provide you redundancy because, in case of failure, the IP will be moved with the Pod to another node. Unfortunately, pipework is not native and a less secure method.
If you have some better option for that please let me know.</p><p>Here is example for deployment with hostNetwork:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>extensions/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ltsp-server<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>ltsp-server<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ltsp-server<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ltsp-server<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostNetwork</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>tftpd<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.example.org/example/ltsp:latest<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;/usr/sbin/in.tftpd&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-L&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-u&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;tftp&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-a&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;:69&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-s&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/lib/tftpboot&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>lifecycle</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>postStart</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>exec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;cd /var/lib/tftpboot/ltsp/amd64; ln -sf config/lts.conf .&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/lib/tftpboot/ltsp/amd64/config&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nbd-server<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.example.org/example/ltsp:latest<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;/bin/nbd-server-wrapper.sh&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ltsp-config<span style=color:#bbb>
</span></code></pre></div><p>As you can see it also requires configmap with <strong>lts.conf</strong> file.
Here is example part from mine:</p><pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: ltsp-config
data:
  lts.conf: |
    [default]
    KEEP_SYSTEM_SERVICES           = &quot;ssh ureadahead dbus-org.freedesktop.login1 systemd-logind polkitd cgmanager ufw rpcbind nfs-kernel-server&quot;

    PREINIT_00_TIME                = &quot;ln -sf /usr/share/zoneinfo/Europe/Prague /etc/localtime&quot;
    PREINIT_01_FIX_HOSTNAME        = &quot;sed -i '/^127.0.0.2/d' /etc/hosts&quot;
    PREINIT_02_DOCKER_OPTIONS      = &quot;sed -i 's|^ExecStart=.*|ExecStart=/usr/bin/dockerd -H fd:// --storage-driver overlay2 --iptables=false --ip-masq=false --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5|' /etc/systemd/system/docker.service&quot;

    FSTAB_01_SSH                   = &quot;/dev/data/ssh     /etc/ssh          ext4 nofail,noatime,nodiratime 0 0&quot;
    FSTAB_02_JOURNALD              = &quot;/dev/data/journal /var/log/journal  ext4 nofail,noatime,nodiratime 0 0&quot;
    FSTAB_03_DOCKER                = &quot;/dev/data/docker  /var/lib/docker   ext4 nofail,noatime,nodiratime 0 0&quot;

    # Each command will stop script execution when fail
    RCFILE_01_SSH_SERVER           = &quot;cp /rofs/etc/ssh/*_config /etc/ssh; ssh-keygen -A&quot;
    RCFILE_02_SSH_CLIENT           = &quot;mkdir -p /root/.ssh/; echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDBSLYRaORL2znr1V4a3rjDn3HDHn2CsvUNK1nv8+CctoICtJOPXl6zQycI9KXNhANfJpc6iQG1ZPZUR74IiNhNIKvOpnNRPyLZ5opm01MVIDIZgi9g0DUks1g5gLV5LKzED8xYKMBmAfXMxh/nsP9KEvxGvTJB3OD+/bBxpliTl5xY3Eu41+VmZqVOz3Yl98+X8cZTgqx2dmsHUk7VKN9OZuCjIZL9MtJCZyOSRbjuo4HFEssotR1mvANyz+BUXkjqv2pEa0I2vGQPk1VDul5TpzGaN3nOfu83URZLJgCrX+8whS1fzMepUYrbEuIWq95esjn0gR6G4J7qlxyguAb9 admin@kubernetes' &gt;&gt; /root/.ssh/authorized_keys&quot;
    RCFILE_03_KERNEL_DEBUG         = &quot;sysctl -w kernel.unknown_nmi_panic=1 kernel.softlockup_panic=1; modprobe netconsole netconsole=@/vmbr0,@10.9.0.15/&quot;
    RCFILE_04_SYSCTL               = &quot;sysctl -w fs.file-max=20000000 fs.nr_open=20000000 net.ipv4.neigh.default.gc_thresh1=80000 net.ipv4.neigh.default.gc_thresh2=90000 net.ipv4.neigh.default.gc_thresh3=100000&quot;
    RCFILE_05_FORWARD              = &quot;echo 1 &gt; /proc/sys/net/ipv4/ip_forward&quot;
    RCFILE_06_MODULES              = &quot;modprobe br_netfilter&quot;
    RCFILE_07_JOIN_K8S             = &quot;kubeadm join --token 2a4576.504356e45fa3d365 10.9.0.20:6443 --discovery-token-ca-cert-hash sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&quot;
</code></pre><ul><li><strong>KEEP_SYSTEM_SERVICES</strong> - during the boot, LTSP automatically removes some services, this variable is needed to prevent this behavior.</li><li>**PREINIT_*** - commands listed here will be executed before systemd runs (this function was added by the <a href=#used-patches-list>feature_preinit.diff</a> patch)</li><li>**FSTAB_*** - entries written here will be added to the <code>/etc/fstab</code> file.
As you can see, I use the <code>nofail</code> option, that means that if a partition doesn't exist, it will continue to boot without error.
If you have fully diskless nodes you can remove the FSTAB settings or configure the remote filesystem there.</li><li>**RCFILE_*** - those commands will be written to <code>rc.local</code> file, which will be called by systemd during the boot.
Here I load the kernel modules and add some sysctl tunes, then call the <code>kubeadm join</code> command, which adds my node to the Kubernetes cluster.</li></ul><p>You can get more details on all the variables used from <a href=http://manpages.ubuntu.com/manpages/xenial/man5/lts.conf.5.html>lts.conf manpage</a>.</p><p>Now you can configure your DHCP. Basically you should set the <code>next-server</code> and <code>filename</code> options.</p><p>I use ISC-DHCP server, and here is an example <code>dhcpd.conf</code>:</p><pre><code>shared-network ltsp-netowrk {
    subnet 10.9.0.0 netmask 255.255.0.0 {
        authoritative;
        default-lease-time -1;
        max-lease-time -1;

        option domain-name              &quot;example.org&quot;;
        option domain-name-servers      10.9.0.1;
        option routers                  10.9.0.1;
        next-server                     ltsp-1;  # write LTSP-server hostname here

        if option architecture = 00:07 {
            filename &quot;/ltsp/amd64/grub/x86_64-efi/core.efi&quot;;
        } else {
            filename &quot;/ltsp/amd64/grub/i386-pc/core.0&quot;;
        }

        range 10.9.200.0 10.9.250.254; 
    }
</code></pre><p>You can start from this, but what about me, I have multiple LTSP-servers and I configure leases statically for each node via the Ansible playbook.</p><p>Try to run your first node. If everything was right, you will have a running system there.
The node also will be added to your Kubernetes cluster.</p><p>Now you can try to make your own changes.</p><p>If you need something more, note that LTSP can be easily changed to meet your needs.
Feel free to look into the source code and you can find many answers there.</p><p><em><strong>UPD:</strong> Many people asking me: Why not simple use CoreOS and Ignition?</em></p><p><em>I can answer. The main feature here is image preparation process, not configuration. In case with LTSP you have classic Ubuntu system, and everything that can be installed on Ubuntu it can also be written here in the Dockerfile. In case CoreOS you have no so many freedom and you can’t easily add custom kernel modules and packages at the build stage of the boot image.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e38db7f640df9065a21c341cfbe4705c>Health checking gRPC servers on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-10-01 class=text-muted>Monday, October 01, 2018</time></div><p><strong>Author</strong>: <a href=https://twitter.com/ahmetb>Ahmet Alp Balkan</a> (Google)</p><p><a href=https://grpc.io>gRPC</a> is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
<a href=https://github.com/grpc-ecosystem/grpc-health-probe/>grpc-health-probe</a>, a
Kubernetes-native way to health check gRPC apps.</p><p>If you're unfamiliar, Kubernetes <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/>health
checks</a>
(liveness and readiness probes) is what's keeping your applications available
while you're sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.</p><p>Kubernetes <a href=https://github.com/kubernetes/kubernetes/issues/21493>does not
support</a> gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:</p><p><a href=/images/blog/2019-09-30-health-checking-grpc/options.png><img src=/images/blog/2019-09-30-health-checking-grpc/options.png alt="options for health checking grpc on kubernetes today"></a></p><ol><li><strong>httpGet probe:</strong> Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).</li><li><strong>tcpSocket probe:</strong> Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.</li><li><strong>exec probe:</strong> This invokes a program in a container's ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.</li></ol><p>Can we do better? Absolutely.</p><h2 id=introducing-grpc-health-probe>Introducing “grpc-health-probe”</h2><p>To standardize the "exec probe" approach mentioned above, we need:</p><ul><li>a <strong>standard</strong> health check "protocol" that can be implemented in any gRPC
server easily.</li><li>a <strong>standard</strong> health check "tool" that can query the health protocol easily.</li></ul><p>Thankfully, gRPC has a <a href=https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md>standard health checking
protocol</a>. It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.</p><p>If you
<a href=https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto>implement</a>
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this <code>Check()</code> method to determine server status.</p><p>The next thing you need is the "standard tool", and it's the
<a href=https://github.com/grpc-ecosystem/grpc-health-probe/><strong>grpc-health-probe</strong></a>.</p><a href=/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png><img width=768 title="grpc-health-probe on kubernetes" src=/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png></a><p>With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:</p><ol><li>Find the gRPC "health" module in your favorite language and start using it
(example <a href=https://godoc.org/github.com/grpc/grpc-go/health>Go library</a>).</li><li>Ship the
<a href=https://github.com/grpc-ecosystem/grpc-health-probe/>grpc_health_probe</a>
binary in your container.</li><li><a href=https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes>Configure</a>
Kubernetes "exec" probe to invoke the "grpc_health_probe" tool in the
container.</li></ol><p>In this case, executing "grpc_health_probe" will call your gRPC server over
<code>localhost</code>, since they are in the same pod.</p><h2 id=what-s-next>What's next</h2><p><strong>grpc-health-probe</strong> project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.</p><p>If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and <a href=https://github.com/grpc-ecosystem/grpc-health-probe/>give
feedback</a>.</p><h2 id=further-reading>Further reading</h2><ul><li>Protocol: <a href=https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md>GRPC Health Checking Protocol</a> (<a href=https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto>health.proto</a>)</li><li>Documentation: <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/>Kubernetes liveness and readiness probes</a></li><li>Article: <a href=https://ahmet.im/blog/advanced-kubernetes-health-checks/>Advanced Kubernetes Health Check Patterns</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-16e83787afdcc9efee9bb0c798bd9134>Kubernetes 1.12: Kubelet TLS Bootstrap and Azure Virtual Machine Scale Sets (VMSS) Move to General Availability</h1><div class="td-byline mb-4"><time datetime=2018-09-27 class=text-muted>Thursday, September 27, 2018</time></div><p><strong>Author</strong>: The 1.12 <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md>Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.12, our third release of 2018!</p><p>Today’s release continues to focus on internal improvements and graduating features to stable in Kubernetes. This newest version graduates key features such as security and Azure. Notable additions in this release include two highly-anticipated features graduating to general availability: Kubelet TLS Bootstrap and Support for Azure Virtual Machine Scale Sets (VMSS).</p><p>These new features mean increased security, availability, resiliency, and ease of use to get production applications to market faster. The release also signifies the increasing maturation and sophistication of Kubernetes on the developer side.</p><p>Let’s dive into the key features of this release:</p><h2 id=introducing-general-availability-of-kubelet-tls-bootstrap>Introducing General Availability of Kubelet TLS Bootstrap</h2><p>We’re excited to announce General Availability (GA) of <a href=https://github.com/kubernetes/features/issues/43>Kubelet TLS Bootstrap</a>. In Kubernetes 1.4, we introduced an API for requesting certificates from a cluster-level Certificate Authority (CA). The original intent of this API is to enable provisioning of TLS client certificates for kubelets. This feature allows for a kubelet to bootstrap itself into a TLS-secured cluster. Most importantly, it automates the provision and distribution of signed certificates.</p><p>Before, when a kubelet ran for the first time, it had to be given client credentials in an out-of-band process during cluster startup. The burden was on the operator to provision these credentials. Because this task was so onerous to manually execute and complex to automate, many operators deployed clusters with a single credential and single identity for all kubelets. These setups prevented deployment of node lockdown features like the Node authorizer and the NodeRestriction admission controller.</p><p>To alleviate this, <a href=https://github.com/kubernetes/community/tree/master/sig-auth>SIG Auth</a> introduced a way for kubelet to generate a private key and a CSR for submission to a cluster-level certificate signing process. The v1 (GA) designation indicates production hardening and readiness, and comes with the guarantee of long-term backwards compatibility.</p><p>Alongside this, <a href=https://github.com/kubernetes/features/issues/267>Kubelet server certificate bootstrap and rotation</a> is moving to beta. Currently, when a kubelet first starts, it generates a self-signed certificate/key pair that is used for accepting incoming TLS connections. This feature introduces a process for generating a key locally and then issuing a Certificate Signing Request to the cluster API server to get an associated certificate signed by the cluster’s root certificate authority. Also, as certificates approach expiration, the same mechanism will be used to request an updated certificate.</p><h2 id=support-for-azure-virtual-machine-scale-sets-vmss-and-cluster-autoscaler-is-now-stable>Support for Azure Virtual Machine Scale Sets (VMSS) and Cluster-Autoscaler is Now Stable</h2><p>Azure Virtual Machine Scale Sets (VMSS) allow you to create and manage a homogenous VM pool that can automatically increase or decrease based on demand or a set schedule. This enables you to easily manage, scale, and load balance multiple VMs to provide high availability and application resiliency, ideal for large-scale applications that can run as Kubernetes workloads.</p><p>With this new stable feature, Kubernetes supports the <a href=https://github.com/kubernetes/features/issues/514>scaling of containerized applications with Azure VMSS</a>, including the ability to <a href=https://github.com/kubernetes/features/issues/513>integrate it with cluster-autoscaler</a> to automatically adjust the size of the Kubernetes clusters based on the same conditions.</p><h2 id=additional-notable-feature-updates>Additional Notable Feature Updates</h2><p><a href=https://github.com/kubernetes/features/issues/585><code>RuntimeClass</code></a> is a new cluster-scoped resource that surfaces container runtime properties to the control plane being released as an alpha feature.</p><p><a href=https://github.com/kubernetes/features/issues/177>Snapshot / restore functionality for Kubernetes and CSI</a> is being introduced as an alpha feature. This provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers.</p><p><a href=https://github.com/kubernetes/features/issues/561>Topology aware dynamic provisioning</a> is now in beta, meaning storage resources can now understand where they live. This also includes beta support to <a href=https://github.com/kubernetes/features/issues/567>AWS EBS</a> and <a href=https://github.com/kubernetes/features/issues/558>GCE PD</a>.</p><p><a href=https://github.com/kubernetes/features/issues/495>Configurable pod process namespace sharing</a> is moving to beta, meaning users can configure containers within a pod to share a common PID namespace by setting an option in the PodSpec.</p><p><a href=https://github.com/kubernetes/features/issues/382>Taint node by condition</a> is now in beta, meaning users have the ability to represent node conditions that block scheduling by using taints.</p><p><a href=https://github.com/kubernetes/features/issues/117>Arbitrary / Custom Metrics</a> in the Horizontal Pod Autoscaler is moving to a second beta to test some additional feature enhancements. This reworked Horizontal Pod Autoscaler functionality includes support for custom metrics and status conditions.</p><p>Improvements that will allow the <a href=https://github.com/kubernetes/features/issues/591>Horizontal Pod Autoscaler to reach proper size faster</a> are moving to beta.</p><p><a href=https://github.com/kubernetes/features/issues/21>Vertical Scaling of Pods</a> is now in beta, which makes it possible to vary the resource limits on a pod over its lifetime. In particular, this is valuable for pets (i.e., pods that are very costly to destroy and re-create).</p><p><a href=https://github.com/kubernetes/features/issues/460>Encryption at rest via KMS</a> is now in beta. This adds multiple encryption providers, including Google Cloud KMS, Azure Key Vault, AWS KMS, and Hashicorp Vault, that will encrypt data as it is stored to etcd.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.12 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.12.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=/docs/tutorials/>interactive tutorials</a>. You can also install 1.12 using <a href=/docs/setup/independent/create-cluster-kubeadm/>Kubeadm</a>.</p><h2 id=5-day-features-blog-series>5 Day Features Blog Series</h2><p>If you’re interested in exploring these features more in depth, check back next week for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:</p><ul><li>Day 1 - Kubelet TLS Bootstrap</li><li>Day 2 - Support for Azure Virtual Machine Scale Sets (VMSS) and Cluster-Autoscaler</li><li>Day 3 - Snapshots Functionality</li><li>Day 4 - RuntimeClass</li><li>Day 5 - Topology Resources</li></ul><h2 id=release-team>Release team</h2><p>This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md>release team</a> led by Tim Pepper, Orchestration & Containers Lead, at VMware Open Source Technology Center. The 36 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 22,000 individual contributors to date and an active community of more than 45,000 people.</p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average, 259 different companies and over 1,400 individuals contribute to Kubernetes each month. <a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&var-period=m&var-repogroup_name=All">Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=user-highlights>User Highlights</h2><p>Established, global organizations are using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at massive scale. Recently published user stories from the community include:</p><ul><li><strong>Ygrene</strong>, a PACE (Property Assessed Clean Energy) financing company, is using cloud native to <a href=https://kubernetes.io/case-studies/ygrene/>bring security and scalability to the finance industry</a>, cutting deployment times down to five minutes with Kubernetes.</li><li><strong>Sling TV</strong>, a live TV streaming service, uses Kubernetes to <a href=https://kubernetes.io/case-studies/slingtv/>enable their hybrid cloud strategy</a> and deliver a high-quality service for their customers.</li><li><strong>ING</strong>, a Dutch multinational banking and financial services corporation, moved to Kubernetes with the intent to eventually be able to go from <a href=https://kubernetes.io/case-studies/ing/>idea to production within 48 hours</a>.</li><li><strong>Pinterest</strong>, a web and mobile application company that is running on 1,000 microservices and hundreds of thousands of data jobs, moved to Kubernetes to <a href=https://kubernetes.io/case-studies/pinterest/>build on-demand scaling and simply the deployment process</a>.</li><li><strong>Pearson</strong>, a global education company serving 75 million learners, is using Kubernetes to <a href=https://kubernetes.io/case-studies/pearson/>transform the way that educational content is delivered online</a> and has saved 15-20% in developer productivity.</li></ul><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community.</p><h2 id=ecosystem-updates>Ecosystem Updates</h2><ul><li>CNCF recently released the findings of their <a href=https://www.cncf.io/blog/2018/08/29/cncf-survey-use-of-cloud-native-technologies-in-production-has-grown-over-200-percent/>bi-annual CNCF survey</a>, finding that the use of cloud native technologies in production has grown over 200% within the last six months.</li><li>CNCF expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual's ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found <a href=https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/>here</a>.</li><li>CNCF added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more <a href=https://www.cncf.io/certification/training/>here</a>.</li><li>CNCF also offers <a href=https://www.cncf.io/certification/training/>online training</a> that teaches the skills needed to create and configure a real-world Kubernetes cluster.</li><li>Kubernetes documentation now features <a href=https://k8s.io/docs/home/>user journeys</a>: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.</li></ul><h2 id=kubecon>KubeCon</h2><p>The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to <a href=https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/>Shanghai</a> from November 13-15, 2018 and <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/>Seattle</a> from December 10-13, 2018. This conference will feature technical sessions, case studies, developer deep dives, salons and more! <a href=https://www.cncf.io/community/kubecon-cloudnativecon-events/>Register today</a>!</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.12 release team on November 6th at 10am PDT to learn about the major features in this release. Register <a href=https://zoom.us/webinar/register/WN_DYMejau3TMaTbk91oC3YkA>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a027c0eb9a8cd0ade81a98a50bed51b1>Hands On With Linkerd 2.0</h1><div class="td-byline mb-4"><time datetime=2018-09-18 class=text-muted>Tuesday, September 18, 2018</time></div><p><strong>Author</strong>: Thomas Rampelberg (Buoyant)</p><p>Linkerd 2.0 was recently announced as generally available (GA), signaling its readiness for production use. In this tutorial, we’ll walk you through how to get Linkerd 2.0 up and running on your Kubernetes cluster in a matter seconds.</p><p>But first, what is Linkerd and why should you care? Linkerd is a service sidecar that augments a Kubernetes service, providing zero-config dashboards and UNIX-style CLI tools for runtime debugging, diagnostics, and reliability. Linkerd is also a service mesh, applied to multiple (or all) services in a cluster to provide a uniform layer of telemetry, security, and control across them.</p><p>Linkerd works by installing ultralight proxies into each pod of a service. These proxies report telemetry data to, and receive signals from, a control plane. This means that using Linkerd doesn’t require any code changes, and can even be installed live on a running service. Linkerd is fully open source, Apache v2 licensed, and is hosted by the Cloud Native Computing Foundation (just like Kubernetes itself!)</p><p>Without further ado, let’s see just how quickly you can get Linkerd running on your Kubernetes cluster. In this tutorial, we’ll walk you through how to deploy Linkerd on any Kubernetes 1.9+ cluster and how to use it to debug failures in a sample gRPC application.</p><h2 id=step-1-install-the-demo-app>Step 1: Install the demo app 🚀</h2><p>Before we install Linkerd, let’s start by installing a basic gRPC demo application called Emojivoto onto your Kubernetes cluster. To install Emojivoto, run:</p><p><code>curl https://run.linkerd.io/emojivoto.yml | kubectl apply -f -</code></p><p>This command downloads the Kubernetes manifest for Emojivoto, and uses kubectl to apply it to your Kubernetes cluster. Emojivoto is comprised of several services that run in the “emojivoto” namespace. You can see the services by running:</p><p><code>kubectl get -n emojivoto deployments</code></p><p>You can also see the app live by running</p><p><code>minikube -n emojivoto service web-svc --url # if you’re on minikube</code></p><p>… or:</p><p><code>kubectl get svc web-svc -n emojivoto -o jsonpath="{.status.loadBalancer.ingress[0].*}" #</code></p><p>… if you’re somewhere else</p><p>Click around. You might notice that some parts of the application are broken! If you were to inspect your handly local Kubernetes dashboard, you wouldn’t see very much interesting---as far as Kubernetes is concerned, the app is running just fine. This is a very common situation! Kubernetes understands whether your pods are running, but not whether they are responding properly.</p><p>In the next few steps, we’ll walk you through how to use Linkerd to diagnose the problem.</p><h2 id=step-2-install-linkerd-s-cli>Step 2: Install Linkerd’s CLI</h2><p>We’ll start by installing Linkerd’s command-line interface (CLI) onto your local machine. Visit the <a href=https://github.com/linkerd/linkerd2/releases/>Linkerd releases page</a>, or simply run:</p><p><code>curl -sL https://run.linkerd.io/install | sh</code></p><p>Once installed, add the <code>linkerd</code> command to your path with:</p><p><code>export PATH=$PATH:$HOME/.linkerd2/bin</code></p><p>You should now be able to run the command <code>linkerd version</code>, which should display:</p><pre><code>Client version: v2.0
Server version: unavailable
</code></pre><p>“Server version: unavailable” means that we need to add Linkerd’s control plane to the cluster, which we’ll do next. But first, let’s validate that your cluster is prepared for Linkerd by running:</p><p><code>linkerd check --pre</code></p><p>This handy command will report any problems that will interfere with your ability to install Linkerd. Hopefully everything looks OK and you’re ready to move on to the next step.</p><h2 id=step-3-install-linkerd-s-control-plane-onto-the-cluster>Step 3: Install Linkerd’s control plane onto the cluster</h2><p>In this step, we’ll install Linkerd’s lightweight control plane into its own namespace (“linkerd”) on your cluster. To do this, run:</p><p><code>linkerd install | kubectl apply -f -</code></p><p>This command generates a Kubernetes manifest and uses <code>kubectl</code> command to apply it to your Kubernetes cluster. (Feel free to inspect the manifest before you apply it.)</p><p>(Note: if your Kubernetes cluster is on GKE with RBAC enabled, you’ll need an extra step: you must grant a ClusterRole of cluster-admin to your Google Cloud account first, in order to install certain telemetry features in the control plane. To do that, run: <code>kubectl create clusterrolebinding cluster-admin-binding-$USER --clusterrole=cluster-admin --user=$(gcloud config get-value account)</code>.)</p><p>Depending on the speed of your internet connection, it may take a minute or two for your Kubernetes cluster to pull the Linkerd images. While that’s happening, we can validate that everything’s happening correctly by running:</p><p><code>linkerd check</code></p><p>This command will patiently wait until Linkerd has been installed and is running.</p><p>Finally, we’re ready to view Linkerd’s dashboard! Just run:</p><p><code>linkerd dashboard</code></p><p>If you see something like below, Linkerd is now running on your cluster. 🎉</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/1-dashboard.png width=700></center><h2 id=step-4-add-linkerd-to-the-web-service>Step 4: Add Linkerd to the web service</h2><p>At this point we have the Linkerd control plane installed in the “linkerd” namespace, and we have our emojivoto demo app installed in the “emojivoto” namespace. But we haven’t actually added Linkerd to our service yet. So let’s do that.</p><p>In this example, let’s pretend we are the owners of the “web” service. Other services, like “emoji” and “voting”, are owned by other teams--so we don’t want to touch them.</p><p>There are a couple ways to add Linkerd to our service. For demo purposes, the easiest is to do something like this:</p><p><code>kubectl get -n emojivoto deploy/web -o yaml | linkerd inject - | kubectl apply -f -</code></p><p>This command retrieves the manifest of the “web” service from Kubernetes, runs this manifest through <code>linkerd inject</code>, and finally reapplies it to the Kubernetes cluster. The <code>linkerd inject</code> command augments the manifest to include Linkerd’s data plane proxies. As with <code>linkerd install</code>, <code>linkerd inject</code> is a pure text operation, meaning that you can inspect the input and output before you use it. Since “web” is a Deployment, Kubernetes is kind enough to slowly roll the service one pod at a time--meaning that “web” can be serving traffic live while we add Linkerd to it!</p><p>We now have a service sidecar running on the “web” service!</p><h2 id=step-5-debugging-for-fun-and-for-profit>Step 5: Debugging for Fun and for Profit</h2><p>Congratulations! You now have a full gRPC application running on your Kubernetes cluster with Linkerd installed on the “web” service. Of course, that application is failing when you use it--so now let’s use Linkerd to track down those errors.</p><p>If you glance at the Linkerd dashboard (the <code>linkerd dashboard</code> command), you should see all services in the “emojivoto” namespace show up. Since “web” has the Linkerd service sidecar installed on it, you’ll also see success rate, requests per second, and latency percentiles show up.</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/2-web-overview.png width=700></center><p>That’s pretty neat, but the first thing you might notice is that success rate is well below 100%! Click on “web” and let’s dig in.</p><p>You should now be looking at the Deployment page for the web service. The first thing you’ll see here is that web is taking traffic from vote-bot (a service included in the Emojivoto manifest to continually generate a low level of live traffic), and has two outgoing dependencies, emoji and voting.</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/3-web-detail.png width=700></center><p>The emoji service is operating at 100%, but the voting service is failing! A failure in a dependent service may be exactly what’s causing the errors that web is returning.</p><p>Let’s scroll a little further down the page, we’ll see a live list of all traffic endpoints that “web” is receiving. This is interesting:</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/4-web-top.png width=700></center><p>There are two calls that are not at 100%: the first is vote-bot’s call the “/api/vote” endpoint. The second is the “VotePoop” call from the web service to the voting service. Very interesting! Since /api/vote is an incoming call, and “/VotePoop” is an outgoing call, this is a good clue that the failure of the vote service’s VotePoop endpoint is what’s causing the problem!</p><p>Finally, if we click on the “tap” icon for that row in the far right column, we’ll be taken to live list of requests that match this endpoint. This allows us to confirm that the requests are failing (they all have <a href=https://godoc.org/google.golang.org/grpc/codes#Code>gRPC status code 2</a>, indicating an error).</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/5-web-tap.png width=700></center><p>At this point we have the ammunition we need to talk to the owners of the vote “voting” service. We’ve identified an endpoint on their service that consistently returns an error, and have found no other obvious sources of failures in the system.</p><p>We hope you’ve enjoyed this journey through Linkerd 2.0. There is much more for you to explore. For example, everything we did above using the web UI can also be accomplished via pure CLI commands, e.g. <code>linkerd top</code>, <code>linkerd stat</code>, and <code>linkerd tap</code>.</p><p>Also, did you notice the little Grafana icon on the very first page we looked at? Linkerd ships with automatic Grafana dashboards for all those metrics, allowing you to view everything you’re seeing in the Linkerd dashboard in a time series format. Check it out!</p><center><img src=/images/blog/2018-09-18-2018-linkerd-2.0/6-grafana.png width=700></center><h2 id=want-more>Want more?</h2><p>In this tutorial, we’ve shown you how to install Linkerd on a cluster, add it as a service sidecar to just one service--while the service is receiving live traffic!---and use it to debug a runtime issue. But this is just the tip of the iceberg. We haven’t even touched any of Linkerd’s reliability or security features!</p><p>Linkerd has a thriving community of adopters and contributors, and we’d love for YOU to be a part of it. For more, check out the <a href=https://linkerd.io/docs>docs</a> and <a href=https://github.com/linkerd/linkerd>GitHub</a> repo, join the <a href=https://slack.linkerd.io/>Linkerd Slack</a> and mailing lists (<a href=https://lists.cncf.io/g/cncf-linkerd-users>users</a>, <a href=https://lists.cncf.io/g/cncf-linkerd-dev>developers</a>, <a href=https://lists.cncf.io/g/cncf-linkerd-announce>announce</a>), and, of course, follow <a href=https://twitter.com/linkerd>@linkerd</a> on Twitter! We can’t wait to have you aboard!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7855b5e12fb40ef21a91c33a9b5b9b59>2018 Steering Committee Election Cycle Kicks Off</h1><div class="td-byline mb-4"><time datetime=2018-09-06 class=text-muted>Thursday, September 06, 2018</time></div><p><strong>Author</strong>: Paris Pittman (Google), Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF)</p><p>Having a clear, definable governance model is crucial for the health of open source projects. For one of the highest velocity projects in the open source world, governance is critical especially for one as large and active as Kubernetes, which is one of the most high-velocity projects in the open source world. A clear structure helps users trust that the project will be nurtured and progress forward. Initially, this structure was laid by the former 7 member bootstrap committee composed of founders and senior contributors with a goal to create the foundational governance building blocks.</p><p>The <a href=https://git.k8s.io/steering/charter.md>initial charter</a> and establishment of an election process to seat a full Steering Committee was a part of those first building blocks. Last year, the bootstrap committee <a href=https://groups.google.com/d/msg/kubernetes-dev/piPuoqFkJwA/mCjwLH81BgAJ>kicked off</a> the first Kubernetes Steering Committee election which brought forth 6 new members from the community as voted on by contributors. These new members plus the bootstrap committee formed the <a href=https://github.com/kubernetes/steering>Steering Committee that we know today</a>. This yearly election cycle will continue to ensure that new representatives get cycled through to add different voices and thoughts on the Kubernetes project strategy.</p><p>The committee has worked hard on topics that will streamline the project and how we operate. SIG (Special Interest Group) governance was an overarching recurring theme this year: Kubernetes community is not a monolithic organization, but a huge, distributed community, where <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups (SIGs) and Working Groups (WGs)</a> are the atomic community units, that are making Kubernetes so successful from the ground.</p><h3 id=contributors-this-is-where-you-come-in>Contributors - this is where you come in.</h3><p>There are three seats up for election this year. The <a href=https://git.k8s.io/community/events/elections/2018>voters guide</a> will get you up to speed on the specifics of this years election including candidate bios as they are updated in real time. The <a href=https://github.com/kubernetes/steering/blob/master/elections.md>elections process doc</a> will steer you towards eligibility, operations, and the fine print.</p><ol><li>Nominate yourself, someone else, and/or put your support to others.</li></ol><p>Want to help chart our course? Interested in governance and community topics? Add your name! <em>The nomination process is optional</em>.</p><ol start=2><li>Vote.</li></ol><p>On September 19th, eligible voters will receive an email poll invite conducted by <a href=https://civs.cs.cornell.edu>CIVS</a>. The newly elected will be announced at the <a href=https://github.com/kubernetes/community/tree/master/communication#weekly-meeting>weekly community meeting</a> on Thursday, October 4th at 5pm UTC.</p><p>To those who are running:</p><img src=/images/blog/2018-09-06-2018-steering-committee-election-cycle-kicks-off/sc-elections.png width=400><h3 id=helpful-resources>Helpful resources</h3><ul><li><a href=https://github.com/kubernetes/steering>Steering Committee</a> - who sits on the committee and terms, their projects and meetings info</li><li><a href=https://git.k8s.io/steering/charter.md>Steering Committee Charter</a> - this is a great read if you’re interested in running (or assessing for the best candidates!)</li><li><a href=https://git.k8s.io/steering/elections.md>Election Process</a></li><li><a href=https://git.k8s.io/community/events/elections/2018>Voters Guide!</a> - Updated on a rolling basis. This guide will always have the latest information throughout the election cycle. The complete schedule of events and candidate bios will be housed here.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-976a1c2afde41e47eb984e28c88c7085>The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</h1><div class="td-byline mb-4"><time datetime=2018-08-29 class=text-muted>Wednesday, August 29, 2018</time></div><p><strong>Author</strong>: Aaron Crickenberger (Google) and Benjamin Elder (Google)</p><p><em>“Large projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.”</em> - <a href=https://git.k8s.io/community/values.md#automation-over-process>Kubernetes Community Values</a></p><p>Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.</p><p>This strategy worked. It worked so well that the project quickly scaled past its contributors’ capacity as humans. What followed was an incredible journey of automation and innovation. We didn’t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.</p><h2 id=the-work>The Work</h2><p>Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.</p><p>Further experience revealed other areas where machines could do the work for us:</p><ul><li>PR Workflow<ul><li>Did the contributor sign our CLA?</li><li>Did the PR pass tests?</li><li>Is the PR mergeable?</li><li>Did the merge commit pass tests?</li></ul></li><li>Triage<ul><li>Who should be reviewing PRs?</li><li>Is there enough information to route an issue to the right people?</li><li>Is an issue still relevant?</li></ul></li><li>Project Health<ul><li>What is happening in the project?</li><li>What should we be paying attention to?</li></ul></li></ul><p>As we developed automation to improve our situation, we followed a few guiding principles:</p><ul><li>Follow the push/poll control loop patterns that worked well for Kubernetes</li><li>Prefer stateless loosely coupled services that do one thing well</li><li>Prefer empowering the entire community over empowering a few core contributors</li><li>Eat our own dogfood and avoid reinventing wheels</li></ul><h2 id=enter-prow>Enter Prow</h2><p>This led us to create <a href=https://git.k8s.io/test-infra/prow>Prow</a> as the central component for our automation. Prow is sort of like an <a href=https://ifttt.com/>If This, Then That</a> for GitHub events, with a built-in library of <a href=https://prow.k8s.io/command-help>commands</a>, <a href=https://prow.k8s.io/plugins>plugins</a>, and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.</p><p>Prow lets us do things like:</p><ul><li>Allow our community to triage issues/PRs by commenting commands such as “/priority critical-urgent”, “/assign mary” or “/close”</li><li>Auto-label PRs based on how much code they change, or which files they touch</li><li>Age out issues/PRs that have remained inactive for too long</li><li>Auto-merge PRs that meet our PR workflow requirements</li><li>Run CI jobs defined as <a href=https://github.com/knative/build>Knative Builds</a>, Kubernetes Pods, or Jenkins jobs</li><li>Enforce org-wide and per-repo GitHub policies like <a href=https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector>branch protection</a> and <a href=https://github.com/kubernetes/test-infra/tree/master/label_sync>GitHub labels</a></li></ul><p>Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. <a href=https://github.com/kubernetes/test-infra/tree/master/prow#getting-started>Getting started with Prow</a> takes a Kubernetes cluster and <code>kubectl apply starter.yaml</code> (running pods on a Kubernetes cluster).</p><p>Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:</p><ul><li><a href=https://github.com/kubernetes/test-infra/tree/master/boskos>Boskos</a>: manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically (<a href="http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1">with monitoring</a>)</li><li><a href=https://github.com/kubernetes/test-infra/tree/master/ghproxy>ghProxy</a>: a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesn’t hit API limits (<a href="http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&orgId=1">with monitoring</a>)</li><li><a href=https://github.com/kubernetes/test-infra/tree/master/greenhouse>Greenhouse</a>: allows us to use a remote bazel cache to provide faster build and test results for PRs (<a href="http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1">with monitoring</a>)</li><li><a href=https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice>Splice</a>: allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity</li><li><a href=https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide>Tide</a>: allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice</li></ul><h2 id=scaling-project-health>Scaling Project Health</h2><p>With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:</p><ul><li><a href=https://github.com/kubernetes/test-infra/tree/master/gubernator>Gubernator</a>: display the results and test history for a given PR</li><li><a href=https://github.com/kubernetes/test-infra/tree/master/kettle>Kettle</a>: transfer data from GCS to a publicly accessible bigquery dataset</li><li><a href=https://k8s-gubernator.appspot.com/pr>PR dashboard</a>: a workflow-aware dashboard that allows contributors to understand which PRs require attention and why</li><li><a href=https://storage.googleapis.com/k8s-gubernator/triage/index.html>Triage</a>: identify common failures that happen across all jobs and tests</li><li><a href=https://k8s-testgrid.appspot.com/>Testgrid</a>: display test results for a given job across all runs, summarize test results across groups of jobs</li></ul><p>We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:</p><ul><li><a href="https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1">Which prow commands are people most actively using</a></li><li><a href="https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&var-period=d7&var-repo_name=All&var-reviewers=All">PR reviews by contributor over time</a></li><li><a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1">Time spent in each phase of our PR workflow</a></li></ul><h2 id=into-the-beyond>Into the Beyond</h2><p>Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had <a href="https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&var-period_name=Last%20year&var-metric=contributions&var-repogroup_name=All">participation from over 13,800 unique developers</a> on GitHub.</p><p>On any given weekday our Prow instance <a href="http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&fullscreen&orgId=1&from=now-6M&to=now">runs over 10,000 CI jobs</a>; from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.</p><p>With today’s <a href=https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google>announcement from CNCF</a> – noting that Google Cloud has begun transferring ownership and management of the Kubernetes project’s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.</p><p>Want to find out more? Come check out these resources:</p><ul><li><a href=https://elder.dev/posts/prow>Prow: Testing the way to Kubernetes Next</a></li><li><a href="https://www.youtube.com/watch?v=BsIC7gPkH5M">Automation and the Kubernetes Contributor Experience</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0e02457e323dc3bf8fea133fc9657a2c>Introducing Kubebuilder: an SDK for building Kubernetes APIs using CRDs</h1><div class="td-byline mb-4"><time datetime=2018-08-10 class=text-muted>Friday, August 10, 2018</time></div><p><strong>Author</strong>: Phillip Wittrock (Google), Sunil Arora (Google)</p><p>How can we enable applications such as MySQL, Spark and Cassandra to manage themselves just like Kubernetes Deployments and Pods do? How do we configure these applications as their own first class APIs instead of a collection of StatefulSets, Services, and ConfigMaps?</p><p>We have been working on a solution and are happy to introduce <a href=https://github.com/kubernetes-sigs/kubebuilder><em>kubebuilder</em></a>, a comprehensive development kit for rapidly building and publishing Kubernetes APIs and Controllers using CRDs. Kubebuilder scaffolds projects and API definitions and is built on top of the <a href=https://github.com/kubernetes-sigs/controller-runtime>controller-runtime</a> libraries.</p><h3 id=why-kubebuilder-and-kubernetes-apis>Why Kubebuilder and Kubernetes APIs?</h3><p>Applications and cluster resources typically require some operational work - whether it is replacing failed replicas with new ones, or scaling replica counts while resharding data. Running the MySQL application may require scheduling backups, reconfiguring replicas after scaling, setting up failure detection and remediation, etc.</p><p>With the Kubernetes API model, management logic is embedded directly into an application specific Kubernetes API, e.g. a “MySQL” API. Users then declaratively manage the application through YAML configuration using tools such as kubectl, just like they do for Kubernetes objects. This approach is referred to as an Application Controller, also known as an Operator. Controllers are a powerful technique backing the core Kubernetes APIs that may be used to build many kinds of solutions in addition to Applications; such as Autoscalers, Workload APIs, Configuration APIs, CI/CD systems, and more.</p><p>However, while it has been possible for trailblazers to build new Controllers on top of the raw API machinery, doing so has been a DIY “from scratch” experience, requiring developers to learn low level details about how Kubernetes libraries are implemented, handwrite boilerplate code, and wrap their own solutions for integration testing, RBAC configuration, documentation, etc. Kubebuilder makes this experience simple and easy by applying the lessons learned from building the core Kubernetes APIs.</p><h3 id=getting-started-building-application-controllers-and-kubernetes-apis>Getting Started Building Application Controllers and Kubernetes APIs</h3><p>By providing an opinionated and structured solution for creating Controllers and Kubernetes APIs, developers have a working “out of the box” experience that uses the lessons and best practices learned from developing the core Kubernetes APIs. Creating a new "Hello World" Controller with <code>kubebuilder</code> is as simple as:</p><ol><li>Create a project with <code>kubebuilder init</code></li><li>Define a new API with <code>kubebuilder create api</code></li><li>Build and run the provided main function with <code>make install & make run</code></li></ol><p>This will scaffold the API and Controller for users to modify, as well as scaffold integration tests, RBAC rules, Dockerfiles, Makefiles, etc.
After adding their implementation to the project, users create the artifacts to publish their API through:</p><ol><li>Build and push the container image from the provided Dockerfile using <code>make docker-build</code> and <code>make docker-push</code> commands</li><li>Deploy the API using <code>make deploy</code> command</li></ol><p>Whether you are already a Controller aficionado or just want to learn what the buzz is about, check out the <a href=https://github.com/kubernetes-sigs/kubebuilder>kubebuilder repo</a> or take a look at an example in the <a href=https://book.kubebuilder.io>kubebuilder book</a> to learn about how simple and easy it is to build Controllers.</p><h3 id=get-involved>Get Involved</h3><p>Kubebuilder is a project under <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API Machinery</a> and is being actively developed by contributors from many companies such as Google, Red Hat, VMware, Huawei and others. Get involved by giving us feedback through these channels:</p><ul><li>Kubebuilder <a href=https://slack.k8s.io/#kubebuilder>chat room on Slack</a></li><li>SIG <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery>mailing list</a></li><li><a href=https://github.com/kubernetes-sigs/kubebuilder/issues/new>GitHub issues</a></li><li>Send a pull request in the <a href=https://github.com/kubernetes-sigs/kubebuilder>kubebuilder repo</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ee824f5efff26f6e8a06bcfecb11a9ce>Out of the Clouds onto the Ground: How to Make Kubernetes Production Grade Anywhere</h1><div class="td-byline mb-4"><time datetime=2018-08-03 class=text-muted>Friday, August 03, 2018</time></div><p><strong>Authors</strong>: Steven Wong (VMware), Michael Gasch (VMware)</p><p>This blog offers some guidelines for running a production grade Kubernetes cluster in an environment like an on-premise data center or edge location.</p><p>What does it mean to be “production grade”?</p><ul><li>The installation is secure</li><li>The deployment is managed with a repeatable and recorded process</li><li>Performance is predictable and consistent</li><li>Updates and configuration changes can be safely applied</li><li>Logging and monitoring is in place to detect and diagnose failures and resource shortages</li><li>Service is “highly available enough” considering available resources, including constraints on money, physical space, power, etc.</li><li>A recovery process is available, documented, and tested for use in the event of failures</li></ul><p>In short, production grade means anticipating accidents and preparing for recovery with minimal pain and delay.</p><p>This article is directed at on-premise Kubernetes deployments on a hypervisor or bare-metal platform, facing finite backing resources compared to the expansibility of the major public clouds. However, some of these recommendations may also be useful in a public cloud if budget constraints limit the resources you choose to consume.</p><p>A single node bare-metal Minikube deployment may be cheap and easy, but is not production grade. Conversely, you’re not likely to achieve Google’s Borg experience in a retail store, branch office, or edge location, nor are you likely to need it.</p><p>This blog offers some guidance on achieving a production worthy Kubernetes deployment, even when dealing with some resource constraints.</p><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/without-incidence.png alt="without incidence"></p><h2 id=critical-components-in-a-kubernetes-cluster>Critical components in a Kubernetes cluster</h2><p>Before we dive into the details, it is critical to understand the overall Kubernetes architecture.</p><p>A Kubernetes cluster is a highly distributed system based on a control plane and clustered worker node architecture as depicted below.</p><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/api-server.png alt="api server"></p><p>Typically the API server, Controller Manager and Scheduler components are co-located within multiple instances of control plane (aka Master) nodes. Master nodes usually include etcd too, although there are high availability and large cluster scenarios that call for running etcd on independent hosts. The components can be run as containers, and optionally be supervised by Kubernetes, i.e. running as statics pods.</p><p>For high availability, redundant instances of these components are used. The importance and required degree of redundancy varies.</p><h3 id=kubernetes-components-from-an-ha-perspective>Kubernetes components from an HA perspective</h3><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/kubernetes-components-ha.png alt="kubernetes components HA"></p><p>Risks to these components include hardware failures, software bugs, bad updates, human errors, network outages, and overloaded systems resulting in resource exhaustion. Redundancy can mitigate the impact of many of these hazards. In addition, the resource scheduling and high availability features of a hypervisor platform can be useful to surpass what can be achieved using the Linux operating system, Kubernetes, and a container runtime alone.</p><p>The API Server uses multiple instances behind a load balancer to achieve scale and availability. The load balancer is a critical component for purposes of high availability. Multiple DNS API Server ‘A’ records might be an alternative if you don’t have a load balancer.</p><p>The kube-scheduler and kube-controller-manager engage in a leader election process, rather than utilizing a load balancer. Since a <a href=/docs/tasks/administer-cluster/running-cloud-controller/>cloud-controller-manager</a> is used for selected types of hosting infrastructure, and these have implementation variations, they will not be discussed, beyond indicating that they are a control plane component.</p><p>Pods running on Kubernetes worker nodes are managed by the kubelet agent. Each worker instance runs the kubelet agent and a <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>CRI-compatible</a> container runtime. Kubernetes itself is designed to monitor and recover from worker node outages. But for critical workloads, hypervisor resource management, workload isolation and availability features can be used to enhance availability and make performance more predictable.</p><h2 id=etcd>etcd</h2><p>etcd is the persistent store for all Kubernetes objects. The availability and recoverability of the etcd cluster should be the first consideration in a production-grade Kubernetes deployment.</p><p>A five-node etcd cluster is a best practice if you can afford it. Why? Because you could engage in maintenance on one and still tolerate a failure. A three-node cluster is the minimum <a href=https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size>recommendation</a> for production-grade service, even if only a single hypervisor host is available. More than seven nodes is not recommended except for <a href=https://monzo.com/blog/2017/11/29/very-robust-etcd/>very large installations</a> straddling multiple availability zones.</p><p>The minimum recommendation for hosting an etcd cluster node is 2GB of RAM with 8GB of SSD-backed disk. Usually, 8GB RAM and a 20GB disk will be enough. Disk performance affects failed node recovery time. See <a href=https://coreos.com/etcd/docs/latest/op-guide/hardware.html>https://coreos.com/etcd/docs/latest/op-guide/hardware.html</a> for more on this.</p><h3 id=consider-multiple-etcd-clusters-in-special-situations>Consider multiple etcd clusters in special situations</h3><p>For very large Kubernetes clusters, consider using a separate etcd cluster for Kubernetes events so that event storms do not impact the main Kubernetes API service. If you use flannel networking, it retains configuration in etcd and may have differing version requirements than Kubernetes, which can complicate etcd backup -- consider using a dedicated etcd cluster for flannel.</p><h2 id=single-host-deployments>Single host deployments</h2><p>The availability risk list includes hardware, software and people. If you are limited to a single host, the use of redundant storage, error-correcting memory and dual power supplies can reduce hardware failure exposure. Running a hypervisor on the physical host will allow operation of redundant software components and add operational advantages related to deployment, upgrade, and resource consumption governance, with predictable and repeatable performance under stress. For example, even if you can only afford to run singletons of the master services, they need to be protected from overload and resource exhaustion while competing with your application workload. A hypervisor can be more effective and easier to manage than configuring Linux scheduler priorities, cgroups, Kubernetes flags, etc.</p><p>If resources on the host permit, you can deploy three etcd VMs. Each of the etcd VMs should be backed by a different physical storage device, or they should use separate partitions of a backing store using redundancy (mirroring, RAID, etc).</p><p>Dual redundant instances of the API server, scheduler and controller manager would be the next upgrade, if your single host has the resources.</p><h3 id=single-host-deployment-options-least-production-worthy-to-better>Single host deployment options, least production worthy to better</h3><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/single-host-deployment.png alt="single host deployment"></p><h2 id=dual-host-deployments>Dual host deployments</h2><p>With two hosts, storage concerns for etcd are the same as a single host, you want redundancy. And you would preferably run 3 etcd instances. Although possibly counter-intuitive, it is better to concentrate all etcd nodes on a single host. You do not gain reliability by doing a 2+1 split across two hosts - because loss of the node holding the majority of etcd instances results in an outage, whether that majority is 2 or 3. If the hosts are not identical, put the whole etcd cluster on the most reliable host.</p><p>Running redundant API Servers, kube-schedulers, and kube-controller-managers is recommended. These should be split across hosts to minimize risk due to container runtime, OS and hardware failures.</p><p>Running a hypervisor layer on the physical hosts will allow operation of redundant software components with resource consumption governance, and can have planned maintenance operational advantages.</p><h3 id=dual-host-deployment-options-least-production-worthy-to-better>Dual host deployment options, least production worthy to better</h3><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/dual-host-deployment.png alt="dual host deployment"></p><p>Triple (or larger) host deployments -- Moving into uncompromised production-grade service
Splitting etcd across three hosts is recommended. A single hardware failure will reduce application workload capacity, but should not result in a complete service outage.</p><p>With very large clusters, more etcd instances will be required.</p><p>Running a hypervisor layer offers operational advantages and better workload isolation. It is beyond the scope of this article, but at the three-or-more host level, advanced features may be available (clustered redundant shared storage, resource governance with dynamic load balancing, automated health monitoring with live migration or failover).</p><h3 id=triple-or-more-host-options-least-production-worthy-to-better>Triple (or more) host options, least production worthy to better</h3><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/triple-host-deployment.png alt="triple host deployment"></p><h2 id=kubernetes-configuration-settings>Kubernetes configuration settings</h2><p>Master and Worker nodes should be protected from overload and resource exhaustion. Hypervisor features can be used to isolate critical components and reserve resources. There are also Kubernetes configuration settings that can throttle things like API call rates and pods per node. Some install suites and commercial distributions take care of this, but if you are performing a custom Kubernetes deployment, you may find that the defaults are not appropriate, particularly if your resources are small or your cluster is large.</p><p>Resource consumption by the control plane will correlate with the number of pods and the pod churn rate. Very large and very small clusters will benefit from non-default <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>settings</a> of kube-apiserver request throttling and memory. Having these too high can lead to request limit exceeded and out of memory errors.</p><p>On worker nodes, <a href=/docs/tasks/administer-cluster/reserve-compute-resources/>Node Allocatable</a> should be configured based on a reasonable supportable workload density at each node. Namespaces can be created to subdivide the worker node cluster into multiple virtual clusters with resource CPU and memory <a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>quotas</a>. Kubelet handling of <a href=/docs/tasks/administer-cluster/out-of-resource/>out of resource</a> conditions can be configured.</p><h2 id=security>Security</h2><p>Every Kubernetes cluster has a cluster root Certificate Authority (CA). The Controller Manager, API Server, Scheduler, kubelet client, kube-proxy and administrator certificates need to be generated and installed. If you use an install tool or a distribution this may be handled for you. A manual process is described <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md>here</a>. You should be prepared to reinstall certificates in the event of node replacements or expansions.</p><p>As Kubernetes is entirely API driven, controlling and limiting who can access the cluster and what actions they are allowed to perform is essential. Encryption and authentication options are addressed in this <a href=/docs/tasks/administer-cluster/securing-a-cluster/>documentation</a>.</p><p>Kubernetes application workloads are based on container images. You want the source and content of these images to be trustworthy. This will almost always mean that you will host a local container image repository. Pulling images from the public Internet can present both reliability and security issues. You should choose a repository that supports image signing, security scanning, access controls on pushing and pulling images, and logging of activity.</p><p>Processes must be in place to support applying updates for host firmware, hypervisor, OS, Kubernetes, and other dependencies. Version monitoring should be in place to support audits.</p><p>Recommendations:</p><ul><li>Tighten security settings on the control plane components beyond defaults (e.g., <a href=http://blog.kontena.io/locking-down-kubernetes-workers/>locking down worker nodes</a>)</li><li>Utilize <a href=/docs/concepts/policy/pod-security-policy/>Pod Security Policies</a></li><li>Consider the <a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy</a> integration available with your networking solution, including how you will accomplish tracing, monitoring and troubleshooting.</li><li>Use RBAC to drive authorization decisions and enforcement.</li><li>Consider physical security, especially when deploying to edge or remote office locations that may be unattended. Include storage encryption to limit exposure from stolen devices and protection from attachment of malicious devices like USB keys.</li><li>Protect Kubernetes plain-text cloud provider credentials (access keys, tokens, passwords, etc.)</li></ul><p>Kubernetes <a href=/docs/concepts/configuration/secret/>secret</a> objects are appropriate for holding small amounts of sensitive data. These are retained within etcd. These can be readily used to hold credentials for the Kubernetes API but there are times when a workload or an extension of the cluster itself needs a more full-featured solution. The HashiCorp Vault project is a popular solution if you need more than the built-in secret objects can provide.</p><h2 id=disaster-recovery-and-backup>Disaster Recovery and Backup</h2><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/disaster-recovery.png alt="disaster recovery"></p><p>Utilizing redundancy through the use of multiple hosts and VMs helps reduce some classes of outages, but scenarios such as a sitewide natural disaster, a bad update, getting hacked, software bugs, or human error could still result in an outage.</p><p>A critical part of a production deployment is anticipating a possible future recovery.</p><p>It’s also worth noting that some of your investments in designing, documenting, and automating a recovery process might also be re-usable if you need to do large-scale replicated deployments at multiple sites.</p><p>Elements of a DR plan include backups (and possibly replicas), replacements, a planned process, people who can carry out the process, and recurring training. Regular test exercises and <a href=https://github.com/dastergon/awesome-chaos-engineering>chaos engineering principles</a> can be used to audit your readiness.</p><p>Your availability requirements might demand that you retain local copies of the OS, Kubernetes components, and container images to allow recovery even during an Internet outage. The ability to deploy replacement hosts and nodes in an “air-gapped” scenario can also offer security and speed of deployment advantages.</p><p>All Kubernetes objects are stored on etcd. Periodically backing up the etcd cluster data is important to recover Kubernetes clusters under disaster scenarios, such as losing all master nodes.</p><p>Backing up an etcd cluster can be accomplished with etcd’s <a href=https://coreos.com/etcd/docs/latest/op-guide/recovery.html>built-in</a> snapshot mechanism, and copying the resulting file to storage in a different failure domain. The snapshot file contains all the Kubernetes states and critical information. In order to keep the sensitive Kubernetes data safe, encrypt the snapshot files.</p><p>Using disk volume based snapshot recovery of etcd can have issues; see <a href=https://github.com/kubernetes/kubernetes/issues/40027>#40027</a>. API-based backup solutions (e.g., <a href=https://github.com/heptio/ark>Ark</a>) can offer more granular recovery than a etcd snapshot, but also can be slower. You could utilize both snapshot and API-based backups, but you should do one form of etcd backup as a minimum.</p><p>Be aware that some Kubernetes extensions may maintain state in independent etcd clusters, on persistent volumes, or through other mechanisms. If this state is critical, it should have a backup and recovery plan.</p><p>Some critical state is held outside etcd. Certificates, container images, and other configuration- and operation-related state may be managed by your automated install/update tooling. Even if these items can be regenerated, backup or replication might allow for faster recovery after a failure. Consider backups with a recovery plan for these items:</p><ul><li>Certificate and key pairs<ul><li>CA</li><li>API Server</li><li>Apiserver-kubelet-client</li><li>ServiceAccount signing</li><li>“Front proxy”</li><li>Front proxy client</li></ul></li><li>Critical DNS records</li><li>IP/subnet assignments and reservations</li><li>External load-balancers</li><li>kubeconfig files</li><li>LDAP or other authentication details</li><li>Cloud provider specific account and configuration data</li></ul><h2 id=considerations-for-your-production-workloads>Considerations for your production workloads</h2><p>Anti-affinity specifications can be used to split clustered services across backing hosts, but at this time the settings are used only when the pod is scheduled. This means that Kubernetes can restart a failed node of your clustered application, but does not have a native mechanism to rebalance after a fail back. This is a topic worthy of a separate blog, but supplemental logic might be useful to achieve optimal workload placements after host or worker node recoveries or expansions. The <a href=/docs/concepts/configuration/pod-priority-preemption/>Pod Priority and Preemption feature</a> can be used to specify a preferred triage in the event of resource shortages caused by failures or bursting workloads.</p><p>For stateful services, external attached volume mounts are the standard Kubernetes recommendation for a non-clustered service (e.g., a typical SQL database). At this time Kubernetes managed snapshots of these external volumes is in the category of a <a href="https://docs.google.com/presentation/d/1dgxfnroRAu0aF67s-_bmeWpkM1h2LCxe6lB1l1oS0EQ/edit#slide=id.g3ca07c98c2_0_47">roadmap feature request</a>, likely to align with the Container Storage Interface (CSI) integration. Thus performing backups of such a service would involve application specific, in-pod activity that is beyond the scope of this document. While awaiting better Kubernetes support for a snapshot and backup workflow, running your database service in a VM rather than a container, and exposing it to your Kubernetes workload may be worth considering.</p><p>Cluster-distributed stateful services (e.g., Cassandra) can benefit from splitting across hosts, using <a href=https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/#disclaimer>local persistent volumes</a> if resources allow. This would require deploying multiple Kubernetes worker nodes (could be VMs on hypervisor hosts) to preserve a quorum under single point failures.</p><h2 id=other-considerations>Other considerations</h2><p><a href=/docs/concepts/cluster-administration/logging/>Logs</a> and <a href=/docs/tasks/debug-application-cluster/resource-usage-monitoring/>metrics</a> (if collected and persistently retained) are valuable to diagnose outages, but given the variety of technologies available it will not be addressed in this blog. If Internet connectivity is available, it may be desirable to retain logs and metrics externally at a central location.</p><p>Your production deployment should utilize an automated installation, configuration and update tool (e.g., <a href=https://github.com/kubernetes-incubator/kubespray>Ansible</a>, <a href=https://github.com/cloudfoundry-incubator/kubo-deployment>BOSH</a>, <a href=https://github.com/chef-cookbooks/kubernetes>Chef</a>, <a href=/docs/getting-started-guides/ubuntu/installation/>Juju</a>, <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>, <a href=https://forge.puppet.com/puppetlabs/kubernetes>Puppet</a>, etc.). A manual process will have repeatability issues, be labor intensive, error prone, and difficult to scale. <a href=https://www.cncf.io/certification/software-conformance/#logos>Certified distributions</a> are likely to include a facility for retaining configuration settings across updates, but if you implement your own install and config toolchain, then retention, backup and recovery of the configuration artifacts is essential. Consider keeping your deployment components and settings under a version control system such as Git.</p><h2 id=outage-recovery>Outage recovery</h2><p><a href=https://en.wikipedia.org/wiki/Runbook>Runbooks</a> documenting recovery procedures should be tested and retained offline -- perhaps even printed. When an on-call staff member is called up at 2 am on a Friday night, it may not be a great time to improvise. Better to execute from a pre-planned, tested checklist -- with shared access by remote and onsite personnel.</p><h2 id=final-thoughts>Final thoughts</h2><p><img src=/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/airplane.png alt=airplane></p><p>Buying a ticket on a commercial airline is convenient and safe. But when you travel to a remote location with a short runway, that commercial Airbus A320 flight isn’t an option. This doesn’t mean that air travel is off the table. It does mean that some compromises are necessary.</p><p>The adage in aviation is that on a single engine aircraft, an engine failure means you crash. With twin engines, at the very least, you get more choices of where you crash. Kubernetes on a small number of hosts is similar, and if your business case justifies it, you might scale up to a larger fleet of mixed large and small vehicles (e.g., FedEx, Amazon).</p><p>Those designing a production-grade Kubernetes solution have a lot of options and decisions. A blog-length article can’t provide all the answers, and can’t know your specific priorities. We do hope this offers a checklist of things to consider, along with some useful guidance. Some options were left “on the cutting room floor” (e.g., running Kubernetes components using <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-and-alpha-in-v19-self-hosting>self-hosting</a> instead of static pods). These might be covered in a follow up if there is interest. Also, Kubernetes’ high enhancement rate means that if your search engine found this article after 2019, some content might be past the “sell by” date.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5cd188187bd5cac97e56eebd5891dd04>Dynamically Expand Volume with CSI and Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-08-02 class=text-muted>Thursday, August 02, 2018</time></div><p><strong>Author</strong>: Orain Xiong (Co-Founder, WoquTech)</p><p><em>There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity.</em></p><h2 id=introduction>Introduction</h2><p>As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.</p><p>They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.</p><p>Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.</p><p>With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.</p><p>Currently, expanding volume is only available with those storage provisioners:</p><ul><li>gcePersistentDisk</li><li>awsElasticBlockStore</li><li>OpenStack Cinder</li><li>glusterfs</li><li>rbd</li></ul><p>In order to enable this feature, we should set feature gate <code>ExpandPersistentVolumes</code> true and turn on the <code>PersistentVolumeClaimResize</code> admission plugin. Once <code>PersistentVolumeClaimResize</code> has been enabled, resizing will be allowed by a Storage Class whose <code>allowVolumeExpansion</code> field is set to true.</p><p>Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.</p><p>This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.</p><h2 id=container-storage-interface-csi>Container Storage Interface (CSI)</h2><p>To have a better understanding of what we're going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.</p><p>CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.</p><p>This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:</p><p><img src=/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png alt="csi diagram"></p><ul><li>Three new external components are introduced to decouple Kubernetes and Storage Provider logic</li><li>Blue arrows present the conventional way to call against API Server</li><li>Red arrows present gRPC to call against Volume Driver</li></ul><p>For more details, please visit: <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>https://github.com/container-storage-interface/spec/blob/master/spec.md</a></p><h2 id=extend-csi-and-kubernetes>Extend CSI and Kubernetes</h2><p>In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, “in-tree” volume plugin, external-provisioner and external-attacher.</p><h2 id=extend-csi-spec>Extend CSI spec</h2><p>The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including <code>RequiresFSResize</code> and <code>ControllerResizeVolume</code> and <code>NodeResizeVolume</code>, should be introduced.</p><pre><code>service Controller {
 rpc CreateVolume (CreateVolumeRequest)
   returns (CreateVolumeResponse) {}
……
 rpc RequiresFSResize (RequiresFSResizeRequest)
   returns (RequiresFSResizeResponse) {}
 rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
   returns (ControllerResizeVolumeResponse) {}
}

service Node {
 rpc NodeStageVolume (NodeStageVolumeRequest)
   returns (NodeStageVolumeResponse) {}
……
 rpc NodeResizeVolume (NodeResizeVolumeRequest)
   returns (NodeResizeVolumeResponse) {}
}
</code></pre><h2 id=extend-in-tree-volume-plugin>Extend “In-Tree” Volume Plugin</h2><p>In addition to the extend CSI specification, the <code>csiPlugin﻿</code> interface within Kubernetes should also implement <code>expandablePlugin</code>. The <code>csiPlugin</code> interface will expand <code>PersistentVolumeClaim</code> representing for <code>ExpanderController</code>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> ExpandableVolumePlugin <span style=color:#a2f;font-weight:700>interface</span> {
VolumePlugin
<span style=color:#00a000>ExpandVolumeDevice</span>(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, <span style=color:#0b0;font-weight:700>error</span>)
<span style=color:#00a000>RequiresFSResize</span>() <span style=color:#0b0;font-weight:700>bool</span>
}
</code></pre></div><h3 id=implement-volume-driver>Implement Volume Driver</h3><p>Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:</p><ul><li>CreateVolume</li><li>DeleteVolume</li><li>ControllerPublishVolume</li><li>ControllerUnpublishVolume</li><li>ValidateVolumeCapabilities</li><li>ListVolumes</li><li>GetCapacity</li><li>ControllerGetCapabilities</li><li>RequiresFSResize</li><li>ControllerResizeVolume</li></ul><h2 id=demonstration>Demonstration</h2><p>Let’s demonstrate this feature with a concrete user case.</p><ul><li>Create storage class for CSI storage provisioner</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>allowVolumeExpansion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-qcfs<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csiProvisionerSecretName</span>:<span style=color:#bbb> </span>orain-test<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>csiProvisionerSecretNamespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>csi-qcfsplugin<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>reclaimPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>volumeBindingMode</span>:<span style=color:#bbb> </span>Immediate<span style=color:#bbb>
</span></code></pre></div><ul><li><p>Deploy CSI Volume Driver including storage provisioner <code>csi-qcfsplugin</code> across Kubernetes cluster</p></li><li><p>Create PVC <code>qcfs-pvc</code> which will be dynamically provisioned by storage class <code>csi-qcfs</code></p></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>qcfs-pvc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span>.<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- ReadWriteOnce<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>300Gi<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>csi-qcfs<span style=color:#bbb>
</span></code></pre></div><ul><li>Create MySQL 5.7 instance to use PVC <code>qcfs-pvc</code></li><li>In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:<ul><li>Batch insert to make MySQL consuming more file system capacity</li><li>Surge query request</li></ul></li><li>Dynamically expand volume capacity through edit pvc <code>qcfs-pvc</code> configuration</li></ul><p>The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.</p><p><img src=/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png alt="prometheus grafana"></p><p>We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.</p><h2 id=conclusion>Conclusion</h2><p>Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-952c1a872bccffe28c4f0737dd9a3548>KubeVirt: Extending Kubernetes with CRDs for Virtualized Workloads</h1><div class="td-byline mb-4"><time datetime=2018-07-27 class=text-muted>Friday, July 27, 2018</time></div><p><strong>Author</strong>: David Vossel (Red Hat)</p><h2 id=what-is-kubevirt>What is KubeVirt?</h2><p><a href=https://github.com/kubevirt/kubevirt>KubeVirt</a> is a Kubernetes addon that provides users the ability to schedule traditional virtual machine workloads side by side with container workloads. Through the use of <a href=/docs/concepts/extend-Kubernetes/api-extension/custom-resources/>Custom Resource Definitions</a> (CRDs) and other Kubernetes features, KubeVirt seamlessly extends existing Kubernetes clusters to provide a set of virtualization APIs that can be used to manage virtual machines.</p><h2 id=why-use-crds-over-an-aggregated-api-server>Why Use CRDs Over an Aggregated API Server?</h2><p>Back in the middle of 2017, those of us working on KubeVirt were at a crossroads. We had to make a decision whether or not to extend Kubernetes using an aggregated API server or to make use of the new Custom Resource Definitions (CRDs) feature.</p><p>At the time, CRDs lacked much of the functionality we needed to deliver our feature set. The ability to create our own aggregated API server gave us all the flexibility we needed, but it had one major flaw. <strong>An aggregated API server significantly increased the complexity involved with installing and operating KubeVirt.</strong></p><p>The crux of the issue for us was that aggregated API servers required access to etcd for object persistence. This meant that cluster admins would have to either accept that KubeVirt needs a separate etcd deployment which increases complexity, or provide KubeVirt with shared access to the Kubernetes etcd store which introduces risk.</p><p>We weren’t okay with this tradeoff. Our goal wasn’t to just extend Kubernetes to run virtualization workloads, it was to do it in the most seamless and effortless way possible. We felt that the added complexity involved with an aggregated API server sacrificed the part of the user experience involved with installing and operating KubeVirt.</p><p><strong>Ultimately we chose to go with CRDs and trust that the Kubernetes ecosystem would grow with us to meet the needs of our use case.</strong> Our bets were well placed. At this point there are either solutions in place or solutions under discussion that solve every feature gap we encountered back in 2017 when were evaluating CRDs vs an aggregated API server.</p><h2 id=building-layered-kubernetes-like-apis-with-crds>Building Layered “Kubernetes like” APIs with CRDs</h2><p>We designed KubeVirt’s API to follow the same patterns users are already familiar with in the Kubernetes core API.</p><p>For example, in Kubernetes the lowest level unit that users create to perform work is a Pod. Yes, Pods do have multiple containers but logically the Pod is the unit at the bottom of the stack. A Pod represents a mortal workload. The Pod gets scheduled, eventually the Pod’s workload terminates, and that’s the end of the Pod’s lifecycle.</p><p>Workload controllers such as the ReplicaSet and StatefulSet are layered on top of the Pod abstraction to help manage scale out and stateful applications. From there we have an even higher level controller called a Deployment which is layered on top of ReplicaSets help manage things like rolling updates.</p><p>In KubeVirt, this concept of layering controllers is at the very center of our design. The KubeVirt VirtualMachineInstance (VMI) object is the lowest level unit at the very bottom of the KubeVirt stack. Similar in concept to a Pod, a VMI represents a single mortal virtualized workload that executes once until completion (powered off).</p><p>Layered on top of VMIs we have a workload controller called a VirtualMachine (VM). The VM controller is where we really begin to see the differences between how users manage virtualized workloads vs containerized workloads. Within the context of existing Kubernetes functionality, the best way to describe the VM controller’s behavior is to compare it to a StatefulSet of size one. This is because the VM controller represents a single stateful (immortal) virtual machine capable of persisting state across both node failures and multiple restarts of its underlying VMI. This object behaves in the way that is familiar to users who have managed virtual machines in AWS, GCE, OpenStack or any other similar IaaS cloud platform. The user can shutdown a VM, then choose to start that exact same VM up again at a later time.</p><p>In addition to VMs, we also have a VirtualMachineInstanceReplicaSet (VMIRS) workload controller which manages scale out of identical VMI objects. This controller behaves nearly identically to the Kubernetes ReplicSet controller. The primary difference being that the VMIRS manages VMI objects and the ReplicaSet manages Pods. Wouldn’t it be nice if we could come up with a way to <a href=https://github.com/kubernetes/kubernetes/issues/65622>use the Kubernetes ReplicaSet controller to scale out CRDs?</a></p><p>Each one of these KubeVirt objects (VMI, VM, VMIRS) are registered with Kubernetes as a CRD when the KubeVirt install manifest is posted to the cluster. By registering our APIs as CRDs with Kubernetes, all the tooling involved with managing Kubernetes clusters (like kubectl) have access to the KubeVirt APIs just as if they are native Kubernetes objects.</p><h2 id=dynamic-webhooks-for-api-validation>Dynamic Webhooks for API Validation</h2><p>One of the responsibilities of the Kubernetes API server is to intercept and validate requests prior to allowing objects to be persisted into etcd. For example, if someone tries to create a Pod using a malformed Pod specification, the Kubernetes API server immediately catches the error and rejects the POST request. This all occurs before the object is persistent into etcd preventing the malformed Pod specification from making its way into the cluster.</p><p>This validation occurs during a process called admission control. Until recently, it was not possible to extend the default Kubernetes admission controllers without altering code and compiling/deploying an entirely new Kubernetes API server. This meant that if we wanted to perform admission control on KubeVirt’s CRD objects while they are posted to the cluster, we’d have to build our own version of the Kubernetes API server and convince our users to use that instead. That was not a viable solution for us.</p><p>Using the new <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>Dynamic Admission Control</a> feature that first landed in Kubernetes 1.9, we now have a path for performing custom validation on KubeVirt API through the use of a <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#external-admission-webhooks>ValidatingAdmissionWebhook</a>. This feature allows KubeVirt to dynamically register an HTTPS webhook with Kubernetes at KubeVirt install time. After registering the custom webhook, all requests related to KubeVirt API objects are forwarded from the Kubernetes API server to our HTTPS endpoint for validation. If our endpoint rejects a request for any reason, the object will not be persisted into etcd and the client receives our response outlining the reason for the rejection.</p><p>For example, if someone posts a malformed VirtualMachine object, they’ll receive an error indicating what the problem is.</p><pre><code>$ kubectl create -f my-vm.yaml 
Error from server: error when creating &quot;my-vm.yaml&quot;: admission webhook &quot;virtualmachine-validator.kubevirt.io&quot; denied the request: spec.template.spec.domain.devices.disks[0].volumeName 'registryvolume' not found.
</code></pre><p>In the example output above, that error response is coming directly from KubeVirt’s admission control webhook.</p><h2 id=crd-openapiv3-validation>CRD OpenAPIv3 Validation</h2><p>In addition to the validating webhook, KubeVirt also uses the ability to provide an <a href=/docs/tasks/access-kubernetes-API/extend-api-custom-resource-definitions/#advanced-topics>OpenAPIv3 validation schema</a> when registering a CRD with the cluster. While the OpenAPIv3 schema does not let us express some of the more advanced validation checks that the validation webhook provides, it does offer the ability to enforce simple validation checks involving things like required fields, max/min value lengths, and verifying that values are formatted in a way that matches a regular expression string.</p><h2 id=dynamic-webhooks-for-podpreset-like-behavior>Dynamic Webhooks for “PodPreset Like” Behavior</h2><p>The Kubernetes Dynamic Admission Control feature is not only limited to validation logic, it also provides the ability for applications like KubeVirt to both intercept and mutate requests as they enter the cluster. This is achieved through the use of a <strong>MutatingAdmissionWebhook</strong> object. In KubeVirt, we are looking to use a mutating webhook to support our VirtualMachinePreset (VMPreset) feature.</p><p>A VMPreset acts in a similar way to a PodPreset. Just like a PodPreset allows users to define values that should automatically be injected into pods at creation time, a VMPreset allows users to define values that should be injected into VMs at creation time. Through the use of a mutating webhook, KubeVirt can intercept a request to create a VM, apply VMPresets to the VM spec, and then validate that the resulting VM object. This all occurs before the VM object is persisted into etcd which allows KubeVirt to immediately notify the user of any conflicts at the time the request is made.</p><h2 id=subresources-for-crds>Subresources for CRDs</h2><p>When comparing the use of CRDs to an aggregated API server, one of the features CRDs lack is the ability to support subresources. Subresources are used to provide additional resource functionality. For example, the <code>pod/logs</code> and <code>pod/exec</code> subresource endpoints are used behind the scenes to provide the <code>kubectl logs</code> and <code>kubectl exec</code> command functionality.</p><p>Just like Kubernetes uses the <code>pod/exec</code> subresource to provide access to a pod’s environment, in KubeVirt we want subresources to provide serial-console, VNC, and SPICE access to a virtual machine. By adding virtual machine guest access through subresources, we can leverage RBAC to provide access control for these features.</p><p>So, given that the KubeVirt team decided to use CRD’s instead of an aggregated API server for custom resource support, how can we have subresources for CRDs when the CRD feature expiclity does not support subresources?</p><p>We created a workaround for this limitation by implementing a stateless aggregated API server that exists only to serve subresource requests. With no state, we don’t have to worry about any of the issues we identified earlier with regards to access to etcd. This means the KubeVirt API is actually supported through a combination of both CRDs for resources and an aggregated API server for stateless subresources.</p><p>This isn’t a perfect solution for us. Both aggregated API servers and CRDs require us to register an API GroupName with Kubernetes. This API GroupName field essentially namespaces the API’s REST path in a way that prevents API naming conflicts between other third party applications. Because CRDs and aggregated API servers can’t share the same GroupName, we have to register two separate GroupNames. One is used by our CRDs and the other is used by the aggregated API server for subresource requests.</p><p>Having two GroupNames in our API is slightly inconvenient because it means the REST path for the endpoints that serve the KubeVirt subresource requests have a slightly different base path than the resources.</p><p>For example, the endpoint to create a VMI object is as follows.</p><p><strong>/apis/kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm</strong></p><p>However, the subresource endpoint to access graphical VNC looks like this.</p><p><strong>/apis/subresources.kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm/vnc</strong></p><p>Notice that the first request uses <strong>kubevirt.io</strong> and the second request uses <strong>subresource.kubevirt.io</strong>. We don’t like that, but that’s how we’ve managed to combine CRDs with a stateless aggregated API server for subresources.</p><p>One thing worth noting is that in Kubernetes 1.10 a very basic form of CRD subresource support was added in the form of the <code>/status</code> and <code>/scale</code> subresources. This support does not help us deliver the virtualization features we want subresources for. However, there have been discussions about exposing custom CRD subresources as webhooks in a future Kubernetes version. If this functionality lands, we will gladly transition away from our stateless aggregated API server workaround to use a subresource webhook feature.</p><h2 id=crd-finalizers>CRD Finalizers</h2><p>A <a href=/docs/tasks/access-kubernetes-API/extend-api-custom-resource-definitions/#advanced-topics>CRD finalizer</a> is a feature that lets us provide a pre-delete hook in order to perform actions before allowing a CRD object to be removed from persistent storage. In KubeVirt, we use finalizers to guarantee a virtual machine has completely terminated before we allow the corresponding VMI object to be removed from etcd.</p><h2 id=api-versioning-for-crds>API Versioning for CRDs</h2><p>The Kubernetes core APIs have the ability to support multiple versions for a single object type and perform conversions between those versions. This gives the Kubernetes core APIs a path for advancing the <code>v1alpha1</code> version of an object to a <code>v1beta1</code> version and so forth.</p><p>Prior to Kubernetes 1.11, CRDs did not have support for multiple versions. This meant when we wanted to progress a CRD from <code>kubevirt.io/v1alpha1</code> to <code>kubevirt.io/v1beta1</code>, the only path available to was to backup our CRD objects, delete the registered CRD from Kubernetes, register a new CRD with the updated version, convert the backed up CRD objects to the new version, and finally post the migrated CRD objects back to the cluster.</p><p>That strategy was not exactly a viable option for us.</p><p>Fortunately thanks to some recent <a href=https://github.com/kubernetes/features/issues/544>work to rectify this issue in Kubernetes</a>, the latest Kubernetes v1.11 now supports <a href=https://github.com/kubernetes/kubernetes/pull/63830>CRDs with multiple versions</a>. Note however that this initial multi version support is limited. While a CRD can now have multiple versions, the feature does not currently contain a path for performing conversions between versions. In KubeVirt, the lack of conversion makes it difficult us to evolve our API as we progress versions. Luckily, support for conversions between versions is underway and we look forward to taking advantage of that feature once it lands in a future Kubernetes release.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-04b7e5d7125964da493f2ae401baca35>Feature Highlight: CPU Manager</h1><div class="td-byline mb-4"><time datetime=2018-07-24 class=text-muted>Tuesday, July 24, 2018</time></div><p><strong>Authors</strong>: Balaji Subramaniam (<a href=mailto:balaji.subramaniam@intel.com>Intel</a>), Connor Doyle (<a href=mailto:connor.p.doyle@intel.com>Intel</a>)</p><p>This blog post describes the <a href=/docs/tasks/administer-cluster/cpu-management-policies/>CPU Manager</a>, a beta feature in <a href=https://kubernetes.io/>Kubernetes</a>. The CPU manager feature enables better placement of workloads in the <a href=/docs/reference/command-line-tools-reference/kubelet/>Kubelet</a>, the Kubernetes node agent, by allocating exclusive CPUs to certain pod containers.</p><p><img src=/images/blog/2018-07-24-cpu-manager/cpu-manager.png alt="cpu manager"></p><h2 id=sounds-good-but-does-the-cpu-manager-help-me>Sounds Good! But Does the CPU Manager Help Me?</h2><p>It depends on your workload. A single compute node in a Kubernetes cluster can run many <a href=/docs/concepts/workloads/pods/pod/>pods</a> and some of these pods could be running CPU-intensive workloads. In such a scenario, the pods might contend for the CPU resources available in that compute node. When this contention intensifies, the workload can move to different CPUs depending on whether the pod is throttled and the availability of CPUs at scheduling time. There might also be cases where the workload could be sensitive to context switches. In all the above scenarios, the performance of the workload might be affected.</p><p>If your workload is sensitive to such scenarios, then CPU Manager can be enabled to provide better performance isolation by allocating exclusive CPUs for your workload.</p><p>CPU manager might help workloads with the following characteristics:</p><ul><li>Sensitive to CPU throttling effects.</li><li>Sensitive to context switches.</li><li>Sensitive to processor cache misses.</li><li>Benefits from sharing a processor resources (e.g., data and instruction caches).</li><li>Sensitive to cross-socket memory traffic.</li><li>Sensitive or requires hyperthreads from the same physical CPU core.</li></ul><h2 id=ok-how-do-i-use-it>Ok! How Do I use it?</h2><p>Using the CPU manager is simple. First, <a href=/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies>enable CPU manager with the Static policy</a> in the Kubelet running on the compute nodes of your cluster. Then configure your pod to be in the <a href=/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed>Guaranteed Quality of Service (QoS) class</a>. Request whole numbers of CPU cores (e.g., <code>1000m</code>, <code>4000m</code>) for containers that need exclusive cores. Create your pod in the same way as before (e.g., <code>kubectl create -f pod.yaml</code>). And <em>voilà</em>, the CPU manager will assign exclusive CPUs to each of container in the pod according to their CPU requests.</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: exclusive-2
spec:
  containers:
  - image: quay.io/connordoyle/cpuset-visualizer
    name: exclusive-2
    resources:
      # Pod is in the Guaranteed QoS class because requests == limits
      requests:
        # CPU request is an integer
        cpu: 2
        memory: &quot;256M&quot;
      limits:
        cpu: 2
        memory: &quot;256M&quot;
</code></pre><p><em>Pod specification requesting two exclusive CPUs.</em></p><h2 id=hmm-how-does-the-cpu-manager-work>Hmm … How Does the CPU Manager Work?</h2><p>For Kubernetes, and the purposes of this blog post, we will discuss three kinds of CPU resource controls available in most Linux distributions. The first two are CFS shares (what's my weighted fair share of CPU time on this system) and CFS quota (what's my hard cap of CPU time over a period). The CPU manager uses a third control called CPU affinity (on what logical CPUs am I allowed to execute).</p><p>By default, all the pods and the containers running on a compute node of your Kubernetes cluster can execute on any available cores in the system. The total amount of allocatable shares and quota are limited by the CPU resources explicitly <a href=/docs/tasks/administer-cluster/reserve-compute-resources/>reserved for kubernetes and system daemons</a>. However, limits on the CPU time being used can be specified using <a href=/docs/tasks/configure-pod-container/assign-cpu-resource/#specify-a-cpu-request-and-a-cpu-limit>CPU limits in the pod spec</a>. Kubernetes uses <a href=https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt>CFS quota</a> to enforce CPU limits on pod containers.</p><p>When CPU manager is enabled with the "static" policy, it manages a shared pool of CPUs. Initially this shared pool contains all the CPUs in the compute node. When a container with integer CPU request in a Guaranteed pod is created by the Kubelet, CPUs for that container are removed from the shared pool and assigned exclusively for the lifetime of the container. Other containers are migrated off these exclusively allocated CPUs.</p><p>All non-exclusive-CPU containers (Burstable, BestEffort and Guaranteed with non-integer CPU) run on the CPUs remaining in the shared pool. When a container with exclusive CPUs terminates, its CPUs are added back to the shared CPU pool.</p><h2 id=more-details-please>More Details Please ...</h2><p><img src=/images/blog/2018-07-24-cpu-manager/cpu-manager-anatomy.png alt="cpu manager"></p><p>The figure above shows the anatomy of the CPU manager. The CPU Manager uses the Container Runtime Interface's <code>UpdateContainerResources</code> method to modify the CPUs on which containers can run. The Manager periodically reconciles the current State of the CPU resources of each running container with <code>cgroupfs</code>.</p><p>The CPU Manager uses <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cpumanager/policy.go#L25>Policies</a> to decide the allocation of CPUs. There are two policies implemented: None and Static. By default, the CPU manager is enabled with the None policy from Kubernetes version 1.10.</p><p>The Static policy allocates exclusive CPUs to pod containers in the Guaranteed QoS class which request integer CPUs. On a best-effort basis, the Static policy tries to allocate CPUs topologically in the following order:</p><ol><li>Allocate all the CPUs in the same processor socket if available and the container requests at least an entire socket worth of CPUs.</li><li>Allocate all the logical CPUs (hyperthreads) from the same physical CPU core if available and the container requests an entire core worth of CPUs.</li><li>Allocate any available logical CPU, preferring to acquire CPUs from the same socket.</li></ol><h2 id=how-is-performance-isolation-improved-by-cpu-manager>How is Performance Isolation Improved by CPU Manager?</h2><p>With CPU manager static policy enabled, the workloads might perform better due to one of the following reasons:</p><ol><li>Exclusive CPUs can be allocated for the workload container but not the other containers. These containers do not share the CPU resources. As a result, we expect better performance due to isolation when an aggressor or a co-located workload is involved.</li><li>There is a reduction in interference between the resources used by the workload since we can partition the CPUs among workloads. These resources might also include the cache hierarchies and memory bandwidth and not just the CPUs. This helps improve the performance of workloads in general.</li><li>CPU Manager allocates CPUs in a topological order on a best-effort basis. If a whole socket is free, the CPU Manager will exclusively allocate the CPUs from the free socket to the workload. This boosts the performance of the workload by avoiding any cross-socket traffic.</li><li>Containers in Guaranteed QoS pods are subject to CFS quota. Very bursty workloads may get scheduled, burn through their quota before the end of the period, and get throttled. During this time, there may or may not be meaningful work to do with those CPUs. Because of how the resource math lines up between CPU quota and number of exclusive CPUs allocated by the static policy, these containers are not subject to CFS throttling (quota is equal to the maximum possible cpu-time over the quota period).</li></ol><h2 id=ok-ok-do-you-have-any-results>Ok! Ok! Do You Have Any Results?</h2><p>Glad you asked! To understand the performance improvement and isolation provided by enabling the CPU Manager feature in the Kubelet, we ran experiments on a dual-socket compute node (Intel Xeon CPU E5-2680 v3) with hyperthreading enabled. The node consists of 48 logical CPUs (24 physical cores each with 2-way hyperthreading). Here we demonstrate the performance benefits and isolation provided by the CPU Manager feature using benchmarks and real-world workloads for three different scenarios.</p><h3 id=how-do-i-interpret-the-plots>How Do I Interpret the Plots?</h3><p>For each scenario, we show box plots that illustrates the normalized execution time and its variability of running a benchmark or real-world workload with and without CPU Manager enabled. The execution time of the runs are normalized to the best-performing run (1.00 on y-axis represents the best performing run and lower is better). The height of the box plot shows the variation in performance. For example if the box plot is a line, then there is no variation in performance across runs. In the box, middle line is the median, upper line is 75th percentile and lower line is 25th percentile. The height of the box (i.e., difference between 75th and 25th percentile) is defined as the interquartile range (IQR). Whiskers shows data outside that range and the points show outliers. The outliers are defined as any data 1.5x IQR below or above the lower or upper quartile respectively. Every experiment is run ten times.</p><h3 id=protection-from-aggressor-workloads>Protection from Aggressor Workloads</h3><p>We ran six benchmarks from the <a href=http://parsec.cs.princeton.edu/>PARSEC benchmark suite</a> (the victim workloads) co-located with a CPU stress container (the aggressor workload) with and without the CPU Manager feature enabled. The CPU stress container is run <a href=https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c>as a pod</a> in the Burstable QoS class requesting 23 CPUs with <code>--cpus 48</code> flag. <a href=https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902>The benchmarks are run as pods</a> in the Guaranteed QoS class requesting a full socket worth of CPUs (24 CPUs on this system). The figure below plots the normalized execution time of running a benchmark pod co-located with the stress pod, with and without the CPU Manager static policy enabled. We see improved performance and reduced performance variability when static policy is enabled for all test cases.</p><p><img src=/images/blog/2018-07-24-cpu-manager/execution-time.png alt="execution time"></p><h3 id=performance-isolation-for-co-located-workloads>Performance Isolation for Co-located Workloads</h3><p>In this section, we demonstrate how CPU manager can be beneficial to multiple workloads in a co-located workload scenario. In the box plots below we show the performance of two benchmarks (Blackscholes and Canneal) from the PARSEC benchmark suite run in the Guaranteed (Gu) and Burstable (Bu) QoS classes co-located with each other, with and without the CPU manager static policy enabled.</p><p>Starting from the top left and proceeding clockwise, we show the performance of Blackscholes in the Bu QoS class (top left), Canneal in the Bu QoS class (top right), Canneal in Gu QoS class (bottom right) and Blackscholes in the Gu QoS class (bottom left, respectively. In each case, they are co-located with Canneal in the Gu QoS class (top left), Blackscholes in the Gu QoS class (top right), Blackscholes in the Bu QoS class (bottom right) and Canneal in the Bu QoS class (bottom left) going clockwise from top left, respectively. For example, Bu-blackscholes-Gu-canneal plot (top left) is showing the performance of Blackscholes running in the Bu QoS class when co-located with Canneal running in the Gu QoS class. In each case, the pod in Gu QoS class requests cores worth a whole socket (i.e., 24 CPUs) and the pod in Bu QoS class request 23 CPUs.</p><p>There is better performance and less performance variation for both the co-located workloads in all the tests. For example, consider the case of Bu-blackscholes-Gu-canneal (top left) and Gu-canneal-Bu-blackscholes (bottom right). They show the performance of Blackscholes and Canneal run simultaneously with and without the CPU manager enabled. In this particular case, Canneal gets exclusive cores due to CPU manager since it is in the Gu QoS class and requesting integer number of CPU cores. But Blackscholes also gets exclusive set of CPUs as it is the only workload in the shared pool. As a result, both Blackscholes and Canneal get some performance isolation benefits due to the CPU manager.</p><p><img src=/images/blog/2018-07-24-cpu-manager/performance-comparison.png alt="performance comparison"></p><h3 id=performance-isolation-for-stand-alone-workloads>Performance Isolation for Stand-Alone Workloads</h3><p>This section shows the performance improvement and isolation provided by the CPU manager for stand-alone real-world workloads. We use two workloads from the <a href=https://github.com/tensorflow/models/tree/master/official>TensorFlow official models</a>: <a href=https://github.com/tensorflow/models/tree/master/official/r1/wide_deep>wide and deep</a> and <a href=https://github.com/tensorflow/models/tree/master/official/r1/resnet>ResNet</a>. We use the census and CIFAR10 dataset for the wide and deep and ResNet models respectively. In each case the <a href=https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58>pods</a> (<a href=https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c>wide and deep</a>, <a href=https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3>ResNet</a> request 24 CPUs which corresponds to a whole socket worth of cores. As shown in the plots, CPU manager enables better performance isolation in both cases.</p><p><img src=/images/blog/2018-07-24-cpu-manager/performance-comparison-2.png alt="performance comparison"></p><h2 id=limitations>Limitations</h2><p>Users might want to get CPUs allocated on the socket near to the bus which connects to an external device, such as an accelerator or high-performance network card, in order to avoid cross-socket traffic. This type of alignment is not yet supported by CPU manager.
Since the CPU manager provides a best-effort allocation of CPUs belonging to a socket and physical core, it is susceptible to corner cases and might lead to fragmentation.
The CPU manager does not take the isolcpus Linux kernel boot parameter into account, although this is reportedly common practice for some low-jitter use cases.</p><h2 id=acknowledgements>Acknowledgements</h2><p>We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management and SIG-Node.
cmx.io (for the fun drawing tool).</p><h4 id=notices-and-disclaimers>Notices and Disclaimers</h4><p>Software and workloads used in performance tests may have been optimized for performance only on Intel microprocessors. Performance tests, such as SYSmark and MobileMark, are measured using specific computer systems, components, software, operations and functions. Any change to any of those factors may cause the results to vary. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases, including the performance of that product when combined with other products. For more information go to <a href=http://www.intel.com/benchmarks>www.intel.com/benchmarks</a>.</p><p>Intel technologies’ features and benefits depend on system configuration and may require enabled hardware, software or service activation. Performance varies depending on system configuration. No computer system can be absolutely secure. Check with your system manufacturer or retailer or learn more at intel.com.</p><p>Workload Configuration:
<a href=https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902>https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902</a>
<a href=https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c>https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c</a>
<a href=https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58>https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58</a>
<a href=https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3>https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3</a>
<a href=https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c>https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c</a></p><p>System Configuration:
CPU
Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
CPU(s): 48
On-line CPU(s) list: 0-47
Thread(s) per core: 2
Core(s) per socket: 12
Socket(s): 2
NUMA node(s): 2
Vendor ID: GenuineIntel
Model name: Intel(R) Xeon(R) CPU E5-2680 v3
Memory
256 GB
OS/Kernel
Linux 3.10.0-693.21.1.el7.x86_64</p><p>Intel, the Intel logo, Xeon are trademarks of Intel Corporation or its subsidiaries in the U.S. and/or other countries.<br>*Other names and brands may be claimed as the property of others.
© Intel Corporation.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-82df37be5b315b12749701fd3a3426c2>The History of Kubernetes & the Community Behind It</h1><div class="td-byline mb-4"><time datetime=2018-07-20 class=text-muted>Friday, July 20, 2018</time></div><p><strong>Authors</strong>: Brendan Burns (Distinguished Engineer, Microsoft)</p><p><img src=/images/blog/2018-07-20-history-kubernetes-community.png alt="oscon award"></p><p>It is remarkable to me to return to Portland and OSCON to stand on stage with members of the Kubernetes community and accept this award for Most Impactful Open Source Project. It was scarcely three years ago, that on this very same stage we declared Kubernetes 1.0 and the project was added to the newly formed Cloud Native Computing Foundation.</p><p>To think about how far we have come in that short period of time and to see the ways in which this project has shaped the cloud computing landscape is nothing short of amazing. The success is a testament to the power and contributions of this amazing open source community. And the daily passion and quality contributions of our endlessly engaged, world-wide community is nothing short of humbling.</p><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>Congratulations <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> for winning the "most impact" award at <a href="https://twitter.com/hashtag/OSCON?src=hash&ref_src=twsrc%5Etfw">#OSCON</a> I'm so proud to be a part of this amazing community! <a href="https://twitter.com/CloudNativeFdn?ref_src=twsrc%5Etfw">@CloudNativeFdn</a> <a href=https://t.co/5sRUYyefAK>pic.twitter.com/5sRUYyefAK</a></p>&mdash; Jaice Singer DuMars (@jaydumars) <a href="https://twitter.com/jaydumars/status/1019993233487613952?ref_src=twsrc%5Etfw">July 19, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>👏 congrats <a href="https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw">@kubernetesio</a> community on winning the <a href="https://twitter.com/hashtag/oscon?src=hash&ref_src=twsrc%5Etfw">#oscon</a> Most Impact Award, we are proud of you! <a href=https://t.co/5ezDphi6J6>pic.twitter.com/5ezDphi6J6</a></p>&mdash; CNCF (@CloudNativeFdn) <a href="https://twitter.com/CloudNativeFdn/status/1019996928296095744?ref_src=twsrc%5Etfw">July 19, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>At a meetup in Portland this week, I had a chance to tell the story of Kubernetes’ past, its present and some thoughts about its future, so I thought I would write down some pieces of what I said for those of you who couldn’t be there in person.</p><p>It all began in the fall of 2013, with three of us: Craig McLuckie, Joe Beda and I were working on public cloud infrastructure. If you cast your mind back to the world of cloud in 2013, it was a vastly different place than it is today. Imperative bash scripts were only just starting to give way to declarative configuration of IaaS with systems. Netflix was popularizing the idea of immutable infrastructure but doing it with heavy-weight full VM images. The notion of orchestration, and certainly container orchestration existed in a few internet scale companies, but not in cloud and certainly not in the enterprise.</p><p>Docker changed all of that. By popularizing a lightweight container runtime and providing a simple way to package, distributed and deploy applications onto a machine, the Docker tooling and experience popularized a brand-new cloud native approach to application packaging and maintenance. Were it not for Docker’s shifting of the cloud developer’s perspective, Kubernetes simply would not exist.</p><p>I think that it was Joe who first suggested that we look at Docker in the summer of 2013, when Craig, Joe and I were all thinking about how we could bring a cloud native application experience to a broader audience. And for all three of us, the implications of this new tool were immediately obvious. We knew it was a critical component in the development of cloud native infrastructure.</p><p>But as we thought about it, it was equally obvious that Docker, with its focus on a single machine, was not the complete solution. While Docker was great at building and packaging individual containers and running them on individual machines, there was a clear need for an orchestrator that could deploy and manage large numbers of containers across a fleet of machines.</p><p>As we thought about it some more, it became increasingly obvious to Joe, Craig and I, that not only was such an orchestrator necessary, it was also inevitable, and it was equally inevitable that this orchestrator would be open source. This realization crystallized for us in the late fall of 2013, and thus began the rapid development of first a prototype, and then the system that would eventually become known as Kubernetes. As 2013 turned into 2014 we were lucky to be joined by some incredibly talented developers including Ville Aikas, Tim Hockin, Dawn Chen, Brian Grant and Daniel Smith.</p><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>Happy to see k8s team members winning the “most impact” award. <a href="https://twitter.com/hashtag/oscon?src=hash&ref_src=twsrc%5Etfw">#oscon</a> <a href=https://t.co/D6mSIiDvsU>pic.twitter.com/D6mSIiDvsU</a></p>&mdash; Bridget Kromhout (@bridgetkromhout) <a href="https://twitter.com/bridgetkromhout/status/1019992441825341440?ref_src=twsrc%5Etfw">July 19, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>Kubernetes won the O'Reilly Most Impact Award. Thanks to our contributors and users! <a href=https://t.co/T6Co1wpsAh>pic.twitter.com/T6Co1wpsAh</a></p>&mdash; Brian Grant (@bgrant0607) <a href="https://twitter.com/bgrant0607/status/1019995276235325440?ref_src=twsrc%5Etfw">July 19, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>The initial goal of this small team was to develop a “minimally viable orchestrator.” From experience we knew that the basic feature set for such an orchestrator was:</p><ul><li>Replication to deploy multiple instances of an application</li><li>Load balancing and service discovery to route traffic to these replicated containers</li><li>Basic health checking and repair to ensure a self-healing system</li><li>Scheduling to group many machines into a single pool and distribute work to them</li></ul><p>Along the way, we also spent a significant chunk of our time convincing executive leadership that open sourcing this project was a good idea. I’m endlessly grateful to Craig for writing numerous whitepapers and to Eric Brewer, for the early and vocal support that he lent us to ensure that Kubernetes could see the light of day.</p><p>In June of 2014 when Kubernetes was released to the world, the list above was the sum total of its basic feature set. As an early stage open source community, we then spent a year building, expanding, polishing and fixing this initial minimally viable orchestrator into the product that we released as a 1.0 in OSCON in 2015. We were very lucky to be joined early on by the very capable OpenShift team which lent significant engineering and real world enterprise expertise to the project. Without their perspective and contributions, I don’t think we would be standing here today.</p><p>Three years later, the Kubernetes community has grown exponentially, and Kubernetes has become synonymous with cloud native container orchestration. There are more than 1700 people who have contributed to Kubernetes, there are more than 500 Kubernetes meetups worldwide and more than 42000 users have joined the #kubernetes-dev channel. What’s more, the community that we have built works successfully across geographic, language and corporate boundaries. It is a truly open, engaged and collaborative community, and in-and-of-itself and amazing achievement. Many thanks to everyone who has helped make it what it is today. Kubernetes is a commodity in the public cloud because of you.</p><p>But if Kubernetes is a commodity, then what is the future? Certainly, there are an endless array of tweaks, adjustments and improvements to the core codebase that will occupy us for years to come, but the true future of Kubernetes are the applications and experiences that are being built on top of this new, ubiquitous platform.</p><p>Kubernetes has dramatically reduced the complexity to build new developer experiences, and a myriad of new experiences have been developed or are in the works that provide simplified or targeted developer experiences like Functions-as-a-Service, on top of core Kubernetes-as-a-Service.</p><p>The Kubernetes cluster itself is being extended with custom resource definitions (which I first described to Kelsey Hightower on a walk from OSCON to a nearby restaurant in 2015), these new resources allow cluster operators to enable new plugin functionality that extend and enhance the APIs that their users have access to.</p><p>By embedding core functionality like logging and monitoring in the cluster itself and enabling developers to take advantage of such services simply by deploying their application into the cluster, Kubernetes has reduced the learning necessary for developers to build scalable reliable applications.</p><p>Finally, Kubernetes has provided a new, common vocabulary for expressing the patterns and paradigms of distributed system development. This common vocabulary means that we can more easily describe and discuss the common ways in which our distributed systems are built, and furthermore we can build standardized, re-usable implementations of such systems. The net effect of this is the development of higher quality, reliable distributed systems, more quickly.</p><p>It’s truly amazing to see how far Kubernetes has come, from a rough idea in the minds of three people in Seattle to a phenomenon that has redirected the way we think about cloud native development across the world. It has been an amazing journey, but what’s truly amazing to me, is that I think we’re only just now scratching the surface of the impact that Kubernetes will have. Thank you to everyone who has enabled us to get this far, and thanks to everyone who will take us further.</p><p>Brendan</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c61517aae0c272388dc0a2b9867a2b0d>Kubernetes Wins the 2018 OSCON Most Impact Award</h1><div class="td-byline mb-4"><time datetime=2018-07-19 class=text-muted>Thursday, July 19, 2018</time></div><p><strong>Authors</strong>: Brian Grant (Principal Engineer, Google) and Tim Hockin (Principal Engineer, Google)</p><p>We are humbled to be recognized by the community with this award.</p><p>We had high hopes when we created Kubernetes. We wanted to change the way cloud applications were deployed and managed. Whether we’d succeed or not was very uncertain. And look how far we’ve come in such a short time.</p><p>The core technology behind Kubernetes was informed by <a href=https://ai.google/research/pubs/pub44843>lessons learned from Google’s internal infrastructure</a>, but nobody can deny the enormous role of the Kubernetes community in the success of the project. <a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1">The community, of which Google is a part</a>, now drives every aspect of the project: the design, development, testing, documentation, releases, and more. That is what makes Kubernetes fly.</p><p>While we actively sought partnerships and community engagement, none of us anticipated just how important the open-source community would be, how fast it would grow, or how large it would become. Honestly, we really didn’t have much of a plan.</p><p>We looked to other open-source projects for inspiration and advice: Docker (now Moby), other open-source projects at Google such as Angular and Go, the Apache Software Foundation, OpenStack, Node.js, Linux, and others. But it became clear that there was no clear-cut recipe we could follow. So we winged it.</p><p>Rather than rehashing history, we thought we’d share two high-level lessons we learned along the way.</p><p>First, in order to succeed, community health and growth needs to be treated as a top priority. It’s hard, and it is time-consuming. It requires attention to both internal project dynamics and outreach, as well as constant vigilance to build and sustain relationships, be inclusive, maintain open communication, and remain responsive to contributors and users. Growing existing contributors and onboarding new ones is critical to sustaining project growth, but that takes time and energy that might otherwise be spent on development. These things have to become core values in order for contributors to keep them going.</p><p>Second, start simple with how the project is organized and operated, but be ready to adopt to more scalable approaches as it grows. Over time, Kubernetes has transitioned from what was effectively a single team and git repository to many subgroups (Special Interest Groups and Working Groups), sub-projects, and repositories. From manual processes to fully automated ones. From informal policies to formal governance.</p><p>We certainly didn’t get everything right or always adapt quickly enough, and we constantly struggle with scale. <a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">At this point</a>, Kubernetes has more than 20,000 contributors and is approaching one million comments on its issues and pull requests, <a href=https://www.cncf.io/blog/2017/02/27/measuring-popularity-kubernetes-using-bigquery/>making it one of the fastest moving projects in the history of open source</a>.</p><p>Thank you to all our contributors and to all the users who’ve stuck with us on the sometimes bumpy journey. This project would not be what it is today without the community.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b5c84fcebf9b418559579d3f0ae1ba90>11 Ways (Not) to Get Hacked</h1><div class="td-byline mb-4"><time datetime=2018-07-18 class=text-muted>Wednesday, July 18, 2018</time></div><p><strong>Author</strong>: Andrew Martin (ControlPlane)</p><p>Kubernetes security has come a long way since the project's inception, but still contains some gotchas. Starting with the control plane, building up through workload and network security, and finishing with a projection into the future of security, here is a list of handy tips to help harden your clusters and increase their resilience if compromised.</p><ul><li><a href=#part-one-the-control-plane>Part One: The Control Plane</a><ul><li><a href=#1-tls-everywhere>1. TLS Everywhere</a></li><li><a href=#2-enable-rbac-with-least-privilege-disable-abac-and-monitor-logs>2. Enable RBAC with Least Privilege, Disable ABAC, and Monitor Logs</a></li><li><a href=#3-use-third-party-auth-for-api-server>3. Use Third Party Auth for API Server</a></li><li><a href=#4-separate-and-firewall-your-etcd-cluster>4. Separate and Firewall your etcd Cluster</a></li><li><a href=#5-rotate-encryption-keys>5. Rotate Encryption Keys</a></li></ul></li><li><a href=#part-two-workloads>Part Two: Workloads</a><ul><li><a href=#6-use-linux-security-features-and-podsecuritypolicies>6. Use Linux Security Features and PodSecurityPolicies</a></li><li><a href=#7-statically-analyse-yaml>7. Statically Analyse YAML</a></li><li><a href=#8-run-containers-as-a-non-root-user>8. Run Containers as a Non-Root User</a></li><li><a href=#9-use-network-policies>9. Use Network Policies</a></li><li><a href=#10-scan-images-and-run-ids>10. Scan Images and Run IDS</a></li></ul></li><li><a href=#part-three-the-future>Part Three: The Future</a><ul><li><a href=#11-run-a-service-mesh>11. Run a Service Mesh</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul><h1 id=part-one-the-control-plane>Part One: The Control Plane</h1><p>The control plane is Kubernetes' brain. It has an overall view of every container and pod running on the cluster, can schedule new pods (which can include containers with root access to their parent node), and can read all the secrets stored in the cluster. This valuable cargo needs protecting from accidental leakage and malicious intent: when it's accessed, when it's at rest, and when it's being transported across the network.</p><h2 id=1-tls-everywhere>1. TLS Everywhere</h2><p><strong>TLS should be enabled for every component that supports it to prevent traffic sniffing, verify the identity of the server, and (for mutual TLS) verify the identity of the client.</strong></p><blockquote><p>Note that some components and installation methods may enable local ports over HTTP and administrators should familiarize themselves with the settings of each component to identify potentially unsecured traffic.</p></blockquote><p><a href=/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-level-security-tls-for-all-api-traffic>Source</a></p><p>This network diagram by <a href="https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g1e639c415b_0_56">Lucas Käldström</a> demonstrates some of the places TLS should ideally be applied: between every component on the master, and between the Kubelet and API server. <a href=https://twitter.com/kelseyhightower/>Kelsey Hightower</a>'s canonical <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/1.9.0/docs/04-certificate-authority.md>Kubernetes The Hard Way</a> provides detailed manual instructions, as does <a href=https://coreos.com/etcd/docs/latest/op-guide/security.html>etcd's security model</a> documentation.</p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/kubernetes-control-plane.png width=800></p><p>Autoscaling Kubernetes nodes was historically difficult, as each node requires a TLS key to connect to the master, and baking secrets into base images is not good practice. <a href=https://medium.com/@toddrosner/kubernetes-tls-bootstrapping-cf203776abc7>Kubelet TLS bootstrapping</a> provides the ability for a new kubelet to create a certificate signing request so that certificates are generated at boot time.</p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/node-tls-bootstrap.png width=800></p><h2 id=2-enable-rbac-with-least-privilege-disable-abac-and-monitor-logs>2. Enable RBAC with Least Privilege, Disable ABAC, and Monitor Logs</h2><p><strong>Role-based access control provides fine-grained policy management for user access to resources, such as access to namespaces.</strong></p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/rbac2.png width=800></p><p>Kubernetes' ABAC (Attribute Based Access Control) has been <a href=http://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes.html>superseded by RBAC</a> since release 1.6, and should not be enabled on the API server. Use RBAC instead:</p><pre><code>--authorization-mode=RBAC
</code></pre><p>Or use this flag to disable it in GKE:</p><pre><code>--no-enable-legacy-authorization
</code></pre><p>There are plenty of <a href=https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/>good examples</a> of <a href=https://github.com/uruddarraju/kubernetes-rbac-policies>RBAC policies for cluster services</a>, as well as <a href=/docs/admin/authorization/rbac/#role-binding-examples>the docs</a>. And it doesn't have to stop there - fine-grained RBAC policies can be extracted from audit logs with <a href=https://github.com/liggitt/audit2rbac>audit2rbac</a>.</p><p>Incorrect or excessively permissive RBAC policies are a security threat in case of a compromised pod. Maintaining least privilege, and continuously reviewing and improving RBAC rules, should be considered part of the "technical debt hygiene" that teams build into their development lifecycle.</p><p><a href=/docs/tasks/debug-application-cluster/audit/>Audit Logging</a> (beta in 1.10) provides customisable API logging at the payload (e.g. request and response), and also metadata levels. Log levels can be tuned to your organisation's security policy - <a href=https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging#audit_policy>GKE</a> provides sane defaults to get you started.</p><p>For read requests such as get, list, and watch, only the request object is saved in the audit logs; the response object is not. For requests involving sensitive data such as Secret and ConfigMap, only the metadata is exported. For all other requests, both request and response objects are saved in audit logs.</p><p>Don't forget: keeping these logs inside the cluster is a security threat in case of compromise. These, like all other security-sensitive logs, should be transported outside the cluster to prevent tampering in the event of a breach.</p><h2 id=3-use-third-party-auth-for-api-server>3. Use Third Party Auth for API Server</h2><p><strong>Centralising authentication and authorisation across an organisation (aka Single Sign On) helps onboarding, offboarding, and consistent permissions for users</strong>.</p><p>Integrating Kubernetes with third party auth providers (like Google or GitHub) uses the remote platform's identity guarantees (backed up by things like 2FA) and prevents administrators having to reconfigure the Kubernetes API server to add or remove users.</p><p><a href=https://github.com/coreos/dex>Dex</a> is an OpenID Connect Identity (OIDC) and OAuth 2.0 provider with pluggable connectors. Pusher takes this a stage further with <a href=https://thenewstack.io/kubernetes-single-sign-one-less-identity/>some custom tooling</a>, and there are some <a href=https://github.com/negz/kuberos>other</a> <a href=https://github.com/micahhausler/k8s-oidc-helper>helpers</a> available with slightly different use cases.</p><h2 id=4-separate-and-firewall-your-etcd-cluster>4. Separate and Firewall your etcd Cluster</h2><p><strong>etcd stores information on state and secrets, and is a critical Kubernetes component - it should be protected differently from the rest of your cluster.</strong></p><p>Write access to the API server's etcd is equivalent to gaining root on the entire cluster, and even read access can be used to escalate privileges fairly easily.</p><p>The Kubernetes scheduler will search etcd for pod definitions that do not have a node. It then sends the pods it finds to an available kubelet for scheduling. Validation for submitted pods is performed by the API server before it writes them to etcd, so malicious users writing directly to etcd can bypass many security mechanisms - e.g. PodSecurityPolicies.</p><p>etcd should be configured with <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/security.md>peer and client TLS certificates</a>, and deployed on dedicated nodes. To mitigate against private keys being stolen and used from worker nodes, the cluster can also be firewalled to the API server.</p><h2 id=5-rotate-encryption-keys>5. Rotate Encryption Keys</h2><p><strong>A security best practice is to regularly rotate encryption keys and certificates, in order to limit the "blast radius" of a key compromise.</strong></p><p>Kubernetes will <a href=/docs/tasks/tls/certificate-rotation/>rotate some certificates automatically</a> (notably, the kubelet client and server certs) by creating new CSRs as its existing credentials expire.</p><p>However, the <a href=/docs/tasks/administer-cluster/encrypt-data/>symmetric encryption keys</a> that the API server uses to encrypt etcd values are not automatically rotated - they must be <a href=https://www.twistlock.com/2017/08/02/kubernetes-secrets-encryption/>rotated manually</a>. Master access is required to do this, so managed services (such as GKE or AKS) abstract this problem from an operator.</p><h1 id=part-two-workloads>Part Two: Workloads</h1><p>With minimum viable security on the control plane the cluster is able to operate securely. But, like a ship carrying potentially dangerous cargo, the ship's containers must be protected to contain that cargo in the event of an unexpected accident or breach. The same is true for Kubernetes workloads (pods, deployments, jobs, sets, etc.) - they may be trusted at deployment time, but if they're internet-facing there's always a risk of later exploitation. Running workloads with minimal privileges and hardening their runtime configuration can help to mitigate this risk.</p><h2 id=6-use-linux-security-features-and-podsecuritypolicies>6. Use Linux Security Features and PodSecurityPolicies</h2><p><strong>The Linux kernel has a number of overlapping security extensions (capabilities, SELinux, AppArmor, seccomp-bpf) that can be configured to provide least privilege to applications</strong>.</p><p>Tools like <a href=https://github.com/genuinetools/bane>bane</a> can help to generate AppArmor profiles, and <a href=https://github.com/docker-slim/docker-slim#quick-seccomp-example>docker-slim</a> for seccomp profiles, but beware - a comprehensive test suite it required to exercise all code paths in your application when verifying the side effects of applying these policies.</p><p><a href=/docs/concepts/policy/pod-security-policy/>PodSecurityPolicies</a> can be used to mandate the use of security extensions and other Kubernetes security directives. They provide a minimum contract that a pod must fulfil to be submitted to the API server - including security profiles, the privileged flag, and the sharing of host network, process, or IPC namespaces.</p><p>These directives are important, as they help to prevent containerised processes from escaping their isolation boundaries, and <a href=https://twitter.com/tallclair>Tim Allclair</a>'s <a href=https://gist.github.com/tallclair/11981031b6bfa829bb1fb9dcb7e026b0>example PodSecurityPolicy</a> is a comprehensive resource that you can customise to your use case.</p><h2 id=7-statically-analyse-yaml>7. Statically Analyse YAML</h2><p><strong>Where PodSecurityPolicies deny access to the API server, static analysis can also be used in the development workflow to model an organisation's compliance requirements or risk appetite.</strong></p><p>Sensitive information should not be stored in pod-type YAML resource (deployments, pods, sets, etc.), and sensitive configmaps and secrets should be encrypted with tools such as <a href=https://github.com/coreos/vault-operator>vault</a> (with CoreOS's operator), <a href=https://github.com/AGWA/git-crypt>git-crypt</a>, <a href=https://github.com/bitnami-labs/sealed-secrets>sealed secrets</a>, or <a href=https://cloud.google.com/kms/>cloud provider KMS</a>.</p><p>Static analysis of YAML configuration can be used to establish a baseline for runtime security. <a href=https://kubesec.io/>kubesec</a> generates risk scores for resources:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;score&#34;</span>: <span style=color:#666>-30</span>,
  <span style=color:green;font-weight:700>&#34;scoring&#34;</span>: {
    <span style=color:green;font-weight:700>&#34;critical&#34;</span>: [{
      <span style=color:green;font-weight:700>&#34;selector&#34;</span>: <span style=color:#b44>&#34;containers[] .securityContext .privileged == true&#34;</span>,
      <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;Privileged containers can allow almost completely unrestricted host access&#34;</span>
    }],
    <span style=color:green;font-weight:700>&#34;advise&#34;</span>: [{
      <span style=color:green;font-weight:700>&#34;selector&#34;</span>: <span style=color:#b44>&#34;containers[] .securityContext .runAsNonRoot == true&#34;</span>,
      <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;Force the running image to run as a non-root user to ensure least privilege&#34;</span>
    }, {
      <span style=color:green;font-weight:700>&#34;selector&#34;</span>: <span style=color:#b44>&#34;containers[] .securityContext .capabilities .drop&#34;</span>,
      <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;Reducing kernel capabilities available to a container limits its attack surface&#34;</span>,
      <span style=color:green;font-weight:700>&#34;href&#34;</span>: <span style=color:#b44>&#34;/docs/tasks/configure-pod-container/security-context/&#34;</span>
    }]
  }
}
</code></pre></div><p>And <a href=https://github.com/garethr/kubetest>kubetest</a> is a unit test framework for Kubernetes configurations:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic>#// vim: set ft=python:</span>
<span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>test_for_team_label</span>():
    <span style=color:#a2f;font-weight:700>if</span> spec[<span style=color:#b44>&#34;kind&#34;</span>] <span style=color:#666>==</span> <span style=color:#b44>&#34;Deployment&#34;</span>:
        labels <span style=color:#666>=</span> spec[<span style=color:#b44>&#34;spec&#34;</span>][<span style=color:#b44>&#34;template&#34;</span>][<span style=color:#b44>&#34;metadata&#34;</span>][<span style=color:#b44>&#34;labels&#34;</span>]
        assert_contains(labels, <span style=color:#b44>&#34;team&#34;</span>, <span style=color:#b44>&#34;should indicate which team owns the deployment&#34;</span>)

test_for_team_label()
</code></pre></div><p>These tools "<a href=https://en.wikipedia.org/wiki/Shift_left_testing>shift left</a>" (moving checks and verification earlier in the development cycle). Security testing in the development phase gives users fast feedback about code and configuration that may be rejected by a later manual or automated check, and can reduce the friction of introducing more secure practices.</p><h2 id=8-run-containers-as-a-non-root-user>8. Run Containers as a Non-Root User</h2><p><strong>Containers that run as root frequently have far more permissions than their workload requires which, in case of compromise, could help an attacker further their attack.</strong></p><p>Containers still rely on the traditional Unix security model (called <a href=https://www.linux.com/learn/overview-linux-kernel-security-features>discretionary access control</a> or DAC) - everything is a file, and permissions are granted to users and groups.</p><p>User namespaces are not enabled in Kubernetes. This means that a container's user ID table maps to the host's user table, and running a process as the root user inside a container runs it as root on the host. Although we have layered security mechanisms to prevent container breakouts, running as root inside the container is still not recommended.</p><p>Many container images use the root user to run PID 1 - if that process is compromised, the attacker has root in the container, and any mis-configurations become much easier to exploit.</p><p><a href=https://engineering.bitnami.com/articles/running-non-root-containers-on-openshift.html>Bitnami has done a lot of work</a> moving their container images to <a href=https://github.com/bitnami/bitnami-docker-nginx/blob/b068b8bd01eb2f5a7314c09466724f86aa4548f9/1.12/Dockerfile#L28>non-root users</a> (especially as OpenShift requires this by default), which may ease a migration to non-root container images.</p><p>This PodSecurityPolicy snippet prevents running processes as root inside a container, and also escalation to root:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#080;font-style:italic># Required to prevent escalations to root.</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>allowPrivilegeEscalation</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>runAsUser</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Require the container to run without root privileges.</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rule</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;MustRunAsNonRoot&#39;</span><span style=color:#bbb>
</span></code></pre></div><p>Non-root containers cannot bind to the privileged ports under 1024 (this is gated by the CAP_NET_BIND_SERVICE kernel capability), but services can be used to disguise this fact. In this example the fictional MyApp application is bound to port 8443 in its container, but the service exposes it on 443 by proxying the request to the targetPort:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>443</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>8443</span><span style=color:#bbb>
</span></code></pre></div><p>Having to run workloads as a non-root user is not going to change until user namespaces are usable, or the ongoing work to <a href=https://rootlesscontaine.rs/>run containers without root</a> lands in container runtimes.</p><h2 id=9-use-network-policies>9. Use Network Policies</h2><p><strong>By default, Kubernetes networking allows all pod to pod traffic; this can be restricted using a</strong> <a href=/docs/concepts/services-networking/network-policies/><strong>Network Policy</strong></a> <strong>.</strong></p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/kubernetes-networking.png width=800></p><p>Traditional services are restricted with firewalls, which use static IP and port ranges for each service. As these IPs very rarely change they have historically been used as a form of identity. Containers rarely have static IPs - they are built to fail fast, be rescheduled quickly, and use service discovery instead of static IP addresses. These properties mean that firewalls become much more difficult to configure and review.</p><p>As Kubernetes stores all its system state in etcd it can configure dynamic firewalling - if it is supported by the CNI networking plugin. Calico, Cilium, kube-router, Romana, and Weave Net all support network policy.</p><p>It should be noted that these policies fail-closed, so the absence of a podSelector here defaults to a wildcard:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></code></pre></div><p>Here's an example NetworkPolicy that denies all egress except UDP 53 (DNS), which also prevents inbound connections to your application. <a href=https://www.weave.works/blog/securing-microservices-kubernetes/>NetworkPolicies are stateful</a>, so the replies to outbound requests still reach the application.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-deny-external-egress<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>myapp<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>53</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>UDP<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></code></pre></div><p>Kubernetes network policies can not be applied to DNS names. This is because DNS can resolve round-robin to many IPs, or dynamically based on the calling IP, so network policies can be applied to a fixed IP or podSelector (for dynamic Kubernetes IPs) only.</p><p>Best practice is to start by denying all traffic for a namespace and incrementally add routes to allow an application to pass its acceptance test suite. This can become complex, so ControlPlane hacked together <a href=https://github.com/controlplaneio/netassert>netassert</a> - network security testing for DevSecOps workflows with highly parallelised nmap:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>k8s</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># used for Kubernetes pods</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deployment</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># only deployments currently supported</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>test-frontend</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># pod name, defaults to `default` namespace</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>test-microservice</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># `test-microservice` is the DNS name of the target service</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>test-database</span>:<span style=color:#bbb> </span>-<span style=color:#666>80</span><span style=color:#bbb>     </span><span style=color:#080;font-style:italic># `test-frontend` should not be able to access test-database’s port 80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>169.254.169.254</span>:<span style=color:#bbb> </span>-<span style=color:#666>80</span>,<span style=color:#bbb> </span>-<span style=color:#666>443</span><span style=color:#bbb>           </span><span style=color:#080;font-style:italic># AWS metadata API</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>metadata.google.internal</span>:<span style=color:#bbb> </span>-<span style=color:#666>80</span>,<span style=color:#bbb> </span>-<span style=color:#666>443</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># GCP metadata API</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>new-namespace:test-microservice</span>:<span style=color:#bbb>  </span><span style=color:#080;font-style:italic># `new-namespace` is the namespace name</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>test-database.new-namespace</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># longer DNS names can be used for other namespaces</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>test-frontend.default</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>169.254.169.254</span>:<span style=color:#bbb> </span>-<span style=color:#666>80</span>,<span style=color:#bbb> </span>-<span style=color:#666>443</span><span style=color:#bbb>           </span><span style=color:#080;font-style:italic># AWS metadata API</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>metadata.google.internal</span>:<span style=color:#bbb> </span>-<span style=color:#666>80</span>,<span style=color:#bbb> </span>-<span style=color:#666>443</span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># GCP metadata API</span><span style=color:#bbb>
</span></code></pre></div><p>Cloud provider metadata APIs are a constant source of escalation (as the recent <a href=https://hackerone.com/reports/341876>Shopify</a> <a href=https://hackerone.com/reports/341876>bug bounty</a> demonstrates), so specific tests to confirm that the APIs are blocked on the container network helps to guard against accidental misconfiguration.</p><h2 id=10-scan-images-and-run-ids>10. Scan Images and Run IDS</h2><p><strong>Web servers present an attack surface to the network they're attached to: scanning an image's installed files ensures the absence of known vulnerabilities that an attacker could exploit to gain remote access to the container. An IDS (Intrusion Detection System) detects them if they do.</strong></p><p>Kubernetes permits pods into the cluster through a series of <a href=/docs/admin/admission-controllers/>admission controller</a> gates, which are applied to pods and other resources like deployments. These gates can validate each pod for admission or change its contents, and they now support backend webhooks.</p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/admission-controllers.png width=800></p><p>These webhooks can be used by container image scanning tools to validate images before they are deployed to the cluster. Images that have failed checks can be refused admission.</p><p>Scanning container images for known vulnerabilities can reduce the window of time that an attacker can exploit a disclosed CVE. Free tools such as CoreOS's <a href=https://github.com/coreos/clair>Clair</a> and Aqua's <a href=https://github.com/aquasecurity/microscanner>Micro Scanner</a> should be used in a deployment pipeline to prevent the deployment of images with critical, exploitable vulnerabilities.</p><p>Tools such as <a href=https://grafeas.io/>Grafeas</a> can store image metadata for constant compliance and vulnerability checks against a container's unique signature (a <a href=https://en.wikipedia.org/wiki/Content-addressable_storage>content addressable</a> hash). This means that scanning a container image with that hash is the same as scanning the images deployed in production, and can be done continually without requiring access to production environments.</p><p>Unknown Zero Day vulnerabilities will always exist, and so intrusion detection tools such as <a href=https://www.twistlock.com/>Twistlock</a>, <a href=https://www.aquasec.com/>Aqua</a>, and <a href=https://sysdig.com/product/secure/>Sysdig Secure</a> should be deployed in Kubernetes. IDS detects unusual behaviours in a container and pauses or kills it - <a href=https://github.com/draios/falco>Sysdig's Falco</a> is a an Open Source rules engine, and an entrypoint to this ecosystem.</p><h1 id=part-three-the-future>Part Three: The Future</h1><p>The next stage of security's "cloud native evolution" looks to be the service mesh, although adoption may take time - migration involves shifting complexity from applications to the mesh infrastructure, and organisations will be keen to understand best-practice.</p><p><img src=/images/blog/2018-06-05-11-ways-not-to-get-hacked/service-mesh-@sebiwicb.png width=800></p><h2 id=11-run-a-service-mesh>11. Run a Service Mesh</h2><p><strong>A service mesh is a web of encrypted persistent connections, made between high performance "sidecar" proxy servers like Envoy and Linkerd. It adds traffic management, monitoring, and policy - all without microservice changes.</strong></p><p>Offloading microservice security and networking code to a shared, battle tested set of libraries was already possible with <a href=https://linkerd.io/>Linkerd</a>, and the introduction of <a href=https://istio.io/>Istio</a> by Google, IBM, and Lyft, has added an alternative in this space. With the addition of <a href=https://spiffe.io>SPIFFE</a> for per-pod cryptographic identity and a plethora of <a href=https://istio.io/docs/concepts/what-is-istio/>other features</a>, Istio could simplify the deployment of the next generation of network security.</p><p>In "Zero Trust" networks there may be no need for traditional firewalling or Kubernetes network policy, as every interaction occurs over mTLS (mutual TLS), ensuring that both parties are not only communicating securely, but that the identity of both services is known.</p><p>This shift from traditional networking to Cloud Native security principles is not one we expect to be easy for those with a traditional security mindset, and the <a href=https://amzn.to/2Gg6Pav>Zero Trust Networking book</a> from SPIFFE's <a href=https://twitter.com/evan2645>Evan Gilman</a> is a highly recommended introduction to this brave new world.</p><p>Istio <a href=https://istio.io/about/notes/0.8/>0.8 LTS</a> is out, and the project is rapidly approaching a 1.0 release. Its stability versioning is the same as the Kubernetes model: a stable core, with individual APIs identifying themselves under their own alpha/beta stability namespace. Expect to see an uptick in Istio adoption over the coming months.</p><h1 id=conclusion>Conclusion</h1><p>Cloud Native applications have a more fine-grained set of lightweight security primitives to lock down workloads and infrastructure. The power and flexibility of these tools is both a blessing and curse - with insufficient automation it has become easier to expose insecure workloads which permit breakouts from the container or its isolation model.</p><p>There are more defensive tools available than ever, but caution must be taken to reduce attack surfaces and the potential for misconfiguration.</p><p>However if security slows down an organisation's pace of feature delivery it will never be a first-class citizen. Applying Continuous Delivery principles to the software supply chain allows an organisation to achieve compliance, continuous audit, and enforced governance without impacting the business's bottom line.</p><p>Iteratating quickly on security is easiest when supported by a comprehensive test suite. This is achieved with Continuous Security - an alternative to point-in-time penetration tests, with constant pipeline validation ensuring an organisation's attack surface is known, and the risk constantly understood and managed.</p><p>This is ControlPlane's modus operandi: if we can help kickstart a Continuous Security discipline, deliver Kubernetes security and operations training, or co-implement a secure cloud native evolution for you, please <a href=https://control-plane.io>get in touch</a>.</p><hr><p>Andrew Martin is a co-founder at <a href=https://twitter.com/controlplaneio>@controlplaneio</a> and tweets about cloud native security at <a href=https://twitter.com/sublimino>@sublimino</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c7a6473357fd6808fc9ac06453b45883>How the sausage is made: the Kubernetes 1.11 release interview, from the Kubernetes Podcast</h1><div class="td-byline mb-4"><time datetime=2018-07-16 class=text-muted>Monday, July 16, 2018</time></div><p><b>Author</b>: Craig Box (Google)</p><p>At KubeCon EU, my colleague Adam Glick and I were pleased to announce the <a href=https://kubernetespodcast.com/>Kubernetes Podcast from Google</a>. In this weekly conversation, we focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.</p><p>We <a href=https://kubernetespodcast.com/episode/010-kubernetes-1.11/>recently had the pleasure of speaking</a> to the release manager for Kubernetes 1.11, Josh Berkus from Red Hat, and the release manager for the upcoming 1.12, Tim Pepper from VMware.</p><p>In this conversation we learned about the release process, the impact of quarterly releases on end users, and how Kubernetes is like baking.</p><p>I encourage you to listen to <a href=https://kubernetespodcast.com/episode/010-kubernetes-1.11/>the podcast version</a> if you have a commute, or a dog to walk. If you like what you hear, <a href=https://kubernetespodcast.com/subscribe>we encourage you to subscribe</a>! In case you're short on time, or just want to browse quickly, we are delighted to share the transcript with you.</p><hr><p><b>CRAIG BOX: First of all, congratulations both, and thank you.</b></p><p>JOSH BERKUS: Well, thank you. Congratulations for me, because my job is done.</p><p>[LAUGHTER]</p><p>Congratulations and sympathy for Tim.</p><p>[LAUGH]</p><p>TIM PEPPER: Thank you, and I guess thank you?</p><p>[LAUGH]</p><p><b>ADAM GLICK: For those that don't know a lot about the process, why don't you help people understand — what is it like to be the release manager? What's the process that a release goes through to get to the point when everyone just sees, OK, it's released — 1.11 is available? What does it take to get up to that?</b></p><p>JOSH BERKUS: We have a quarterly release cycle. So every three months, we're releasing. And ideally and fortunately, this is actually now how we are doing things. Somewhere around two, three weeks before the previous release, somebody volunteers to be the release lead. That person is confirmed by <a href=https://github.com/kubernetes/sig-release>SIG Release</a>. So far, we've never had more than one volunteer, so there hasn't been really a fight about it.</p><p>And then that person starts working with others to put together <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md>a team</a> called the release team. Tim's just gone through this with Stephen Augustus and <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md>picking out a whole bunch of people</a>. And then after or a little before— probably after, because we want to wait for the retrospective from the previous release— the release lead then sets <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release-1.11.md>a schedule</a> for the upcoming release, as in when all the deadlines will be.</p><p>And this is a thing, because we're still tinkering with relative deadlines, and how long should code freeze be, and how should we track features? Because we don't feel that we've gotten down that sort of cadence perfectly yet. I mean, like, we've done pretty well, but we don't feel like we want to actually set [in stone], this is the schedule for each and every release.</p><p>Also, we have to adjust the schedule because of holidays, right? Because you can't have the code freeze deadline starting on July 4 or in the middle of design or sometime else when we're going to have a large group of contributors who are out on vacation.</p><p>TIM PEPPER: This is something I've had to spend some time looking at, thinking about 1.12. Going back to early June as we were tinkering with the code freeze date, starting to think about, well, what are the implications going to be on 1.12? When would these things start falling on the calendar? And then also for 1.11, we had one complexity. If we slipped the release past this week, we start running into the US 4th of July holiday, and we're not likely to get a lot done.</p><p>So much of a slip would mean slipping into the middle of July before we'd really know that we were successfully triaging things. And worst case maybe, we're quite a bit later into July.</p><p>So instead of quarterly with a three-month sort of cadence, well, maybe we've accidentally ended up chopping out one month out of the next release or pushing it quite a bit into the end of the year. And that made the deliberation around things quite complex, but thankfully this week, everything's gone smoothly in the end.</p><p><b>CRAIG BOX: All the releases so far have been one quarter — they've been a 12-week release cycle, give or take. Is that something that you think will continue going forward, or is the release team thinking about different ways they could run releases?</b></p><p>TIM PEPPER: The whole community is thinking about this. There are voices who'd like the cadence to be faster, and there are voices who'd like it to be shorter. And there's good arguments for both.</p><p><b>ADAM GLICK: Because it's interesting. It sounds like it is a date-driven release cycle versus a feature-driven release cycle.</b></p><p>JOSH BERKUS: Yeah, certainly. I really honestly think everybody in the world of software recognizes that feature-driven release cycles just don't work. And a big part of the duties of the release team collectively— several members of the team do this— is yanking things out of the release that are not ready. And the hard part of that is figuring out which things aren't ready, right? Because the person who's working on it tends to be super optimistic about what they can get done and what they can get fixed before the deadline.</p><p><b>ADAM GLICK: Of course.</b></p><p>TIM PEPPER: And this is one of the things I think that's useful about the process we have in place on the release team for having shadows who spend some time on the release team, working their way up into more of a lead position and gaining some experience, starting to get some exposure to see that optimism and see the processes for vetting.</p><p>And it's even an overstatement to say the process. Just see the way that we build the intuition for how to vet and understand and manage the risk, and really go after and chase things down proactively and early to get resolution in a timely way versus continuing to just all be optimistic and letting things maybe languish and put a release at risk.</p><p><b>CRAIG BOX: I've been reading this week about the introduction of <a href=https://github.com/kubernetes/community/issues/566>feature branches</a> to Kubernetes. The new server-side apply feature, for example, is being built in a branch so that it didn't have to be half-built in master and then ripped out again as the release approached, if the feature wasn't ready. That seems to me like something that's a normal part of software development? Is there a reason it's taken so long to bring that to core Kubernetes?</b></p><p>JOSH BERKUS: I don't actually know the history of why we're not using feature branches. I mean, the reason why we're not using feature branches pervasively now is that we have to transition from a different system. And I'm not really clear on how we adopted that linear development system. But it's certainly something we discussed on the release team, because there were issues of features that we thought were going to be ready, and then developed major problems. And we're like, if we have to back this out, that's going to be painful. And we did actually have to back one feature out, which involved not pulling out a Git commit, but literally reversing the line changes, which is really not how you want to be doing things.</p><p><b>CRAIG BOX: No.</b></p><p>TIM PEPPER: The other big benefit, I think, to the release branches if they are well integrated with the CI system for continuous integration and testing, you really get the feedback, and you can demonstrate, this set of stuff is ready. And then you can do deferred commitment on the master branch. And what comes in to a particular release on the timely cadence that users are expecting is stuff that's ready. You don't have potentially destabilizing things, because you can get a lot more proof and evidence of readiness.</p><p><b>ADAM GLICK: What are you looking at in terms of the tool chain that you're using to do this? You mentioned a couple of things, and I know it's obviously run through GitHub. But I imagine you have a number of other tools that you're using in order to manage the release, to make sure that you understand what's ready, what's not. You mentioned balancing between people who are very optimistic about the feature they're working on making it in versus the time-driven deadline, and balancing those two. Is that just a manual process, or do you have a set of tools that help you do that?</b></p><p>JOSH BERKUS: Well, there's code review, obviously. So just first of all, process was somebody wants to actually put in a feature, commit, or any kind of merge really, right? So that has to be assigned to one of the SIGs, one of these Special Interest Groups. Possibly more than one, depending on what areas it touches.</p><p>And then two generally overlapping groups of people have to approve that. One would be the SIG that it's assigned to, and the second would be anybody represented in the OWNERS files in the code tree of the directories which get touched.</p><p>Now sometimes those are the same group of people. I'd say often, actually. But sometimes they're not completely the same group of people, because sometimes you're making a change to the network, but that also happens to touch GCP support and OpenStack support, and so they need to review it as well.</p><p>So the first part is the human part, which is a bunch of other people need to look at this. And possibly they're going to comment "Hey. This is a really weird way to do this. Do you have a reason for it?"</p><p>Then the second part of it is the automated testing that happens, the automated acceptance testing that happens via webhook on there. And actually, one of the things that we did that was a significant advancement in this release cycle— and by we, I actually mean not me, but the great folks at <a href=https://github.com/kubernetes/community/tree/master/sig-scalability>SIG Scalability</a> did— was add an <a href=https://k8s-testgrid.appspot.com/sig-release-master-blocking#gce-scale-performance>additional acceptance test</a> that does a mini performance test.</p><p>Because one of the problems we've had historically is our major performance tests are large and take a long time to run, and so by the time we find out that we're failing the performance tests, we've already accumulated, you know, 40, 50 commits. And so now we're having to do git bisect to find out which of those commits actually caused the performance regression, which can make them very slow to address.</p><p>And so adding that performance pre-submit, the performance acceptance test really has helped stabilize performance in terms of new commits. So then we have that level of testing that you have to get past.</p><p>And then when we're done with that level of testing, we run a whole large battery of larger tests— end-to-end tests, performance tests, upgrade and downgrade tests. And one of the things that we've added recently and we're integrating to the process something called conformance tests. And the conformance test is we're testing whether or not you broke backwards compatibility, because it's obviously a big deal for Kubernetes users if you do that when you weren't intending to.</p><p>One of the busiest roles in the release team is a role called <a href=https://github.com/kubernetes/sig-release#ci-signal-lead>CI Signal</a>. And it's that person's job just to watch all of the tests for new things going red and then to try to figure out why they went red and bring it to people's attention.</p><p><b>ADAM GLICK: I've often heard what you're referring to kind of called a breaking change, because it breaks the existing systems that are running. How do you identify those to people so when they see, hey, there's a new version of Kubernetes out there, I want to try it out, is that just going to release notes? Or is there a special way that you identify breaking changes as opposed to new features?</b></p><p>JOSH BERKUS: That goes into release notes. I mean, keep in mind that one of the things that happens with Kubernetes' features is we go through this alpha, beta, general availability phase, right? So a feature's alpha for a couple of releases and then becomes beta for a release or two, and then it becomes generally available. And part of the idea of having this that may require a feature to go through that cycle for a year or more before its general availability is by the time it's general availability, we really want it to be, we are not going to change the API for this.</p><p>However, stuff happens, and we do occasionally have to do those. And so far, our main way to identify that to people actually is in the release notes. If you look at <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#no-really-you-must-do-this-before-you-upgrade>the current release notes</a>, there are actually two things in there right now that are sort of breaking changes.</p><p>One of them is the bit with <a href=/docs/concepts/configuration/pod-priority-preemption/>priority and preemption</a> in that preemption being on by default now allows badly behaved users of the system to cause trouble in new ways. I'd actually have to look at the release notes to see what the second one was...</p><p>TIM PEPPER: The <a href=https://github.com/kubernetes/kubernetes/issues/64612>JSON capitalization case sensitivity</a>.</p><p>JOSH BERKUS: Right. Yeah. And that was one of those cases where you have to break backwards compatibility, because due to a library switch, we accidentally enabled people using JSON in a case-insensitive way in certain APIs, which was never supposed to be the case. But because we didn't have a specific test for that, we didn't notice that we'd done it.</p><p>And so for three releases, people could actually shove in malformed JSON, and Kubernetes would accept it. Well, we have to fix that now. But that does mean that there are going to be users out in the field who have malformed JSON in their configuration management that is now going to break.</p><p><b>CRAIG BOX: But at least the good news is Kubernetes was always outputting correct formatted JSON during this period, I understand.</b></p><p>JOSH BERKUS: Mm-hmm.</p><p>TIM PEPPER: I think that also kind of reminds of one of the other areas— so kind of going back to the question of, well, how do you share word of breaking changes? Well, one of the ways you do that is to have as much quality CI that you can to catch these things that are important. Give the feedback to the developer who's making the breaking change, such that they don't make the breaking change. And then you don't actually have to communicate it out to users.</p><p>So some of this is bound to happen, because you always have test escapes. But it's also a reminder of the need to ensure that you're also really building and maintaining your test cases and the quality and coverage of your CI system over time.</p><p><b>ADAM GLICK: What do you mean when you say test escapes?</b></p><p>TIM PEPPER: So I guess it's a term in the art, but for those who aren't familiar with it, you have intended behavior that wasn't covered by test, and as a result, an unintended change happens to that. And instead of your intended behavior being shipped, you're shipping something else.</p><p>JOSH BERKUS: The JSON change is a textbook example of this, which is we were testing that the API would continue to accept correct JSON. We were not testing adequately that it wouldn't accept incorrect JSON.</p><p>TIM PEPPER: A test escape, another way to think of it as you shipped a bug because there was not a test case highlighting the possibility of the bug.</p><p><b>ADAM GLICK: It's the classic, we tested to make sure the feature worked. We didn't test to make sure that breaking things didn't work.</b></p><p>TIM PEPPER: It's common for us to focus on "I've created this feature and I'm testing the positive cases". And this also comes to thinking about things like secure by default and having a really robust system. A harder piece of engineering often is to think about the failure cases and really actively manage those well.</p><p>JOSH BERKUS: I had a conversation with a contributor recently where it became apparent that contributor had never worked on a support team, because their conception of a badly behaved user was, like, a hacker, right? An attacker who comes from outside.</p><p>And I'm like, no, no, no. You're stable of badly behaved users is your own staff. You know, they will do bad things, not necessarily intending to do bad things, but because they're trying to take a shortcut. And that is actually your primary concern in terms of preventing breaking the system.</p><p><b>CRAIG BOX: Josh, what was your preparation to be release manager for 1.11?</b></p><p>JOSH BERKUS: I was on the release team for two cycles, plus I was kind of auditing the release team for half a cycle before that. So in 1.9, I originally joined to be the shadow for bug triage, except I ended up not being the shadow, because the person who was supposed to be the lead for bug triage then dropped out. Then I ended up being the bug triage lead, and had to kind of improvise it because there wasn't documentation on what was involved in the role at the time.</p><p>And then I was <a href=https://github.com/kubernetes/sig-release/blob/master/README.md#bug-triage-lead>bug triage lead</a> for a second cycle, for the 1.10 cycle, and then took over as release lead for the cycle. And one of the things on my to-do list is to update the requirements to be release lead, because we actually do have written requirements, and to say that the expectation now is that you spend at least two cycles on the release team, one of them either as a lead or as a shadow to the release lead.</p><p><b>CRAIG BOX: And is bug triage lead just what it sounds like?</b></p><p>JOSH BERKUS: Yeah. Pretty much. There's more tracking involved than triage. Part of it is just deficiencies in tooling, something we're looking to address. But things like GitHub API limitations make it challenging to build automated tools that help us intelligently track issues. And we are actually working with GitHub on that. Like, they've been helpful. It's just, they have their own scaling problems.</p><p>But then beyond that, you know, a lot of that, it's what you would expect it to be in terms of what triage says, right? Which is looking at every issue and saying, first of all, is this a real issue? Second, is it a serious issue? Third, who needs to address this?</p><p>And that's a lot of the work, because for anybody who is a regular contributor to Kubernetes, the number of GitHub notifications that they receive per day means that most of us turn our GitHub notifications off.</p><p><b>CRAIG BOX: Indeed.</b></p><p>JOSH BERKUS: Because it's just this fire hose. And as a result, when somebody really needs to pay attention to something right now, that generally requires a human to go and track them down by email or Slack or whatever they prefer. Twitter in some cases. I've done that. And say, hey. We really need you to look at this issue, because it's about to hold up the beta release.</p><p><b>ADAM GLICK: When you look at the process that you're doing now, what are the changes that are coming in the future that will make the release process even better and easier?</b></p><p>JOSH BERKUS: Well, we just went through this whole retro, and I put in some recommendations for things. Obviously, some additional automation, which I'm going to be looking at doing now that I'm cycling off of the release team for a quarter and can actually look at more longer term goals, will help, particularly now that we've addressed actually some of our GitHub data flow issues.</p><p>Beyond that, I put in a whole bunch of recommendations in the retro, but it's actually up to Tim which recommendations he's going to try to implement. So I'll let him [comment].</p><p>TIM PEPPER: I think one of the biggest changes that happened in the 1.11 cycle is this emphasis on trying to keep our continuous integration test status always green. That is huge for software development and keeping velocity. If you have this more, I guess at this point antiquated notion of waterfall development, where you do feature development for a while and are accepting of destabilization, and somehow later you're going to come back and spend a period on stabilization and fixing, that really elongates the feedback loop for developers.</p><p>And they don't realize what was broken, and the problems become much more complex to sort out as time goes by. One, developers aren't thinking about what it was that they'd been working on anymore. They've lost the context to be able to efficiently solve the problem.</p><p>But then you start also getting interactions. Maybe a bug was introduced, and other people started working around it or depending on it, and you get complex dependencies then that are harder to fix. And when you're trying to do that type of complex resolution late in the cycle, it becomes untenable over time. So I think continuing on that and building on it, I'm seeing a little bit more focus on test cases and meaningful test coverage. I think that's a great cultural change to have happening.</p><p>And maybe because I'm following Josh into this role from a bug triage position and in his mentions earlier of just the communications and tracking involved with that versus triage, I do have a bit of a concern that at times, email and Slack are relatively quiet. Some of the SIG meeting notes are a bit sparse or YouTube videos slow to upload. So the general artifacts around choice making I think is an area where we need a little more rigor. So I'm hoping to see some of that.</p><p>And that can be just as subtle as commenting on issues like, hey, this commit doesn't say what it's doing. And for that reason on the release team, we can't assess its risk versus value. So could you give a little more information here? Things like that give more information both to the release team and the development community as well, because this is open source. And to collaborate, you really do need to communicate in depth.</p><p><b>CRAIG BOX: Speaking of cultural changes, professional baker to Kubernetes' release lead sounds like quite a journey.</b></p><p>JOSH BERKUS: There was a lot of stuff in between.</p><p><b>CRAIG BOX: Would you say there are a lot of similarities?</b></p><p>JOSH BERKUS: You know, believe it or not, there actually are similarities. And here's where it's similar, because I was actually thinking about this earlier. So when I was a professional baker, one of the things that I had to do was morning pastry. Like, I was actually in charge of doing several other things for custom orders, but since I had to come to work at 3:00 AM anyway— which also distressingly has similarities with some of this process. Because I had to come to work at 3:00 AM anyway, one of my secondary responsibilities was traying the morning pastry.</p><p>And one of the parts of that is you have this great big gas-fired oven with 10 rotating racks in it that are constantly rotating. Like, you get things in and out in the oven by popping them in and out while the racks are moving. That takes a certain amount of skill. You get burn marks on your wrists for your first couple of weeks of work. And then different pastries require a certain number of rotations to be done.</p><p>And there's a lot of similarities to the release cadence, because what you're doing is you're popping something in the oven or you're seeing something get kicked off, and then you have a certain amount of time before you need to check on it or you need to pull it out. And you're doing that in parallel with a whole bunch of other things. You know, with 40 other trays.</p><p><b>CRAIG BOX: And with presumably a bunch of colleagues who are all there at the same time.</b></p><p>JOSH BERKUS: Yeah. And the other thing is that these deadlines are kind of absolute, right? You can't say, oh, well, I was reading a magazine article, and I didn't have time to pull that tray out. It's too late. The pastry is burned, and you're going to have to throw it away, and they're not going to have enough pastry in the front case for the morning rush. And the customers are not interested in your excuses for that.</p><p>So from that perspective, from the perspective of saying, hey, we have a bunch of things that need to happen in parallel, they have deadlines and those deadlines are hard deadlines, there it's actually fairly similar.</p><p><b>CRAIG BOX: Tim, do you have any other history that helped get you to where you are today?</b></p><p>TIM PEPPER: I think in some ways I'm more of a traditional journey. I've got a computer engineering bachelor's degree. But I'm also maybe a bit of an outlier. In the late '90s, I found a passion for open source and Linux. Maybe kind of an early adopter, early believer in that.</p><p>And was working in the industry in the Bay Area for a while. Got involved in the Silicon Valley and Bay Area Linux users groups a bit, and managed to find work as a Linux sysadmin, and then doing device driver and kernel work and on up into distro. So that was all kind of standard in a way. And then I also did some other work around hardware enablement, high-performance computing, non-uniform memory access. Things that are really, really systems work.</p><p>And then about three years ago, my boss was really bending my ear and trying to get me to work on this cloud-related project. And that just felt so abstract and different from the low-level bits type of stuff that I'd been doing.</p><p>But kind of grudgingly, I eventually came around to the realization that the cloud is interesting, and it's so much more complex than local machine-only systems work, the type of things that I'd been doing before. It's massively distributed and you have a high-latency, low-reliability interconnect on all the nodes in the distributed network. So it's wildly complex engineering problems that need solved.</p><p>And so that got me interested. Started working then on this open source orchestrator for virtual machines and containers. It was written in Go and was having a lot of fun. But it wasn't Kubernetes, and it was becoming clear that Kubernetes was taking off. So about a year ago, I made the deliberate choice to move over to Kubernetes work.</p><p><b>ADAM GLICK: Previously, Josh, you spoke a little bit about your preparation for becoming a release manager. For other folks that are interested in getting involved in the community and maybe getting involved in release management, should they follow the same path that you did? Or what are ways that would be good for them to get involved? And for you, Tim, how you've approached the preparation for taking on the next release.</b></p><p>JOSH BERKUS: The great thing with the release team is that we have this formal mentorship path. And it's fast, right? That's the advantage of releasing quarterly, right? Is that within six months, you can go from joining the team as a shadow to being the release lead if you have the time. And you know, by the time you work your way up to release time, you better have support from your boss about this, because you're going to end up spending a majority of your work time towards the end of the release on release management.</p><p>So the answer is to sign up to look when we're getting into the latter half of release cycle, to sign up as a shadow. Or at the beginning of a release cycle, to sign up as a shadow. Some positions actually can reasonably use more than one shadow. There's some position that just require a whole ton of legwork like release notes. And as a result, could actually use more than one shadow meaningfully. So there's probably still places where people could sign up for 1.12. Is that true, Tim?</p><p>TIM PEPPER: Definitely. I think— gosh, right now we have 34 volunteers on the release team, which is—</p><p><b>ADAM GLICK: Wow.</b></p><p>JOSH BERKUS: OK. OK. Maybe not then.</p><p>[LAUGH]</p><p>TIM PEPPER: It's potentially becoming a lot of cats to herd. But I think even outside of that formal volunteering to be a named shadow, anybody is welcome to show up to the release team meetings, follow the release team activities on <a href=http://slack.k8s.io>Slack</a>, start understanding how the process works. And really, this is the case all across open source. It doesn't even have to be the release team. If you're passionate about networking, start following what SIG Network is doing. It's the same sort of path, I think, into any area on the project.</p><p>Each of the SIGs [has] a channel. So it would be #SIG-whatever the name is. [In our] case, #SIG-Release.</p><p>I'd also maybe give a plug for a <a href=https://youtu.be/goAph8A20gQ>talk I did at KubeCon</a> in Copenhagen this spring, talking about how the release team specifically can be a path for new contributors coming in. And had some ideas and suggestions there for newcomers.</p><p><b>CRAIG BOX: There's three questions in the Google SRE postmortem template that I really like. And I'm sure you will have gone through these in the retrospective process as you released 1.11, so I'd like to ask them now one at a time.</p><p>First of all, what went well?</b></p><p>JOSH BERKUS: Two things, I think, really improved things, both for contributors and for the release team. Thing number one was putting a strong emphasis on getting the test grid green well ahead of code freeze.</p><p>TIM PEPPER: Definitely.</p><p>JOSH BERKUS: Now partly that went well because we had a spectacular CI lead, <a href=https://github.com/aishsundar>Aish Sundar</a>, who's now in training to become the release lead.</p><p>TIM PEPPER: And I'd count that partly as one of the "Where were you lucky?" areas. We happened upon a wonderful person who just popped up and volunteered.</p><p>JOSH BERKUS: Yes. And then but part of that was also that we said, hey. You know, we're not going to do what we've done before which is not really care about these tests until code slush. We're going to care about these tests now.</p><p>And importantly— this is really important to the Kubernetes community— when we went to the various SIGs, the SIG Cluster Lifecycle and SIG Scalability and SIG Node and the other ones who were having test failures, and we said this to them. They didn't say, get lost. I'm busy. They said, what's failing?</p><p><b>CRAIG BOX: Great.</b></p><p>JOSH BERKUS: And so that made a big difference. And the second thing that was pretty much allowed by the first thing was to shorten the code freeze period. Because the code freeze period is frustrating for developers, because if they don't happen to be working on a 1.11 feature, even if they worked on one before, and they delivered it early in the cycle, and it's completely done, they're kind of paralyzed, and they can't do anything during code freeze. And so it's very frustrating for them, and we want to make that period as short as possible. And we did that this time, and I think it helped everybody.</p><p><b>CRAIG BOX: What went poorly?</b></p><p>JOSH BERKUS: We had a lot of problems with flaky tests. We have a lot of old tests that are not all that well maintained, and they're testing very complicated things like upgrading a cluster that has 40 nodes. And as a result, these tests have high failure rates that have very little to do with any change in the code.</p><p>And so one of the things that happened, and the reason we had a one-day delay in the release is, you know, we're a week out from release, and just by random luck of the draw, a bunch of these tests all at once got a run of failures. And it turned out that run of failures didn't actually mean anything, having anything to do with Kubernetes. But there was no way for us to tell that without a lot of research, and we were not going to have enough time for that research without delaying the release.</p><p>So one of the things we're looking to address in the 1.12 cycle is to actually move some of those flaky tests out. Either fix them or move them out of the release blocking category.</p><p>TIM PEPPER: In a way, I think this also highlights one of the things that Josh mentioned that went well, the emphasis early on getting the test results green, it allows us to see the extent to which these flakes are such a problem. And then the unlucky occurrence of them all happening to overlap on a failure, again, highlights that these flakes have been called out in the community for quite some time. I mean, at least a year. I know one contributor who was really concerned about them.</p><p>But they became a second order concern versus just getting things done in the short term, getting features and proving that the features worked, and kind of accepting in a risk management way on the release team that, yes, those are flakes. We don't have time to do something about them, and it's OK. But because of the emphasis on keeping the test always green now, we have the luxury maybe to focus on improving these flakes, and really get to where we have truly high quality CI signal, and can really believe in the results that we have on an ongoing basis.</p><p>JOSH BERKUS: And having solved some of the more basic problems, we're now seeing some of the other problems like coordination between related features. Like we right now have a feature where— and this is one of the sort of backwards compatibility release notes— where the feature went into beta, and is on by default.</p><p>And the second feature that was supposed to provide access control for the first feature did not go in as beta, and is not on by default. And the team for the first feature did not realize the second feature was being held up until two days before the release. So it's going to result in us actually patching something in 11.1.</p><p>And so like, we put that into something that didn't go well. But on the other hand, as Tim points out, a few release cycles ago, we wouldn't even have identified that as a problem, because we were still struggling with just individual features having a clear idea well ahead of the release of what was going in and what wasn't going in.</p><p>TIM PEPPER: I think something like this also is a case that maybe advocates for the use of feature branches. If these things are related, we might have seen it and done more pre-testing within that branch and pre-integration, and decide maybe to merge a couple of what initially had been disjoint features into a single feature branch, and really convince ourselves that together they were good. And cross all the Ts, dot all the Is on them, and not have something that's gated on an alpha feature that's possibly falling away.</p><p><b>CRAIG BOX: And then the final question, which I think you've both touched on a little. Where did you get lucky, or unlucky perhaps?</b></p><p>JOSH BERKUS: I would say number one where I got lucky is truly having a fantastic team. I mean, we just had a lot of terrific people who were very good and very energetic and very enthusiastic about taking on their release responsibilities including Aish and Tim and Ben and Nick and Misty who took over Docs four weeks into the release. And then went crazy with it and said, well, I'm new here, so I'm going to actually change a bunch of things we've been doing that didn't work in the first place. So that was number one. I mean, that really made honestly all the difference.</p><p>And then the second thing, like I said, is that we didn't have sort of major, unexpected monkey wrenches thrown at us. So in the 1.10 cycle, we actually had two of those, which is why I still count Jace as heroic for pulling off a release that was only a week late.</p><p>You know, number one was having the scalability tests start failing for unrelated reasons for a long period, which then masked the fact that they were actually failing for real reasons when we actually got them working again. And as a result, ending up debugging a major and super complicated scalability issue within days of what was supposed to be the original release date. So that was monkey wrench number one for the 1.10 cycle.</p><p>Monkey wrench number two for the 1.10 cycle was we got a security hole that needed to be patched. And so again, a week out from what was supposed to be the original release date, we were releasing a security update, and that security update required patching the release branch. And it turns out that patch against the release branch broke a bunch of incoming features. And we didn't get anything of that magnitude in the 1.11 release, and I'm thankful for that.</p><p>TIM PEPPER: Also, I would maybe argue in a way that a portion of that wasn't just luck. The extent to which this community has a good team, not just the release team but beyond, some of this goes to active work that folks all across the project, but especially in the contributor experience SIG are doing to cultivate a positive and inclusive culture here. And you really see that. When problems crop up, you're seeing people jump on and really try to constructively tackle them. And it's really fun to be a part of that.</p><hr><p><i>Thanks to Josh Berkus and Tim Pepper for talking to the Kubernetes Podcast from Google.</p><p><a href=https://github.com/jberkus>Josh Berkus</a> hangs out in #sig-release on the <a href=https://slack.k8s.io>Kubernetes Slack</a>. He maintains a newsletter called "<a href=http://lwkd.info/>Last Week in Kubernetes Development</a>", with Noah Kantrowitz. You can read him on Twitter at <a href=https://twitter.com/fuzzychef>@fuzzychef</a>, but he does warn you that there's a lot of politics there as well.</p><p><a href=https://github.com/tpepper>Tim Pepper</a> is also on Slack - he's always open to folks reaching out with a question, looking for help or advice. On Twitter you'll find him at <a href=https://twitter.com/pythomit>@pythomit</a>, which is "Timothy P" backwards. Tim is an avid soccer fan and season ticket holder for the <a href=https://portlandtimbers.com/>Portland Timbers</a> and the <a href=https://portlandthorns.com/>Portland Thorns</a>, so you'll get all sorts of opinions on soccer in addition to technology!</p><p>You can find the <a href=http://www.kubernetespodcast.com/>Kubernetes Podcast from Google</a> at <a href=https://twitter.com/KubernetesPod>@kubernetespod</a> on Twitter, and you can <a href=https://kubernetespodcast.com/subscribe/>subscribe</a> so you never miss an episode.</i></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4ce1d602e0512565b4d1b6e9a57367e1>Resizing Persistent Volumes using Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-07-12 class=text-muted>Thursday, July 12, 2018</time></div><p><strong>Author</strong>: Hemant Kumar (Red Hat)</p><p><strong>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/>series of in-depth articles</a> on what’s new in Kubernetes 1.11</strong></p><p>In Kubernetes v1.11 the persistent volume expansion feature is being promoted to beta. This feature allows users to easily resize an existing volume by editing the <code>PersistentVolumeClaim</code> (PVC) object. Users no longer have to manually interact with the storage backend or delete and recreate PV and PVC objects to increase the size of a volume. Shrinking persistent volumes is not supported.</p><p>Volume expansion was introduced in v1.8 as an Alpha feature, and versions prior to v1.11 required enabling the feature gate, <code>ExpandPersistentVolumes</code>, as well as the admission controller, <code>PersistentVolumeClaimResize</code> (which prevents expansion of PVCs whose underlying storage provider does not support resizing). In Kubernetes v1.11+, both the feature gate and admission controller are enabled by default.</p><p>Although the feature is enabled by default, a cluster admin must opt-in to allow users to resize their volumes. Kubernetes v1.11 ships with volume expansion support for the following in-tree volume plugins: AWS-EBS, GCE-PD, Azure Disk, Azure File, Glusterfs, Cinder, Portworx, and Ceph RBD. Once the admin has determined that volume expansion is supported for the underlying provider, they can make the feature available to users by setting the <code>allowVolumeExpansion</code> field to <code>true</code> in their <code>StorageClass</code> object(s). Only PVCs created from that <code>StorageClass</code> will be allowed to trigger volume expansion.</p><pre><code>~&gt; cat standard.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
parameters:
  type: pd-standard
provisioner: kubernetes.io/gce-pd
allowVolumeExpansion: true
reclaimPolicy: Delete
</code></pre><p>Any PVC created from this <code>StorageClass</code> can be edited (as illustrated below) to request more space. Kubernetes will interpret a change to the storage field as a request for more space, and will trigger automatic volume resizing.</p><p><img src=/images/blog/2018-07-12-resizing-persistent-volumes-using-kubernetes/pvc-storageclass.png alt="PVC StorageClass"></p><h2 id=file-system-expansion>File System Expansion</h2><p>Block storage volume types such as GCE-PD, AWS-EBS, Azure Disk, Cinder, and Ceph RBD typically require a file system expansion before the additional space of an expanded volume is usable by pods. Kubernetes takes care of this automatically whenever the pod(s) referencing your volume are restarted.</p><p>Network attached file systems (like Glusterfs and Azure File) can be expanded without having to restart the referencing Pod, because these systems do not require special file system expansion.</p><p>File system expansion must be triggered by terminating the pod using the volume. More specifically:</p><ul><li>Edit the PVC to request more space.</li><li>Once underlying volume has been expanded by the storage provider, then the PersistentVolume object will reflect the updated size and the PVC will have the <code>FileSystemResizePending</code> condition.</li></ul><p>You can verify this by running <code>kubectl get pvc &lt;pvc_name> -o yaml</code></p><pre><code>~&gt; kubectl get pvc myclaim -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
  namespace: default
  uid: 02d4aa83-83cd-11e8-909d-42010af00004
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 14Gi
  storageClassName: standard
  volumeName: pvc-xxx
status:
  capacity:
    storage: 9G
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2018-07-11T14:51:10Z
    message: Waiting for user to (re-)start a pod to finish file system resize of
      volume on node.
    status: &quot;True&quot;
    type: FileSystemResizePending
  phase: Bound
</code></pre><ul><li>Once the PVC has the condition <code>FileSystemResizePending</code> then pod that uses the PVC can be restarted to finish file system resizing on the node. Restart can be achieved by deleting and recreating the pod or by scaling down the deployment and then scaling it up again.</li><li>Once file system resizing is done, the PVC will automatically be updated to reflect new size.</li></ul><p>Any errors encountered while expanding file system should be available as events on pod.</p><h2 id=online-file-system-expansion>Online File System Expansion</h2><p>Kubernetes v1.11 also introduces an alpha feature called online file system expansion. This feature enables file system expansion while a volume is still in-use by a pod. Because this feature is alpha, it requires enabling the feature gate, <code>ExpandInUsePersistentVolumes</code>. It is supported by the in-tree volume plugins GCE-PD, AWS-EBS, Cinder, and Ceph RBD. When this feature is enabled, pod referencing the resized volume do not need to be restarted. Instead, the file system will automatically be resized while in use as part of volume expansion. File system expansion does not happen until a pod references the resized volume, so if no pods referencing the volume are running file system expansion will not happen.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>Check out additional documentation on this feature here: <a href=http://k8s.io/docs/concepts/storage/persistent-volumes>http://k8s.io/docs/concepts/storage/persistent-volumes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-37f9b93223232a11f579d220e199b9d1>Dynamic Kubelet Configuration</h1><div class="td-byline mb-4"><time datetime=2018-07-11 class=text-muted>Wednesday, July 11, 2018</time></div><p><strong>Author</strong>: Michael Taufen (Google)</p><p><strong>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/>series of in-depth articles</a> on what’s new in Kubernetes 1.11</strong></p><h2 id=why-dynamic-kubelet-configuration>Why Dynamic Kubelet Configuration?</h2><p>Kubernetes provides API-centric tooling that significantly improves workflows for managing applications and infrastructure. Most Kubernetes installations, however, run the Kubelet as a native process on each host, outside the scope of standard Kubernetes APIs.</p><p>In the past, this meant that cluster administrators and service providers could not rely on Kubernetes APIs to reconfigure Kubelets in a live cluster. In practice, this required operators to either ssh into machines to perform manual reconfigurations, use third-party configuration management automation tools, or create new VMs with the desired configuration already installed, then migrate work to the new machines. These approaches are environment-specific and can be expensive.</p><p>Dynamic Kubelet configuration gives cluster administrators and service providers the ability to reconfigure Kubelets in a live cluster via Kubernetes APIs.</p><h2 id=what-is-dynamic-kubelet-configuration>What is Dynamic Kubelet Configuration?</h2><p>Kubernetes v1.10 made it possible to configure the Kubelet via a beta <a href=/docs/tasks/administer-cluster/kubelet-config-file/>config file</a> API. Kubernetes already provides the ConfigMap abstraction for storing arbitrary file data in the API server.</p><p>Dynamic Kubelet configuration extends the Node object so that a Node can refer to a ConfigMap that contains the same type of config file. When a Node is updated to refer to a new ConfigMap, the associated Kubelet will attempt to use the new configuration.</p><h2 id=how-does-it-work>How does it work?</h2><p>Dynamic Kubelet configuration provides the following core features:</p><ul><li>Kubelet attempts to use the dynamically assigned configuration.</li><li>Kubelet "checkpoints" configuration to local disk, enabling restarts without API server access.</li><li>Kubelet reports assigned, active, and last-known-good configuration sources in the Node status.</li><li>When invalid configuration is dynamically assigned, Kubelet automatically falls back to a last-known-good configuration and reports errors in the Node status.</li></ul><p>To use the dynamic Kubelet configuration feature, a cluster administrator or service provider will first post a ConfigMap containing the desired configuration, then set each Node.Spec.ConfigSource.ConfigMap reference to refer to the new ConfigMap. Operators can update these references at their preferred rate, giving them the ability to perform controlled rollouts of new configurations.</p><p>Each Kubelet watches its associated Node object for changes. When the Node.Spec.ConfigSource.ConfigMap reference is updated, the Kubelet will "checkpoint" the new ConfigMap by writing the files it contains to local disk. The Kubelet will then exit, and the OS-level process manager will restart it. Note that if the Node.Spec.ConfigSource.ConfigMap reference is not set, the Kubelet uses the set of flags and config files local to the machine it is running on.</p><p>Once restarted, the Kubelet will attempt to use the configuration from the new checkpoint. If the new configuration passes the Kubelet's internal validation, the Kubelet will update Node.Status.Config to reflect that it is using the new configuration. If the new configuration is invalid, the Kubelet will fall back to its last-known-good configuration and report an error in Node.Status.Config.</p><p>Note that the default last-known-good configuration is the combination of Kubelet command-line flags with the Kubelet's local configuration file. Command-line flags that overlap with the config file always take precedence over both the local configuration file and dynamic configurations, for backwards-compatibility.</p><p>See the following diagram for a high-level overview of a configuration update for a single Node:</p><p><img src=/images/blog/2018-07-11-dynamic-kubelet-configuration/kubelet-diagram.png alt=kubelet-diagram></p><h2 id=how-can-i-learn-more>How can I learn more?</h2><p>Please see the official tutorial at /docs/tasks/administer-cluster/reconfigure-kubelet/, which contains more in-depth details on user workflow, how a configuration becomes "last-known-good," how the Kubelet "checkpoints" config, and possible failure modes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d1b8243b08fd75f7c6f7a1ee1b31e1c>CoreDNS GA for Kubernetes Cluster DNS</h1><div class="td-byline mb-4"><time datetime=2018-07-10 class=text-muted>Tuesday, July 10, 2018</time></div><p><strong>Author</strong>: John Belamaric (Infoblox)</p><p><strong>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/>series of in-depth articles</a> on what’s new in Kubernetes 1.11</strong></p><h2 id=introduction>Introduction</h2><p>In Kubernetes 1.11, <a href=https://coredns.io>CoreDNS</a> has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.</p><p>DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.</p><p>CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.</p><p>In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.</p><h2 id=implementation-differences>Implementation differences</h2><p>In kube-dns, several containers are used within a single pod: <code>kubedns</code>, <code>dnsmasq</code>, and <code>sidecar</code>. The <code>kubedns</code>
container watches the Kubernetes API and serves DNS records based on the <a href=https://github.com/kubernetes/dns/blob/master/docs/specification.md>Kubernetes DNS specification</a>, <code>dnsmasq</code> provides caching and stub domain support, and <code>sidecar</code> provides metrics and health checks.</p><p>This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in <code>dnsmasq</code> have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because <code>dnsmasq</code> handles the stub domains,
but <code>kubedns</code> handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see <a href=https://github.com/kubernetes/dns/issues/131>dns#131</a>).</p><p>All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.</p><h2 id=configuring-coredns>Configuring CoreDNS</h2><p>In kube-dns, you can <a href=https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/>modify a ConfigMap</a> to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.</p><p>In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS <a href=https://coredns.io/2017/07/23/corefile-explained/>Corefile</a> to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.</p><p>When upgrading from kube-dns to CoreDNS using <code>kubeadm</code>, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See <a href=/docs/tasks/administer-cluster/coredns/>Using CoreDNS for Service Discovery</a> for more details.</p><h2 id=bug-fixes-and-enhancements>Bug fixes and enhancements</h2><p>There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.</p><ul><li><p><a href=https://github.com/kubernetes/dns/issues/55>dns#55 - Custom DNS entries for kube-dns</a> may be handled using the "fallthrough" mechanism in the <a href=https://coredns.io/plugins/kubernetes>kubernetes plugin</a>, using the <a href=https://coredns.io/plugins/rewrite>rewrite plugin</a>, or simply serving a subzone with a different plugin such as the <a href=https://coredns.io/plugins/file>file plugin</a>.</p></li><li><p><a href=https://github.com/kubernetes/dns/issues/116>dns#116 - Only one A record set for headless service with pods having single hostname</a>. This issue is fixed without any additional configuration.</p></li><li><p><a href=https://github.com/kubernetes/dns/issues/131>dns#131 - externalName not using stubDomains settings</a>. This issue is fixed without any additional configuration.</p></li><li><p><a href=https://github.com/kubernetes/dns/issues/167>dns#167 - enable skyDNS round robin A/AAAA records</a>. The equivalent functionality can be configured using the <a href=https://coredns.io/plugins/loadbalance>load balance plugin</a>.</p></li><li><p><a href=https://github.com/kubernetes/dns/issues/190>dns#190 - kube-dns cannot run as non-root user</a>. This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.</p></li><li><p><a href=https://github.com/kubernetes/dns/issues/232>dns#232 - fix pod hostname to be podname for dns srv records</a> is an enhancement that is supported through the "endpoint_pod_names" feature described below.</p></li></ul><h2 id=metrics>Metrics</h2><p>The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for <code>dnsmasq</code> and <code>kubedns</code> (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS <a href=https://coredns.io/plugins/metrics/>Prometheus plugin</a> page.</p><h2 id=some-special-features>Some special features</h2><p>The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the <a href=https://github.com/kubernetes/dns/blob/master/docs/specification.md>Kubernetes DNS specification</a>;
they enhance functionality but remain backward compatible. Since CoreDNS is not
<em>only</em> made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.</p><h3 id=pods-verified-mode>Pods verified mode</h3><p>In kube-dns, pod name records are "fake". That is, any "a-b-c-d.namespace.pod.cluster.local" query will
return the IP address "a.b.c.d". In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a "pods verified" mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.</p><h3 id=endpoint-names-based-on-pod-names>Endpoint names based on pod names</h3><p>In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:</p><pre><code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
</code></pre><p>However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:</p><pre><code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
</code></pre><p>For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example <a href=https://github.com/kubernetes/kubernetes/issues/47992>kubernetes#47992</a> and <a href=https://github.com/coredns/coredns/pull/1190>coredns#1190</a>). To enable this in CoreDNS, you specify the "endpoint_pod_names" option in your Corefile, which results in this:</p><pre><code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
</code></pre><h3 id=autopath>Autopath</h3><p>CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, "headless" above, rather than "headless.default.svc.cluster.local". However,
when requesting an external name - "infoblox.com", for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to <code>dnsmasq</code> and then to <code>kubedns</code>, since <a href=https://github.com/kubernetes/dns/issues/121>negative caching is disabled</a>):</p><ul><li>infoblox.com.default.svc.cluster.local -> NXDOMAIN</li><li>infoblox.com.svc.cluster.local -> NXDOMAIN</li><li>infoblox.com.cluster.local -> NXDOMAIN</li><li>infoblox.com.your-internal-domain.com -> NXDOMAIN</li><li>infoblox.com -> returns a valid record</li></ul><p>In CoreDNS, an optional feature called <a href=https://coredns.io/plugins/autopath>autopath</a> can be enabled that will cause this search path to be followed
<em>in the server</em>. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.</p><h3 id=a-few-other-kubernetes-specific-features>A few other Kubernetes specific features</h3><p>In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.</p><p>You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.</p><h2 id=extensibility>Extensibility</h2><p>In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the <a href=https://coredns.io/explugins/unbound>unbound plugin</a>, to server records directly from a database with the <a href=https://coredns.io/explugins/pdsql>pdsql plugin</a>, and to allow multiple CoreDNS instances to share a common level 2 cache with the <a href=https://coredns.io/explugins/redisc>redisc plugin</a>.</p><p>Many other interesting extensions have been added, which you will find on the <a href=https://coredns.io/explugins/>External Plugins</a> page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the <a href=https://coredns.io/explugins/kubernetai>kubernetai plugin</a>, which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.</p><h2 id=what-s-next>What's Next?</h2><p>CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!</p><p>The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the <a href=https://coredns.io/blog>CoreDNS Blog</a>.</p><h3 id=get-involved-with-coredns>Get involved with CoreDNS</h3><p>CoreDNS is an incubated <a href=https://cncf.io>CNCF</a> project.</p><p>We're most active on Slack (and GitHub):</p><ul><li>Slack: #coredns on <a href=https://slack.cncf.io>https://slack.cncf.io</a></li><li>GitHub: <a href=https://github.com/coredns/coredns>https://github.com/coredns/coredns</a></li></ul><p>More resources can be found:</p><ul><li>Website: <a href=https://coredns.io>https://coredns.io</a></li><li>Blog: <a href=https://blog.coredns.io>https://blog.coredns.io</a></li><li>Twitter: <a href=https://twitter.com/corednsio>@corednsio</a></li><li>Mailing list/group: <a href=mailto:coredns-discuss@googlegroups.com>coredns-discuss@googlegroups.com</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2e5e79eb95ef7d07df43b968c4f8c326>Meet Our Contributors - Monthly Streaming YouTube Mentoring Series</h1><div class="td-byline mb-4"><time datetime=2018-07-10 class=text-muted>Tuesday, July 10, 2018</time></div><p><strong>Author</strong>: Paris Pittman (Google)</p><p><img src=/images/blog/2018-06-05-meet-our-contributors-youtube-mentoring-series/meet-our-contributors.png alt=meet_our_contributors></p><p>July 11th at 2:30pm and 8pm UTC kicks off our next installment of Meet Our Contributors YouTube series. This month is special: members of the steering committee will be on to answer any and all questions from the community on the first 30 minutes of the 8pm UTC session. More on submitting questions below.</p><p><a href=https://github.com/kubernetes/community/blob/master/mentoring/meet-our-contributors.md>Meet Our Contributors</a> was created to give an opportunity to new and current contributors alike to get time in front of our upstream community to ask questions that you would typically ask a mentor. We have 3-6 contributors on each session (an AM and PM session depending on where you are in the world!) answer questions <a href=https://www.youtube.com/c/KubernetesCommunity/live>live on a YouTube stream</a>. If you miss it, don’t stress, the recording is up after it’s over. Check out a past episode <a href="https://www.youtube.com/watch?v=EVsXi3Zhlo0&list=PL69nYSiGNLP3QpQrhZq_sLYo77BVKv09F">here</a>.</p><p>As you can imagine, the questions span broadly from introductory - “what’s a SIG?” to more advanced - “why’s my test flaking?” You’ll also hear growth related advice questions such as “what’s my best path to becoming an approver?” We’re happy to do a live code/docs review or explain part of the codebase as long as we have a few days notice.</p><p>We answer at least 10 questions per session and have helped 500+ people to date. This is a scalable mentoring initiative that makes it easy for all parties to share information, get advice, and get going with what they are trying to accomplish. We encourage you to submit questions for our next session:</p><ul><li>Join the Kubernetes Slack channel - #meet-our-contributors - to ask your question or for more detailed information. DM paris@ if you would like to remain anonymous.</li><li>Twitter works, too, with the hashtag #k8smoc</li></ul><p>If you are contributor reading this that has wanted to mentor but just can’t find the time - this is for you! <a href=https://goo.gl/forms/ZcnFiqNR5EQH03zm2>Reach out to us</a>.</p><p>You can join us live on June 6th at 2:30pm and 8pm UTC, and every first Wednesday of the month, on the <a href=https://www.youtube.com/c/KubernetesCommunity/live>Kubernetes Community live stream</a>. We look forward to seeing you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ff1a9a29a39941763cf19d669d7fbbcc>IPVS-Based In-Cluster Load Balancing Deep Dive</h1><div class="td-byline mb-4"><time datetime=2018-07-09 class=text-muted>Monday, July 09, 2018</time></div><p><strong>Author</strong>: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)</p><p><strong>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/>series of in-depth articles</a> on what’s new in Kubernetes 1.11</strong></p><h2 id=introduction>Introduction</h2><p>Per <a href=https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/>the Kubernetes 1.11 release blog post </a>, we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.</p><h2 id=what-is-ipvs>What Is IPVS?</h2><p><strong>IPVS</strong> (<strong>IP Virtual Server</strong>) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.</p><p>IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.</p><h2 id=why-ipvs-for-kubernetes>Why IPVS for Kubernetes?</h2><p>As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.</p><p>Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.</p><p>Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.</p><p>On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.</p><h2 id=ipvs-based-kube-proxy>IPVS-based Kube-proxy</h2><h3 id=parameter-changes>Parameter Changes</h3><p><strong>Parameter: --proxy-mode</strong> In addition to existing userspace and iptables modes, IPVS mode is configured via <code>--proxy-mode=ipvs</code>. It implicitly uses IPVS NAT mode for service port mapping.</p><p><strong>Parameter: --ipvs-scheduler</strong></p><p>A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being <code>--ipvs-scheduler</code>. If it’s not configured, then round-robin (rr) is the default value.</p><ul><li>rr: round-robin</li><li>lc: least connection</li><li>dh: destination hashing</li><li>sh: source hashing</li><li>sed: shortest expected delay</li><li>nq: never queue</li></ul><p>In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.</p><p><strong>Parameter: <code>--cleanup-ipvs</code></strong> Similar to the <code>--cleanup-iptables</code> parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.</p><p><strong>Parameter: <code>--ipvs-sync-period</code></strong> Maximum interval of how often IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.</p><p><strong>Parameter: <code>--ipvs-min-sync-period</code></strong> Minimum interval of how often the IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.</p><p><strong>Parameter: <code>--ipvs-exclude-cidrs</code></strong> A comma-separated list of CIDR's which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can't distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.</p><h3 id=design-considerations>Design Considerations</h3><h4 id=ipvs-service-network-topology>IPVS Service Network Topology</h4><p>When creating a ClusterIP type Service, IPVS proxier will do the following three things:</p><ul><li>Make sure a dummy interface exists in the node, defaults to kube-ipvs0</li><li>Bind Service IP addresses to the dummy interface</li><li>Create IPVS virtual servers for each Service IP address respectively</li></ul><p>Here comes an example:</p><pre><code># kubectl describe svc nginx-service
Name:			nginx-service
...
Type:			ClusterIP
IP:			    10.102.128.4
Port:			http	3080/TCP
Endpoints:		10.244.0.235:8080,10.244.1.237:8080
Session Affinity:	None

# ip addr
...
73: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.102.128.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn     
TCP  10.102.128.4:3080 rr
  -&gt; 10.244.0.235:8080            Masq    1      0          0         
  -&gt; 10.244.1.237:8080            Masq    1      0          0   
</code></pre><p>Please note that the relationship between a Kubernetes Service and IPVS virtual servers is <code>1:N</code>. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is <code>1:1</code>.</p><p>Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.</p><h4 id=port-mapping>Port Mapping</h4><p>There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.</p><pre><code>TCP  10.102.128.4:3080 rr
  -&gt; 10.244.0.235:8080            Masq    1      0          0         
  -&gt; 10.244.1.237:8080            Masq    1      0       
</code></pre><h4 id=session-affinity>Session Affinity</h4><p>IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:</p><pre><code># kubectl describe svc nginx-service
Name:			nginx-service
...
IP:			    10.102.128.4
Port:			http	3080/TCP
Session Affinity:	ClientIP

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr persistent 10800
</code></pre><h4 id=iptables-ipset-in-ipvs-proxier>Iptables & Ipset in IPVS Proxier</h4><p>IPVS is for load balancing and it can't handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.</p><p>IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:</p><ul><li>kube-proxy start with --masquerade-all=true</li><li>Specify cluster CIDR in kube-proxy startup</li><li>Support Loadbalancer type service</li><li>Support NodePort type service</li></ul><p>However, we don't want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:</p><table><thead><tr><th>set name</th><th>members</th><th>usage</th></tr></thead><tbody><tr><td>KUBE-CLUSTER-IP</td><td>All Service IP + port</td><td>masquerade for cases that <code>masquerade-all=true</code> or <code>clusterCIDR</code> specified</td></tr><tr><td>KUBE-LOOP-BACK</td><td>All Service IP + port + IP</td><td>masquerade for resolving hairpin issue</td></tr><tr><td>KUBE-EXTERNAL-IP</td><td>Service External IP + port</td><td>masquerade for packets to external IPs</td></tr><tr><td>KUBE-LOAD-BALANCER</td><td>Load Balancer ingress IP + port</td><td>masquerade for packets to Load Balancer type service</td></tr><tr><td>KUBE-LOAD-BALANCER-LOCAL</td><td>Load Balancer ingress IP + port with <code>externalTrafficPolicy=local</code></td><td>accept packets to Load Balancer with <code>externalTrafficPolicy=local</code></td></tr><tr><td>KUBE-LOAD-BALANCER-FW</td><td>Load Balancer ingress IP + port with <code>loadBalancerSourceRanges</code></td><td>Drop packets for Load Balancer type Service with <code>loadBalancerSourceRanges</code> specified</td></tr><tr><td>KUBE-LOAD-BALANCER-SOURCE-CIDR</td><td>Load Balancer ingress IP + port + source CIDR</td><td>accept packets for Load Balancer type Service with <code>loadBalancerSourceRanges</code> specified</td></tr><tr><td>KUBE-NODE-PORT-TCP</td><td>NodePort type Service TCP port</td><td>masquerade for packets to NodePort(TCP)</td></tr><tr><td>KUBE-NODE-PORT-LOCAL-TCP</td><td>NodePort type Service TCP port with <code>externalTrafficPolicy=local</code></td><td>accept packets to NodePort Service with <code>externalTrafficPolicy=local</code></td></tr><tr><td>KUBE-NODE-PORT-UDP</td><td>NodePort type Service UDP port</td><td>masquerade for packets to NodePort(UDP)</td></tr><tr><td>KUBE-NODE-PORT-LOCAL-UDP</td><td>NodePort type service UDP port with <code>externalTrafficPolicy=local</code></td><td>accept packets to NodePort Service with <code>externalTrafficPolicy=local</code></td></tr></tbody></table><p>In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.</p><h3 id=run-kube-proxy-in-ipvs-mode>Run kube-proxy in IPVS Mode</h3><p>Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (<code>KUBE_PROXY_MODE=ipvs</code>) or specifying flag (<code>--proxy-mode=ipvs</code>). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.</p><pre><code>ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
</code></pre><p>Finally, for Kubernetes v1.10, feature gate <code>SupportIPVSProxyMode</code> is set to <code>true</code> by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable <code>--feature-gates=SupportIPVSProxyMode=true</code> explicitly for Kubernetes before v1.10.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.
Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a>
Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a>
Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates
Chat with the community on <a href=http://slack.k8s.io/>Slack</a>
Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-56b1754609f34aa3b199a3e9af607d92>Airflow on Kubernetes (Part 1): A Different Kind of Operator</h1><div class="td-byline mb-4"><time datetime=2018-06-28 class=text-muted>Thursday, June 28, 2018</time></div><p><strong>Author</strong>: Daniel Imberman (Bloomberg LP)</p><h2 id=introduction>Introduction</h2><p>As part of Bloomberg's <a href=https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/>continued commitment to developing the Kubernetes ecosystem</a>, we are excited to announce the Kubernetes Airflow Operator; a mechanism for <a href=https://airflow.apache.org/>Apache Airflow</a>, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.</p><h2 id=what-is-airflow>What Is Airflow?</h2><p>Apache Airflow is one realization of the DevOps philosophy of "Configuration As Code." Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.</p><p><img src=/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow_dags.png width=85% alt="Airflow DAGs">
<img src=/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow.png width=85% alt="Airflow UI"></p><h2 id=why-airflow-on-kubernetes>Why Airflow on Kubernetes?</h2><p>Since its inception, Airflow's greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.</p><p>To address this issue, we've utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an "any job you want" workflow orchestrator.</p><h2 id=the-kubernetes-operator>The Kubernetes Operator</h2><p>Before we move any further, we should clarify that an <a href=https://airflow.apache.org/concepts.html#operators>Operator</a> in Airflow is a task definition. When a user creates a DAG, they would use an operator like the "SparkSubmitOperator" or the "PythonOperator" to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.</p><p>Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:</p><ul><li><p><strong>Increased flexibility for deployments:</strong><br>Airflow's plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.</p></li><li><p><strong>Flexibility of configurations and dependencies:</strong>
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires <a href=https://www.scipy.org>SciPy</a> and another that requires <a href=http://www.numpy.org>NumPy</a>, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.</p></li><li><p><strong>Usage of kubernetes secrets for added security:</strong>
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.</p></li></ul><h1 id=architecture>Architecture</h1><p><img src=/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow-architecture.png width=85% alt="Airflow Architecture"></p><p>The Kubernetes Operator uses the <a href=https://github.com/kubernetes-client/Python>Kubernetes Python Client</a> to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you've defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.</p><h1 id=using-the-kubernetes-operator>Using the Kubernetes Operator</h1><h2 id=a-basic-example>A Basic Example</h2><p>The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the <code>passing-task</code> pod should complete, while the <code>failing-task</code> pod returns a failure to the Airflow webserver.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>airflow</span> <span style=color:#a2f;font-weight:700>import</span> DAG
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>datetime</span> <span style=color:#a2f;font-weight:700>import</span> datetime, timedelta
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>airflow.contrib.operators.kubernetes_pod_operator</span> <span style=color:#a2f;font-weight:700>import</span> KubernetesPodOperator
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>airflow.operators.dummy_operator</span> <span style=color:#a2f;font-weight:700>import</span> DummyOperator


default_args <span style=color:#666>=</span> {
    <span style=color:#b44>&#39;owner&#39;</span>: <span style=color:#b44>&#39;airflow&#39;</span>,
    <span style=color:#b44>&#39;depends_on_past&#39;</span>: False,
    <span style=color:#b44>&#39;start_date&#39;</span>: datetime<span style=color:#666>.</span>utcnow(),
    <span style=color:#b44>&#39;email&#39;</span>: [<span style=color:#b44>&#39;airflow@example.com&#39;</span>],
    <span style=color:#b44>&#39;email_on_failure&#39;</span>: False,
    <span style=color:#b44>&#39;email_on_retry&#39;</span>: False,
    <span style=color:#b44>&#39;retries&#39;</span>: <span style=color:#666>1</span>,
    <span style=color:#b44>&#39;retry_delay&#39;</span>: timedelta(minutes<span style=color:#666>=</span><span style=color:#666>5</span>)
}

dag <span style=color:#666>=</span> DAG(
    <span style=color:#b44>&#39;kubernetes_sample&#39;</span>, default_args<span style=color:#666>=</span>default_args, schedule_interval<span style=color:#666>=</span>timedelta(minutes<span style=color:#666>=</span><span style=color:#666>10</span>))


start <span style=color:#666>=</span> DummyOperator(task_id<span style=color:#666>=</span><span style=color:#b44>&#39;run_this_first&#39;</span>, dag<span style=color:#666>=</span>dag)

passing <span style=color:#666>=</span> KubernetesPodOperator(namespace<span style=color:#666>=</span><span style=color:#b44>&#39;default&#39;</span>,
                          image<span style=color:#666>=</span><span style=color:#b44>&#34;Python:3.6&#34;</span>,
                          cmds<span style=color:#666>=</span>[<span style=color:#b44>&#34;Python&#34;</span>,<span style=color:#b44>&#34;-c&#34;</span>],
                          arguments<span style=color:#666>=</span>[<span style=color:#b44>&#34;print(&#39;hello world&#39;)&#34;</span>],
                          labels<span style=color:#666>=</span>{<span style=color:#b44>&#34;foo&#34;</span>: <span style=color:#b44>&#34;bar&#34;</span>},
                          name<span style=color:#666>=</span><span style=color:#b44>&#34;passing-test&#34;</span>,
                          task_id<span style=color:#666>=</span><span style=color:#b44>&#34;passing-task&#34;</span>,
                          get_logs<span style=color:#666>=</span>True,
                          dag<span style=color:#666>=</span>dag
                          )

failing <span style=color:#666>=</span> KubernetesPodOperator(namespace<span style=color:#666>=</span><span style=color:#b44>&#39;default&#39;</span>,
                          image<span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu:1604&#34;</span>,
                          cmds<span style=color:#666>=</span>[<span style=color:#b44>&#34;Python&#34;</span>,<span style=color:#b44>&#34;-c&#34;</span>],
                          arguments<span style=color:#666>=</span>[<span style=color:#b44>&#34;print(&#39;hello world&#39;)&#34;</span>],
                          labels<span style=color:#666>=</span>{<span style=color:#b44>&#34;foo&#34;</span>: <span style=color:#b44>&#34;bar&#34;</span>},
                          name<span style=color:#666>=</span><span style=color:#b44>&#34;fail&#34;</span>,
                          task_id<span style=color:#666>=</span><span style=color:#b44>&#34;failing-task&#34;</span>,
                          get_logs<span style=color:#666>=</span>True,
                          dag<span style=color:#666>=</span>dag
                          )

passing<span style=color:#666>.</span>set_upstream(start)
failing<span style=color:#666>.</span>set_upstream(start)
</code></pre></div><p><img src=/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-basic-dag-run.png width=85% alt="Basic DAG Run"></p><h2 id=but-how-does-this-relate-to-my-workflow>But how does this relate to my workflow?</h2><p>While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.</p><h3 id=1-pr-in-github>1: PR in github</h3><p>Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR'ing your code, and merge to the master branch to trigger an automated CI build.</p><h3 id=2-ci-cd-via-jenkins-docker-image>2: CI/CD via Jenkins -> Docker Image</h3><p><a href=https://getintodevops.com/blog/building-your-first-docker-image-with-jenkins-2-guide-for-developers>Generate your Docker images and bump release version within your Jenkins build</a>.</p><h3 id=3-airflow-launches-task>3: Airflow launches task</h3><p>Finally, update your DAGs to reflect the new release version and you should be ready to go!</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python>production_task <span style=color:#666>=</span> KubernetesPodOperator(namespace<span style=color:#666>=</span><span style=color:#b44>&#39;default&#39;</span>,
                          <span style=color:#080;font-style:italic># image=&#34;my-production-job:release-1.0.1&#34;, &lt;-- old release</span>
                          image<span style=color:#666>=</span><span style=color:#b44>&#34;my-production-job:release-1.0.2&#34;</span>,
                          cmds<span style=color:#666>=</span>[<span style=color:#b44>&#34;Python&#34;</span>,<span style=color:#b44>&#34;-c&#34;</span>],
                          arguments<span style=color:#666>=</span>[<span style=color:#b44>&#34;print(&#39;hello world&#39;)&#34;</span>],
                          name<span style=color:#666>=</span><span style=color:#b44>&#34;fail&#34;</span>,
                          task_id<span style=color:#666>=</span><span style=color:#b44>&#34;failing-task&#34;</span>,
                          get_logs<span style=color:#666>=</span>True,
                          dag<span style=color:#666>=</span>dag
                          )
</code></pre></div><h1 id=launching-a-test-deployment>Launching a test deployment</h1><p>Since the Kubernetes Operator is not yet released, we haven't released an official <a href=https://helm.sh/>helm</a> chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:</p><h2 id=step-1-set-your-kubeconfig-to-point-to-a-kubernetes-cluster>Step 1: Set your kubeconfig to point to a kubernetes cluster</h2><h2 id=step-2-clone-the-airflow-repo>Step 2: Clone the Airflow Repo:</h2><p>Run <code>git clone https://github.com/apache/incubator-airflow.git</code> to clone the official Airflow repo.</p><h2 id=step-3-run>Step 3: Run</h2><p>To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:</p><pre><code>sed -ie &quot;s/KubernetesExecutor/LocalExecutor/g&quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
</code></pre><p>Before we move on, let's discuss what these commands are doing:</p><h3 id=sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml>sed -ie "s/KubernetesExecutor/LocalExecutor/g" scripts/ci/kubernetes/kube/configmaps.yaml</h3><p>The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.</p><h3 id=scripts-ci-kubernetes-docker-build-sh>./scripts/ci/kubernetes/Docker/build.sh</h3><p>This script will tar the Airflow master source code build a Docker container based on the Airflow distribution</p><h3 id=scripts-ci-kubernetes-kube-deploy-sh>./scripts/ci/kubernetes/kube/deploy.sh</h3><p>Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml</p><h2 id=step-4-log-into-your-webserver>Step 4: Log into your webserver</h2><p>Now that your Airflow instance is running let's take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run</p><pre><code>WEB=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}' | grep &quot;airflow&quot; | head -1)
kubectl port-forward $WEB 8080:8080
</code></pre><p>Now the Airflow UI will exist on http://localhost:8080. To log in simply enter <code>airflow</code>/<code>airflow</code> and you should have full access to the Airflow web UI.</p><h2 id=step-5-upload-a-test-document>Step 5: Upload a test document</h2><p>To modify/add your own DAGs, you can use <code>kubectl cp</code> to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:</p><p><code>kubectl cp &lt;local file> &lt;namespace>/&lt;pod>:/root/airflow/dags -c scheduler</code></p><h2 id=step-6-enjoy>Step 6: Enjoy!</h2><h1 id=so-when-will-i-be-able-to-use-this>So when will I be able to use this?</h1><p>While this feature is still in the early stages, we hope to see it released for wide release in the next few months.</p><h1 id=get-involved>Get Involved</h1><p>This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the <a href=https://github.com/apache/incubator-airflow/tree/v1-10-test>1.10 release branch of Airflow</a> (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributers can have a huge influence on the future of these features.</p><p>For those interested in joining these efforts, I'd recommend checkint out these steps:</p><ul><li>Join the airflow-dev mailing list at <a href=mailto:dev@airflow.apache.org>dev@airflow.apache.org</a>.</li><li>File an issue in <a href=https://issues.apache.org/jira/projects/AIRFLOW/issues/>Apache Airflow JIRA</a></li><li>Join our SIG-BigData meetings on Wednesdays at 10am PST.</li><li>Reach us on slack at #sig-big-data on kubernetes.slack.com</li></ul><p>Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9398fd1cfd158f73710f9e089de0fbac>Kubernetes 1.11: In-Cluster Load Balancing and CoreDNS Plugin Graduate to General Availability</h1><div class="td-byline mb-4"><time datetime=2018-06-27 class=text-muted>Wednesday, June 27, 2018</time></div><p><strong>Author</strong>: Kubernetes 1.11 <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md>Release Team</a></p><p>We’re pleased to announce the delivery of Kubernetes 1.11, our second release of 2018!</p><p>Today’s release continues to advance maturity, scalability, and flexibility of Kubernetes, marking significant progress on features that the team has been hard at work on over the last year. This newest version graduates key features in networking, opens up two major features from SIG-API Machinery and SIG-Node for beta testing, and continues to enhance storage features that have been a focal point of the past two releases. The features in this release make it increasingly possible to plug any infrastructure, cloud or on-premise, into the Kubernetes system.</p><p>Notable additions in this release include two highly-anticipated features graduating to general availability: IPVS-based In-Cluster Load Balancing and CoreDNS as a cluster DNS add-on option, which means increased scalability and flexibility for production applications.</p><p>Let’s dive into the key features of this release:</p><h2 id=ipvs-based-in-cluster-service-load-balancing-graduates-to-general-availability>IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability</h2><p>In this release, <a href=https://github.com/kubernetes/features/issues/265>IPVS-based in-cluster service load balancing</a> has moved to stable. IPVS (IP Virtual Server) provides high-performance in-kernel load balancing, with a simpler programming interface than iptables. This change delivers better network throughput, better programming latency, and higher scalability limits for the cluster-wide distributed load-balancer that comprises the Kubernetes Service model. IPVS is not yet the default but clusters can begin to use it for production traffic.</p><h2 id=coredns-promoted-to-general-availability>CoreDNS Promoted to General Availability</h2><p><a href=https://coredns.io>CoreDNS</a> is now available as a <a href=https://github.com/kubernetes/features/issues/427>cluster DNS add-on option</a>, and is the default when using kubeadm. CoreDNS is a flexible, extensible authoritative DNS server and directly integrates with the Kubernetes API. CoreDNS has fewer moving parts than the previous DNS server, since it’s a single executable and a single process, and supports flexible use cases by creating custom DNS entries. It’s also written in Go making it memory-safe. You can learn more about CoreDNS <a href=https://youtu.be/dz9S7R8r5gw>here</a>.</p><h2 id=dynamic-kubelet-configuration-moves-to-beta>Dynamic Kubelet Configuration Moves to Beta</h2><p>This feature makes it possible for new Kubelet configurations to be rolled out in a live cluster. Currently, Kubelets are configured via command-line flags, which makes it difficult to update Kubelet configurations in a running cluster. With this beta feature, <a href=/docs/tasks/administer-cluster/reconfigure-kubelet/>users can configure Kubelets in a live cluster</a> via the API server.</p><h2 id=custom-resource-definitions-can-now-define-multiple-versions>Custom Resource Definitions Can Now Define Multiple Versions</h2><p>Custom Resource Definitions are no longer restricted to defining a single version of the custom resource, a restriction that was difficult to work around. Now, with this beta <a href=https://github.com/kubernetes/features/issues/544>feature</a>, multiple versions of the resource can be defined. In the future, this will be expanded to support some automatic conversions; for now, this feature allows custom resource authors to “promote with safe changes, e.g. v1beta1 to v1,” and to create a migration path for resources which do have changes.</p><p>Custom Resource Definitions now also support <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/customresources-subresources.md>"status" and "scale" subresources</a>, which integrate with monitoring and high-availability frameworks. These two changes advance the ability to run cloud-native applications in production using Custom Resource Definitions.</p><h2 id=enhancements-to-csi>Enhancements to CSI</h2><p>Container Storage Interface (CSI) has been a major topic over the last few releases. After moving to <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>beta in 1.10</a>, the 1.11 release continues enhancing CSI with a number of features. The 1.11 release adds alpha support for raw block volumes to CSI, integrates CSI with the new kubelet plugin registration mechanism, and makes it easier to pass secrets to CSI plugins.</p><h2 id=new-storage-features>New Storage Features</h2><p>Support for <a href=https://github.com/kubernetes/features/issues/284>online resizing of Persistent Volumes</a> has been introduced as an alpha feature. This enables users to increase the size of PVs without having to terminate pods and unmount volume first. The user will update the PVC to request a new size and kubelet will resize the file system for the PVC.</p><p>Support for <a href=https://github.com/kubernetes/features/issues/554>dynamic maximum volume count</a> has been introduced as an alpha feature. This new feature enables in-tree volume plugins to specify the maximum number of volumes that can be attached to a node and allows the limit to vary depending on the type of node. Previously, these limits were hard coded or configured via an environment variable.</p><p>The StorageObjectInUseProtection feature is now stable and prevents the removal of both <a href=https://github.com/kubernetes/features/issues/499>Persistent Volumes</a> that are bound to a Persistent Volume Claim, and <a href=https://github.com/kubernetes/features/issues/498>Persistent Volume Claims</a> that are being used by a pod. This safeguard will help prevent issues from deleting a PV or a PVC that is currently tied to an active pod.</p><p>Each Special Interest Group (SIG) within the community continues to deliver the most-requested enhancements, fixes, and functionality for their respective specialty areas. For a complete list of inclusions by SIG, please visit the <a href=https://github.com/kubernetes/kubernetes/blob/release-1.11/CHANGELOG-1.11.md#111-release-notes>release notes</a>.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.11 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.11.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=/docs/tutorials/>interactive tutorials</a>.</p><p>You can also install 1.11 using Kubeadm. Version 1.11.0 will be available as Deb and RPM packages, installable using the <a href=/docs/setup/independent/create-cluster-kubeadm/>Kubeadm cluster installer</a> sometime on June 28th.</p><h2 id=4-day-features-blog-series>4 Day Features Blog Series</h2><p>If you’re interested in exploring these features more in depth, check back in two weeks for our 4 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:</p><ul><li>Day 1: <a href=/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/>IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability</a></li><li>Day 2: <a href=/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/>CoreDNS Promoted to General Availability</a></li><li>Day 3: <a href=/blog/2018/07/11/dynamic-kubelet-configuration/>Dynamic Kubelet Configuration Moves to Beta</a></li><li>Day 4: <a href=/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/>Resizing Persistent Volumes using Kubernetes</a></li></ul><h2 id=release-team>Release team</h2><p>This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md>release team</a> led by Josh Berkus, Kubernetes Community Manager at Red Hat. The 20 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 20,000 individual contributors to date and an active community of more than 40,000 people.</p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average, 250 different companies and over 1,300 individuals contribute to Kubernetes each month. <a href=https://devstats.k8s.io>Check out DevStats</a> to learn more about the overall velocity of the Kubernetes project and community.</p><h2 id=user-highlights>User Highlights</h2><p>Established, global organizations are using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at massive scale. Recently published user stories from the community include:</p><ul><li><strong>The New York Times</strong>, known as the newspaper of record, <a href=https://kubernetes.io/case-studies/newyorktimes/>moved out of its data centers and into the public cloud</a> with the help of Google Cloud Platform and Kubernetes. This move meant a significant increase in speed of delivery, from 45 minutes to just a few seconds with Kubernetes.</li><li><strong>Nordstrom</strong>, a leading fashion retailer based in the U.S., began their cloud native journey by <a href=https://kubernetes.io/case-studies/nordstrom/>adopting Docker containers orchestrated with Kubernetes</a>. The results included a major increase in Ops efficiency, improving CPU utilization from 5x to 12x depending on the workload.</li><li><strong>Squarespace</strong>, a SaaS solution for easily building and hosting websites, <a href=https://kubernetes.io/case-studies/squarespace/>moved their monolithic application to microservices with the help of Kubernetes</a>. This resulted in a deployment time reduction of almost 85%.</li><li><strong>Crowdfire</strong>, a leading social media management platform, moved from a monolithic application to a <a href=https://kubernetes.io/case-studies/crowdfire/>custom Kubernetes-based setup</a>. This move reduced deployment time from 15 minutes to less than a minute.</li></ul><p>Is Kubernetes helping your team? Share your story with the community.</p><h2 id=ecosystem-updates>Ecosystem Updates</h2><ul><li>The CNCF recently expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual's ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found <a href=https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/>here</a>.</li><li>The CNCF recently added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more <a href=https://www.cncf.io/certification/training/>here</a>.</li><li>CNCF also offers <a href=https://www.cncf.io/certification/training/>online training</a> that teaches the skills needed to create and configure a real-world Kubernetes cluster.</li><li>Kubernetes documentation now features <a href=https://k8s.io/docs/home/>user journeys</a>: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.</li></ul><h2 id=kubecon>KubeCon</h2><p>The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to [Shanghai](<a href=https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/>https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/</a> from November 14-15, 2018 and <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/>Seattle</a> from December 11-13, 2018. This conference will feature technical sessions, case studies, developer deep dives, salons and more! The CFP for both event is currently open. <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/program/call-for-proposals-cfp/>Submit your talk</a> and <a href="https://www.regonline.com/registration/Checkin.aspx?EventID=2246960">register</a> today!</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.11 release team on July 31st at 10am PDT to learn about the major features in this release including In-Cluster Load Balancing and the CoreDNS Plugin. Register <a href=https://www.cncf.io/event/webinar-kubernetes-1-11/>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-416d1fed0ff270b6e7f8838c494f3117>Dynamic Ingress in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-06-07 class=text-muted>Thursday, June 07, 2018</time></div><p><strong>Author</strong>: Richard Li (Datawire)</p><p>Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services. One approach is <a href=https://www.getambassador.io>Ambassador</a>, a Kubernetes-native open source API Gateway built on the <a href=https://www.envoyproxy.io>Envoy Proxy</a>. Ambassador is designed for dynamic environment where services may come and go frequently.</p><p>Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.</p><h2 id=a-basic-ambassador-example>A Basic Ambassador Example</h2><p>Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: httpbin
  annotations:
    getambassador.io/config: |
      ---
      apiVersion: ambassador/v0
      kind:  Mapping
      name:  httpbin_mapping
      prefix: /httpbin/
      service: httpbin.org:80
      host_rewrite: httpbin.org
spec:
  type: ClusterIP
  ports:
    - port: 80
</code></pre><p>A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP <code>host</code> header should be set to httpbin.org.</p><h2 id=kubeflow>Kubeflow</h2><p><a href=https://github.com/kubeflow/kubeflow>Kubeflow</a> provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.</p><p><img src=/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow.png alt=kubeflow></p><center><i>Kubeflow architecture, pre-Ambassador</center></i><h2 id=service-configuration>Service configuration</h2><p>With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:</p><pre><code>---
apiVersion: ambassador/v0
kind:  Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
</code></pre><p>In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.</p><h2 id=kubeflow-and-ambassador>Kubeflow and Ambassador</h2><p><img src=/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow-ambassador.png alt=kubeflow-ambassador></p><p>With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services, Kubeflow creates and annotates a K8s service so that the model will be served at https://<ingress host>/models/<model name>/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.</p><p>If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.</p><p>If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the <a href=https://www.getambassador.io/user-guide/getting-started>Getting Started with Ambassador guide</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bfad75a93816a9020a268af7543370fe>4 Years of K8s</h1><div class="td-byline mb-4"><time datetime=2018-06-06 class=text-muted>Wednesday, June 06, 2018</time></div><p><strong>Author</strong>: Joe Beda (CTO and Founder, Heptio)</p><p>On June 6, 2014 I checked in the <a href=https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56>first commit</a> of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.</p><p><img src=/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png alt=k8s_first_commit></p><p>The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.</p><p>Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.</p><p>Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.</p><p>After we got the nod, it was time to actually build the system. We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across. By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith. Once we had something working, someone had to sign up to clean things up to get it ready for public launch. That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in. So while I have the first public commit to the repo, there was work underway well before that.</p><p>The version of Kubernetes at that point was really just a shadow of what it was to become. The core concepts were there but it was very raw. For example, Pods were called Tasks. That was changed a day before we went public. All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon. You can watch that video here:</p><center><iframe width=560 height=315 src=https://www.youtube.com/embed/YrxnVKZeqK8 frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe></center><p>But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger. Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt. The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special. The best expression of this is the <a href=https://git.k8s.io/community/values.md>set of Kubernetes values</a> that Sarah Novotny helped curate.</p><p>Here is to another 4 years and beyond! 🎉🎉🎉</p></div><div class=td-content style=page-break-before:always><h1 id=pg-819e0ad707ce8b6e6e6d031bb89f354d>Say Hello to Discuss Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-05-30 class=text-muted>Wednesday, May 30, 2018</time></div><p><strong>Author</strong>: Jorge Castro (Heptio)</p><p>Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way.</p><p>Slack is great for casual and timely conversations and keeping up with other community members, but communication can't be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like "What's your favorite CI/CD tool" or "<a href=https://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192>Kubectl tips and tricks</a>" are offtopic there.</p><p>While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of <a href=https://www.discourse.org/features>Discourse</a>, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There's only one way to find out: Welcome to <a href=https://discuss.kubernetes.io>discuss.kubernetes.io</a></p><p><img src=/images/blog/2018-05-30-say-hello-to-discuss-kubernetes.png alt=discuss_screenshot></p><p>Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email.</p><p>Ecosystem partners and developers now have a place where they can <a href=https://discuss.kubernetes.io/c/announcements>announce projects</a> that they're working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building.</p><p>This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience.</p><p>Hop in and take a look. We're just getting started, so you might want to begin by <a href=https://discuss.kubernetes.io/t/introduce-yourself-here/56>introducing yourself</a> and then browsing around. Apps are also available for <a href="https://play.google.com/store/apps/details?id=com.discourse&hl=en_US&rdid=com.discourse&pli=1">Android </a>and <a href="https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8">iOS</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb3baac87f981215a7ff26debd0db5d3>Introducing kustomize; Template-free Configuration Customization for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-05-29 class=text-muted>Tuesday, May 29, 2018</time></div><p><strong>Authors:</strong> Jeff Regan (Google), Phil Wittrock (Google)</p><p>If you run a Kubernetes environment, chances are you’ve
customized a Kubernetes configuration — you've copied
some API object YAML files and edited them to suit
your needs.</p><p>But there are drawbacks to this approach — it can be
hard to go back to the source material and incorporate
any improvements that were made to it. Today Google is
announcing <a href=https://github.com/kubernetes-sigs/kustomize><strong>kustomize</strong></a>, a command-line tool
contributed as a <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/0008-kustomize.md>subproject</a> of <a href=https://github.com/kubernetes/community/tree/master/sig-cli>SIG-CLI</a>. The tool
provides a new, purely <em>declarative</em> approach to
configuration customization that adheres to and
leverages the familiar and carefully designed
Kubernetes API.</p><p>Here’s a common scenario. Somewhere on the internet you
find someone’s Kubernetes configuration for a content
management system. It's a set of files containing YAML
specifications of Kubernetes API objects. Then, in some
corner of your own company you find a configuration for
a database to back that CMS — a database you prefer
because you know it well.</p><p>You want to use these together, somehow. Further, you
want to customize the files so that your resource
instances appear in the cluster with a label that
distinguishes them from a colleague’s resources who’s
doing the same thing in the same cluster.
You also want to set appropriate values for CPU, memory
and replica count.</p><p>Additionally, you’ll want <em>multiple variants</em> of the
entire configuration: a small variant (in terms of
computing resources used) devoted to testing and
experimentation, and a much larger variant devoted to
serving outside users in production. Likewise, other
teams will want their own variants.</p><p>This raises all sorts of questions. Do you copy your
configuration to multiple locations and edit them
independently? What if you have dozens of development
teams who need slightly different variations of the
stack? How do you maintain and upgrade the aspects of
configuration that they share in common? Workflows
using <strong>kustomize</strong> provide answers to these questions.</p><h2 id=customization-is-reuse>Customization is reuse</h2><p>Kubernetes configurations aren't code (being YAML
specifications of API objects, they are more strictly
viewed as data), but configuration lifecycle has many
similarities to code lifecycle.</p><p>You should keep configurations in version
control. Configuration owners aren’t necessarily the
same set of people as configuration
users. Configurations may be used as parts of a larger
whole. Users will want to <em>reuse</em> configurations for
different purposes.</p><p>One approach to configuration reuse, as with code
reuse, is to simply copy it all and customize the
copy. As with code, severing the connection to the
source material makes it difficult to benefit from
ongoing improvements to the source material. Taking
this approach with many teams or environments, each
with their own variants of a configuration, makes a
simple upgrade intractable.</p><p>Another approach to reuse is to express the source
material as a parameterized template. A tool processes
the template—executing any embedded scripting and
replacing parameters with desired values—to generate
the configuration. Reuse comes from using different
sets of values with the same template. The challenge
here is that the templates and value files are not
specifications of Kubernetes API resources. They are,
necessarily, a new thing, a new language, that wraps
the Kubernetes API. And yes, they can be powerful, but
bring with them learning and tooling costs. Different
teams want different changes—so almost every
specification that you can include in a YAML file
becomes a parameter that needs a value. As a result,
the value sets get large, since all parameters (that
don't have trusted defaults) must be specified for
replacement. This defeats one of the goals of
reuse—keeping the differences between the variants
small in size and easy to understand in the absence of
a full resource declaration.</p><h2 id=a-new-option-for-configuration-customization>A new option for configuration customization</h2><p>Compare that to <strong>kustomize</strong>, where the tool’s
behavior is determined by declarative specifications
expressed in a file called <code>kustomization.yaml</code>.</p><p>The <strong>kustomize</strong> program reads the file and the
Kubernetes API resource files it references, then emits
complete resources to standard output. This text output
can be further processed by other tools, or streamed
directly to <strong>kubectl</strong> for application to a cluster.</p><p>For example, if a file called <code>kustomization.yaml</code>
containing</p><pre><code>   commonLabels:
     app: hello
   resources:
   - deployment.yaml
   - configMap.yaml
   - service.yaml
</code></pre><p>is in the current working directory, along with
the three resource files it mentions, then running</p><pre><code>kustomize build
</code></pre><p>emits a YAML stream that includes the three given
resources, and adds a common label <code>app: hello</code> to
each resource.</p><p>Similarly, you can use a <em>commonAnnotations</em> field to
add an annotation to all resources, and a <em>namePrefix</em>
field to add a common prefix to all resource
names. This trivial yet common customization is just
the beginning.</p><p>A more common use case is that you’ll need multiple
variants of a common set of resources, e.g., a
<em>development</em>, <em>staging</em> and <em>production</em> variant.</p><p>For this purpose, <strong>kustomize</strong> supports the idea of an
<em>overlay</em> and a <em>base</em>. Both are represented by a
kustomization file. The base declares things that the
variants share in common (both resources and a common
customization of those resources), and the overlays
declare the differences.</p><p>Here’s a file system layout to manage a <em>staging</em> and
<em>production</em> variant of a given cluster app:</p><pre><code>   someapp/
   ├── base/
   │   ├── kustomization.yaml
   │   ├── deployment.yaml
   │   ├── configMap.yaml
   │   └── service.yaml
   └── overlays/
      ├── production/
      │   └── kustomization.yaml
      │   ├── replica_count.yaml
      └── staging/
          ├── kustomization.yaml
          └── cpu_count.yaml
</code></pre><p>The file <code>someapp/base/kustomization.yaml</code> specifies the
common resources and common customizations to those
resources (e.g., they all get some label, name prefix
and annotation).</p><p>The contents of
<code>someapp/overlays/production/kustomization.yaml</code> could
be</p><pre><code>   commonLabels:
    env: production
   bases:
   - ../../base
   patches:
   - replica_count.yaml
</code></pre><p>This kustomization specifies a <em>patch</em> file
<code>replica_count.yaml</code>, which could be:</p><pre><code>   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: the-deployment
   spec:
     replicas: 100
</code></pre><p>A patch is a partial resource declaration, in this case
a patch of the deployment in
<code>someapp/base/deployment.yaml</code>, modifying only the
<em>replicas</em> count to handle production traffic.</p><p>The patch, being a partial deployment spec, has a clear
context and purpose and can be validated even if it’s
read in isolation from the remaining
configuration. It’s not just a context free <em>{parameter
name, value}</em> tuple.</p><p>To create the resources for the production variant, run</p><pre><code>kustomize build someapp/overlays/production
</code></pre><p>The result is printed to stdout as a set of complete
resources, ready to be applied to a cluster. A
similar command defines the staging environment.</p><h2 id=in-summary>In summary</h2><p>With <strong>kustomize</strong>, you can manage an arbitrary number
of distinctly customized Kubernetes configurations
using only Kubernetes API resource files. Every
artifact that <strong>kustomize</strong> uses is plain YAML and can
be validated and processed as such. kustomize encourages
a fork/modify/rebase <a href=https://github.com/kubernetes-sigs/kustomize/blob/master/docs/workflows.md>workflow</a>.</p><p>To get started, try the <a href=https://github.com/kubernetes-sigs/kustomize/blob/master/examples/helloWorld>hello world</a> example.
For discussion and feedback, join the <a href=https://groups.google.com/forum/#!forum/kustomize>mailing list</a> or
<a href=https://github.com/kubernetes-sigs/kustomize/issues/new>open an issue</a>. Pull requests are welcome.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-567d5b08adf4dd0e7e63748193b37c61>Kubernetes Containerd Integration Goes GA</h1><div class="td-byline mb-4"><time datetime=2018-05-24 class=text-muted>Thursday, May 24, 2018</time></div><h1 id=kubernetes-containerd-integration-goes-ga>Kubernetes Containerd Integration Goes GA</h1><p><strong>Authors</strong>: Lantao Liu, Software Engineer, Google and Mike Brown, Open Source Developer Advocate, IBM</p><p>In a previous blog - <a href=https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes>Containerd Brings More Container Runtime Options for Kubernetes</a>, we introduced the alpha version of the Kubernetes containerd integration. With another 6 months of development, the integration with containerd is now generally available! You can now use <a href=https://github.com/containerd/containerd/releases/tag/v1.1.0>containerd 1.1</a> as the container runtime for production Kubernetes clusters!</p><p>Containerd 1.1 works with Kubernetes 1.10 and above, and supports all Kubernetes features. The test coverage of containerd integration on <a href=https://cloud.google.com/>Google Cloud Platform</a> in Kubernetes test infrastructure is now equivalent to the Docker integration (See: <a href=https://k8s-testgrid.appspot.com/sig-node-containerd>test dashboard)</a>.</p><p><em>We're very glad to see containerd rapidly grow to this big milestone. Alibaba Cloud started to use containerd actively since its first day, and thanks to the simplicity and robustness emphasise, make it a perfect container engine running in our Serverless Kubernetes product, which has high qualification on performance and stability. No doubt, containerd will be a core engine of container era, and continue to driving innovation forward.</em></p><p style=text-align:right><em>— Xinwei, Staff Engineer in Alibaba Cloud</em></p><h1 id=architecture-improvements>Architecture Improvements</h1><p>The Kubernetes containerd integration architecture has evolved twice. Each evolution has made the stack more stable and efficient.</p><h2 id=containerd-1-0-cri-containerd-end-of-life>Containerd 1.0 - CRI-Containerd (end of life)</h2><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png width=100% alt="cri-containerd architecture"></p><p>For containerd 1.0, a daemon called cri-containerd was required to operate between Kubelet and containerd. Cri-containerd handled the <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>Container Runtime Interface (CRI)</a> service requests from Kubelet and used containerd to manage containers and container images correspondingly. Compared to the Docker CRI implementation (<a href=https://github.com/kubernetes/kubernetes/tree/v1.10.2/pkg/kubelet/dockershim>dockershim</a>), this eliminated one extra hop in the stack.</p><p>However, cri-containerd and containerd 1.0 were still 2 different daemons which interacted via grpc. The extra daemon in the loop made it more complex for users to understand and deploy, and introduced unnecessary communication overhead.</p><h2 id=containerd-1-1-cri-plugin-current>Containerd 1.1 - CRI Plugin (current)</h2><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/containerd.png width=100% alt="containerd architecture"></p><p>In containerd 1.1, the cri-containerd daemon is now refactored to be a containerd CRI plugin. The CRI plugin is built into containerd 1.1, and enabled by default. Unlike cri-containerd, the CRI plugin interacts with containerd through direct function calls. This new architecture makes the integration more stable and efficient, and eliminates another grpc hop in the stack. Users can now use Kubernetes with containerd 1.1 directly. The cri-containerd daemon is no longer needed.</p><h1 id=performance>Performance</h1><p>Improving performance was one of the major focus items for the containerd 1.1 release. Performance was optimized in terms of pod startup latency and daemon resource usage.</p><p>The following results are a comparison between containerd 1.1 and Docker 18.03 CE. The containerd 1.1 integration uses the CRI plugin built into containerd; and the Docker 18.03 CE integration uses the dockershim.</p><p>The results were generated using the Kubernetes node performance benchmark, which is part of <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md>Kubernetes node e2e test</a>. Most of the containerd benchmark data is publicly accessible on the <a href=http://node-perf-dash.k8s.io/>node performance dashboard</a>.</p><h3 id=pod-startup-latency>Pod Startup Latency</h3><p>The "105 pod batch startup benchmark" results show that the containerd 1.1 integration has lower pod startup latency than Docker 18.03 CE integration with dockershim (lower is better).</p><p style=text-align:center><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/latency.png alt=latency></p><h3 id=cpu-and-memory>CPU and Memory</h3><p>At the steady state, with 105 pods, the containerd 1.1 integration consumes less CPU and memory overall compared to Docker 18.03 CE integration with dockershim. The results vary with the number of pods running on the node, 105 is chosen because it is the current default for the maximum number of user pods per node.</p><p>As shown in the figures below, compared to Docker 18.03 CE integration with dockershim, the containerd 1.1 integration has 30.89% lower kubelet cpu usage, 68.13% lower container runtime cpu usage, 11.30% lower kubelet resident set size (RSS) memory usage, 12.78% lower container runtime RSS memory usage.</p><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cpu.png alt=cpu width=50%><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/memory.png alt=memory width=50%></p><h1 id=crictl>crictl</h1><p>Container runtime command-line interface (CLI) is a useful tool for system and application troubleshooting. When using Docker as the container runtime for Kubernetes, system administrators sometimes login to the Kubernetes node to run Docker commands for collecting system and/or application information. For example, one may use <em>docker ps</em> and <em>docker inspect</em> to check application process status, <em>docker images</em> to list images on the node, and <em>docker info</em> to identify container runtime configuration, etc.</p><p>For containerd and all other CRI-compatible container runtimes, e.g. dockershim, we recommend using <em>crictl</em> as a replacement CLI over the Docker CLI for troubleshooting pods, containers, and container images on Kubernetes nodes.</p><p><em>crictl</em> is a tool providing a similar experience to the Docker CLI for Kubernetes node troubleshooting and <em>crictl</em> works consistently across all CRI-compatible container runtimes. It is hosted in the <a href=https://github.com/kubernetes-incubator/cri-tools>kubernetes-incubator/cri-tools</a> repository and the current version is <a href=https://github.com/kubernetes-incubator/cri-tools/releases/tag/v1.0.0-beta.1>v1.0.0-beta.1</a>. <em>crictl</em> is designed to resemble the Docker CLI to offer a better transition experience for users, but it is not exactly the same. There are a few important differences, explained below.</p><h2 id=limited-scope-crictl-is-a-troubleshooting-tool>Limited Scope - crictl is a Troubleshooting Tool</h2><p>The scope of <em>crictl</em> is limited to troubleshooting, it is not a replacement to docker or kubectl. Docker's CLI provides a rich set of commands, making it a very useful development tool. But it is not the best fit for troubleshooting on Kubernetes nodes. Some Docker commands are not useful to Kubernetes, such as <em>docker network</em> and <em>docker build</em>; and some may even break the system, such as <em>docker rename</em>. <em>crictl</em> provides just enough commands for node troubleshooting, which is arguably safer to use on production nodes.</p><h2 id=kubernetes-oriented>Kubernetes Oriented</h2><p><em>crictl</em> offers a more kubernetes-friendly view of containers. Docker CLI lacks core Kubernetes concepts, e.g. <em>pod</em> and <em><a href=/docs/concepts/overview/working-with-objects/namespaces/>namespace</a></em>, so it can't provide a clear view of containers and pods. One example is that <em>docker ps</em> shows somewhat obscure, long Docker container names, and shows pause containers and application containers together:</p><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/docker-ps.png width=100% alt="docker ps"></p><p>However, <a href=https://www.ianlewis.org/en/almighty-pause-container>pause containers</a> are a pod implementation detail, where one pause container is used for each pod, and thus should not be shown when listing containers that are members of pods.</p><p><em>crictl</em>, by contrast, is designed for Kubernetes. It has different sets of commands for pods and containers. For example, <em>crictl pods</em> lists pod information, and <em>crictl ps</em> only lists application container information. All information is well formatted into table columns.</p><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-pods.png width=100% alt="crictl pods">
<img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-ps.png width=100% alt="crictl ps"></p><p>As another example, <em>crictl pods</em> includes a <em>--namespace</em> option for filtering pods by the namespaces specified in Kubernetes.</p><p><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/crictl-pods-filter.png width=100% alt="crictl pods filter"></p><p>For more details about how to use <em>crictl</em> with containerd:</p><ul><li><a href=https://github.com/containerd/cri/blob/master/docs/crictl.md>Document</a></li><li><a href=https://asciinema.org/a/179047>Demo video</a></li></ul><h1 id=what-about-docker-engine>What about Docker Engine?</h1><p>"Does switching to containerd mean I can't use Docker Engine anymore?" We hear this question a lot, the short answer is NO.</p><p>Docker Engine is built on top of containerd. The next release of <a href=https://www.docker.com/community-edition>Docker Community Edition (Docker CE)</a> will use containerd version 1.1. Of course, it will have the CRI plugin built-in and enabled by default. This means users will have the option to continue using Docker Engine for other purposes typical for Docker users, while also being able to configure Kubernetes to use the underlying containerd that came with and is simultaneously being used by Docker Engine on the same node. See the architecture figure below showing the same containerd being used by Docker Engine and Kubelet:</p><p style=text-align:center><img src=/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/docker-ce.png width=60% alt=docker-ce></p><p>Since containerd is being used by both Kubelet and Docker Engine, this means users who choose the containerd integration will not just get new Kubernetes features, performance, and stability improvements, they will also have the option of keeping Docker Engine around for other use cases.</p><p>A containerd <a href=https://github.com/containerd/containerd/blob/master/docs/namespaces.md>namespace</a> mechanism is employed to guarantee that Kubelet and Docker Engine won't see or have access to containers and images created by each other. This makes sure they won't interfere with each other. This also means that:</p><ul><li>Users won't see Kubernetes created containers with the <em>docker ps</em> command. Please use <em>crictl ps</em> instead. And vice versa, users won't see Docker CLI created containers in Kubernetes or with <em>crictl ps</em> command. The <em>crictl create</em> and <em>crictl runp</em> commands are only for troubleshooting. Manually starting pod or container with <em>crictl</em> on production nodes is not recommended.</li><li>Users won't see Kubernetes pulled images with the <em>docker images</em> command. Please use the <em>crictl images</em> command instead. And vice versa, Kubernetes won't see images created by <em>docker pull</em>, <em>docker load</em> or <em>docker build</em> commands. Please use the <em>crictl pull</em> command instead, and <em><a href=https://github.com/containerd/containerd/blob/release/1.2/docs/man/ctr.1.md>ctr</a> cri load</em> if you have to load an image.</li></ul><h1 id=summary>Summary</h1><ul><li>Containerd 1.1 natively supports CRI. It can be used directly by Kubernetes.</li><li>Containerd 1.1 is production ready.</li><li>Containerd 1.1 has good performance in terms of pod startup latency and system resource utilization.</li><li><em>crictl</em> is the CLI tool to talk with containerd 1.1 and other CRI-conformant container runtimes for node troubleshooting.</li><li>The next stable release of Docker CE will include containerd 1.1. Users have the option to continue using Docker for use cases not specific to Kubernetes, and configure Kubernetes to use the same underlying containerd that comes with Docker.</li></ul><p>We'd like to thank all the contributors from Google, IBM, Docker, ZTE, ZJU and many other individuals who made this happen!</p><p>For a detailed list of changes in the containerd 1.1 release, please see the release notes here: <a href=https://github.com/containerd/containerd/releases/tag/v1.1.0>https://github.com/containerd/containerd/releases/tag/v1.1.0</a></p><h1 id=try-it-out>Try it out</h1><p>To setup a Kubernetes cluster using containerd as the container runtime:</p><ul><li>For a production quality cluster on GCE brought up with kube-up.sh, see <a href=https://github.com/containerd/cri/blob/v1.0.0/docs/kube-up.md>here</a>.</li><li>For a multi-node cluster installer and bring up steps using Ansible and kubeadm, see <a href=https://github.com/containerd/cri/blob/v1.0.0/contrib/ansible/README.md>here</a>.</li><li>For creating a cluster from scratch on Google Cloud, see <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kubernetes the Hard Way</a>.</li><li>For a custom installation from release tarball, see <a href=https://github.com/containerd/cri/blob/v1.0.0/docs/installation.md>here</a>.</li><li>To install using LinuxKit on a local VM, see <a href=https://github.com/linuxkit/linuxkit/tree/master/projects/kubernetes>here</a>.</li></ul><h1 id=contribute>Contribute</h1><p>The containerd CRI plugin is an open source github project within containerd <a href=https://github.com/containerd/cri>https://github.com/containerd/cri</a>. Any contributions in terms of ideas, issues, and/or fixes are welcome. The <a href=https://github.com/containerd/cri#getting-started-for-developers>getting started guide for developers</a> is a good place to start for contributors.</p><h1 id=community>Community</h1><p>The project is developed and maintained jointly by members of the Kubernetes SIG-Node community and the containerd community. We'd love to hear feedback from you. To join the communities:</p><ul><li><a href=https://github.com/kubernetes/community/tree/master/sig-node>sig-node community site</a></li><li>Slack:<ul><li>#sig-node channel in <a href=http://kubernetes.slack.com>kubernetes.slack.com</a></li><li>#containerd channel in <a href=https://dockr.ly/community>https://dockr.ly/community</a></li></ul></li><li>Mailing List: <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-node>https://groups.google.com/forum/#!forum/kubernetes-sig-node</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c66e92b0acdfea516c8983cc62d032fd>Getting to Know Kubevirt</h1><div class="td-byline mb-4">By <b>kbarnard</b> |
<time datetime=2018-05-22 class=text-muted>Tuesday, May 22, 2018</time></div><p><strong>Author</strong>: <a href=mailto:jbrooks@redhat.com>Jason Brooks</a> (Red Hat)</p><p>Once you've become accustomed to running Linux container workloads on Kubernetes, you may find yourself wishing that you could run other sorts of workloads on your Kubernetes cluster. Maybe you need to run an application that isn't architected for containers, or that requires a different version of the Linux kernel -- or an all together different operating system -- than what's available on your container host.</p><p>These sorts of workloads are often well-suited to running in virtual machines (VMs), and <a href=http://www.kubevirt.io/>KubeVirt</a>, a virtual machine management add-on for Kubernetes, is aimed at allowing users to run VMs right alongside containers in the their Kubernetes or OpenShift clusters.</p><p>KubeVirt extends Kubernetes by adding resource types for VMs and sets of VMs through Kubernetes' <a href=/docs/concepts/api-extension/custom-resources/#customresourcedefinitions>Custom Resource Definitions API</a> (CRD). KubeVirt VMs run within regular Kubernetes pods, where they have access to standard pod networking and storage, and can be managed using standard Kubernetes tools such as kubectl.</p><p>Running VMs with Kubernetes involves a bit of an adjustment compared to using something like oVirt or OpenStack, and understanding the basic architecture of KubeVirt is a good place to begin.</p><p>In this post, we’ll talk about some of the components that are involved in KubeVirt at a high level. The components we’ll check out are CRDs, the KubeVirt virt-controller, virt-handler and virt-launcher components, libvirt, storage, and networking.</p><h2 id=kubevirt-components>KubeVirt Components</h2><p><img src=/images/blog/2018-05-22-getting-to-know-kubevirt/kubevirt-components.png width=70% alt="Kubevirt Components"></p><h2 id=custom-resource-definitions>Custom Resource Definitions</h2><p>Kubernetes resources are endpoints in the Kubernetes API that store collections of related API objects. For instance, the built-in pods resource contains a collection of Pod objects. The Kubernetes <a href=/docs/concepts/api-extension/custom-resources/#customresourcedefinitions>Custom Resource Definition</a> API allows users to extend Kubernetes with additional resources by defining new objects with a given name and schema. Once you've applied a custom resource to your cluster, the Kubernetes API server serves and handles the storage of your custom resource.</p><p>KubeVirt's primary CRD is the VirtualMachine (VM) resource, which contains a collection of VM objects inside the Kubernetes API server. The VM resource defines all the properties of the Virtual machine itself, such as the machine and CPU type, the amount of RAM and vCPUs, and the number and type of NICs available in the VM.</p><h2 id=virt-controller>virt-controller</h2><p>The virt-controller is a Kubernetes <a href=https://coreos.com/operators/>Operator</a> that’s responsible for cluster-wide virtualization functionality. When new VM objects are posted to the Kubernetes API server, the virt-controller takes notice and creates the pod in which the VM will run. When the pod is scheduled on a particular node, the virt-controller updates the VM object with the node name, and hands off further responsibilities to a node-specific KubeVirt component, the virt-handler, an instance of which runs on every node in the cluster.</p><h2 id=virt-handler>virt-handler</h2><p>Like the virt-controller, the virt-handler is also reactive, watching for changes to the VM object, and performing all necessary operations to change a VM to meet the required state. The virt-handler references the VM specification and signals the creation of a corresponding domain using a libvirtd instance in the VM's pod. When a VM object is deleted, the virt-handler observes the deletion and turns off the domain.</p><h2 id=virt-launcher>virt-launcher</h2><p>For every VM object one pod is created. This pod's primary container runs the virt-launcher KubeVirt component. The main purpose of the virt-launcher Pod is to provide the cgroups and namespaces which will be used to host the VM process.</p><p>virt-handler signals virt-launcher to start a VM by passing the VM's CRD object to virt-launcher. virt-launcher then uses a local libvirtd instance within its container to start the VM. From there virt-launcher monitors the VM process and terminates once the VM has exited.</p><p>If the Kubernetes runtime attempts to shutdown the virt-launcher pod before the VM has exited, virt-launcher forwards signals from Kubernetes to the VM process and attempts to hold off the termination of the pod until the VM has shutdown successfully.</p><pre><code># kubectl get pods

NAME                                   READY     STATUS        RESTARTS   AGE
virt-controller-7888c64d66-dzc9p   1/1       Running   0          2h
virt-controller-7888c64d66-wm66x   0/1       Running   0          2h
virt-handler-l2xkt                 1/1       Running   0          2h
virt-handler-sztsw                 1/1       Running   0          2h
virt-launcher-testvm-ephemeral-dph94   2/2       Running       0          2h
</code></pre><h2 id=libvirtd>libvirtd</h2><p>An instance of libvirtd is present in every VM pod. virt-launcher uses libvirtd to manage the life-cycle of the VM process.</p><h2 id=storage-and-networking>Storage and Networking</h2><p>KubeVirt VMs may be configured with disks, backed by volumes.</p><p><a href=https://github.com/kubevirt/kubevirt/blob/master/docs/direct-pv-disks.md>Persistent Volume Claim</a> volumes make Kubernetes persistent volume available as disks directly attached to the VM. This is the primary way to provide KubeVirt VMs with persistent storage. Currently, persistent volumes must be iscsi block devices, although work is underway to enable <a href=https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#persistentvolumeclaim>file-based pv disks</a>.</p><p><a href=https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#ephemeral-volume>Ephemeral Volumes</a> are a local copy on write images that use a network volume as a read-only backing store. KubeVirt dynamically generates the ephemeral images associated with a VM when the VM starts, and discards the ephemeral images when the VM stops. Currently, ephemeral volumes must be backed by pvc volumes.</p><p><a href=https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#registrydisk>Registry Disk</a> volumes reference docker image that embed a qcow or raw disk. As the name suggests, these volumes are pulled from a container registry. Like regular ephemeral container images, data in these volumes persists only while the pod lives.</p><p><a href=https://kubevirt.gitbooks.io/user-guide/disks-and-volumes.html#cloudinitnocloud>CloudInit NoCloud</a> volumes provide VMs with a cloud-init NoCloud user-data source, which is added as a disk to the VM, where it's available to provide configuration details to guests with cloud-init installed. Cloud-init details can be provided in clear text, as base64 encoded UserData files, or via Kubernetes secrets.</p><p>In the example below, a Registry Disk is configured to provide the image from which to boot the VM. A cloudInit NoCloud volume, paired with an ssh-key stored as clear text in the userData field, is provided for authentication with the VM:</p><pre><code>apiVersion: kubevirt.io/v1alpha1
kind: VirtualMachine
metadata:
  name: myvm
spec:
  terminationGracePeriodSeconds: 5
  domain:
    resources:
      requests:
        memory: 64M
    devices:
      disks:
      - name: registrydisk
        volumeName: registryvolume
        disk:
          bus: virtio
      - name: cloudinitdisk
        volumeName: cloudinitvolume
        disk:
          bus: virtio
  volumes:
    - name: registryvolume
      registryDisk:
        image: kubevirt/cirros-registry-disk-demo:devel
    - name: cloudinitvolume
      cloudInitNoCloud:
        userData: |
          ssh-authorized-keys:
            - ssh-rsa AAAAB3NzaK8L93bWxnyp test@test.com
</code></pre><p>Just as with regular Kubernetes pods, basic networking functionality is made available automatically to each KubeVirt VM, and particular TCP or UDP ports can be exposed to the outside world using regular Kubernetes services. No special network configuration is required.</p><h2 id=getting-involved>Getting Involved</h2><p>KubeVirt development is accelerating, and the project is eager for new contributors. If you're interested in getting involved, check out the project's <a href=https://github.com/kubevirt/kubevirt/issues>open issues</a> and check out the <a href="https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com">project calendar</a>.</p><p>If you need some help or want to chat you can connect to the team via freenode IRC in #kubevirt, or on the KubeVirt <a href=https://groups.google.com/forum/#!forum/kubevirt-dev>mailing list</a>. User documentation is available at <a href=https://kubevirt.gitbooks.io/user-guide/>https://kubevirt.gitbooks.io/user-guide/</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-86c8f3b0d16c35a20cb363fb5f3f686d>Gardener - The Kubernetes Botanist</h1><div class="td-byline mb-4">By <b>rfranzke</b> |
<time datetime=2018-05-17 class=text-muted>Thursday, May 17, 2018</time></div><p><strong>Authors</strong>: <a href=mailto:rafael.franzke@sap.com>Rafael Franzke</a> (SAP), <a href=mailto:vasu.chandrasekhara@sap.com>Vasu Chandrasekhara</a> (SAP)</p><p>Today, Kubernetes is the natural choice for running software in the Cloud. More and more developers and corporations are in the process of containerizing their applications, and many of them are adopting Kubernetes for automated deployments of their Cloud Native workloads.</p><p>There are many Open Source tools which help in creating and updating single Kubernetes clusters. However, the more clusters you need the harder it becomes to operate, monitor, manage, and keep all of them alive and up-to-date.</p><p>And that is exactly what project "<a href=https://github.com/gardener>Gardener</a>" focuses on. It is not just another provisioning tool, but it is rather designed to manage Kubernetes clusters as a service. It provides <a href=https://github.com/cncf/k8s-conformance>Kubernetes-conformant</a> clusters on various cloud providers and the ability to maintain hundreds or thousands of them at scale. At SAP, we face this heterogeneous multi-cloud & on-premise challenge not only in our own platform, but also encounter the same demand at all our larger and smaller customers implementing Kubernetes & Cloud Native.</p><p>Inspired by the possibilities of Kubernetes and the ability to self-host, the foundation of Gardener is Kubernetes itself. While self-hosting, as in, to run Kubernetes components inside Kubernetes is a popular topic in the community, we apply a special pattern catering to the needs of operating a huge number of clusters with minimal total cost of ownership. We take an initial Kubernetes cluster (called "seed" cluster) and seed the control plane components (such as the API server, scheduler, controller-manager, etcd and others) of an end-user cluster as simple Kubernetes pods. In essence, the focus of the seed cluster is to deliver a robust Control-Plane-as-a-Service at scale. Following our botanical terminology, the end-user clusters when ready to sprout are called "shoot" clusters. Considering network latency and other fault scenarios, we recommend a seed cluster per cloud provider and region to host the control planes of the many shoot clusters.</p><p>Overall, this concept of reusing Kubernetes primitives already simplifies deployment, management, scaling & patching/updating of the control plane. Since it builds upon highly available initial seed clusters, we can evade multiple quorum number of master node requirements for shoot cluster control planes and reduce waste/costs. Furthermore, the actual shoot cluster consists only of worker nodes for which full administrative access to the respective owners could be granted, thereby structuring a necessary separation of concerns to deliver a higher level of SLO. The architectural role & operational ownerships are thus defined as following (cf. <code>Figure 1</code>):</p><ul><li>Kubernetes as a Service provider owns, operates, and manages the garden and the seed clusters. They represent parts of the required landscape/infrastructure.</li><li>The control planes of the shoot clusters are run in the seed and, consequently, within the separate security domain of the service provider.</li><li>The shoot clusters' machines are run under the ownership of and in the cloud provider account and the environment of the customer, but still managed by the Gardener.</li><li>For on-premise or private cloud scenarios the delegation of ownership & management of the seed clusters (and the IaaS) is feasible.</li></ul><p><img src=/images/blog/2018-05-17-gardener-the-kubernetes-botanist/architecture.png width=70% alt="Gardener architecture"></p><p><em>Figure 1 Technical Gardener landscape with components.</em></p><p>The Gardener is developed as an aggregated API server and comes with a bundled set of controllers. It runs inside another dedicated Kubernetes cluster (called "garden" cluster) and it extends the Kubernetes API with custom resources. Most prominently, the Shoot resource allows a description of the entire configuration of a user's Kubernetes cluster in a declarative way. Corresponding controllers will, just like native Kubernetes controllers, watch these resources and bring the world's actual state to the desired state (resulting in create, reconcile, update, upgrade, or delete operations.)
The following example manifest shows what needs to be specified:</p><pre><code>apiVersion: garden.sapcloud.io/v1beta1
kind: Shoot
metadata:
  name: dev-eu1
  namespace: team-a
spec:
  cloud:
    profile: aws
    region: us-east-1
    secretBindingRef:
      name: team-a-aws-account-credentials
    aws:
      machineImage:
        ami: ami-34237c4d
        name: CoreOS
      networks:
        vpc:
          cidr: 10.250.0.0/16
        ...
      workers:
      - name: cpu-pool
        machineType: m4.xlarge
        volumeType: gp2
        volumeSize: 20Gi
        autoScalerMin: 2
        autoScalerMax: 5
  dns:
    provider: aws-route53
    domain: dev-eu1.team-a.example.com
  kubernetes:
    version: 1.10.2
  backup:
    ...
  maintenance:
    ...
  addons:
    cluster-autoscaler:
      enabled: true
    ...
</code></pre><p>Once sent to the garden cluster, Gardener will pick it up and provision the actual shoot. What is not shown above is that each action will enrich the <code>Shoot</code>'s <code>status</code> field indicating whether an operation is currently running and recording the last error (if there was any) and the health of the involved components. Users are able to configure and monitor their cluster's state in true Kubernetes style. Our users have even written their own custom controllers watching & mutating these <code>Shoot</code> resources.</p><h1 id=technical-deep-dive>Technical deep dive</h1><p>The Gardener implements a Kubernetes inception approach; thus, it leverages Kubernetes capabilities to perform its operations. It provides a couple of controllers (cf. <code>[A]</code>) watching <code>Shoot</code> resources whereas the main controller is responsible for the standard operations like create, update, and delete. Another controller named "shoot care" is performing regular health checks and garbage collections, while a third's ("shoot maintenance") tasks are to cover actions like updating the shoot's machine image to the latest available version.</p><p>For every shoot, Gardener creates a dedicated <code>Namespace</code> in the seed with appropriate security policies and within it pre-creates the later required certificates managed as <code>Secrets</code>.</p><h3 id=etcd>etcd</h3><p>The backing data store etcd (cf. <code>[B]</code>) of a Kubernetes cluster is deployed as a <code>StatefulSet</code> with one replica and a <code>PersistentVolume(Claim)</code>. Embracing best practices, we run another etcd shard-instance to store <code>Events</code> of a shoot. Anyway, the main etcd pod is enhanced with a sidecar validating the data at rest and taking regular snapshots which are then efficiently backed up to an object store. In case etcd's data is lost or corrupt, the sidecar restores it from the latest available snapshot. We plan to develop incremental/continuous backups to avoid discrepancies (in case of a recovery) between a restored etcd state and the actual state [1].</p><h3 id=kubernetes-control-plane>Kubernetes control plane</h3><p>As already mentioned above, we have put the other Kubernetes control plane components into native <code>Deployments</code> and run them with the rolling update strategy. By doing so, we can not only leverage the existing deployment and update capabilities of Kubernetes, but also its monitoring and liveliness proficiencies. While the control plane itself uses in-cluster communication, the API Servers' <code>Service</code> is exposed via a load balancer for external communication (cf. <code>[C]</code>). In order to uniformly generate the deployment manifests (mainly depending on both the Kubernetes version and cloud provider), we decided to utilize <a href=https://github.com/kubernetes/helm>Helm</a> charts whereas Gardener leverages only Tillers rendering capabilities, but deploys the resulting manifests directly without running Tiller at all [2].</p><h3 id=infrastructure-preparation>Infrastructure preparation</h3><p>One of the first requirements when creating a cluster is a well-prepared infrastructure on the cloud provider side including networks and security groups. In our current provider specific in-tree implementation of Gardener (called the "Botanist"), we employ <a href=https://github.com/hashicorp/terraform>Terraform</a> to accomplish this task. Terraform provides nice abstractions for the major cloud providers and implements capabilities like parallelism, retry mechanisms, dependency graphs, idempotency, and more. However, we found that Terraform is challenging when it comes to error handling and it does not provide a technical interface to extract the root cause of an error. Currently, Gardener generates a Terraform script based on the shoot specification and stores it inside a <code>ConfigMap</code> in the respective namespace of the seed cluster. The <a href=https://github.com/gardener/terraformer>Terraformer component</a> then runs as a <code>Job</code> (cf. <code>[D]</code>), executes the mounted Terraform configuration, and writes the produced state back into another <code>ConfigMap</code>. Using the Job primitive in this manner helps to inherit its retry logic and achieve fault tolerance against temporary connectivity issues or resource constraints. Moreover, Gardener only needs to access the Kubernetes API of the seed cluster to submit the Job for the underlying IaaS. This design is important for private cloud scenarios in which typically the IaaS API is not exposed publicly.</p><h3 id=machine-controller-manager>Machine controller manager</h3><p>What is required next are the nodes to which the actual workload of a cluster is to be scheduled. However, Kubernetes offers no primitives to request nodes forcing a cluster administrator to use external mechanisms. The considerations include the full lifecycle, beginning with initial provisioning and continuing with providing security fixes, and performing health checks and rolling updates. While we started with instantiating static machines or utilizing instance templates of the cloud providers to create the worker nodes, we concluded (also from our previous production experience with running a cloud platform) that this approach requires extensive effort. During discussions at KubeCon 2017, we recognized that the best way, of course, to manage cluster nodes is to again apply core Kubernetes concepts and to teach the system to self-manage the nodes/machines it runs. For that purpose, we developed the <a href=https://github.com/gardener/machine-controller-manager>machine controller manager</a> (cf. <code>[E]</code>) which extends Kubernetes with <code>MachineDeployment</code>, <code>MachineClass</code>, <code>MachineSet</code> & <code>Machine</code> resources and enables declarative management of (virtual) machines from within the Kubernetes context just like <code>Deployments</code>, <code>ReplicaSets</code> & <code>Pods</code>. We reused code from existing Kubernetes controllers and just needed to abstract a few IaaS/cloud provider specific methods for creating, deleting, and listing machines in dedicated drivers. When comparing Pods and Machines a subtle difference becomes evident: creating virtual machines directly results in costs, and if something unforeseen happens, these costs can increase very quickly. To safeguard against such rampage, the machine controller manager comes with a safety controller that terminates orphaned machines and freezes the rollout of MachineDeployments and MachineSets beyond certain thresholds and time-outs. Furthermore, we leverage the existing official <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>cluster-autoscaler</a> already including the complex logic of determining which node pool to scale out or down. Since its cloud provider interface is well-designed, we enabled the autoscaler to directly modify the number of replicas in the respective <code>MachineDeployment</code> resource when triggering to scale out or down.</p><h3 id=addons>Addons</h3><p>Besides providing a properly setup control plane, every Kubernetes cluster requires a few system components to work. Usually, that's the kube-proxy, an overlay network, a cluster DNS, and an ingress controller. Apart from that, Gardener allows to order optional add-ons configurable by the user (in the shoot resource definition), e.g. Heapster, the Kubernetes Dashboard, or Cert-Manager. Again, the Gardener renders the manifests for all these components via Helm charts (partly adapted and curated from the <a href=https://github.com/kubernetes/charts/tree/master/stable>upstream charts repository</a>). However, these resources are managed in the shoot cluster and can thus be tweaked by users with full administrative access. Hence, Gardener ensures that these deployed resources always match the computed/desired configuration by utilizing an existing watch dog, the <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager>kube-addon-manager</a> (cf. <code>[F]</code>).</p><h3 id=network-air-gap>Network air gap</h3><p>While the control plane of a shoot cluster runs in a seed managed & supplied by your friendly platform-provider, the worker nodes are typically provisioned in a separate cloud provider (billing) account of the user. Typically, these worker nodes are placed into private networks [3] to which the API Server in the seed control plane establishes direct communication, using a simple <a href=https://github.com/gardener/vpn>VPN</a> solution based on ssh (cf. <code>[G]</code>). We have recently migrated the SSH-based implementation to an <a href=https://github.com/OpenVPN/openvpn>OpenVPN</a>-based implementation which significantly increased the network bandwidth.</p><h3 id=monitoring-logging>Monitoring & Logging</h3><p>Monitoring, alerting, and logging are crucial to supervise clusters and keep them healthy so as to avoid outages and other issues. <a href=https://github.com/prometheus/prometheus>Prometheus</a> has become the most used monitoring system in the Kubernetes domain. Therefore, we deploy a central Prometheus instance into the <code>garden</code> namespace of every seed. It collects metrics from all the seed's kubelets including those for all pods running in the seed cluster. In addition, next to every control plane a dedicated tenant Prometheus instance is provisioned for the shoot itself (cf. <code>[H]</code>). It gathers metrics for its own control plane as well as for the pods running on the shoot's worker nodes. The former is done by fetching data from the central Prometheus' federation endpoint and filtering for relevant control plane pods of the particular shoot. Other than that, Gardener deploys two <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> instances, one responsible for the control plane and one for the workload, exposing cluster-level metrics to enrich the data. The <a href=https://github.com/prometheus/node_exporter>node exporter</a> provides more detailed node statistics. A dedicated tenant <a href=http://github.com/grafana/grafana>Grafana</a> dashboard displays the analytics and insights via lucid dashboards. We also defined alerting rules for critical events and employed the <a href=https://github.com/prometheus/alertmanager>AlertManager</a> to send emails to operators and support teams in case any alert is fired.</p><p>[1] This is also the reason for not supporting point-in-time recovery. There is no reliable infrastructure reconciliation implemented in Kubernetes so far. Thus, restoring from an old backup without refreshing the actual workload and state of the concerned cluster would generally not be of much help.</p><p>[2] The most relevant criteria for this decision was that Tiller requires a port-forward connection for communication which we experienced to be too unstable and error-prone for our automated use case. Nevertheless, we are looking forward to Helm v3 hopefully interacting with Tiller using <code>CustomResourceDefinitions</code>.</p><p>[3] Gardener offers to either create & prepare these networks with the Terraformer or it can be instructed to reuse pre-existing networks.</p><h1 id=usability-and-interaction>Usability and Interaction</h1><p>Despite requiring only the familiar <code>kubectl</code> command line tool for managing all of Gardener, we provide a central <a href=https://github.com/gardener/dashboard>dashboard</a> for comfortable interaction. It enables users to easily keep track of their clusters' health, and operators to monitor, debug, and analyze the clusters they are responsible for. Shoots are grouped into logical projects in which teams managing a set of clusters can collaborate and even track issues via an integrated ticket system (e.g. GitHub Issues). Moreover, the dashboard helps users to add & manage their infrastructure account secrets and to view the most relevant data of all their shoot clusters in one place while being independent from the cloud provider they are deployed to.</p><p><img src=/images/blog/2018-05-17-gardener-the-kubernetes-botanist/dashboard.gif alt="Gardener architecture"></p><p><em>Figure 2 Animated Gardener dashboard.</em></p><p>More focused on the duties of developers and operators, the Gardener command line client <a href=https://github.com/gardener/gardenctl><code>gardenctl</code></a> simplifies administrative tasks by introducing easy higher-level abstractions with simple commands that help condense and multiplex information & actions from/to large amounts of seed and shoot clusters.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ gardenctl ls shoots
projects:
- project: team-a
  shoots:
  - dev-eu1
  - prod-eu1

$ gardenctl target shoot prod-eu1
<span style=color:#666>[</span>prod-eu1<span style=color:#666>]</span>

$ gardenctl show prometheus
NAME           READY     STATUS    RESTARTS   AGE       IP              NODE
prometheus-0   3/3       Running   <span style=color:#666>0</span>          106d      10.241.241.42   ip-10-240-7-72.eu-central-1.compute.internal

URL: https://user:password@p.prod-eu1.team-a.seed.aws-eu1.example.com
</code></pre></div><h1 id=outlook-and-future-plans>Outlook and future plans</h1><p>The Gardener is already capable of managing Kubernetes clusters on AWS, Azure, GCP, OpenStack [4]. Actually, due to the fact that it relies only on Kubernetes primitives, it nicely connects to private cloud or on-premise requirements. The only difference from Gardener's point of view would be the quality and scalability of the underlying infrastructure - the lingua franca of Kubernetes ensures strong portability guarantees for our approach.</p><p>Nevertheless, there are still challenges ahead. We are probing a possibility to include an option to create a federation control plane delegating to multiple shoot clusters in this Open Source project. In the previous sections we have not explained how to <a href=https://en.wikipedia.org/wiki/M%C3%BCnchhausen_trilemma>bootstrap</a> the garden and the seed clusters themselves. You could indeed use any production ready cluster provisioning tool or the cloud providers' Kubernetes as a Service offering. We have built an uniform tool called <a href=https://github.com/gardener/kubify>Kubify</a> based on Terraform and reused many of the mentioned Gardener components. We envision the required Kubernetes infrastructure to be able to be spawned in its entirety by an initial bootstrap Gardener and are already discussing how we could achieve that.</p><p>Another important topic we are focusing on is disaster recovery. When a seed cluster fails, the user's static workload will continue to operate. However, administrating the cluster won't be possible anymore. We are considering to move control planes of the shoots hit by a disaster to another seed. Conceptually, this approach is feasible and we already have the required components in place to implement that, e.g. automated etcd backup and restore. The contributors for this project not only have a mandate for developing Gardener for production, but most of us even run it in true DevOps mode as well. We completely trust the Kubernetes concepts and are committed to follow the "eat your own dog food" approach.</p><p>In order to enable a more independent evolution of the Botanists, which contain the infrastructure provider specific parts of the implementation, we plan to describe well-defined interfaces and factor out the Botanists into their own components. This is similar to what Kubernetes is currently doing with the cloud-controller-manager. Currently, all the cloud specifics are part of the core Gardener repository presenting a soft barrier to extending or supporting new cloud providers.</p><p>When taking a look at how the shoots are actually provisioned, we need to gain more experience on how really large clusters with thousands of nodes and pods (or more) behave. Potentially, we will have to deploy e.g. the API server and other components in a scaled-out fashion for large clusters to spread the load. Fortunately, horizontal pod autoscaling based on custom metrics from Prometheus will make this relatively easy with our setup. Additionally, the feedback from teams who run production workloads on our clusters, is that Gardener should support with prearranged Kubernetes <a href=/docs/tasks/configure-pod-container/quality-service-pod/>QoS</a>. Needless to say, our aspiration is going to be the integration and contribution to the vision of <a href=https://speakerdeck.com/thockin/a-few-things-to-know-about-resource-scheduling>Kubernetes Autopilot</a>.</p><p>[4] Prototypes already validated CTyun & Aliyun.</p><h1 id=gardener-is-open-source>Gardener is open source</h1><p>The Gardener project is developed as Open Source and hosted on GitHub: <a href=https://github.com/gardener>https://github.com/gardener</a></p><p>SAP is working on Gardener since mid 2017 and is focused on building up a project that can easily be evolved and extended. Consequently, we are now looking for further partners and contributors to the project. As outlined above, we completely rely on Kubernetes primitives, add-ons, and specifications and adapt its innovative Cloud Native approach. We are looking forward to aligning with and contributing to the Kubernetes community. In fact, we envision contributing the complete project to the CNCF.</p><p>At the moment, an important focus on collaboration with the community is the <a href=https://sigs.k8s.io/cluster-api>Cluster API working group</a> within the SIG Cluster Lifecycle founded a few months ago. Its primary goal is the definition of a portable API representing a Kubernetes cluster. That includes the configuration of control planes and the underlying infrastructure. The overlap of what we have already in place with Shoot and Machine resources compared to what the community is working on is striking. Hence, we joined this working group and are actively participating in their regular meetings, trying to contribute back our learnings from production. Selfishly, it is also in our interest to shape a robust API.</p><p>If you see the potential of the Gardener project then please learn more about it on GitHub and help us make Gardener even better by asking questions, engaging in discussions, and by contributing code. Also, try out our <a href=https://github.com/gardener/landscape-setup-template>quick start setup</a>.</p><p>We are looking forward to seeing you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-097264f98e024a5d7313268988d5ac8f>Docs are Migrating from Jekyll to Hugo</h1><div class="td-byline mb-4">By <b>zcorleissen</b> |
<time datetime=2018-05-05 class=text-muted>Saturday, May 05, 2018</time></div><p><strong>Author</strong>: <a href=https://www.cncf.io/people/staff/>Zach Corleissen</a> (CNCF)</p><h2 id=changing-the-site-framework>Changing the site framework</h2><p>After nearly a year of investigating how to enable multilingual support for Kubernetes docs, we've decided to migrate the site's static generator from Jekyll to <a href=https://gohugo.io/>Hugo</a>.</p><p>What does the Hugo migration mean for users and contributors?</p><h3 id=things-will-break>Things will break</h3><p>Hugo's Markdown parser is <a href=https://gohugo.io/getting-started/configuration/#configure-blackfriday>differently strict than Jekyll's</a>. As a consequence, some Markdown formatting that rendered fine in Jekyll now produces some unexpected results: <a href=https://github.com/kubernetes/website/issues/8258>strange left nav ordering</a>, <a href=https://github.com/kubernetes/website/issues/8247>vanishing tutorials</a>, and <a href=https://github.com/kubernetes/website/issues/8246>broken links</a>, among others.</p><p>If you encounter any site weirdness or broken formatting, please <a href=https://github.com/kubernetes/website/issues/new>open an issue</a>. You can see the list of issues that are <a href="https://github.com/kubernetes/website/issues?q=is%3Aissue+is%3Aopen+Hugo+label%3A%22Needs+Docs+Review%22">specific to Hugo migration</a>.</p><h3 id=multilingual-support-is-coming>Multilingual support is coming</h3><p>Our initial search focused on finding a language selector that would play well with Jekyll. The projects we found weren't well-supported, and a prototype of one plugin made it clear that a Jekyll implementation would create technical debt that drained resources away from the quality of the docs.</p><p>We chose Hugo after months of research and conversations with other open source translation projects. (Special thanks to <a href="https://twitter.com/jaegerandi?lang=da">Andreas Jaeger</a> and his experience at OpenStack). Hugo's <a href=https://gohugo.io/content-management/multilingual/>multilingual support</a> is built in and easy.</p><h3 id=pain-now-relief-later>Pain now, relief later</h3><p>Another advantage of Hugo is that <a href=https://gohugo.io/troubleshooting/build-performance/>build performance</a> scales well at size. At 250+ pages, the Kubernetes site's build times suffered significantly with Jekyll. We're excited about removing the barrier to contribution created by slow site build times.</p><p>Again, if you encounter any broken, missing, or unexpected content, please <a href=https://github.com/kubernetes/website/issues/new>open an issue</a> and let us know.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ff99d5de7d522c022ef82151d5e698fa>Announcing Kubeflow 0.1</h1><div class="td-byline mb-4">By <b>aronchick</b> |
<time datetime=2018-05-04 class=text-muted>Friday, May 04, 2018</time></div><h1 id=since-last-we-met>Since Last We Met</h1><p>Since the <a href=https://kubernetes.io/blog/2017/12/introducing-kubeflow-composable>initial announcement</a> of Kubeflow at <a href=https://kccncna17.sched.com/event/CU5v/hot-dogs-or-not-at-scale-with-kubernetes-i-vish-kannan-david-aronchick-google>the last KubeCon+CloudNativeCon</a>, we have been both surprised and delighted by the excitement for building great ML stacks for Kubernetes. In just over five months, the <a href=https://github.com/kubeflow>Kubeflow project</a> now has:</p><ul><li>70+ contributors</li><li>20+ contributing organizations</li><li>15 repositories</li><li>3100+ GitHub stars</li><li>700+ commits</li></ul><p>and already is among the top 2% of GitHub projects <strong><em>ever</em></strong>.</p><p>People are excited to chat about Kubeflow as well! The Kubeflow community has also held meetups, talks and public sessions all around the world with thousands of attendees. With all this help, we’ve started to make substantial in every step of ML, from building your first model all the way to building a production-ready, high-scale deployments. At the end of the day, our mission remains the same: we want to let data scientists and software engineers focus on the things they do well by giving them an easy-to-use, portable and scalable ML stack.</p><h1 id=introducing-kubeflow-0-1>Introducing Kubeflow 0.1</h1><p>Today, we’re proud to announce the availability of Kubeflow 0.1, which provides a minimal set of packages to begin developing, training and deploying ML. In just a few commands, you can get:</p><ul><li><strong>Jupyter Hub</strong> - for collaborative & interactive training</li><li>A <strong>TensorFlow Training Controller</strong> with native distributed training</li><li>A <strong>TensorFlow Serving</strong> for hosting</li><li><strong>Argo</strong> for workflows</li><li><strong>SeldonCore</strong> for complex inference and non TF models</li><li><strong>Ambassador</strong> for Reverse Proxy</li><li>Wiring to make it work on any Kubernetes anywhere</li></ul><p>To get started, it’s just as easy as it always has been:</p><pre><code># Create a namespace for kubeflow deployment
NAMESPACE=kubeflow
kubectl create namespace ${NAMESPACE}
VERSION=v0.1.3

# Initialize a ksonnet app. Set the namespace for it's default environment.
APP_NAME=my-kubeflow
ks init ${APP_NAME}
cd ${APP_NAME}
ks env set default --namespace ${NAMESPACE}

# Install Kubeflow components
ks registry add kubeflow github.com/kubeflow/kubeflow/tree/${VERSION}/kubeflow
ks pkg install kubeflow/core@${VERSION}
ks pkg install kubeflow/tf-serving@${VERSION}
ks pkg install kubeflow/tf-job@${VERSION}

# Create templates for core components
ks generate kubeflow-core kubeflow-core

# Deploy Kubeflow
ks apply default -c kubeflow-core
</code></pre><p>And thats it! JupyterHub is deployed so we can now use Jupyter to begin developing models. Once we have python code to build our model we can build a docker image and train our model using our TFJob operator by running commands like the following:</p><pre><code>ks generate tf-job my-tf-job --name=my-tf-job --image=gcr.io/my/image:latest
ks apply default -c my-tf-job

We could then deploy the model by doing

ks generate tf-serving ${MODEL_COMPONENT} --name=${MODEL_NAME}
ks param set ${MODEL_COMPONENT} modelPath ${MODEL_PATH}
ks apply ${ENV} -c ${MODEL_COMPONENT}
</code></pre><p>Within just a few commands, data scientists and software engineers can now create even complicated ML solutions and focus on what they do best: answering business critical questions.</p><h1 id=community-contributions>Community Contributions</h1><p>It’d be impossible to have gotten where we are without enormous help from everyone in the community. Some specific contributions that we want to highlight include:</p><ul><li><a href=https://github.com/kubeflow/kubeflow/tree/v0.7.0/kubeflow/argo>Argo</a> for managing ML workflows</li><li><a href=https://github.com/kubeflow/caffe2-operator>Caffe2 Operator</a> for running Caffe2 jobs</li><li><a href=https://github.com/kubeflow/kubeflow/tree/master/components/openmpi-controller>Horovod & OpenMPI</a> for improved distributed training performance of TensorFlow</li><li><a href=https://github.com/kubeflow/kubeflow/blob/master/docs/gke/iap_request.py>Identity Aware Proxy</a>, which enables using security your services with identities, rather than VPNs and Firewalls</li><li><a href=https://github.com/kubeflow/katib>Katib</a> for hyperparameter tuning</li><li><a href=https://github.com/kubeflow/experimental-kvc>Kubernetes volume controller</a> which provides basic volume and data management using volumes and volume sources in a Kubernetes cluster.</li><li><a href=https://github.com/kubeflow/kubebench>Kubebench</a> for benchmarking of HW and ML stacks</li><li><a href=https://github.com/kubeflow/kubeflow/tree/v0.7.0/kubeflow/pachyderm>Pachyderm</a> for managing complex data pipelines</li><li><a href=https://github.com/kubeflow/pytorch-operator>PyTorch operator</a> for running PyTorch jobs</li><li><a href=https://github.com/kubeflow/kubeflow/tree/v0.7.0/kubeflow/seldon>Seldon Core</a> for running complex model deployments and non-TensorFlow serving</li></ul><p>It’s difficult to overstate how much the community has helped bring all these projects (and more) to fruition. Just a few of the contributing companies include: Alibaba Cloud, Ant Financial, Caicloud, Canonical, Cisco, Datawire, Dell, GitHub, Google, Heptio, Huawei, Intel, Microsoft, Momenta, One Convergence, Pachyderm, Project Jupyter, Red Hat, Seldon, Uber and Weaveworks.</p><h1 id=learning-more>Learning More</h1><p>If you’d like to try out Kubeflow, we have a number of options for you:</p><ol><li>You can use sample walkthroughs hosted on <a href=https://www.katacoda.com/kubeflow>Katacoda</a></li><li>You can follow a guided tutorial with existing models from the <a href=https://github.com/kubeflow/examples>examples repository</a>. These include the <a href=https://github.com/kubeflow/examples/tree/master/github_issue_summarization>GitHub Issue Summarization</a>, <a href=https://github.com/kubeflow/examples/tree/master/mnist>MNIST</a> and <a href=https://github.com/kubeflow/examples/tree/v0.5.1/agents>Reinforcement Learning with Agents</a>.</li><li>You can start a cluster on your own and try your own model. Any Kubernetes conformant cluster will support Kubeflow including those from contributors <a href=https://www.prnewswire.com/news-releases/caicloud-releases-its-kubernetes-based-cluster-as-a-service-product-claas-20-and-the-first-tensorflow-as-a-service-taas-11-while-closing-6m-series-a-funding-300418071.html>Caicloud</a>, <a href=https://jujucharms.com/canonical-kubernetes/>Canonical</a>, <a href=https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-container-cluster>Google</a>, <a href=https://heptio.com/products/kubernetes-subscription/>Heptio</a>, <a href=https://github.com/mesosphere/dcos-kubernetes-quickstart>Mesosphere</a>, <a href=https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough>Microsoft</a>, <a href="https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial">IBM</a>, <a href=https://docs.openshift.com/container-platform/3.3/install_config/install/quick_install.html#install-config-install-quick-install>Red Hat/Openshift </a>and <a href=https://www.weave.works/product/cloud/>Weaveworks</a>.</li></ol><p>There were also a number of sessions at KubeCon + CloudNativeCon EU 2018 covering Kubeflow. The links to the talks are here; the associated videos will be posted in the coming days.</p><ul><li><p>Wednesday, May 2:</p><ul><li><a href=http://sched.co/Drmt>Kubeflow Intro - Michał Jastrzębski & Ala Raddaoui, Intel</a></li></ul></li><li><p>Thursday, May 3:</p><ul><li><a href=http://sched.co/Drnd>Kubeflow Deep Dive - Jeremy Lewi, Google</a></li><li><a href=http://sched.co/Dquu>Write Once, Train & Predict Everywhere: Portable ML Stacks with Kubeflow - Jeremy Lewi, Google & Stephan Fabel, Canonical</a></li><li><a href=http://sched.co/DqvC>Compliant Data Management and Machine Learning on Kubernetes - Daniel Whitenack, Pachyderm</a></li><li><a href=https://kccnceu18.sched.com/event/E46y/bringing-your-data-pipeline-into-the-machine-learning-era-chris-gaun-jorg-schad-mesosphere-intermediate-skill-level>Bringing Your Data Pipeline into The Machine Learning Era - Chris Gaun & Jörg Schad, Mesosphere</a></li></ul></li><li><p>Friday, May 4:</p><ul><li><a href=http://sched.co/Duoq>Keynote: Kubeflow ML on Kubernetes - David Aronchick & Vishnu Kannan, Google</a></li><li><a href=http://sched.co/Dqv6>Conquering a Kubeflow Kubernetes Cluster with ksonnet, Ark, and Sonobuoy - Kris Nova, Heptio & David Aronchick, Google</a></li><li><a href=http://sched.co/Dqvw>Serving ML Models at Scale with Seldon and Kubeflow - Clive Cox, Seldon.io</a></li></ul></li></ul><h1 id=what-s-next>What’s Next?</h1><p>Our next major release will be 0.2 coming this summer. In it, we expect to land the following new features:</p><ul><li>Simplified setup via a bootstrap container</li><li>Improved accelerator integration</li><li>Support for more ML frameworks, e.g., Spark ML, XGBoost, sklearn</li><li>Autoscaled TF Serving</li><li>Programmatic data transforms, e.g., tf.transform</li></ul><p>But the most important feature is the one we haven’t heard yet. Please tell us! Some options for making your voice heard include:</p><ul><li>The <a href=https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg>Kubeflow Slack channel</a></li><li>The <a href=https://groups.google.com/forum/#!forum/kubeflow-discuss>Kubeflow-discuss</a> email list</li><li>The <a href=http://twitter.com/kubeflow>Kubeflow twitter</a> account</li><li>Our <a href=https://github.com/kubeflow/community>weekly community meeting</a></li><li>Please download and run <a href=https://github.com/kubeflow/kubeflow/pull/330/files>kubeflow</a>, and submit bugs!</li></ul><p>Thank you for all your support so far!
<em>Jeremy Lewi & David Aronchick</em> Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9b38d1f6d5473fb1c25057fda021e39c>Current State of Policy in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-05-02 class=text-muted>Wednesday, May 02, 2018</time></div><p>Kubernetes has grown dramatically in its impact to the industry; and with rapid growth, we are beginning to see variations across components in how they define and apply policies.</p><p>Currently, policy related components could be found in identity services, networking services, storage services, multi-cluster federation, RBAC and many other areas, with different degree of maturity and also different motivations for specific problems. Within each component, some policies are extensible while others are not. The languages used by each project to express intent vary based on the original authors and experience. Driving consistent views of policies across the entire domain is a daunting task.</p><p>Adoption of Kubernetes in regulated industries will also drive the need to ensure that a deployed cluster confirms with various legal requirements, such as PCI, HIPPA, or GDPR. Each of these compliance standards enforces a certain level of privacy around user information, data, and isolation.</p><p>The core issues with the current Kubernetes policy implementations are identified as follows:</p><ul><li>Lack of big picture across the platform</li><li>Lack of coordination and common language among different policy components</li><li>Lack of consistency for extensible policy creation across the platform.<ul><li>There are areas where policy components are extensible, and there are also areas where strict end-to-end solutions are enforced. No consensus is established on the preference to a extensible and pluggable architecture.</li></ul></li><li>Lack of consistent auditability across the Kubernetes architecture of policies which are created, modified, or disabled as well as the actions performed on behalf of the policies which are applied.</li></ul><h2 id=forming-kubernetes-policy-wg>Forming Kubernetes Policy WG</h2><p>We have established a new WG to directly address these issues. We intend to provide an overall architecture that describes both the current policy related implementations as well as future policy related proposals in Kubernetes. Through a collaborative method, we want to present both dev and end user a universal view of policy in Kubernetes.</p><p>We are not seeking to redefine and replace existing implementations which have been reached by thorough discussion and consensus. Rather to establish a summarized review of current implementation and addressing gaps to address broad end to end scenarios as defined in our initial design proposal.</p><p>Kubernetes Policy WG has been focusing on the design proposal document and using the weekly meeting for discussions among WG members. The design proposal outlines the background and motivation of why we establish the WG, the concrete use cases from which the gaps/requirement analysis is deduced, the overall architecture and the container policy interface proposal.</p><h2 id=key-policy-scenarios-in-kubernetes>Key Policy Scenarios in Kubernetes</h2><p>Among several use cases the workgroup has brainstormed, eventually three major scenario stands out.</p><p>The first one is about legislation/regulation compliance which requires the Kubernetes clusters conform to. The compliance scenario takes GDPR as an legislation example and the suggested policy architecture out of the discussion is to have a datapolicy controller responsible for the auditing.</p><p>The second scenario is about capacity leasing, or multi-tenant quota in traditional IaaS concept, which deals with when a large enterprise want to delegate the resource control to various Lines Of Business it has, how the Kubernetes cluster should be configured to have a policy driven mechanism to enforce the quota system. The ongoing multi-tenant controller design proposed in the multi-tenancy working group could be an ideal enforcement point for the quota policy controller, which in turn might take a look at kube-arbitrator for inspiration.</p><p>The last scenario is about cluster policy which refers to the general security and resource oriented policy control. Luster policy will involve both cluster level and namespace level policy control as well as enforcement, and there is a proposal called Kubernetes Security Profile that is under development by the Policy WG member to provide a PoC for this use case.</p><h2 id=kubernetes-policy-architecture>Kubernetes Policy Architecture</h2><p>Building upon the three scenarios, the WG is now working on three concrete proposals together with sig-arch, sig-auth and other related projects. Besides the Kubernetes security profile proposal aiming at the cluster policy use case, we also have the scheduling policy proposal which partially targets the capacity leasing use case and the topology service policy proposal which deals with affinity based upon service requirement and enforcement on routing level.</p><p>When these concrete proposals got clearer the WG will be able to provide a high level Kubernetes policy architecture as part of the motivation of the establishment of the Policy WG.</p><h2 id=towards-cloud-native-policy-driven-architecture>Towards Cloud Native Policy Driven Architecture</h2><p>Policy is definitely something goes beyond Kubernetes and applied to a broader cloud native context. Our work in the Kubernetes Policy WG will provide the foundation of building a CNCF wide policy architecture, with the integration of Kubernetes and various other cloud native components such as open policy agent, Istio, Envoy, SPIFEE/SPIRE and so forth. The Policy WG has already collaboration with the CNCF SAFE WG (in-forming) team, and will work on more alignments to make sure a community driven cloud native policy architecture design.</p><p><strong>Authors</strong>: Zhipeng Huang, Torin Sandall, Michael Elder, Erica Von Buelow, Khalid Ahmed, Yisui Hu</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b1db29e1fa629592ea431f588f2145fc>Developing on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-05-01 class=text-muted>Tuesday, May 01, 2018</time></div><p><strong>Authors</strong>: <a href=https://twitter.com/mhausenblas>Michael Hausenblas</a> (Red Hat), <a href=https://twitter.com/errordeveloper>Ilya Dmitrichenko</a> (Weaveworks)</p><p>How do you develop a Kubernetes app? That is, how do you write and test an app that is supposed to run on Kubernetes? This article focuses on the challenges, tools and methods you might want to be aware of to successfully write Kubernetes apps alone or in a team setting.</p><p>We’re assuming you are a developer, you have a favorite programming language, editor/IDE, and a testing framework available. The overarching goal is to introduce minimal changes to your current workflow when developing the app for Kubernetes. For example, if you’re a Node.js developer and are used to a hot-reload setup—that is, on save in your editor the running app gets automagically updated—then dealing with containers and container images, with container registries, Kubernetes deployments, triggers, and more can not only be overwhelming but really take all the fun out if it.</p><p>In the following, we’ll first discuss the overall development setup, then review tools of the trade, and last but not least do a hands-on walkthrough of three exemplary tools that allow for iterative, local app development against Kubernetes.</p><h2 id=where-to-run-your-cluster>Where to run your cluster?</h2><p>As a developer you want to think about where the Kubernetes cluster you’re developing against runs as well as where the development environment sits. Conceptually there are four development modes:</p><p><img src=/images/blog/2018-05-01-developing-on-kubernetes/dok-devmodes_preview.png alt="Dev Modes"></p><p>A number of tools support pure offline development including Minikube, Docker for Mac/Windows, Minishift, and the ones we discuss in detail below. Sometimes, for example, in a microservices setup where certain microservices already run in the cluster, a proxied setup (forwarding traffic into and from the cluster) is preferable and Telepresence is an example tool in this category. The live mode essentially means you’re building and/or deploying against a remote cluster and, finally, the pure online mode means both your development environment and the cluster are remote, as this is the case with, for example, <a href=https://www.eclipse.org/che/docs/che-7/introduction-to-eclipse-che/>Eclipse Che</a> or <a href=https://github.com/errordeveloper/k9c>Cloud 9</a>. Let’s now have a closer look at the basics of offline development: running Kubernetes locally.</p><p><a href=/docs/getting-started-guides/minikube/>Minikube</a> is a popular choice for those who prefer to run Kubernetes in a local VM. More recently Docker for <a href=https://docs.docker.com/docker-for-mac/kubernetes/>Mac</a> and <a href=https://docs.docker.com/docker-for-windows/kubernetes/>Windows</a> started shipping Kubernetes as an experimental package (in the “edge” channel). Some reasons why you may want to prefer using Minikube over the Docker desktop option are:</p><ul><li>You already have Minikube installed and running</li><li>You prefer to wait until Docker ships a stable package</li><li>You’re a Linux desktop user</li><li>You are a Windows user who doesn’t have Windows 10 Pro with Hyper-V</li></ul><p>Running a local cluster allows folks to work offline and that you don’t have to pay for using cloud resources. Cloud provider costs are often rather affordable and free tiers exists, however some folks prefer to avoid having to approve those costs with their manager as well as potentially incur unexpected costs, for example, when leaving cluster running over the weekend.</p><p>Some developers prefer to use a remote Kubernetes cluster, and this is usually to allow for larger compute and storage capacity and also enable collaborative workflows more easily. This means it’s easier for you to pull in a colleague to help with debugging or share access to an app in the team. Additionally, for some developers it can be critical to mirror production environment as closely as possible, especially when it comes down to external cloud services, say, proprietary databases, object stores, message queues, external load balancer, or mail delivery systems.</p><p>In summary, there are good reasons for you to develop against a local cluster as well as a remote one. It very much depends on in which phase you are: from early prototyping and/or developing alone to integrating a set of more stable microservices.</p><p>Now that you have a basic idea of the options around the runtime environment, let’s move on to how to iteratively develop and deploy your app.</p><h2 id=the-tools-of-the-trade>The tools of the trade</h2><p>We are now going to review tooling allowing you to develop apps on Kubernetes with the focus on having minimal impact on your existing workflow. We strive to provide an unbiased description including implications of using each of the tools in general terms.</p><p>Note that this is a tricky area since even for established technologies such as, for example, JSON vs YAML vs XML or REST vs gRPC vs SOAP a lot depends on your background, your preferences and organizational settings. It’s even harder to compare tooling in the Kubernetes ecosystem as things evolve very rapidly and new tools are announced almost on a weekly basis; during the preparation of this post alone, for example, <a href=https://gitkube.sh/>Gitkube</a> and <a href=https://github.com/MinikubeAddon/watchpod>Watchpod</a> came out. To cover these new tools as well as related, existing tooling such as <a href=https://github.com/weaveworks/flux>Weave Flux</a> and OpenShift’s <a href=https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html>S2I</a> we are planning a follow-up blog post to the one you’re reading.</p><h3 id=draft>Draft</h3><p><a href=https://github.com/Azure/draft>Draft</a> aims to help you get started deploying any app to Kubernetes. It is capable of applying heuristics as to what programming language your app is written in and generates a Dockerfile along with a Helm chart. It then runs the build for you and deploys resulting image to the target cluster via the Helm chart. It also allows user to setup port forwarding to localhost very easily.</p><p>Implications:</p><ul><li>User can customise the chart and Dockerfile templates however they like, or even create a <a href=https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md>custom pack</a> (with Dockerfile, the chart and more) for future use</li><li>It’s not very simple to guess how just any app is supposed to be built, in some cases user may need to tweak Dockerfile and the Helm chart that Draft generates</li><li>With <a href=https://github.com/Azure/draft/releases/tag/v0.12.0>Draft version 0.12.0</a> or older, every time user wants to test a change, they need to wait for Draft to copy the code to the cluster, then run the build, push the image and release updated chart; this can timely, but it results in an image being for every single change made by the user (whether it was committed to git or not)</li><li>As of Draft version 0.12.0, builds are executed locally</li><li>User doesn’t have an option to choose something other than Helm for deployment</li><li>It can watch local changes and trigger deployments, but this feature is not enabled by default</li><li>It allows developer to use either local or remote Kubernetes cluster</li><li>Deploying to production is up to the user, Draft authors recommend their other project – Brigade</li><li>Can be used instead of Skaffold, and along the side of Squash</li></ul><p>More info:</p><ul><li><a href=https://kubernetes.io/blog/2017/05/draft-kubernetes-container-development>Draft: Kubernetes container development made easy</a></li><li><a href=https://github.com/Azure/draft/blob/master/docs/getting-started.md>Getting Started Guide</a></li></ul><h3 id=skaffold>Skaffold</h3><p><a href=https://github.com/GoogleCloudPlatform/skaffold>Skaffold</a> is a tool that aims to provide portability for CI integrations with different build system, image registry and deployment tools. It is different from Draft, yet somewhat comparable. It has a basic capability for generating manifests, but it’s not a prominent feature. Skaffold is extendible and lets user pick tools for use in each of the steps in building and deploying their app.</p><p>Implications:</p><ul><li>Modular by design</li><li>Works independently of CI vendor, user doesn’t need Docker or Kubernetes plugin</li><li>Works without CI as such, i.e. from the developer’s laptop</li><li>It can watch local changes and trigger deployments</li><li>It allows developer to use either local or remote Kubernetes cluster</li><li>It can be used to deploy to production, user can configure how exactly they prefer to do it and provide different kind of pipeline for each target environment</li><li>Can be used instead of Draft, and along the side with most other tools</li></ul><p>More info:</p><ul><li><a href=https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html>Introducing Skaffold: Easy and repeatable Kubernetes development</a></li><li><a href=https://github.com/GoogleCloudPlatform/skaffold#getting-started-with-local-tooling>Getting Started Guide</a></li></ul><h3 id=squash>Squash</h3><p><a href=https://github.com/solo-io/squash>Squash</a> consists of a debug server that is fully integrated with Kubernetes, and a IDE plugin. It allows you to insert breakpoints and do all the fun stuff you are used to doing when debugging an application using an IDE. It bridges IDE debugging experience with your Kubernetes cluster by allowing you to attach the debugger to a pod running in your Kubernetes cluster.</p><p>Implications:</p><ul><li>Can be used independently of other tools you chose</li><li>Requires a privileged DaemonSet</li><li>Integrates with popular IDEs</li><li>Supports Go, Python, Node.js, Java and gdb</li><li>User must ensure application binaries inside the container image are compiled with debug symbols</li><li>Can be used in combination with any other tools described here</li><li>It can be used with either local or remote Kubernetes cluster</li></ul><p>More info:</p><ul><li><a href="https://www.youtube.com/watch?v=5TrV3qzXlgI">Squash: A Debugger for Kubernetes Apps</a></li><li><a href=https://squash.solo.io/overview/>Getting Started Guide</a></li></ul><h3 id=telepresence>Telepresence</h3><p><a href=https://www.telepresence.io/>Telepresence</a> connects containers running on developer’s workstation with a remote Kubernetes cluster using a two-way proxy and emulates in-cluster environment as well as provides access to config maps and secrets. It aims to improve iteration time for container app development by eliminating the need for deploying app to the cluster and leverages local container to abstract network and filesystem interface in order to make it appear as if the app was running in the cluster.</p><p>Implications:</p><ul><li>It can be used independently of other tools you chose</li><li>Using together with Squash is possible, although Squash would have to be used for pods in the cluster, while conventional/local debugger would need to be used for debugging local container that’s connected to the cluster via Telepresence</li><li>Telepresence imposes some network latency</li><li>It provides connectivity via a side-car process - sshuttle, which is based on SSH</li><li>More intrusive dependency injection mode with LD_PRELOAD/DYLD_INSERT_LIBRARIES is also available</li><li>It is most commonly used with a remote Kubernetes cluster, but can be used with a local one also</li></ul><p>More info:</p><ul><li><a href=https://www.telepresence.io/>Telepresence: fast, realistic local development for Kubernetes microservices</a></li><li><a href=https://www.telepresence.io/tutorials/docker>Getting Started Guide</a></li><li><a href=https://www.telepresence.io/discussion/how-it-works>How It Works</a></li></ul><h3 id=ksync>Ksync</h3><p><a href=https://github.com/vapor-ware/ksync>Ksync</a> synchronizes application code (and configuration) between your local machine and the container running in Kubernetes, akin to what <a href=https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html>oc rsync</a> does in OpenShift. It aims to improve iteration time for app development by eliminating build and deployment steps.</p><p>Implications:</p><ul><li>It bypasses container image build and revision control</li><li>Compiled language users have to run builds inside the pod (TBC)</li><li>Two-way sync – remote files are copied to local directory</li><li>Container is restarted each time remote filesystem is updated</li><li>No security features – development only</li><li>Utilizes <a href=https://github.com/syncthing/syncthing>Syncthing</a>, a Go library for peer-to-peer sync</li><li>Requires a privileged DaemonSet running in the cluster</li><li>Node has to use Docker with overlayfs2 – no other CRI implementations are supported at the time of writing</li></ul><p>More info:</p><ul><li><a href=https://github.com/vapor-ware/ksync#getting-started>Getting Started Guide</a></li><li><a href=https://github.com/vapor-ware/ksync/blob/master/docs/architecture.md>How It Works</a></li><li><a href=https://www.katacoda.com/vaporio/scenarios/ksync>Katacoda scenario to try out ksync in your browser</a></li><li><a href=https://docs.syncthing.net/specs/>Syncthing Specification</a></li></ul><h2 id=hands-on-walkthroughs>Hands-on walkthroughs</h2><p>The app we will be using for the hands-on walkthroughs of the tools in the following is a simple <a href=https://github.com/kubernauts/dok-example-us>stock market simulator</a>, consisting of two microservices:</p><ul><li>The <code>stock-gen</code> microservice is written in Go and generates stock data randomly and exposes it via HTTP endpoint <code>/stockdata</code>.
‎* A second microservice, <code>stock-con</code> is a Node.js app that consumes the stream of stock data from <code>stock-gen</code> and provides an aggregation in form of a moving average via the HTTP endpoint <code>/average/$SYMBOL</code> as well as a health-check endpoint at <code>/healthz</code>.</li></ul><p>Overall, the default setup of the app looks as follows:</p><p><img src=/images/blog/2018-05-01-developing-on-kubernetes/dok-architecture_preview.png alt="Default Setup"></p><p>In the following we’ll do a hands-on walkthrough for a representative selection of tools discussed above: ksync, Minikube with local build, as well as Skaffold. For each of the tools we do the following:</p><ul><li>Set up the respective tool incl. preparations for the deployment and local consumption of the <code>stock-con</code> microservice.</li><li>Perform a code update, that is, change the source code of the <code>/healthz</code> endpoint in the <code>stock-con</code> microservice and observe the updates.</li></ul><p>Note that for the target Kubernetes cluster we’ve been using Minikube locally, but you can also a remote cluster for ksync and Skaffold if you want to follow along.</p><h2 id=walkthrough-ksync>Walkthrough: ksync</h2><p>As a preparation, install <a href=https://vapor-ware.github.io/ksync/#installation>ksync</a> and then carry out the following steps to prepare the development setup:</p><pre><code>$ mkdir -p $(pwd)/ksync
$ kubectl create namespace dok
$ ksync init -n dok
</code></pre><p>With the basic setup completed we're ready to tell ksync’s local client to watch a certain Kubernetes namespace and then we create a spec to define what we want to sync (the directory <code>$(pwd)/ksync</code> locally with <code>/app</code> in the container). Note that target pod is specified via the selector parameter:</p><pre><code>$ ksync watch -n dok
$ ksync create -n dok --selector=app=stock-con $(pwd)/ksync /app
$ ksync get -n dok
</code></pre><p>Now we deploy the stock generator and the stock consumer microservice:</p><pre><code>$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-gen/app.yaml
$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-con/app.yaml
</code></pre><p>Once both deployments are created and the pods are running, we forward the <code>stock-con</code> service for local consumption (in a separate terminal session):</p><pre><code>$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898
</code></pre><p>With that we should be able to consume the <code>stock-con</code> service from our local machine; we do this by regularly checking the response of the <code>healthz</code> endpoint like so (in a separate terminal session):</p><pre><code>$ watch curl localhost:9898/healthz
</code></pre><p>Now change the code in the <code>ksync/stock-con</code>directory, for example update the <a href=https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52><code>/healthz</code> endpoint code in <code>service.js</code></a> by adding a field to the JSON response and observe how the pod gets updated and the response of the <code>curl localhost:9898/healthz</code> command changes. Overall you should have something like the following in the end:</p><p><img src=/images/blog/2018-05-01-developing-on-kubernetes/dok-ksync_preview.png alt=Preview></p><h3 id=walkthrough-minikube-with-local-build>Walkthrough: Minikube with local build</h3><p>For the following you will need to have Minikube up and running and we will leverage the Minikube-internal Docker daemon for building images, locally. As a preparation, do the following</p><pre><code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;&amp; cd dok-example-us
$ eval $(minikube docker-env)
$ kubectl create namespace dok
</code></pre><p>Now we deploy the stock generator and the stock consumer microservice:</p><pre><code>$ kubectl -n=dok apply -f stock-gen/app.yaml
$ kubectl -n=dok apply -f stock-con/app.yaml
</code></pre><p>Once both deployments are created and the pods are running, we forward the <code>stock-con</code> service for local consumption (in a separate terminal session) and check the response of the <code>healthz</code> endpoint:</p><pre><code>$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;
$ watch curl localhost:9898/healthz
</code></pre><p>Now change the code in the <code>stock-con</code>directory, for example, update the <a href=https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52><code>/healthz</code> endpoint code in <code>service.js</code></a> by adding a field to the JSON response. Once you’re done with your code update, the last step is to build a new container image and kick off a new deployment like shown below:</p><pre><code>$ docker build -t stock-con:dev -f Dockerfile .
$ kubectl -n dok set image deployment/stock-con *=stock-con:dev
</code></pre><p>Overall you should have something like the following in the end:</p><p><img src=/images/blog/2018-05-01-developing-on-kubernetes/dok-minikube-localdev_preview.png alt="Local Preview"></p><h3 id=walkthrough-skaffold>Walkthrough: Skaffold</h3><p>To perform this walkthrough you first need to install <a href=https://github.com/GoogleContainerTools/skaffold#installation>Skaffold</a>. Once that is done, you can do the following steps to prepare the development setup:</p><pre><code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;&amp; cd dok-example-us
$ kubectl create namespace dok
</code></pre><p>Now we deploy the stock generator (but not the stock consumer microservice, that is done via Skaffold):</p><pre><code>$ kubectl -n=dok apply -f stock-gen/app.yaml
</code></pre><p>Note that initially we experienced an authentication error when doing <code>skaffold dev</code> and needed to apply a fix as described in <a href=https://github.com/GoogleContainerTools/skaffold/issues/322>Issue 322</a>. Essentially it means changing the content of <code>~/.docker/config.json</code> to:</p><pre><code>{
   &quot;auths&quot;: {}
}
</code></pre><p>Next, we had to patch <code>stock-con/app.yaml</code> slightly to make it work with Skaffold:</p><p>Add a <code>namespace</code> field to both the <code>stock-con</code> deployment and the service with the value of <code>dok</code>.
Change the <code>image</code> field of the container spec to <code>quay.io/mhausenblas/stock-con</code> since Skaffold manages the container image tag on the fly.</p><p>The resulting <code>app.yaml</code> file stock-con looks as follows:</p><pre><code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: stock-con
    spec:
      containers:
      - name: stock-con
        image: quay.io/mhausenblas/stock-con
        env:
        - name: DOK_STOCKGEN_HOSTNAME
          value: stock-gen
        - name: DOK_STOCKGEN_PORT
          value: &quot;9999&quot;
        ports:
        - containerPort: 9898
          protocol: TCP
        livenessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
        readinessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 9898
  selector:
    app: stock-con
</code></pre><p>The final step before we can start development is to configure Skaffold. So, create a file <code>skaffold.yaml</code> in the <code>stock-con/</code> directory with the following content:</p><pre><code>apiVersion: skaffold/v1alpha2
kind: Config
build:
  artifacts:
  - imageName: quay.io/mhausenblas/stock-con
    workspace: .
    docker: {}
  local: {}
deploy:
  kubectl:
    manifests:
      - app.yaml
</code></pre><p>Now we’re ready to kick off the development. For that execute the following in the <code>stock-con/</code> directory:</p><pre><code>$ skaffold dev
</code></pre><p>Above command triggers a build of the <code>stock-con</code> image and then a deployment. Once the pod of the <code>stock-con</code> deployment is running, we again forward the <code>stock-con</code> service for local consumption (in a separate terminal session) and check the response of the <code>healthz</code> endpoint:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl get -n dok po --selector<span style=color:#666>=</span><span style=color:#b8860b>app</span><span style=color:#666>=</span>stock-con  <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>                     -o<span style=color:#666>=</span>custom-columns<span style=color:#666>=</span>:metadata.name --no-headers |  <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;
$ watch curl localhost:9898/healthz
</code></pre></div><p>If you now change the code in the <code>stock-con</code>directory, for example, by updating the <a href=https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52><code>/healthz</code> endpoint code in <code>service.js</code></a> by adding a field to the JSON response, you should see Skaffold noticing the change and create a new image as well as deploy it. The resulting screen would look something like this:</p><p><img src=/images/blog/2018-05-01-developing-on-kubernetes/dok-skaffold_preview.png alt="Skaffold Preview"></p><p>By now you should have a feeling how different tools enable you to develop apps on Kubernetes and if you’re interested to learn more about tools and or methods, check out the following resources:</p><ul><li>Blog post by Shahidh K Muhammed on <a href=https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948>Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold</a> (03/2018)</li><li>Blog post by Gergely Nemeth on <a href=https://nemethgergely.com/using-kubernetes-for-local-development/index.html>Using Kubernetes for Local Development</a>, with a focus on Skaffold (03/2018)</li><li>Blog post by Richard Li on <a href=https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99>Locally developing Kubernetes services (without waiting for a deploy)</a>, with a focus on Telepresence</li><li>Blog post by Abhishek Tiwari on <a href=https://abhishek-tiwari.com/local-development-environment-for-kubernetes-using-minikube/>Local Development Environment for Kubernetes using Minikube</a> (09/2017)</li><li>Blog post by Aymen El Amri on <a href=https://medium.com/devopslinks/using-kubernetes-minikube-for-local-development-c37c6e56e3db>Using Kubernetes for Local Development — Minikube</a> (08/2017)</li><li>Blog post by Alexis Richardson on <a href=https://www.weave.works/blog/gitops-operations-by-pull-request>​GitOps - Operations by Pull Request</a> (08/2017)</li><li>Slide deck <a href=https://docs.google.com/presentation/d/1d3PigRVt_m5rO89Ob2XZ16bW8lRSkHHH5k816-oMzZo/>GitOps: Drive operations through git</a>, with a focus on Gitkube by Tirumarai Selvan (03/2018)</li><li>Slide deck <a href=https://speakerdeck.com/mhausenblas/developing-apps-on-kubernetes>Developing apps on Kubernetes</a>, a talk Michael Hausenblas gave at a CNCF Paris meetup (04/2018)</li><li>YouTube videos:<ul><li><a href="https://www.youtube.com/watch?v=QW85Y0Ug3KY">TGI Kubernetes 029: Developing Apps with Ksync</a></li><li><a href="https://www.youtube.com/watch?v=McwwWhCXMxc">TGI Kubernetes 030: Exploring Skaffold</a></li><li><a href="https://www.youtube.com/watch?v=zezeBAJ_3w8">TGI Kubernetes 031: Connecting with Telepresence</a></li><li><a href="https://www.youtube.com/watch?v=8B1D7cTMPgA">TGI Kubernetes 033: Developing with Draft</a></li></ul></li><li>Raw responses to the <a href=https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit>Kubernetes Application Survey</a> 2018 by SIG Apps</li></ul><p>With that we wrap up this post on how to go about developing apps on Kubernetes, we hope you learned something and if you have feedback and/or want to point out a tool that you found useful, please let us know via Twitter: <a href=https://twitter.com/errordeveloper>Ilya</a> and <a href=https://twitter.com/mhausenblas>Michael</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c43d9f2ccc16c77f1acf7bacc29a1d6a>Zero-downtime Deployment in Kubernetes with Jenkins</h1><div class="td-byline mb-4">By <b>kbarnard</b> |
<time datetime=2018-04-30 class=text-muted>Monday, April 30, 2018</time></div><p>Ever since we added the <a href=https://aka.ms/azjenkinsk8s>Kubernetes Continuous Deploy</a> and <a href=https://aka.ms/azjenkinsacs>Azure Container Service</a> plugins to the Jenkins update center, "How do I create zero-downtime deployments" is one of our most frequently-asked questions. We created a quickstart template on Azure to demonstrate what zero-downtime deployments can look like. Although our example uses Azure, the concept easily applies to all Kubernetes installations.</p><h2 id=rolling-update>Rolling Update</h2><p>Kubernetes supports the RollingUpdate strategy to replace old pods with new ones gradually, while continuing to serve clients without incurring downtime. To perform a RollingUpdate deployment:</p><ul><li>Set <code>.spec.strategy.type</code> to <code>RollingUpdate</code> (the default value).</li><li>Set <code>.spec.strategy.rollingUpdate.maxUnavailable</code> and <code>.spec.strategy.rollingUpdate.maxSurge</code> to some reasonable value.<ul><li><code>maxUnavailable</code>: the maximum number of pods that can be unavailable during the update process. This can be an absolute number or percentage of the replicas count; the default is 25%.</li><li><code>maxSurge</code>: the maximum number of pods that can be created over the desired number of pods. Again this can be an absolute number or a percentage of the replicas count; the default is 25%.</li></ul></li><li>Configure the <code>readinessProbe</code> for your service container to help Kubernetes determine the state of the pods. Kubernetes will only route the client traffic to the pods with a healthy liveness probe.</li></ul><p>We'll use deployment of the official Tomcat image to demonstrate this:</p><pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tomcat-deployment-rolling-update
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: tomcat
        role: rolling-update
    spec:
      containers:
      - name: tomcat-container
        image: tomcat:${TOMCAT_VERSION}
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /
            port: 8080
  strategy:
    type: RollingUpdate
    rollingUp      maxSurge: 50%
</code></pre><p>If the Tomcat running in the current deployments is version 7, we can replace <code>${TOMCAT_VERSION}</code> with 8 and apply this to the Kubernetes cluster. With the <a href=https://aka.ms/azjenkinsk8s>Kubernetes Continuous Deploy</a> or the <a href=https://aka.ms/azjenkinsacs>Azure Container Service</a> plugin, the value can be fetched from an environment variable which eases the deployment process.</p><p>Behind the scenes, Kubernetes manages the update like so:</p><p><img src=/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/deployment-process.png alt="Deployment Process"></p><ul><li>Initially, all pods are running Tomcat 7 and the frontend Service routes the traffic to these pods.</li><li>During the rolling update, Kubernetes takes down some Tomcat 7 pods and creates corresponding new Tomcat 8 pods. It ensures:<ul><li>at most <code>maxUnavailable</code> pods in the desired Pods can be unavailable, that is, at least (<code>replicas</code> - <code>maxUnavailable</code>) pods should be serving the client traffic, which is 2-1=1 in our case.</li><li>at most maxSurge more pods can be created during the update process, that is 2*50%=1 in our case.</li></ul></li><li>One Tomcat 7 pod is taken down, and one Tomcat 8 pod is created. Kubernetes will not route the traffic to any of them because their readiness probe is not yet successful.</li><li>When the new Tomcat 8 pod is ready as determined by the readiness probe, Kubernetes will start routing the traffic to it. This means during the update process, users may see both the old service and the new service.</li><li>The rolling update continues by taking down Tomcat 7 pods and creating Tomcat 8 pods, and then routing the traffic to the ready pods.</li><li>Finally, all pods are on Tomcat 8.</li></ul><p>The Rolling Update strategy ensures we always have some Ready backend pods serving client requests, so there's no service downtime. However, some extra care is required:</p><ul><li>During the update, both the old pods and new pods may serve the requests. Without well defined session affinity in the Service layer, a user may be routed to the new pods and later back to the old pods.</li><li>This also requires you to maintain well-defined forward and backward compatibility for both data and the API, which can be challenging.</li><li>It may take a long time before a pod is ready for traffic after it is started. There may be a long window of time where the traffic is served with less backend pods than usual. Generally, this should not be a problem as we tend to do production upgrades when the service is less busy. But this will also extend the time window for issue 1.</li><li>We cannot do comprehensive tests for the new pods being created. Moving application changes from dev / QA environments to production can represent a persistent risk of breaking existing functionality. The readiness probe can do some work to check readiness, however, it should be a lightweight task that can be run periodically, and not suitable to be used as an entry point to start the complete tests.</li></ul><h2 id=blue-green-deployment>Blue/green Deployment</h2><p><em>Blue/green deployment quoted from TechTarget</em></p><blockquote><p>A blue/green deployment is a change management strategy for releasing software code. Blue/green deployments, which may also be referred to as A/B deployments require two identical hardware environments that are configured exactly the same way. While one environment is active and serving end users, the other environment remains idle.</p></blockquote><p>Container technology offers a stand-alone environment to run the desired service, which makes it super easy to create identical environments as required in the blue/green deployment. The loosely coupled Services - ReplicaSets, and the label/selector-based service routing in Kubernetes make it easy to switch between different backend environments. With these techniques, the blue/green deployments in Kubernetes can be done as follows:</p><ul><li>Before the deployment, the infrastructure is prepared like so:<ul><li>Prepare the blue deployment and green deployment with <code>TOMCAT_VERSION=7</code> and <code>TARGET_ROLE</code> set to blue or green respectively.</li></ul></li></ul><pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tomcat-deployment-${TARGET_ROLE}
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: tomcat
        role: ${TARGET_ROLE}
    spec:
      containers:
      - name: tomcat-container
        image: tomcat:${TOMCAT_VERSION}
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /
            port: 8080
</code></pre><ul><li>Prepare the public service endpoint, which initially routes to one of the backend environments, say <code>TARGET_ROLE=blue</code>.</li></ul><pre><code>kind: Service
apiVersion: v1
metadata:
  name: tomcat-service
  labels:
    app: tomcat
    role: ${TARGET_ROLE}
    env: prod
spec:
  type: LoadBalancer
  selector:
    app: tomcat
    role: ${TARGET_ROLE}
  ports:
    - port: 80
      targetPort: 8080
</code></pre><ul><li>Optionally, prepare a test endpoint so that we can visit the backend environments for testing. They are similar to the public service endpoint, but they are intended to be accessed internally by the dev/ops team only.</li></ul><pre><code>kind: Service
apiVersion: v1
metadata:
  name: tomcat-test-${TARGET_ROLE}
  labels:
    app: tomcat
    role: test-${TARGET_ROLE}
spec:
  type: LoadBalancer
  selector:
    app: tomcat
    role: ${TARGET_ROLE}
  ports:
    - port: 80
      targetPort: 8080
</code></pre><ul><li>Update the application in the inactive environment, say green environment. Set <code>TARGET_ROLE=green</code> and <code>TOMCAT_VERSION=8</code> in the deployment config to update the green environment.</li><li>Test the deployment via the <code>tomcat-test-green</code> test endpoint to ensure the green environment is ready to serve client traffic.</li><li>Switch the frontend Service routing to the green environment by updating the Service config with <code>TARGET_ROLE=green</code>.</li><li>Run additional tests on the public endpoint to ensure it is working properly.</li><li>Now the blue environment is idle and we can:<ul><li>leave it with the old application so that we can roll back if there's issue with the new application</li><li>update it to make it a hot backup of the active environment</li><li>reduce its replica count to save the occupied resources</li></ul></li></ul><p><img src=/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/resources.png alt=Resources></p><p>As compared to Rolling Update, the blue/green up* The public service is either routed to the old applications, or new applications, but never both at the same time.</p><ul><li>The time it takes for the new pods to be ready does not affect the public service quality, as the traffic will only be routed to the new pods when all of them are tested to be ready.</li><li>We can do comprehensive tests on the new environment before it serves any public traffic. Just keep in mind this is in production, and the tests should not pollute live application data.</li></ul><h2 id=jenkins-automation>Jenkins Automation</h2><p>Jenkins provides easy-to-setup workflow to automate your deployments. With <a href=https://jenkins.io/doc/book/pipeline/>Pipeline</a> support, it is flexible to build the zero-downtime deployment workflow, and visualize the deployment steps.
To facilitate the deployment process for Kubernetes resources, we published the <a href=https://aka.ms/azjenkinsk8s>Kubernetes Continuous Deploy</a> and the <a href=https://aka.ms/azjenkinsacs>Azure Container Service</a> plugins built based on the <a href=https://github.com/fabric8io/kubernetes-client>kubernetes-client</a>. You can deploy the resource to Azure Kubernetes Service (AKS) or the general Kubernetes clusters without the need of kubectl, and it supports variable substitution in the resource configuration so you can deploy environment-specific resources to the clusters without updating the resource config.
We created a Jenkins Pipeline to demonstrate the blue/green deployment to AKS. The flow is like the following:</p><p><img src=/images/blog/2018-04-30-zero-downtime-deployment-kubernetes-jenkins/jenkins-pipeline.png alt="Jenkins Pipeline"></p><ul><li>Pre-clean: clean workspace.</li><li>SCM: pulling code from the source control management system.</li><li>Prepare Image: prepare the application docker images and upload them to some Docker repository.</li><li>Check Env: determine the active and inactive environment, which drives the following deployment.</li><li>Deploy: deploy the new application resource configuration to the inactive environment. With the Azure Container Service plugin, this can be done with:</li></ul><pre><code>acsDeploy azureCredentialsId: 'stored-azure-credentials-id',
          configFilePaths: &quot;glob/path/to/*/resource-config-*.yml&quot;,
          containerService: &quot;aks-name | AKS&quot;,
          resourceGroupName: &quot;resource-group-name&quot;,
          enableConfigSubstitution: true
</code></pre><ul><li>Verify Staged: verify the deployment to the inactive environment to ensure it is working properly. Again, note this is in the production environment, so be careful not to pollute live application data during tests.</li><li>Confirm: Optionally, send email notifications for manual user approval to proceed with the actual environment switch.</li><li>Switch: Switch the frontend service endpoint routing to the inactive environment. This is just another service deployment to the AKS Kubernetes cluster.</li><li>Verify Prod: verify the frontend service endpoint is working properly with the new environment.</li><li>Post-clean: do some post clean on the temporary files.</li></ul><p>For the Rolling Update strategy, simply deploy the deployment configuration to the Kubernetes cluster, which is a simple, single step.</p><h2 id=put-it-all-together>Put It All Together</h2><p>We built a quickstart template on Azure to demonstrate how we can do the zero-downtime deployment to AKS (Kubernetes) with Jenkins. Go to <a href=https://aka.ms/azjenkinsk8sqs>Jenkins Blue-Green Deployment on Kubernetes</a> and click the button Deploy to Azure to get the working demo. This template will provision:</p><ul><li>An AKS cluster, with the following resources:<ul><li>Two similar deployments representing the environments "blue" and "green". Both are initially set up with the <code>tomcat</code>:7 image.</li><li>Two test endpoint services (<code>tomcat-test-blue</code> and <code>tomcat-test-green</code>), which are connected to the corresponding deployments, and can be used to test if the deployments are ready for production use.</li><li>A production service endpoint (<code>tomcat-service</code>) which represents the public endpoint that the users will access. Initially it is routing to the "blue" environment.</li></ul></li><li>A Jenkins master running on an Ubuntu 16.04 VM, with the Azure service principal credentials configured. The Jenkins instance has two sample jobs:<ul><li>AKS Kubernetes Rolling Update Deployment pipeline to demonstrate the Rolling Update deployment to AKS.</li><li>AKS Kubernetes Blue/green Deployment pipeline to demonstrate the blue/green deployment to AKS.</li><li>We didn't include the email confirmation step in the quickstart template. To add that, you need to configure the email SMTP server details in the Jenkins system configuration, and then add a Pipeline stage before Switch:</li></ul></li></ul><pre><code>stage('Confirm') {
    mail (to: 'to@example.com',
        subject: &quot;Job '${env.JOB_NAME}' (${env.BUILD_NUMBER}) is waiting for input&quot;,
        body: &quot;Please go to ${env.BUILD_URL}.&quot;)
    input 'Ready to go?'
}
</code></pre><p>Follow the <a href=https://github.com/Azure/azure-quickstart-templates/tree/master/301-jenkins-aks-zero-downtime-deployment#steps>Steps</a> to setup the resources and you can try it out by start the Jenkins build jobs.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0b6913088687941c6716b37e47562f16>Kubernetes Community - Top of the Open Source Charts in 2017</h1><div class="td-byline mb-4"><time datetime=2018-04-25 class=text-muted>Wednesday, April 25, 2018</time></div><p>2017 was a huge year for Kubernetes, and GitHub’s latest <a href=https://octoverse.github.com>Octoverse report</a> illustrates just how much attention this project has been getting.</p><p>Kubernetes, an <a href=/docs/concepts/overview/what-is-kubernetes/>open source platform for running application containers</a>, provides a consistent interface that enables developers and ops teams to automate the deployment, management, and scaling of a wide variety of applications on just about any infrastructure.</p><p>Solving these shared challenges by leveraging a wide community of expertise and industrial experience, as Kubernetes does, helps engineers focus on building their own products at the top of the stack, rather than needlessly duplicating work that now exists as a standard part of the “cloud native” toolkit.</p><p>However, achieving these gains via ad-hoc collective organizing is its own unique challenge, one which makes it increasingly difficult to support open source, community-driven efforts through periods of rapid growth.</p><p>Read on to find out how the Kubernetes Community has addressed these scaling challenges to reach the top of the charts in GitHub’s 2017 Octoverse report.</p><h2 id=most-discussed-on-github>Most-Discussed on GitHub</h2><p>The top two most-discussed repos of 2017 are both based on Kubernetes:</p><p><img src=/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png alt="Most Discussed"></p><p>Of all the open source repositories on GitHub, none received more issue comments than <a href=https://github.com/kubernetes/kubernetes/>kubernetes/kubernetes</a>. <a href=http://openshift.com/>OpenShift</a>, a <a href=https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/>CNCF certified distribution of Kubernetes</a>, took second place.</p><p>Open discussion with ample time for community feedback and review helps build shared infrastructure and establish new standards for cloud native computing.</p><h2 id=most-reviewed-on-github>Most Reviewed on GitHub</h2><p>Successfully scaling an open source effort’s communications often leads to better coordination and higher-quality feature delivery. The Kubernetes project’s <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Group (SIG)</a> structure has helped it become GitHub’s second most reviewed project:</p><p><img src=/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png alt="Most Reviewed"></p><p>Using SIGs to segment and standardize mechanisms for community participation helps channel more frequent reviews from better-qualified community members.</p><p>When managed effectively, active community discussions indicate more than just a highly contentious codebase, or a project with an extensive list of unmet needs.</p><p>Scaling a project’s capacity to handle issues and community interactions helps to expand the conversation. Meanwhile, large communities come with more diverse use cases and a larger array of support problems to manage. The Kubernetes <a href=https://github.com/kubernetes/community#sigs>SIG organization structure</a> helps to address the challenges of complex communication at scale.</p><p>SIG meetings provide focused opportunities for users, maintainers, and specialists from various disciplines to collaborate together in support of this community effort. These investments in organizing help create an environment where it’s easier to prioritize architecture discussion and planning over commit velocity; enabling the project to sustain this kind of scale.</p><h2 id=join-the-party>Join the party!</h2><p>You may already be using solutions that are successfully managed and scaled on Kubernetes. For example, GitHub.com, which hosts Kubernetes’ upstream source code, <a href=https://githubengineering.com/kubernetes-at-github/>now runs on Kubernetes</a> as well!</p><p>Check out the <a href=https://github.com/kubernetes/community/blob/master/contributors/guide/README.md>Kubernetes Contributors’ guide</a> for more information on how to get started as a contributor.</p><p>You can also join the <a href=https://github.com/kubernetes/community/tree/master/communication#weekly-meeting>weekly Kubernetes Community meeting</a> and consider <a href=https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list>joining a SIG or two</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7873247c56ddad517959742a96b86aa0>Kubernetes Application Survey 2018 Results</h1><div class="td-byline mb-4">By <b>mattfarina</b> |
<time datetime=2018-04-24 class=text-muted>Tuesday, April 24, 2018</time></div><p>Understanding how people use or want to use Kubernetes can help us shape everything from what we build to how we do it. To help us understand how application developers, application operators, and ecosystem tool developers are using and want to use Kubernetes, the Application Definition Working Group recently performed a survey. The survey focused in on these types of user roles and the features and sub-projects owned by the Kubernetes organization. That included kubectl, Dashboard, Minikube, Helm, the Workloads API, etc.</p><p>The results are in and the <a href="https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit?usp=sharing">raw data is now available</a> for everyone.</p><p>There is too much data to summarize in a single blog post and we hope people will be able to find useful information by pouring over the data. Here are some of the highlights that caught our attention.</p><h2 id=participation>Participation</h2><p>First, I would like to thank the 380 people who took the survey and provided feedback. We appreciate the time put into to share so much detail.</p><h2 id=6-8x-response-increase>6.8x Response Increase</h2><p>In the summer of <a href=https://kubernetes.io/blog/2016/08/sig-apps-running-apps-in-kubernetes>2016 we ran a survey on application usage</a>. Kubernetes was much newer and the number of people talking about operating applications was much smaller.</p><p>The number of respondents in the past year and 10 months increased at a rate of 6.8 times.</p><h2 id=where-are-we-in-innovation-lifecycle>Where Are We In Innovation Lifecycle?</h2><p><img src=/images/blog/survey-results/2018-application-survey/minikube-os-usage.png alt="Minikube operating system usage"></p><p>Minikube is used primarily by people on macOS and Linux. Yet, according to the 2018 Stack Overflow survey, <a href=https://insights.stackoverflow.com/survey/2018/#technology-developers-primary-operating-systems>almost half of developers use Windows as their primary operating system</a>. This is where Minikube would run.</p><p>Seeing differences from other data sets is worth looking more deeply at to better understand our audience, where Kubernetes is at, and where it is on the journey it's headed.</p><h2 id=plenty-of-custom-tooling>Plenty of Custom Tooling</h2><p><img src=/images/blog/survey-results/2018-application-survey/custom-tooling.png alt="Custom Tooling"></p><p>Two thirds of respondents work for organizations developing their own tooling to help with application development and operation. We wondered why this might happen so we asked why as a follow-up question. <em>44% of people who took the survey told us why they do it.</em></p><h2 id=app-management-tools>App Management Tools</h2><p><img src=/images/blog/survey-results/2018-application-survey/tool-manage-apps.png alt="Custom Tooling"></p><p>Only 4 tools were in use by more than 10% of those who took the survey with Helm being in use by 64% of them. Many more tools were used by more than 1% of people including those we directly asked about and the space for people to fill in those we didn't ask about. The long tail, captured in the survey, contained more than 80 tools in use.</p><h2 id=want-to-see-more>Want To See More?</h2><p>As the <a href=https://github.com/kubernetes/community/tree/master/sig-apps>Application Definition Working Group</a> is working through the data we're putting observations into a <a href=http://bit.ly/2qTkuhx>Google Slides Document</a>. This is a living document that will continue to grow while we look over and discuss the data.</p><p>There is <a href=https://kccnceu18.sched.com/event/DxV4>a session at KubeCon where the Application Definition Working Group will be meeting</a> and discussing the survey. This is a session open to anyone in attendance, if you would like to attend.</p><p>While this working group is doing analysis and sharing it, we want to encourage others to look at the data and share any insights that might be gained.</p><p><em>Note, the survey questions were generated by the application definition working group with the help of people working on various sub-projects included in the survey. This is the reason some sub-projects have more and varied questions compared to some others. The survey was shared on social media, on mailing lists, in blog posts, in various meetings, and beyond while collecting information for two weeks.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-64a93ffbe0608e658b1e538bcc0cfba4>Local Persistent Volumes for Kubernetes Goes Beta</h1><div class="td-byline mb-4"><time datetime=2018-04-13 class=text-muted>Friday, April 13, 2018</time></div><p>The <a href=/docs/concepts/storage/volumes/#local>Local Persistent Volumes</a> beta feature in Kubernetes 1.10 makes it possible to leverage local disks in your StatefulSets. You can specify directly-attached local disks as PersistentVolumes, and use them in StatefulSets with the same PersistentVolumeClaim objects that previously only supported remote volume types.</p><p>Persistent storage is important for running stateful applications, and Kubernetes has supported these workloads with StatefulSets, PersistentVolumeClaims and PersistentVolumes. These primitives have supported remote volume types well, where the volumes can be accessed from any node in the cluster, but did not support local volumes, where the volumes can only be accessed from a specific node. The demand for using local, fast SSDs in replicated, stateful workloads has increased with demand to run more workloads in Kubernetes.</p><h2 id=addressing-hostpath-challenges>Addressing hostPath challenges</h2><p>The prior mechanism of accessing local storage through hostPath volumes had many challenges. hostPath volumes were difficult to use in production at scale: operators needed to care for local disk management, topology, and scheduling of individual pods when using hostPath volumes, and could not use many Kubernetes features (like StatefulSets). Existing Helm charts that used remote volumes could not be easily ported to use hostPath volumes. The Local Persistent Volumes feature aims to address hostPath volumes’ portability, disk accounting, and scheduling challenges.</p><h2 id=disclaimer>Disclaimer</h2><p>Before going into details about how to use Local Persistent Volumes, note that local volumes are not suitable for most applications. Using local storage ties your application to that specific node, making your application harder to schedule. If that node or local volume encounters a failure and becomes inaccessible, then that pod also becomes inaccessible. In addition, many cloud providers do not provide extensive data durability guarantees for local storage, so you could lose all your data in certain scenarios.</p><p>For those reasons, most applications should continue to use highly available, remotely accessible, durable storage.</p><h2 id=suitable-workloads>Suitable workloads</h2><p>Some use cases that are suitable for local storage include:</p><ul><li>Caching of datasets that can leverage data gravity for fast processing</li><li>Distributed storage systems that shard or replicate data across multiple
nodes. Examples include distributed datastores like Cassandra, or distributed
file systems like Gluster or Ceph.</li></ul><p>Suitable workloads are tolerant of node failures, data unavailability, and data loss. They provide critical, latency-sensitive infrastructure services to the rest of the cluster, and should run with high priority compared to other workloads.</p><h2 id=enabling-smarter-scheduling-and-volume-binding>Enabling smarter scheduling and volume binding</h2><p>An administrator must enable smarter scheduling for local persistent volumes. Before any PersistentVolumeClaims for your local PersistentVolumes are created, a StorageClass must be created with the volumeBindingMode set to “WaitForFirstConsumer”:</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
</code></pre><p>This setting tells the PersistentVolume controller to not immediately bind a PersistentVolumeClaim. Instead, the system waits until a Pod that needs to use a volume is scheduled. The scheduler then chooses an appropriate local PersistentVolume to bind to, taking into account the Pod’s other scheduling constraints and policies. This ensures that the initial volume binding is compatible with any Pod resource requirements, selectors, affinity and anti-affinity policies, and more.</p><p>Note that dynamic provisioning is not supported in beta. All local PersistentVolumes must be statically created.</p><h2 id=creating-a-local-persistent-volume>Creating a local persistent volume</h2><p>For this initial beta offering, local disks must first be pre-partitioned, formatted, and mounted on the local node by an administrator. Directories on a shared file system are also supported, but must also be created before use.</p><p>Once you set up the local volume, you can create a PersistentVolume for it. In this example, the local volume is mounted at “/mnt/disks/vol1” on node “my-node”:</p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-local-pv
spec:
  capacity:
    storage: 500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - my-node
</code></pre><p>Note that there’s a new nodeAffinity field in the PersistentVolume object: this is how the Kubernetes scheduler understands that this PersistentVolume is tied to a specific node. nodeAffinity is a required field for local PersistentVolumes.</p><p>When local volumes are manually created like this, the only supported persistentVolumeReclaimPolicy is “Retain”. When the PersistentVolume is released from the PersistentVolumeClaim, an administrator must manually clean up and set up the local volume again for reuse.</p><h2 id=automating-local-volume-creation-and-deletion>Automating local volume creation and deletion</h2><p>Manually creating and cleaning up local volumes is a big administrative burden, so we’ve written a simple local volume manager to automate some of these pieces. It’s available in the <a href=https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume>external-storage repo</a> as an optional program that you can deploy in your cluster, including instructions and example deployment specs for how to run it.</p><p>To use this, the local volumes must still first be set up and mounted on the local node by an administrator. The administrator needs to mount the local volume into a configurable “discovery directory” that the local volume manager recognizes. Directories on a shared file system are supported, but they must be bind-mounted into the discovery directory.</p><p>This local volume manager monitors the discovery directory, looking for any new mount points. The manager creates a PersistentVolume object with the appropriate storageClassName, path, nodeAffinity, and capacity for any new mount point that it detects. These PersistentVolume objects can eventually be claimed by PersistentVolumeClaims, and then mounted in Pods.</p><p>After a Pod is done using the volume and deletes the PersistentVolumeClaim for it, the local volume manager cleans up the local mount by deleting all files from it, then deleting the PersistentVolume object. This triggers the discovery cycle: a new PersistentVolume is created for the volume and can be reused by a new PersistentVolumeClaim.</p><p>Once the administrator initially sets up the local volume mount, this local volume manager takes over the rest of the PersistentVolume lifecycle without any further administrator intervention required.</p><h2 id=using-local-volumes-in-a-pod>Using local volumes in a pod</h2><p>After all that administrator work, how does a user actually mount a local volume into their Pod? Luckily from the user’s perspective, a local volume can be requested in exactly the same way as any other PersistentVolume type: through a PersistentVolumeClaim. Just specify the appropriate StorageClassName for local volumes in the PersistentVolumeClaim object, and the system takes care of the rest!</p><pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: example-local-claim
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Gi
</code></pre><p>Or in a StatefulSet as a volumeClaimTemplate:</p><pre><code>kind: StatefulSet
...
 volumeClaimTemplates:
  - metadata:
      name: example-local-claim
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 500Gi
</code></pre><h2 id=documentation>Documentation</h2><p>The Kubernetes website provides full documentation for <a href=/docs/concepts/storage/volumes/#local>local persistent volumes</a>.</p><h2 id=future-enhancements>Future enhancements</h2><p>The local persistent volume beta feature is not complete by far. Some notable enhancements under development:</p><ul><li>Starting in 1.10, local raw block volumes is available as an alpha feature. This is useful for workloads that need to directly access block devices and manage their own data format.</li><li>Dynamic provisioning of local volumes using LVM is under design and an alpha implementation will follow in a future release. This will eliminate the current need for an administrator to pre-partition, format and mount local volumes, as long as the workload’s performance requirements can tolerate sharing disks.</li></ul><h2 id=complementary-features>Complementary features</h2><p><a href=/docs/concepts/configuration/pod-priority-preemption/>Pod priority and preemption</a> is another Kubernetes feature that is complementary to local persistent volumes. When your application uses local storage, it must be scheduled to the specific node where the local volume resides. You can give your local storage workload high priority so if that node ran out of room to run your workload, Kubernetes can preempt lower priority workloads to make room for it.</p><p><a href=/docs/concepts/workloads/pods/disruptions/>Pod disruption budget</a> is also very important for those workloads that must maintain quorum. Setting a disruption budget for your workload ensures that it does not drop below quorum due to voluntary disruption events, such as node drains during upgrade.</p><p><a href=/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature>Pod affinity and anti-affinity</a> ensures that your workloads stay either co-located or spread out across failure domains. If you have multiple local persistent volumes available on a single node, it may be preferable to specify an pod anti-affinity policy to spread your workload across nodes. Note that if you want multiple pods to share the same local persistent volume, you do not need to specify a pod affinity policy. The scheduler understands the locality constraints of the local persistent volume and schedules your pod to the correct node.</p><h2 id=getting-involved>Getting involved</h2><p>If you have feedback for this feature or are interested in getting involved with the design and development, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special-Interest-Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p>Special thanks to all the contributors from multiple companies that helped bring this feature to beta, including Cheng Xing (<a href=https://github.com/verult>verult</a>), David Zhu (<a href=https://github.com/davidz627>davidz627</a>), Deyuan Deng (<a href=https://github.com/ddysher>ddysher</a>), Dhiraj Hedge (<a href=https://github.com/dhirajh>dhirajh</a>), Ian Chakeres (<a href=https://github.com/ianchakeres>ianchakeres</a>), Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>), Matthew Wong (<a href=https://github.com/wongma7>wongma7</a>), Michelle Au (<a href=https://github.com/msau42>msau42</a>), Serguei Bezverkhi (<a href=https://github.com/sbezverk>sbezverk</a>), and Yuquan Ren (<a href=https://github.com/nickrenren>nickrenren</a>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-53c6acca1722e82e1dde5f0e07f5d0b7>Migrating the Kubernetes Blog</h1><div class="td-byline mb-4">By <b>zcorleissen</b> |
<time datetime=2018-04-11 class=text-muted>Wednesday, April 11, 2018</time></div><p>We recently migrated the Kubernetes Blog from the Blogger platform to GitHub. With the change in platform comes a change in URL: formerly at <a href=http://blog.kubernetes.io>http://blog.kubernetes.io</a>, the blog now resides at <a href=https://kubernetes.io/blog>https://kubernetes.io/blog</a>.</p><p>All existing posts redirect from their former URLs with <code>&lt;rel=canonical></code> tags, preserving SEO values.</p><h3 id=why-and-how-we-migrated-the-blog>Why and how we migrated the blog</h3><p>Our primary reasons for migrating were to streamline blog submissions and reviews, and to make the overall blog process faster and more transparent. Blogger's web interface made it difficult to provide drafts to multiple reviewers without also granting unnecessary access permissions and compromising security. GitHub's review process offered clear improvements.</p><p>We learned from <a href=https://www.ybrikman.com>Jim Brikman</a>'s experience during <a href=https://www.ybrikman.com/writing/2015/04/20/migrating-from-blogger-to-github-pages/>his own site migration</a> away from Blogger.</p><p>Our migration was broken into several pull requests, but you can see the work that went into the <a href=https://github.com/kubernetes/website/pull/7247>primary migration PR</a>.</p><p>We hope that making blog submissions more accessible will encourage greater community involvement in creating and reviewing blog content.</p><h3 id=how-to-submit-a-blog-post>How to Submit a Blog Post</h3><p>You can submit a blog post for consideration one of two ways:</p><ul><li>Submit a Google Doc through the <a href=https://docs.google.com/forms/d/e/1FAIpQLSch_phFYMTYlrTDuYziURP6nLMijoXx_f7sLABEU5gWBtxJHQ/viewform>blog submission form</a></li><li>Open a pull request against the <a href=https://github.com/kubernetes/website/tree/master/content/en/blog/_posts>website repository</a> as described <a href=/docs/home/contribute/create-pull-request/>here</a></li></ul><p>If you have a post that you want to remain confidential until your publish date, please submit your post via the Google form. Otherwise, you can choose your submission process based on your comfort level and preferred workflow.</p><blockquote class="note callout"><div><strong>Note:</strong> Our workflow hasn't changed for confidential advance drafts. Additionally, we'll coordinate publishing for time sensitive posts to ensure that information isn't released prematurely through an open pull request.</div></blockquote><h3 id=call-for-reviewers>Call for reviewers</h3><p>The Kubernetes Blog needs more reviewers! If you're interested in contributing to the Kubernetes project and can participate on a regular, weekly basis, send an introductory email to <a href=mailto:k8sblog@linuxfoundation.org>k8sblog@linuxfoundation.org</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-71feebf36b2dd94ef96cf22800c61a81>Container Storage Interface (CSI) for Kubernetes Goes Beta</h1><div class="td-byline mb-4"><time datetime=2018-04-10 class=text-muted>Tuesday, April 10, 2018</time></div><p><img src=/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-kubernetes.png alt="Kubernetes Logo">
<img src=/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-logo.png alt="CSI Logo"></p><p>The Kubernetes implementation of the Container Storage Interface (CSI) is now beta in Kubernetes v1.10. CSI was <a href=https://kubernetes.io/blog/2018/01/introducing-container-storage-interface>introduced as alpha</a> in Kubernetes v1.9.</p><p>Kubernetes features are generally introduced as alpha and moved to beta (and eventually to stable/GA) over subsequent Kubernetes releases. This process allows Kubernetes developers to get feedback, discover and fix issues, iterate on the designs, and deliver high quality, production grade features.</p><h2 id=why-introduce-container-storage-interface-in-kubernetes>Why introduce Container Storage Interface in Kubernetes?</h2><p>Although Kubernetes already provides a powerful volume plugin system that makes it easy to consume different types of block and file storage, adding support for new volume plugins has been challenging. Because volume plugins are currently “in-tree”—volume plugins are part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) must align themselves with the Kubernetes release process.</p><p>With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Third party storage developers can now write and deploy volume plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This will result in even more options for the storage that backs Kubernetes users’ stateful containerized workloads.</p><h2 id=what-s-new-in-beta>What’s new in Beta?</h2><p>With the promotion to beta CSI is now enabled by default on standard Kubernetes deployments instead of being opt-in.</p><p>The move of the Kubernetes implementation of CSI to beta also means:</p><ul><li>Kubernetes is compatible with <a href=https://github.com/container-storage-interface/spec/releases/tag/v0.2.0>v0.2</a> of the CSI spec (instead of <a href=https://github.com/container-storage-interface/spec/releases/tag/v0.1.0>v0.1</a>)<ul><li>There were breaking changes between the CSI spec v0.1 and v0.2, so existing CSI drivers must be updated to be 0.2 compatible before use with Kubernetes 1.10.0+.</li></ul></li><li><a href=/docs/concepts/storage/volumes/#mount-propagation>Mount propagation</a>, a feature that allows bidirectional mounts between containers and host (a requirement for containerized CSI drivers), has also moved to beta.</li><li>The Kubernetes <code>VolumeAttachment</code> object, introduced in v1.9 in the storage v1alpha1 group, has been added to the storage v1beta1 group.</li><li>The Kubernetes <code>CSIPersistentVolumeSource</code> object has been promoted to beta.
A <code>VolumeAttributes</code> field was added to Kubernetes <code>CSIPersistentVolumeSource</code> object (in alpha this was passed around via annotations).</li><li>Node authorizer has been updated to limit access to <code>VolumeAttachment</code> objects from kubelet.</li><li>The Kubernetes <code>CSIPersistentVolumeSource</code> object and the CSI external-provisioner have been modified to allow passing of secrets to the CSI volume plugin.</li><li>The Kubernetes <code>CSIPersistentVolumeSource</code> has been modified to allow passing in filesystem type (previously always assumed to be <code>ext4</code>).</li><li>A new optional call, <code>NodeStageVolume</code>, has been added to the CSI spec, and the Kubernetes CSI volume plugin has been modified to call <code>NodeStageVolume</code> during <code>MountDevice</code> (in alpha this step was a no-op).</li></ul><h2 id=how-do-i-deploy-a-csi-driver-on-a-kubernetes-cluster>How do I deploy a CSI driver on a Kubernetes Cluster?</h2><p>CSI plugin authors must provide their own instructions for deploying their plugin on Kubernetes.</p><p>The Kubernetes-CSI implementation team created a <a href=https://kubernetes-csi.github.io/docs/example.html>sample hostpath CSI driver</a>. The sample provides a rough idea of what the deployment process for a CSI driver looks like. Production drivers, however, would deploy node components via a DaemonSet and controller components via a StatefulSet rather than a single pod (for example, see the deployment files for the <a href=https://github.com/GoogleCloudPlatform/compute-persistent-disk-csi-driver/blob/master/deploy/kubernetes/README.md>GCE PD driver</a>).</p><h2 id=how-do-i-use-a-csi-volume-in-my-kubernetes-pod>How do I use a CSI Volume in my Kubernetes pod?</h2><p>Assuming a CSI storage plugin is already deployed on your cluster, you can use it through the familiar Kubernetes storage primitives: <code>PersistentVolumeClaims</code>, <code>PersistentVolumes</code>, and <code>StorageClasses</code>.</p><p>CSI is a beta feature in Kubernetes v1.10. Although it is enabled by default, it may require the following flag:</p><ul><li>API server binary and kubelet binaries:<ul><li><code>--allow-privileged=true</code><ul><li>Most CSI plugins will require bidirectional mount propagation, which can only be enabled for privileged pods. Privileged pods are only permitted on clusters where this flag has been set to true (this is the default in some environments like GCE, GKE, and kubeadm).</li></ul></li></ul></li></ul><h3 id=dynamic-provisioning>Dynamic Provisioning</h3><p>You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a <code>StorageClass</code> pointing to the CSI plugin.</p><p>The following <code>StorageClass</code>, for example, enables dynamic creation of “<code>fast-storage</code>” volumes by a CSI volume plugin called “<code>com.example.csi-driver</code>”.</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-storage
provisioner: com.example.csi-driver
parameters:
  type: pd-ssd
  csiProvisionerSecretName: mysecret
  csiProvisionerSecretNamespace: mynamespace
</code></pre><p>New for beta, the <a href=https://github.com/kubernetes-csi/external-provisioner>default CSI external-provisioner</a> reserves the parameter keys <code>csiProvisionerSecretName</code> and <code>csiProvisionerSecretNamespace</code>. If specified, it fetches the secret and passes it to the CSI driver during provisioning.</p><p>Dynamic provisioning is triggered by the creation of a <code>PersistentVolumeClaim</code> object. The following <code>PersistentVolumeClaim</code>, for example, triggers dynamic provisioning using the <code>StorageClass</code> above.</p><pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-request-for-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
</code></pre><p>When volume provisioning is invoked, the parameter type: <code>pd-ssd</code> and the secret any referenced secret(s) are passed to the CSI plugin <code>com.example.csi-driver</code> via a <code>CreateVolume call</code>. In response, the external volume plugin provisions a new volume and then automatically create a <code>PersistentVolume</code> object to represent the new volume. Kubernetes then binds the new <code>PersistentVolume</code> object to the <code>PersistentVolumeClaim</code>, making it ready to use.</p><p>If the fast-storage StorageClass is marked as “default”, there is no need to include the storageClassName in the PersistentVolumeClaim, it will be used by default.</p><h3 id=pre-provisioned-volumes>Pre-Provisioned Volumes</h3><p>You can always expose a pre-existing volume in Kubernetes by manually creating a <code>PersistentVolume</code> object to represent the existing volume. The following <code>PersistentVolume</code>, for example, exposes a volume with the name “<code>existingVolumeName</code>” belonging to a CSI storage plugin called “<code>com.example.csi-driver</code>”.</p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manually-created-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: com.example.csi-driver
    volumeHandle: existingVolumeName
    readOnly: false
    fsType: ext4
    volumeAttributes:
      foo: bar
    controllerPublishSecretRef:
      name: mysecret1
      namespace: mynamespace
    nodeStageSecretRef:
      name: mysecret2
      namespace: mynamespace
    nodePublishSecretRef
      name: mysecret3
      namespace: mynamespace
</code></pre><h3 id=attaching-and-mounting>Attaching and Mounting</h3><p>You can reference a <code>PersistentVolumeClaim</code> that is bound to a CSI volume in any pod or pod template.</p><pre><code>kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
      - mountPath: &quot;/var/www/html&quot;
        name: my-csi-volume
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: my-request-for-storage
</code></pre><p>When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (<code>ControllerPublishVolume</code>, <code>NodeStageVolume</code>, <code>NodePublishVolume</code>, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.</p><p>For more details please see the CSI implementation <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md>design doc</a> and <a href=https://kubernetes-csi.github.io/>documentation</a>.</p><h2 id=how-do-i-write-a-csi-driver>How do I write a CSI driver?</h2><p>CSI Volume Driver deployments on Kubernetes must meet some <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers>minimum requirements</a>.</p><p>The minimum requirements document also outlines the <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes>suggested mechanism</a> for deploying an arbitrary containerized CSI driver on Kubernetes. This mechanism can be used by a Storage Provider to simplify deployment of containerized CSI compatible volume drivers on Kubernetes.</p><p>As part of the suggested deployment process, the Kubernetes team provides the following sidecar (helper) containers:</p><ul><li><a href=https://github.com/kubernetes-csi/external-attacher>external-attacher</a><ul><li>watches Kubernetes <code>VolumeAttachment</code> objects and triggers <code>ControllerPublish</code> and <code>ControllerUnpublish</code> operations against a CSI endpoint</li></ul></li><li><a href=https://github.com/kubernetes-csi/external-provisioner>external-provisioner</a><ul><li>watches Kubernetes <code>PersistentVolumeClaim</code> objects and triggers <code>CreateVolume</code> and <code>DeleteVolume</code> operations against a CSI endpoint</li></ul></li><li><a href=https://github.com/kubernetes-csi/driver-registrar>driver-registrar</a><ul><li>registers the CSI driver with kubelet (in the future) and adds the drivers custom <code>NodeId</code> (retrieved via <code>GetNodeID</code> call against the CSI endpoint) to an annotation on the Kubernetes Node API Object</li></ul></li><li><a href=https://github.com/kubernetes-csi/livenessprobe>livenessprobe</a><ul><li>can be included in a CSI plugin pod to enable the <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/>Kubernetes Liveness Probe</a> mechanism</li></ul></li></ul><p>Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.</p><h2 id=where-can-i-find-csi-drivers>Where can I find CSI drivers?</h2><p>CSI drivers are developed and maintained by third parties. You can find a non-definitive list of some <a href=https://kubernetes-csi.github.io/docs/drivers.html>sample and production CSI drivers</a>.</p><h2 id=what-about-flexvolumes>What about FlexVolumes?</h2><p>As mentioned in the <a href=https://kubernetes.io/blog/2018/01/introducing-container-storage-interface>alpha release blog post</a>, <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md>FlexVolume plugin</a> was an earlier attempt to make the Kubernetes volume plugin system extensible. Although it enables third party storage vendors to write drivers “out-of-tree”, because it is an exec based API, FlexVolumes requires files for third party driver binaries (or scripts) to be copied to a special plugin directory on the root filesystem of every node (and, in some cases, master) machine. This requires a cluster admin to have write access to the host filesystem for each node and some external mechanism to ensure that the driver file is recreated if deleted, just to deploy a volume plugin.</p><p>In addition to being difficult to deploy, Flex did not address the pain of plugin dependencies: Volume plugins tend to have many external requirements (on mount and filesystem tools, for example). These dependencies are assumed to be available on the underlying host OS, which is often not the case.</p><p>CSI addresses these issues by not only enabling storage plugins to be developed out-of-tree, but also containerized and deployed via standard Kubernetes primitives.</p><p>If you still have questions about in-tree volumes vs CSI vs Flex, please see the <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Volume Plugin FAQ</a>.</p><h2 id=what-will-happen-to-the-in-tree-volume-plugins>What will happen to the in-tree volume plugins?</h2><p>Once CSI reaches stability, we plan to migrate most of the in-tree volume plugins to CSI. Stay tuned for more details as the Kubernetes CSI implementation approaches stable.</p><h2 id=what-are-the-limitations-of-beta>What are the limitations of beta?</h2><p>The beta implementation of CSI has the following limitations:</p><ul><li>Block volumes are not supported; only file.</li><li>CSI drivers must be deployed with the provided external-attacher sidecar plugin, even if they don’t implement <code>ControllerPublishVolume</code>.</li><li>Topology awareness is not supported for CSI volumes, including the ability to share information about where a volume is provisioned (zone, regions, etc.) with the Kubernetes scheduler to allow it to make smarter scheduling decisions, and the ability for the Kubernetes scheduler or a cluster administrator or an application developer to specify where a volume should be provisioned.</li><li><code>driver-registrar</code> requires permissions to modify all Kubernetes node API objects which could result in a compromised node gaining the ability to do the same.</li></ul><h2 id=what-s-next>What’s next?</h2><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI implementation to GA in 1.12.</p><p>The team would like to encourage storage vendors to start developing CSI drivers, deploying them on Kubernetes, and sharing feedback with the team via the Kubernetes Slack channel <a href=https://kubernetes.slack.com/messages/C8EJ01Z46/details/>wg-csi</a>, the Google group <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-storage-wg-csi>kubernetes-sig-storage-wg-csi</a>, or any of the standard <a href=https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact>SIG storage communication channels</a>.</p><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p><p>In addition to the contributors who have been working on the Kubernetes implementation of CSI since alpha:</p><ul><li>Bradley Childs (<a href=https://github.com/childsb>childsb</a>)</li><li>Chakravarthy Nelluri (<a href=https://github.com/chakri-nelluri>chakri-nelluri</a>)</li><li>Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>)</li><li>Luis Pabón (<a href=https://github.com/lpabon>lpabon</a>)</li><li>Saad Ali (<a href=https://github.com/saad-ali>saad-ali</a>)</li><li>Vladimir Vivien (<a href=https://github.com/vladimirvivien>vladimirvivien</a>)</li></ul><p>We offer a huge thank you to the new contributors who stepped up this quarter to help the project reach beta:</p><ul><li>David Zhu (<a href=https://github.com/davidz627>davidz627</a>)</li><li>Edison Xiang (<a href=https://github.com/edisonxiang>edisonxiang</a>)</li><li>Felipe Musse (<a href=https://github.com/musse>musse</a>)</li><li>Lin Ml (<a href=https://github.com/mlmhl>mlmhl</a>)</li><li>Lin Youchong (<a href=https://github.com/linyouchong>linyouchong</a>)</li><li>Pietro Menna (<a href=https://github.com/pietromenna>pietromenna</a>)</li><li>Serguei Bezverkhi (<a href=https://github.com/sbezverk>sbezverk</a>)</li><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li><li>Yuquan Ren (<a href=https://github.com/NickrenREN>NickrenREN</a>)</li></ul><p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special Interest Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2fcbe5a762cb6818b72059b6ba792071>Fixing the Subpath Volume Vulnerability in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-04-04 class=text-muted>Wednesday, April 04, 2018</time></div><p>On March 12, 2018, the Kubernetes Product Security team disclosed <a href=https://issue.k8s.io/60813>CVE-2017-1002101</a>, which allowed containers using <a href=/docs/concepts/storage/volumes/#using-subpath>subpath</a> volume mounts to access files outside of the volume. This means that a container could access any file available on the host, including volumes for other containers that it should not have access to.</p><p>The vulnerability has been fixed and released in the latest Kubernetes patch releases. We recommend that all users upgrade to get the fix. For more details on the impact and how to get the fix, please see the <a href=https://groups.google.com/forum/#!topic/kubernetes-announce/6sNHO_jyBzE>announcement</a>. (Note, some functional regressions were found after the initial fix and are being tracked in <a href=https://github.com/kubernetes/kubernetes/issues/61563>issue #61563</a>).</p><p>This post presents a technical deep dive on the vulnerability and the solution.</p><h2 id=kubernetes-background>Kubernetes Background</h2><p>To understand the vulnerability, one must first understand how volume and subpath mounting works in Kubernetes.</p><p>Before a container is started on a node, the kubelet volume manager locally mounts all the volumes specified in the PodSpec under a directory for that Pod on the host system. Once all the volumes are successfully mounted, it constructs the list of volume mounts to pass to the container runtime. Each volume mount contains information that the container runtime needs, the most relevant being:</p><ul><li>Path of the volume in the container</li><li>Path of the volume on the host (<code>/var/lib/kubelet/pods/&lt;pod uid>/volumes/&lt;volume type>/&lt;volume name></code>)</li></ul><p>When starting the container, the container runtime creates the path in the container root filesystem, if necessary, and then bind mounts it to the provided host path.</p><p>Subpath mounts are passed to the container runtime just like any other volume. The container runtime does not distinguish between a base volume and a subpath volume, and handles them the same way. Instead of passing the host path to the root of the volume, Kubernetes constructs the host path by appending the Pod-specified subpath (a relative path) to the base volume’s host path.</p><p>For example, here is a spec for a subpath volume mount:</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    &lt;snip&gt;
    volumeMounts:
    - mountPath: /mnt/data
      name: my-volume
      subPath: dataset1
  volumes:
  - name: my-volume
    emptyDir: {}
</code></pre><p>In this example, when the Pod gets scheduled to a node, the system will:</p><ul><li>Set up an EmptyDir volume at <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume</code></li><li>Construct the host path for the subpath mount: <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/ + dataset1</code></li><li>Pass the following mount information to the container runtime:<ul><li>Container path: <code>/mnt/data</code></li><li>Host path: <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/dataset1</code></li></ul></li><li>The container runtime bind mounts <code>/mnt/data</code> in the container root filesystem to <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/dataset1</code> on the host.</li><li>The container runtime starts the container.</li></ul><h2 id=the-vulnerability>The Vulnerability</h2><p>The vulnerability with subpath volumes was discovered by Maxim Ivanov, by making a few observations:</p><ul><li>Subpath references files or directories that are controlled by the user, not the system.</li><li>Volumes can be shared by containers that are brought up at different times in the Pod lifecycle, including by different Pods.</li><li>Kubernetes passes host paths to the container runtime to bind mount into the container.</li></ul><p>The basic example below demonstrates the vulnerability. It takes advantage of the observations outlined above by:</p><ul><li>Using an init container to setup the volume with a symlink.</li><li>Using a regular container to mount that symlink as a subpath later.</li><li>Causing kubelet to evaluate the symlink on the host before passing it into the container runtime.</li></ul><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  initContainers:
  - name: prep-symlink
    image: &quot;busybox&quot;
    command: [&quot;bin/sh&quot;, &quot;-ec&quot;, &quot;ln -s / /mnt/data/symlink-door&quot;]
    volumeMounts:
    - name: my-volume
      mountPath: /mnt/data
  containers:
  - name: my-container
    image: &quot;busybox&quot;
    command: [&quot;/bin/sh&quot;, &quot;-ec&quot;, &quot;ls /mnt/data; sleep 999999&quot;]
    volumeMounts:
    - mountPath: /mnt/data
      name: my-volume
      subPath: symlink-door
  volumes:
  - name: my-volume
    emptyDir: {}
</code></pre><p>For this example, the system will:</p><ul><li>Setup an EmptyDir volume at <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume</code></li><li>Pass the following mount information for the init container to the container runtime:<ul><li>Container path: <code>/mnt/data</code></li><li>Host path: <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume</code></li></ul></li><li>The container runtime bind mounts <code>/mnt/data</code> in the container root filesystem to <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume</code> on the host.</li><li>The container runtime starts the init container.</li><li>The init container creates a symlink inside the container: <code>/mnt/data/symlink-door</code> -> <code>/</code>, and then exits.</li><li>Kubelet starts to prepare the volume mounts for the normal containers.</li><li>It constructs the host path for the subpath volume mount: <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/ + symlink-door</code>.</li><li>And passes the following mount information to the container runtime:<ul><li>Container path: <code>/mnt/data</code></li><li>Host path: <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/symlink-door</code></li></ul></li><li>The container runtime bind mounts <code>/mnt/data</code> in the container root filesystem to <code>/var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty~dir/my-volume/symlink-door</code></li><li>However, the bind mount resolves symlinks, which in this case, resolves to <code>/</code> on the host! Now the container can see all of the host’s filesystem through its mount point <code>/mnt/data</code>.</li></ul><p>This is a manifestation of a <a href=https://en.wikipedia.org/wiki/Symlink_race>symlink race</a>, where a malicious user program can gain access to sensitive data by causing a privileged program (in this case, kubelet) to follow a user-created symlink.</p><p>It should be noted that init containers are not always required for this exploit, depending on the volume type. It is used in the EmptyDir example because EmptyDir volumes cannot be shared with other Pods, and only created when a Pod is created, and destroyed when the Pod is destroyed. For persistent volume types, this exploit can also be done across two different Pods sharing the same volume.</p><h2 id=the-fix>The Fix</h2><p>The underlying issue is that the host path for subpaths are untrusted and can point anywhere in the system. The fix needs to ensure that this host path is both:</p><ul><li>Resolved and validated to point inside the base volume.</li><li>Not changeable by the user in between the time of validation and when the container runtime bind mounts it.</li></ul><p>The Kubernetes product security team went through many iterations of possible solutions before finally agreeing on a design.</p><h3 id=idea-1>Idea 1</h3><p>Our first design was relatively simple. For each subpath mount in each container:</p><ul><li>Resolve all the symlinks for the subpath.</li><li>Validate that the resolved path is within the volume.</li><li>Pass the resolved path to the container runtime.</li></ul><p>However, this design is prone to the classic time-of-check-to-time-of-use (<a href=https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>TOCTTOU</a>) problem. In between steps 2) and 3), the user could change the path back to a symlink. The proper solution needs some way to “lock” the path so that it cannot be changed in between validation and bind mounting by the container runtime. All the subsequent ideas use an intermediate bind mount by kubelet to achieve this “lock” step before handing it off to the container runtime. Once a bind mount is performed, the mount source is fixed and cannot be changed.</p><h3 id=idea-2>Idea 2</h3><p>We went a bit wild with this idea:</p><ul><li>Create a working directory under the kubelet’s pod directory. Let’s call it <code>dir1</code>.</li><li>Bind mount the base volume to under the working directory, <code>dir1/volume</code>.</li><li>Chroot to the working directory <code>dir1</code>.</li><li>Inside the chroot, bind mount <code>volume/subpath</code> to <code>subpath</code>. This ensures that any symlinks get resolved to inside the chroot environment.</li><li>Exit the chroot.</li><li>On the host again, pass the bind mounted <code>dir1/subpath</code> to the container runtime.</li></ul><p>While this design does ensure that the symlinks cannot point outside of the volume, it was ultimately rejected due to difficulties of implementing the chroot mechanism in 4) across all the various distros and environments that Kubernetes has to support, including containerized kubelets.</p><h3 id=idea-3>Idea 3</h3><p>Coming back to earth a little bit, our next idea was to:</p><ul><li>Bind mount the subpath to a working directory under the kubelet’s pod directory.</li><li>Get the source of the bind mount, and validate that it is within the base volume.</li><li>Pass the bind mount to the container runtime.</li></ul><p>In theory, this sounded pretty simple, but in reality, 2) was quite difficult to implement correctly. Many scenarios had to be handled where volumes (like EmptyDir) could be on a shared filesystem, on a separate filesystem, on the root filesystem, or not on the root filesystem. NFS volumes ended up handling all bind mounts as a separate mount, instead of as a child to the base volume. There was additional uncertainty about how out-of-tree volume types (that we couldn’t test) would behave.</p><h3 id=the-solution>The Solution</h3><p>Given the amount of scenarios and corner cases that had to be handled with the previous design, we really wanted to find a solution that was more generic across all volume types. The final design that we ultimately went with was to:</p><ul><li>Resolve all the symlinks in the subpath.</li><li>Starting with the base volume, open each path segment one by one, using the <code>openat()</code> syscall, and disallow symlinks. With each path segment, validate that the current path is within the base volume.</li><li>Bind mount <code>/proc/&lt;kubelet pid>/fd/&lt;final fd></code> to a working directory under the kubelet’s pod directory. The proc file is a link to the opened file. If that file gets replaced while kubelet still has it open, then the link will still point to the original file.</li><li>Close the fd and pass the bind mount to the container runtime.</li></ul><p>Note that this solution is different for Windows hosts, where the mounting semantics are different than Linux. In Windows, the design is to:</p><ul><li>Resolve all the symlinks in the subpath.</li><li>Starting with the base volume, open each path segment one by one with a file lock, and disallow symlinks. With each path segment, validate that the current path is within the base volume.</li><li>Pass the resolved subpath to the container runtime, and start the container.</li><li>After the container has started, unlock and close all the files.</li></ul><p>Both solutions are able to address all the requirements of:</p><ul><li>Resolving the subpath and validating that it points to a path inside the base volume.</li><li>Ensuring that the subpath host path cannot be changed in between the time of validation and when the container runtime bind mounts it.</li><li>Being generic enough to support all volume types.</li></ul><h2 id=acknowledgements>Acknowledgements</h2><p>Special thanks to many folks involved with handling this vulnerability:</p><ul><li>Maxim Ivanov, who responsibly disclosed the vulnerability to the Kubernetes Product Security team.</li><li>Kubernetes storage and security engineers from Google, Microsoft, and RedHat, who developed, tested, and reviewed the fixes.</li><li>Kubernetes test-infra team, for setting up the private build infrastructure</li><li>Kubernetes patch release managers, for coordinating and handling all the releases.</li><li>All the production release teams that worked to deploy the fix quickly after release.</li></ul><p>If you find a vulnerability in Kubernetes, please follow our responsible disclosure process and <a href=https://kubernetes.io/security/#report-a-vulnerability>let us know</a>; we want to do our best to make Kubernetes secure for all users.</p><p>-- Michelle Au, Software Engineer, Google; and Jan Šafránek, Software Engineer, Red Hat</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f004182d259941e1fe7af510b3ab62f1>Kubernetes 1.10: Stabilizing Storage, Security, and Networking</h1><div class="td-byline mb-4">By <b>kbarnard</b> |
<time datetime=2018-03-26 class=text-muted>Monday, March 26, 2018</time></div><p><em><strong>Editor's note: today's post is by the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.10/release_team.md>1.10 Release
Team</a></strong></em></p><p>We’re pleased to announce the delivery of Kubernetes 1.10, our first release
of 2018!</p><p>Today’s release continues to advance maturity, extensibility, and pluggability
of Kubernetes. This newest version stabilizes features in 3 key areas,
including storage, security, and networking. Notable additions in this release
include the introduction of external kubectl credential providers (alpha), the
ability to switch DNS service to CoreDNS at install time (beta), and the move
of Container Storage Interface (CSI) and persistent local volumes to beta.</p><p>Let’s dive into the key features of this release:</p><h2 id=storage-csi-and-local-storage-move-to-beta>Storage - CSI and Local Storage move to beta</h2><p>This is an impactful release for <a href=https://github.com/kubernetes/community/tree/master/sig-storage>the Storage Special Interest Group
(SIG)</a>,
marking the culmination of their work on multiple features. The <a href=https://github.com/kubernetes/features/issues/178>Kubernetes
implementation</a> of the
<a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>Container Storage
Interface</a>
(CSI) moves to beta in this release: installing new volume plugins is now as
easy as deploying a pod. This in turn enables third-party storage providers to
develop their solutions independently outside of the core Kubernetes codebase.
This continues the thread of extensibility within the Kubernetes ecosystem.</p><p><a href=https://github.com/kubernetes/features/issues/121>Durable (non-shared) local storage
management</a> progressed to
beta in this release, making locally attached (non-network attached) storage
available as a persistent volume source. This means higher performance and
lower cost for distributed file systems and databases.</p><p>This release also includes many updates to Persistent Volumes. Kubernetes can
automatically <a href=https://github.com/kubernetes/features/issues/498>prevent deletion of Persistent Volume Claims that are in use by
a pod</a> (beta) and <a href=https://github.com/kubernetes/features/issues/499>prevent
deletion of a Persistent Volume that is bound to a Persistent Volume Claim
</a>(beta). This helps ensure
that storage API objects are deleted in the correct order.</p><h2 id=security-external-credential-providers-alpha>Security - External credential providers (alpha)</h2><p>Kubernetes, which is
already highly extensible, gains another extension point in 1.10 with
<a href=https://github.com/kubernetes/features/issues/541>external kubectl credential
providers</a> (alpha). Cloud
providers, vendors, and other platform developers can now release binary
plugins to handle authentication for specific cloud-provider IAM services, or
that integrate with in-house authentication systems that aren’t supported
in-tree, such as Active Directory. This complements the <a href=/docs/tasks/administer-cluster/running-cloud-controller/>Cloud Controller
Manager</a>
feature added in 1.9.</p><h2 id=networking-coredns-as-a-dns-provider-beta>Networking - CoreDNS as a DNS provider (beta)</h2><p>The ability to <a href=https://github.com/kubernetes/website/pull/7638>switch the DNS
service</a> to CoreDNS at
<a href=/docs/tasks/administer-cluster/coredns/>install time</a>
is now in beta. CoreDNS has fewer moving parts: it’s a single executable and a
single process, and supports additional use cases.</p><p>Each Special Interest Group (SIG) within the community continues to deliver
the most-requested enhancements, fixes, and functionality for their respective
specialty areas. For a complete list of inclusions by SIG, please visit the
<a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.10.md#110-release-notes>release
notes</a>.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.10 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.10.0>download on
GitHub</a>. To get
started with Kubernetes, check out these i<a href=/docs/tutorials/>nteractive
tutorials</a>.</p><h2 id=2-day-features-blog-series>2 Day Features Blog Series</h2><p>If you’re interested in exploring these features
more in depth, check back next week for our 2 Days of Kubernetes series where
we’ll highlight detailed walkthroughs of the following features:</p><p>Day 1 - Container Storage Interface (CSI) for Kubernetes going Beta
Day 2 - Local Persistent Volumes for Kubernetes going Beta</p><h2 id=release-team>Release team</h2><p>This release is made possible through the effort of hundreds of
individuals who contributed both technical and non-technical content. Special
thanks to the <a href=https://github.com/kubernetes/sig-release/blob/master/releases/release-1.10/release_team.md>release
team</a>
led by Jaice Singer DuMars, Kubernetes Ambassador for Microsoft. The 10
individuals on the release team coordinate many aspects of the release, from
documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process represents an
amazing demonstration of collaboration in open source software development.
Kubernetes continues to gain new users at a rapid clip. This growth creates a
positive feedback cycle where more contributors commit code creating a more
vibrant ecosystem.</p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has continued refining an ambitious project to
visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io/>K8s
DevStats</a> illustrates the breakdown of contributions
from major company contributors, as well as an impressive set of preconfigured
reports on everything from individual contributors to pull request lifecycle
times. Thanks to increased automation, issue count at the end of the release
was only slightly higher than it was at the beginning. This marks a major
shift toward issue manageability. With 75,000+ comments, Kubernetes remains
one of the most actively discussed projects on GitHub.</p><h2 id=user-highlights>User Highlights</h2><p>According to a <a href=https://www.cncf.io/blog/2018/03/26/cncf-survey-china/>recent CNCF
survey</a>, more than 49%
of Asia-based respondents use Kubernetes in production, with another 49%
evaluating it for use in production. Established, global organizations are
using <a href=https://kubernetes.io/case-studies/>Kubernetes in production</a> at
massive scale. Recently published user stories from the community include:</p><ol><li><strong>Huawei</strong>, the largest telecommunications equipment manufacturer in the
world, <a href=https://kubernetes.io/case-studies/huawei/>moved its internal IT department’s applications to run on
Kubernetes</a>. This resulted in the
global deployment cycles decreasing from a week to minutes, and the efficiency
of application delivery improved by tenfold.</li><li><strong>Jinjiang Travel International</strong>, one of the top 5 largest OTA and hotel
companies, use Kubernetes to <a href=https://www.linux.com/blog/managing-production-systems-kubernetes-chinese-enterprises>speed up their software release
velocity</a>
from hours to just minutes. Additionally, they leverage Kubernetes to increase
the scalability and availability of their online workloads.</li><li><strong>Haufe Group</strong>, the Germany-based media and software company, utilized
Kubernetes to <a href=https://kubernetes.io/case-studies/haufegroup/>deliver a new release in half an
hour</a> instead of days. The
company is also able to scale down to around half the capacity at night,
saving 30 percent on hardware costs.</li><li><strong>BlackRock</strong>, the world’s largest asset manager, was able to move quickly
using Kubernetes and built an investor research web app from <a href=https://kubernetes.io/case-studies/blackrock/>inception to
delivery in under 100 days</a>.
Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your
story</a>
with the community.</li></ol><h2 id=ecosystem-updates>Ecosystem Updates</h2><ol><li>The CNCF is expanding its certification offerings to
include a Certified Kubernetes Application Developer exam. The CKAD exam
certifies an individual's ability to design, build, configure, and expose
cloud native applications for Kubernetes. The CNCF is looking for beta testers
for this new program. More information can be found
<a href=https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/>here</a>.</li><li>Kubernetes documentation now features <a href=https://k8s.io/docs/home/>user
journeys</a>: specific pathways for learning based on
who readers are and what readers want to do. Learning Kubernetes is easier
than ever for beginners, and more experienced users can find task journeys
specific to cluster admins and application developers.</li><li>CNCF also offers <a href=https://www.cncf.io/certification/training/>online
training</a> that teaches the skills
needed to create and configure a real-world Kubernetes cluster.</li></ol><h2 id=kubecon>KubeCon</h2><p>The world’s largest Kubernetes gathering, <a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/>KubeCon +
CloudNativeCon</a>
is coming to Copenhagen from May 2-4, 2018 and will feature technical
sessions, case studies, developer deep dives, salons and more! Check out the
<a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/program/schedule/>schedule</a>
of speakers and
<a href=https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2018/attend/register/>register</a>
today!</p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.10 release team on April 10th at
10am PDT to learn about the major features in this release including Local
Persistent Volumes and the Container Storage Interface (CSI). Register
<a href=https://www.cncf.io/event/webinar-kubernetes-1-10/>here</a>.</p><h2 id=get-involved>Get Involved</h2><p>The simplest way to get involved with Kubernetes is by joining
one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest
Groups</a>
(SIGs) that align with your interests. Have something you’d like to broadcast
to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community
meeting</a>,
and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ol><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack
Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for
latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes
<a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a>.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-58315accba126f61921f8cdcf34ab405>Principles of Container-based Application Design</h1><div class="td-byline mb-4"><time datetime=2018-03-15 class=text-muted>Thursday, March 15, 2018</time></div><p>It's possible nowadays to put almost any application in a container and run it. Creating cloud-native applications, however—containerized applications that are automated and orchestrated effectively by a cloud-native platform such as Kubernetes—requires additional effort. Cloud-native applications anticipate failure; they run and scale reliably even when their infrastructure experiences outages. To offer such capabilities, cloud-native platforms like Kubernetes impose a set of contracts and constraints on applications. These contracts ensure that applications they run conform to certain constraints and allow the platform to automate application management.</p><p>I've outlined <a href=https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper>seven principles</a>for containerized applications to follow in order to be fully cloud-native.</p><p>| ----- |
| <img src=https://lh5.googleusercontent.com/1XqojkVC0CET1yKCJqZ3-0VWxJ3W8Q74zPLlqnn6eHSJsjHOiBTB7EGUX5o_BOKumgfkxVdgBeLyoyMfMIXwVm9p2QXkq_RRy2mDJG1qEExJDculYL5PciYcWfPAKxF2-DGIdiLw alt> |
| Container Design Principles |</p><p>These seven principles cover both build time and runtime concerns.</p><h4 id=build-time>Build time</h4><ul><li><strong>Single Concern:</strong> Each container addresses a single concern and does it well.</li><li><strong>Self-Containment:</strong> A container relies only on the presence of the Linux kernel. Additional libraries are added when the container is built.</li><li><strong>Image Immutability:</strong> Containerized applications are meant to be immutable, and once built are not expected to change between different environments.</li></ul><h4 id=runtime>Runtime</h4><ul><li><strong>High Observability:</strong> Every container must implement all necessary APIs to help the platform observe and manage the application in the best way possible.</li><li><strong>Lifecycle Conformance:</strong> A container must have a way to read events coming from the platform and conform by reacting to those events.</li><li><strong>Process Disposability:</strong> Containerized applications must be as ephemeral as possible and ready to be replaced by another container instance at any point in time.</li><li><strong>Runtime Confinement:</strong> Every container must declare its resource requirements and restrict resource use to the requirements indicated.
The build time principles ensure that containers have the right granularity, consistency, and structure in place. The runtime principles dictate what functionalities must be implemented in order for containerized applications to possess cloud-native function. Adhering to these principles helps ensure that your applications are suitable for automation in Kubernetes.</li></ul><p>The white paper is freely available for download:</p><p>To read more about designing cloud-native applications for Kubernetes, check out my <a href=http://leanpub.com/k8spatterns/>Kubernetes Patterns</a> book.</p><p>— <a href=http://twitter.com/bibryam>Bilgin Ibryam</a>, Principal Architect, Red Hat</p><p>Twitter:  <br>Blog: <a href=http://www.ofbizian.com/>http://www.ofbizian.com</a><br>Linkedin:</p><p>Bilgin Ibryam (@bibryam) is a principal architect at Red Hat, open source committer at ASF, blogger, author, and speaker. He is the author of Camel Design Patterns and Kubernetes Patterns books. In his day-to-day job, Bilgin enjoys mentoring, training and leading teams to be successful with distributed systems, microservices, containers, and cloud-native applications in general.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3fa9fa5e19bf21d00ebfb18ca31a9f61>Expanding User Support with Office Hours</h1><div class="td-byline mb-4"><time datetime=2018-03-14 class=text-muted>Wednesday, March 14, 2018</time></div><p><strong>Today's post is by <a href=https://twitter.com/castrojo>Jorge Castro</a> and <a href=https://twitter.com/errordeveloper>Ilya Dmitichenko</a> on Kubernetes office hours.</strong></p><p>Today's developer has an almost overwhelming amount of resources available for learning. Kubernetes development teams use <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>, <a href=https://kubernetes.io/docs/home>user documentation</a>, <a href=http://slack.k8s.io/>Slack</a>, and the <a href=https://groups.google.com/forum/#!forum/kubernetes-users>mailing lists</a>. Additionally, the community itself continues to amass an <a href=https://github.com/ramitsurana/awesome-kubernetes>awesome list</a> of resources.</p><p>One of the challenges of large projects is keeping user resources relevant and useful. While documentation can be useful, great learning also happens in Q&A sessions at conferences, or by learning with someone whose explanation matches your learning style. Consider that learning Kung Fu from Morpheus would be a lot more fun than reading a book about Kung Fu!</p><p><img src=https://3.bp.blogspot.com/-Iy2GaddJp78/WqnFbVUu9FI/AAAAAAAAAM4/xUzhOSIlRDEMMZNl3SzPBd1Pa0T5y0pKQCLcBGAs/s400/24xkey.jpg alt></p><p>We as Kubernetes developers want to create an interactive experience: where Kubernetes users can get their questions answered by experts in real time, or at least referred to the best known documentation or code example.</p><p>Having discussed a few broad ideas, we eventually decided to make <a href=https://github.com/kubernetes/community/blob/master/events/office-hours.md>Kubernetes Office Hours</a> a live stream where we take user questions from the audience and present them to our panel of contributors and expert users. We run two sessions: one for European time zones, and one for the Americas. These <a href=https://docs.google.com/document/d/1jHSnRzoOxwd1urgxwbANhNgXjMV8fb0B4NS3ZUL10IY/edit>streaming setup guidelines</a> make office hours extensible—for example, if someone wants to run office hours for Asia/Pacific timezones, or for another CNCF project.</p><p>To give you an idea of what Kubernetes office hours are like, here's Josh Berkus answering a question on running databases on Kubernetes. Despite the popularity of this topic, it's still difficult for a new user to get a constructive answer. Here's an excellent response from Josh:</p><p><a href="https://www.youtube.com/embed/Aj0yozuQ0ME?ecver=2"><img src=https://img.youtube.com/vi/Aj0yozuQ0ME/0.jpg alt></a></p><p>It's often easier to field this kind of question in office hours than it is to ask a developer to write a full-length blog post. [Editor's note: That's legit!] Because we don't have infinite developers with infinite time, this kind of focused communication creates high-bandwidth help while limiting developer commitments to 1 hour per month. This allows a rotating set of experts to share the load without overwhelming any one person.</p><p>We hold office hours the third Wednesday of every month on the <a href=https://www.youtube.com/c/kubernetescommunity>Kubernetes YouTube Channel</a>. You can post questions on the <a href=https://kubernetes.slack.com/messages/office-hours>#office-hours channel</a> on Slack, or you can submit your question to Stack Overflow and post a link on Slack. If you post a question in advance, you might get better answers, as volunteers have more time to research and prepare. If a question can't be fully solved during the call, the team will try their best to point you in the right direction and/or ping other people in the community to take a look. <a href=https://github.com/kubernetes/community/blob/master/events/office-hours.md>Check out this page</a> for more details on what's off- and on topic as well as meeting information for your time zone. We hope to hear your questions soon!</p><p>Special thanks to Amazon, Bitnami, Giant Swarm, Heptio, Liquidweb, Northwestern Mutual, Packet.net, Pivotal, Red Hat, Weaveworks, and VMWare for donating engineering time to office hours.</p><p>And thanks to Alan Pope, Joe Beda, and Charles Butler for technical support in making our livestream better.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-949c70280976aa8bb648ff21796fa901>How to Integrate RollingUpdate Strategy for TPR in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-03-13 class=text-muted>Tuesday, March 13, 2018</time></div><p>With Kubernetes, it's easy to manage and scale stateless applications like web apps and API services right out of the box. To date, almost all of the talks about Kubernetes has been about microservices and stateless applications.</p><p>With the popularity of container-based microservice architectures, there is a strong need to deploy and manage RDBMS(Relational Database Management Systems). RDBMS requires experienced database-specific knowledge to correctly scale, upgrade, and re-configure while protecting against data loss or unavailability.</p><p>For example, MySQL (the most popular open source RDBMS) needs to store data in files that are persistent and exclusive to each MySQL database's storage. Each MySQL database needs to be individually distinct, another, more complex is in cluster that need to distinguish one MySQL database from a cluster as a different role, such as master, slave, or shard. High availability and zero data loss are also hard to accomplish when replacing database nodes on failed machines.</p><p>Using powerful Kubernetes API extension mechanisms, we can encode RDBMS domain knowledge into software, named WQ-RDS, running atop Kubernetes like built-in resources.</p><p>WQ-RDS leverages Kubernetes primitive resources and controllers, it deliveries a number of enterprise-grade features and brings a significantly reliable way to automate time-consuming operational tasks like database setup, patching backups, and setting up high availability clusters. WQ-RDS supports mainstream versions of Oracle and MySQL (both compatible with MariaDB).</p><p>Let's demonstrate how to manage a MySQL sharding cluster.</p><h3 id=mysql-sharding-cluster>MySQL Sharding Cluster</h3><p>MySQL Sharding Cluster is a scale-out database architecture. Based on the hash algorithm, the architecture distributes data across all the shards of the cluster. Sharding is entirely transparent to clients: Proxy is able to connect to any Shards in the cluster and issue queries to the correct shards directly.</p><p>| ----- |
| <img src=https://lh5.googleusercontent.com/4WiSkxX-XBqARVqQ0No-1tZ31op90LAUkTco3FdIO1mFScNOTVtMCgnjaO8SRUmms-6MAb46CzxlXDhLBqAAAmbx26atJnu4t1FTTALZx_CbUPqrCxjL746DW4TD42-03Ac9VB2c alt> |
|</p><p>Note: Each shard corresponds to a single MySQL instance. Currently, WQ-RDS supports a maximum of 64 shards.</p><p>|</p><p>All of the shards are built with Kubernetes Statefulset, Services, Storage Class, configmap, secrets and MySQL. WQ-RDS manages the entire lifecycle of the sharding cluster. Advantages of the sharding cluster are obvious:</p><ul><li>Scale out queries per second (QPS) and transactions per second (TPS)</li><li>Scale out storage capacity: gain more storage by distributing data to multiple nodes</li></ul><h3 id=create-a-mysql-sharding-cluster>Create a MySQL Sharding Cluster</h3><p>Let's create a Kubernetes cluster with 8 shards.</p><pre><code> kubectl create -f mysqlshardingcluster.yaml
</code></pre><p>Next, create a MySQL Sharding Cluster including 8 shards.</p><ul><li>TPR : MysqlCluster and MysqlDatabase</li></ul><pre><code>[root@k8s-master ~]# kubectl get mysqlcluster  


NAME             KIND

clustershard-c   MysqlCluster.v1.mysql.orain.com
</code></pre><p>MysqlDatabase from clustershard-c0 to clustershard-c7 belongs to MysqlCluster clustershard-c.</p><pre><code>[root@k8s-master ~]# kubectl get mysqldatabase  

NAME KIND  

clustershard-c0 MysqlDatabase.v1.mysql.orain.com  

clustershard-c1 MysqlDatabase.v1.mysql.orain.com  

clustershard-c2 MysqlDatabase.v1.mysql.orain.com  

clustershard-c3 MysqlDatabase.v1.mysql.orain.com  

clustershard-c4 MysqlDatabase.v1.mysql.orain.com  

clustershard-c5 MysqlDatabase.v1.mysql.orain.com  

clustershard-c6 MysqlDatabase.v1.mysql.orain.com  

clustershard-c7 MysqlDatabase.v1.mysql.orain.com
</code></pre><p>Next, let's look at two main features: high availability and RollingUpdate strategy.</p><p>To demonstrate, we'll start by running sysbench to generate some load on the cluster. In this example, QPS metrics are generated by MySQL export, collected by Prometheus, and visualized in Grafana.</p><h3 id=feature-high-availability>Feature: high availability</h3><p>WQ-RDS handles MySQL instance crashes while protecting against data loss.</p><p>When killing clustershard-c0, WQ-RDS will detect that clustershard-c0 is unavailable and replace clustershard-c0 on failed machine, taking about 35 seconds on average.</p><p><img src=https://lh3.googleusercontent.com/sXqVqfTu6rMWn0mlHLgHHqATe_qsx1tNmMfX60HoTwyhd5HCL4A_ViFBQAZfOoVGioeXcI_XXbzVFUdq2hbKGwS0OXH6PFGqgpZshfBwrT088bz4KqeyTbHpQR2olyzE6eRo1fan alt></p><p>zero data loss at same time.</p><p><img src=https://lh6.googleusercontent.com/7xnN_sODa-3Ch3ScAUlggCTeYfnE3-wxRaCIHrljHCB7LnXgth8zeCv0gk_UU1jbSDBQuACQ2Mf1FO1-E7GvMWwGKjp7irenAKp4DkHlA5LR9OVuLXqubPFhhksA8kfBUh4Z4OuN alt></p><h3 id=feature-rollingupdate-strategy>Feature : RollingUpdate Strategy</h3><p>MySQL Sharding Cluster brings us not only strong scalability but also some level of maintenance complexity. For example, when updating a MySQL configuration like innodb_buffer_pool_size, a DBA has to perform a number of steps:</p><p>1. Apply change time.<br>2. Disable client access to database proxies.<br>3. Start a rolling upgrade.</p><p>Rolling upgrades need to proceed in order and are the most demanding step of the process. One cannot continue a rolling upgrade until and unless previous updates to MySQL instances are running and ready.</p><p>4 Verify the cluster.<br>5. Enable client access to database proxies.</p><p>Possible problems with a rolling upgrade include:</p><ul><li>node reboot</li><li>MySQL instances restart</li><li>human error
Instead, WQ-RDS enables a DBA to perform rolling upgrades automatically.</li></ul><h3 id=statefulset-rollingupdate-in-kubernetes>StatefulSet RollingUpdate in Kubernetes</h3><p>Kubernetes 1.7 includes a major feature that adds automated updates to StatefulSets and supports a range of update strategies including rolling updates.</p><p><strong>Note:</strong> For more information about <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates>StatefulSet RollingUpdate</a>, see the Kubernetes docs.</p><p>Because TPR (currently CRD) does not support the rolling upgrade strategy, we needed to integrate the RollingUpdate strategy into WQ-RDS. Fortunately, the <a href=https://github.com/kubernetes/kubernetes>Kubernetes repo</a> is a treasure for learning. In the process of implementation, there are some points to share:</p><ul><li><p>**MySQL Sharding Cluster has **<strong>changed</strong>: Each StatefulSet has its corresponding ControllerRevision, which records all the revision data and order (like git). Whenever StatefulSet is syncing, StatefulSet Controller will firstly compare it's spec to the latest corresponding ControllerRevision data (similar to git diff). If changed, a new ControllerrRevision will be generated, and the revision number will be incremented by 1. WQ-RDS borrows the process, MySQL Sharding Cluster object will record all the revision and order in ControllerRevision.</p></li><li><p>**How to initialize MySQL Sharding Cluster to meet request **<strong>replicas</strong>: Statefulset supports two <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates>Pod management policies</a>: Parallel and OrderedReady. Because MySQL Sharding Cluster doesn't require ordered creation for its initial processes, we use the Parallel policy to accelerate the initialization of the cluster.</p></li><li><p>**How to perform a Rolling **<strong>Upgrade</strong>: Statefulset recreates pods in strictly decreasing order. The difference is that WQ-RDS updates shards instead of recreating them, as shown below:
<img src=https://lh6.googleusercontent.com/B4ig8krCsXwvMeBy8NamQi1DrihUEzBcRTHCqhn9kUvlcpPrFoYUNAxn61qh8S2HXcdg31QpOhWSsYHP0jI4QxPkKpZ5oY-k9gFp1eK63qt6rwTMMWMiBs45DObY6rw2R7c0lNPu alt></p></li><li><p><strong>When RollingUpdate ends</strong>: Kubernetes signals termination clearly. A rolling update completes when all of a set's Pods have been updated to the updateRevision. The status's currentRevision is set to updateRevision and its updateRevision is set to the empty string. The status's currentReplicas is set to updateReplicas and its updateReplicas are set to 0.</p></li></ul><h3 id=controller-revision-in-wq-rds>Controller revision in WQ-RDS</h3><p>Revision information is stored in MysqlCluster.Status and is no different than Statefulset.Status.</p><pre><code>
root@k8s-master ~]# kubectl get mysqlcluster -o yaml clustershard-c

apiVersion: v1

items:

\- apiVersion: mysql.orain.com/v1

 kind: MysqlCluster

 metadata:

   creationTimestamp: 2017-10-20T08:19:41Z

   labels:

     AppName: clustershard-crm

     Createdby: orain.com

     DBType: MySQL

   name: clustershard-c

   namespace: default

   resourceVersion: &quot;415852&quot;

   uid: 6bb089bb-b56f-11e7-ae02-525400e717a6

 spec:



     dbresourcespec:

       limitedcpu: 1200m

       limitedmemory: 400Mi

       requestcpu: 1000m

       requestmemory: 400Mi



 status:

   currentReplicas: 8

   currentRevision: clustershard-c-648d878965

   replicas: 8

   updateRevision: clustershard-c-648d878965

kind: List

</code></pre><h3 id=example-perform-a-rolling-upgrade>Example: Perform a rolling upgrade</h3><p>Finally, We can now update "clustershard-c" to update configuration "innodb_buffer_pool_size" from 6GB to 7GB and reboot.</p><p>The process takes 480 seconds.</p><p><img src=https://lh4.googleusercontent.com/LOxFDdojYxnPvSHDYwivVge6vGImK7uTdyvCsKrxCMF3rIlVkw7mHeNhJiNJwz1aGzVhZXpqrgzHC6pIbkPk3JPAtuSqX9ovAYBzK01BGfzwkXvMGZomAh4L0DahGyD3QB715B-Z alt></p><p>The upgrade is in monotonically decreasing manner:</p><p><img src=https://lh6.googleusercontent.com/DhFbY3JDh23A_91c04TujxWM9xCX_xq1xOCXHi7XAd75LzKwDtbH6Gr_2VXCscg8AeVCQzw3Inw4M-uvssWq8od4va0wd-fIyClVY63FjRfeU16fQda_XqzBYRIhrG5W3tDnCAwC alt></p><p><img src=https://lh6.googleusercontent.com/bAJtLRRl2TqQrfBOooNm9DIEuezoBhT3f-XuOyGxp8sKePzfRaQYcJ7PFvL30xw9jeUpc-3rVw6Qjr46dFRk7mmUsf3oichNEuC-BFwCEtpbxK0_BjSJxtIE4B5xR4CGw1m6Hf0D alt></p><p><img src=https://lh4.googleusercontent.com/ALYk-EP_rYibA95nIIo8TKx8BYuSY9w1Pqw4JLEiV89K9i06uBhkTrYWX26FjYtheGKVwwVMTtDKH7UTBovGf8AEpK97T3RT23RSAUTs4GyDFaDOGmlRAczbGLm0UjQglbB_NPdF alt></p><p><img src=https://lh4.googleusercontent.com/gmM6UbgVOBWPJBpIMutxeTxGiwtjFv25KAHQw3ebVAF5Kxm-uxkPKEiYKhpwYUTyDe5knYlGmQDDHiN8eefBJx0fbK7jg4IlpG5_DXMUG6rNNFIbpP7Q94ANROIeUfe5JP6t-k37 alt></p><p><img src=https://lh5.googleusercontent.com/aiczeqRNRls8lh-LYbnx112kgZI2gvaBMimAk74KlLhR3EVicuKAemTKr2eKUSFPjmKbsg_gw_nY1G4YU0-3J1EjDPOhz55UUri47Py-s-jRf0dF-lAKn6TRrF6IvGtv2aldWa3k alt></p><p><img src=https://lh6.googleusercontent.com/mwRQP_wXMCzpXsC5sqb0nJ9jU4KdUl4FiUE26gQZMQbrn5zcgqSYZB03CLmGsT2Nuq-7x00W4Ar3IUAh7hxEksQEGl6ugAmY0wo7xjzisNH9VE1qto9Afx8QW2Sr6NR-SBDeJfTt alt></p><p><img src=https://lh6.googleusercontent.com/joraaJ-qX-K8zTdAFBJWeOswQQtNeX6yezKGkSM56FNYQT-XYrgsxvNLYBE0askw9huAmJhebCVU4AMvjz4B6xlIjdLwO3vMX7_dWBzkfu05HZ3-NOsFnqg-jvkLknl-ldRzUcFO alt></p><p><img src=https://lh5.googleusercontent.com/ayoUAhD-azUjUqjut7iSiW8FBFJpCJZLRJDT9mXJoy4QTutAsGgr4yPvbFumaXasOqpsmJ_zZ2k7nrQl2YrjGqPr83PXe-tXjj9OLc-GYhhtJTzBEeddWpZn5pDyBpdw9I4sD-O0 alt></p><h3 id=conclusion>Conclusion</h3><p>RollingUpgrade is meaningful to database administrators. It provides a more effective way to operator database.</p><p>--Orain Xiong, co-founder, Woqutech</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7de8566e09f1f9563eebe44fedd66cc5>Apache Spark 2.3 with Native Kubernetes Support</h1><div class="td-byline mb-4"><time datetime=2018-03-06 class=text-muted>Tuesday, March 06, 2018</time></div><h3 id=kubernetes-and-big-data>Kubernetes and Big Data</h3><p>The open source community has been working over the past year to enable first-class support for data processing, data analytics and machine learning workloads in Kubernetes. New extensibility features in Kubernetes, such as <a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/>custom resources</a> and <a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers>custom controllers</a>, can be used to create deep integrations with individual applications and frameworks.</p><p>Traditionally, data processing workloads have been run in dedicated setups like the YARN/Hadoop stack. However, unifying the control plane for all workloads on Kubernetes simplifies cluster management and can improve resource utilization.</p><p>"Bloomberg has invested heavily in machine learning and NLP to give our clients a competitive edge when it comes to the news and financial information that powers their investment decisions. By building our Data Science Platform on top of Kubernetes, we're making state-of-the-art data science tools like Spark, TensorFlow, and our sizable GPU footprint accessible to the company's 5,000+ software engineers in a consistent, easy-to-use way." - Steven Bower, Team Lead, Search and Data Science Infrastructure at Bloomberg</p><h3 id=introducing-apache-spark-kubernetes>Introducing Apache Spark + Kubernetes</h3><p><a href=http://spark.apache.org/releases/spark-release-2-3-0.html>Apache Spark 2.3</a> with native Kubernetes support combines the best of the two prominent open source projects — Apache Spark, a framework for large-scale data processing; and Kubernetes.</p><p>Apache Spark is an essential tool for data scientists, offering a robust platform for a variety of applications ranging from large scale data transformation to analytics to machine learning. Data scientists are adopting containers en masse to improve their workflows by realizing benefits such as packaging of dependencies and creating reproducible artifacts. Given that Kubernetes is the de facto standard for managing containerized environments, it is a natural fit to have support for Kubernetes APIs within Spark.</p><p>Starting with Spark 2.3, users can run Spark workloads in an existing Kubernetes 1.7+ cluster and take advantage of Apache Spark's ability to manage distributed data processing tasks. Apache Spark workloads can make direct use of Kubernetes clusters for multi-tenancy and sharing through <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/>Namespaces</a> and <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>Quotas</a>, as well as administrative features such as <a href=https://kubernetes.io/docs/admin/authorization/>Pluggable Authorization</a> and <a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>Logging</a>. Best of all, it requires no changes or new installations on your Kubernetes cluster; simply <a href=https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images>create a container image</a> and set up the right <a href=https://spark.apache.org/docs/latest/running-on-kubernetes.html#rbac>RBAC roles</a> for your Spark Application and you're all set.</p><p>Concretely, a native Spark Application in Kubernetes acts as a <a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers>custom controller</a>, which creates Kubernetes resources in response to requests made by the Spark scheduler. In contrast with <a href=https://kubernetes.io/blog/2016/03/using-Spark-and-Zeppelin-to-process-Big-Data-on-Kubernetes>deploying Apache Spark in Standalone Mode</a> in Kubernetes, the native approach offers fine-grained management of Spark Applications, improved elasticity, and seamless integration with logging and monitoring solutions. The community is also exploring advanced use cases such as managing streaming workloads and leveraging service meshes like <a href=https://istio.io/>Istio</a>.</p><p><img src=https://1.bp.blogspot.com/-hl4pnOqiH4M/Wp4w9QmzghI/AAAAAAAAAL4/jcWoDOKEp3Y6lCzGxzTOlbvl2Mq1-2YeQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-03-05%2Bat%2B10.10.14%2BPM.png alt></p><p>To try this yourself on a Kubernetes cluster, simply download the binaries for the official <a href=https://spark.apache.org/downloads.html>Apache Spark 2.3 release</a>. For example, below, we describe running a simple Spark application to compute the mathematical constant Pi across three Spark executors, each running in a separate pod. Please note that this requires a cluster running Kubernetes 1.7 or above, a <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>kubectl</a> client that is configured to access it, and the necessary <a href=https://spark.apache.org/docs/latest/running-on-kubernetes.html#rbac>RBAC rules</a> for the default namespace and service account.</p><pre><code>$ kubectl cluster-info  

Kubernetes master is running at https://xx.yy.zz.ww

$ bin/spark-submit

   --master k8s://https://xx.yy.zz.ww

   --deploy-mode cluster

   --name spark-pi

   --class org.apache.spark.examples.SparkPi

   --conf spark.executor.instances=5

   --conf spark.kubernetes.container.image=

   --conf spark.kubernetes.driver.pod.name=spark-pi-driver

   local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar

</code></pre><p>To watch Spark resources that are created on the cluster, you can use the following kubectl command in a separate terminal window.</p><pre><code>$ kubectl get pods -l 'spark-role in (driver, executor)' -w

NAME              READY STATUS  RESTARTS AGE

spark-pi-driver   1/1 Running  0 14s

spark-pi-da1968a859653d6bab93f8e6503935f2-exec-1   0/1 Pending 0 0s

</code></pre><p>The results can be streamed during job execution by running:</p><pre><code>
$ kubectl logs -f spark-pi-driver

</code></pre><p>When the application completes, you should see the computed value of Pi in the driver logs.</p><p>In Spark 2.3, we're starting with support for Spark applications written in Java and Scala with support for resource localization from a variety of data sources including HTTP, GCS, HDFS, and more. We have also paid close attention to failure and recovery semantics for Spark executors to provide a strong foundation to build upon in the future. Get started with <a href=https://spark.apache.org/docs/latest/running-on-kubernetes.html>the open-source documentation</a> today.</p><h3 id=get-involved>Get Involved</h3><p>There's lots of exciting work to be done in the near future. We're actively working on features such as dynamic resource allocation, in-cluster staging of dependencies, support for PySpark & SparkR, support for Kerberized HDFS clusters, as well as client-mode and popular notebooks' interactive execution environments. For people who fell in love with the Kubernetes way of managing applications declaratively, we've also been working on a <a href=https://coreos.com/operators/>Kubernetes Operator</a> for spark-submit, which allows users to declaratively specify and submit Spark Applications.</p><p>And we're just getting started! We would love for you to get involved and help us evolve the project further.</p><p>Huge thanks to the Apache Spark and Kubernetes contributors spread across multiple organizations who spent many hundreds of hours working on this effort. We look forward to seeing more of you contribute to the project and help it evolve further.</p><p>Anirudh Ramanathan and Palak Bhatia<br>Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-46ba7c4e2f63c0643b347ea9a32b2bed>Kubernetes: First Beta Version of Kubernetes 1.10 is Here</h1><div class="td-byline mb-4"><time datetime=2018-03-02 class=text-muted>Friday, March 02, 2018</time></div><p><strong>Editor's note: Today's post is by Nick Chase. Nick is Head of Content at <a href=https://www.mirantis.com/>Mirantis</a>.</strong>
The Kubernetes community has released the first beta version of Kubernetes 1.10, which means you can now try out some of the new features and give your feedback to the release team ahead of the official release. The release, currently scheduled for March 21, 2018, is targeting the inclusion of more than a dozen brand new alpha features and more mature versions of more than two dozen more.</p><p>Specifically, Kubernetes 1.10 will include production-ready versions of Kubelet TLS Bootstrapping, API aggregation, and more detailed storage metrics.</p><p>Some of these features will look familiar because they emerged at earlier stages in previous releases. Each stage has specific meanings:</p><ul><li><strong>stable</strong>: The same as "generally available", features in this stage have been thoroughly tested and can be used in production environments.</li><li><strong>beta</strong>: The feature has been around long enough that the team is confident that the feature itself is on track to be included as a stable feature, and any API calls aren't going to change. You can use and test these features, but including them in mission-critical production environments is not advised because they are not completely hardened.</li><li><strong>alpha</strong>: New features generally come in at this stage. These features are still being explored. APIs and options may change in future versions, or the feature itself may disappear. Definitely not for production environments.
You can download the latest release of Kubernetes 1.10 from . To give feedback to the development community, <a href=https://github.com/kubernetes/kubernetes/milestone/37>create an issue in the Kubernetes 1.10 milestone</a> and tag the appropriate SIG before March 9.</li></ul><p>Here's what to look for, though you should remember that while this is the current plan as of this writing, there's always a possibility that one or more features may be held for a future release. We'll start with authentication.</p><h3 id=authentication-sig-auth>Authentication (SIG-Auth)</h3><ol><li><a href=https://github.com/kubernetes/features/issues/43>Kubelet TLS Bootstrap</a> (stable): Kubelet TLS bootstrapping is probably the "headliner" of the Kubernetes 1.10 release as it becomes available for production environments. It provides the ability for a new kubelet to create a certificate signing request, which enables you to add new nodes to your cluster without having to either manually add security certificates or use self-signed certificates that eliminate many of the benefits of having certificates in the first place.</li><li><a href=https://github.com/kubernetes/features/issues/5>Pod Security Policy moves to its own API group</a> (beta): The beta release of the Pod Security Policy lets administrators decide what contexts pods can run in. In other words, you have the ability to prevent unprivileged users from creating privileged pods -- that is, pods that can perform actions such as writing files or accessing Secrets -- in particular namespaces.</li><li><a href=https://github.com/kubernetes/features/issues/279>Limit node access to API</a> (beta): Also in beta, you now have the ability to limit calls to the API on a node to just that specific node, and to ensure that a node is only calling its own API, and not those on other nodes.</li><li><a href=https://github.com/kubernetes/features/issues/541>External client-go credential providers</a> (alpha): client-go is the Go language client for accessing the Kubernetes API. This feature adds the ability to add external credential providers. For example, Amazon might want to create its own authenticator to validate interaction with EKS clusters; this feature enables them to do that without having to include their authenticator in the Kubernetes codebase.</li><li><a href=https://github.com/kubernetes/features/issues/542>TokenRequest API</a> (alpha): The TokenRequest API provides the groundwork for much needed improvements to service account tokens; this feature enables creation of tokens that aren't persisted in the Secrets API, that are targeted for specific audiences (such as external secret stores), have configurable expiries, and are bindable to specific pods.</li></ol><h3 id=networking-sig-network>Networking (SIG-Network)</h3><ol><li><a href=https://github.com/kubernetes/features/issues/504>Support configurable pod resolv.conf</a> (beta): You now have the ability to specifically control DNS for a single pod, rather than relying on the overall cluster DNS.</li><li>Although the feature is called <a href=https://github.com/kubernetes/features/issues/427>Switch default DNS plugin to CoreDNS</a> (beta), that's not actually what will happen in this cycle. The community has been working on the switch from kube-dns, which includes dnsmasq, to CoreDNS, another CNCF project with fewer moving parts, for several releases. In Kubernetes 1.10, the default will still be kube-dns, but when CoreDNS reaches feature parity with kube-dns, the team will look at making it the default.</li><li><a href=https://github.com/kubernetes/features/issues/536>Topology aware routing of services</a> (alpha): The ability to distribute workloads is one of the advantages of Kubernetes, but one thing that has been missing until now is the ability to keep workloads and services geographically close together for latency purposes. Topology aware routing will help with this problem. (This functionality may be delayed until Kubernetes 1.11.)</li><li><a href=https://github.com/kubernetes/features/issues/539>Make NodePort IP address configurable</a> (alpha): Not having to specify IP addresses in a Kubernetes cluster is great -- until you actually need to know what one of those addresses is ahead of time, such as for setting up database replication or other tasks. You will now have the ability to specifically configure NodePort IP addresses to solve this problem. (This functionality may be delayed until Kubernetes 1.11.)</li></ol><h3 id=kubernetes-apis-sig-api-machinery>Kubernetes APIs (SIG-API-machinery)</h3><ol><li><a href=https://github.com/kubernetes/features/issues/263>API Aggregation</a> (stable): Kubernetes makes it possible to extend its API by creating your own functionality and registering your functions so that they can be served alongside the core K8s functionality. This capability will be upgraded to "stable" in Kubernetes 1.10, so you can use it in production. Additionally, SIG-CLI is adding a feature called <a href=https://github.com/kubernetes/features/issues/515>kubectl get and describe should work well with extensions</a> (alpha) to make the server, rather than the client, return this information for a smoother user experience.</li><li><a href=https://github.com/kubernetes/features/issues/516>Support for self-hosting authorizer webhook</a> (alpha): Earlier versions of Kubernetes brought us the authorizer webhooks, which make it possible to customize the enforcement of permissions before commands are executed. Those webhooks, however, have to live somewhere, and this new feature makes it possible to host them in the cluster itself.</li></ol><h3 id=storage-sig-storage>Storage (SIG-Storage)</h3><ol><li><a href=https://github.com/kubernetes/features/issues/496>Detailed storage metrics of internal state</a> (stable): With a distributed system such as Kubernetes, it's particularly important to know what's going on inside the system at any given time, either for troubleshooting purposes or simply for automation. This release brings to general availability detailed metrics of what's going in inside the storage systems, including metrics such as mount and unmount time, number of volumes in a particular state, and number of orphaned pod directories. You can find a <a href="https://docs.google.com/document/d/1Fh0T60T_y888LsRwC51CQHO75b2IZ3A34ZQS71s_F0g/edit#heading=h.ys6pjpbasqdu">full list in this design document</a>.</li><li><a href=https://github.com/kubernetes/features/issues/432>Mount namespace propagation</a> (beta): This feature allows a container to mount a volume as rslave so that host mounts can be seen inside the container, or as rshared so that any mounts from inside the container are reflected in the host's mount namespace. The default for this feature is rslave.</li><li><a href=https://github.com/kubernetes/features/issues/361>Local Ephemeral Storage Capacity Isolation</a> (beta): Without this feature in place, every pod on a node that is using ephemeral storage is pulling from the same pool, and allocating storage is on a "best-effort" basis; in other words, a pod never knows for sure how much space it has available. This function provides the ability for a pod to reserve its own storage.</li><li><a href=https://github.com/kubernetes/features/issues/178>Out-of-tree CSI Volume Plugins</a> (beta): Kubernetes 1.9 announced the release of the Container Storage Interface, which provides a standard way for vendors to provide storage to Kubernetes. This function makes it possible for them to create drivers that live "out-of-tree", or out of the normal Kubernetes core. This means that vendors can control their own plugins and don't have to rely on the community for code reviews and approvals.</li><li><a href=https://github.com/kubernetes/features/issues/121>Local Persistent Storage</a> (beta): This feature enables PersistentVolumes to be created with locally attached disks, and not just network volumes.</li><li><a href=https://github.com/kubernetes/features/issues/498>Prevent deletion of Persistent Volume Claims that are used by a pod</a> (beta) and 7. <a href=https://github.com/kubernetes/features/issues/499>Prevent deletion of Persistent Volume that is bound to a Persistent Volume Claim</a> (beta): In previous versions of Kubernetes it was possible to delete storage that is in use by a pod, causing massive problems for the pod. These features provide validation that prevents that from happening.</li><li>Running out of storage space on your Persistent Volume? If you are, you can use <a href=https://github.com/kubernetes/features/issues/531>Add support for online resizing of PVs</a> (alpha) to enlarge the underlying volume it without disrupting existing data. This also works in conjunction with the new <a href=https://github.com/kubernetes/features/issues/304>Add resize support for FlexVolume</a> (alpha); FlexVolumes are vendor-supported volumes implemented through <a href=http://leebriggs.co.uk/blog/2017/03/12/kubernetes-flexvolumes.html>FlexVolume</a> plugins.</li><li><a href=https://github.com/kubernetes/features/issues/490>Topology Aware Volume Scheduling</a> (beta): This feature enables you to specify topology constraints on PersistentVolumes and have those constraints evaluated by the scheduler. It also delays the initial PersistentVolumeClaim binding until the Pod has been scheduled so that the volume binding decision is smarter and considers all Pod scheduling constraints as well. At the moment, this feature is most useful for local persistent volumes, but support for dynamic provisioning is under development.</li></ol><h3 id=node-management-sig-node>Node management (SIG-Node)</h3><ol><li><a href=https://github.com/kubernetes/features/issues/281>Dynamic Kubelet Configuration</a> (beta): Kubernetes makes it easy to make changes to existing clusters, such as increasing the number of replicas or making a service available over the network. This feature makes it possible to change Kubernetes itself (or rather, the Kubelet that runs Kubernetes behind the scenes) without bringing down the node on which Kubelet is running.</li><li><a href=https://github.com/kubernetes/features/issues/292>CRI validation test suite</a> (beta): The Container Runtime Interface (CRI) makes it possible to run containers other than Docker (such as Rkt containers or even virtual machines using Virtlet) on Kubernetes. This features provides a suite of validation tests to make certain that these CRI implementations are compliant, enabling developers to more easily find problems.</li><li><a href=https://github.com/kubernetes/features/issues/495>Configurable Pod Process Namespace Sharing</a> (alpha): Although pods can easily share the Kubernetes namespace, the process, or PID namespace has been a more difficult issue due to lack of support in Docker. This feature enables you to set a parameter on the pod to determine whether containers get their own operating system processes or share a single process.</li><li><a href=https://github.com/kubernetes/features/issues/547>Add support for Windows Container Configuration in CRI</a> (alpha): The Container Runtime Interface was originally designed with Linux-based containers in mind, and it was impossible to implement support for Windows-based containers using CRI. This feature solves that problem, making it possible to specify a WindowsContainerConfig.</li><li><a href=https://github.com/kubernetes/features/issues/277>Debug Containers</a> (alpha): It's easy to debug a container if you have the appropriate utilities. But what if you don't? This feature makes it possible to run debugging tools on a container even if those tools weren't included in the original container image.</li></ol><h3 id=other-changes>Other changes:</h3><ol><li>Deployment (SIG-Cluster Lifecycle): <a href=https://github.com/kubernetes/features/issues/88>Support out-of-process and out-of-tree cloud providers</a> (beta): As Kubernetes gains acceptance, more and more cloud providers will want to make it available. To do that more easily, the community is working on extracting provider-specific binaries so that they can be more easily replaced.</li><li>Kubernetes on Azure (SIG-Azure): Kubernetes has a cluster-autoscaler that automatically adds nodes to your cluster if you're running too many workloads, but until now it wasn't available on Azure. The <a href=https://github.com/kubernetes/features/issues/514>Add Azure support to cluster-autoscaler</a> (alpha) feature aims to fix that. Closely related, the <a href=https://github.com/kubernetes/features/issues/513>Add support for Azure Virtual Machine Scale Sets</a> (alpha) feature makes use of Azure's own autoscaling capabilities to make resources available.
You can download the Kubernetes 1.10 beta from . Again, if you've got feedback (and the community hopes you do) please add an issue to the <a href=https://github.com/kubernetes/kubernetes/milestone/37>1.10 milestone</a> and tag the relevant SIG before March 9.<br>_<br>(Many thanks to community members Michelle Au, Jan Šafránek, Eric Chiang, Michał Nasiadka, Radosław Pieczonka, Xing Yang, Daniel Smith, sylvain boily, Leo Sunmo, Michal Masłowski, Fernando Ripoll, ayodele abejide, Brett Kochendorfer, Andrew Randall, Casey Davenport, Duffie Cooley, Bryan Venteicher, Mark Ayers, Christopher Luciano, and Sandor Szuecs for their invaluable help in reviewing this article for accuracy.)_</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-b417e9a3d65d70ec0a327cce96177503>Reporting Errors from Control Plane to Applications Using Kubernetes Events</h1><div class="td-byline mb-4"><time datetime=2018-01-25 class=text-muted>Thursday, January 25, 2018</time></div><p>At <a href=https://www.box.com/>Box</a>, we manage several large scale Kubernetes clusters that serve as an internal platform as a service (PaaS) for hundreds of deployed microservices. The majority of those microservices are applications that power box.com for over 80,000 customers. The PaaS team also deploys several services affiliated with the platform infrastructure as the <em>control plane</em>.</p><p>One use case of Box’s control plane is <a href=https://en.wikipedia.org/wiki/Public_key_infrastructure>public key infrastructure</a> (<em>PKI</em>) processing. In our infrastructure, applications needing a new SSL certificate also need to trigger some processing in the control plane. The majority of our applications are not allowed to generate new SSL certificates due to security reasons. The control plane has a different security boundary and network access, and is therefore allowed to generate certificates.</p><p>| <img src="https://docs.google.com/a/linuxfoundation.org/drawings/d/snd-Vdn8h65V5wEBwU0KIqg/image?w=624&h=554&rev=303&ac=1" alt> |
| Figure1: Block Diagram of the PKI flow |</p><p>If an application needs a new certificate, the application owner explicitly adds a <a href=/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/>Custom Resource Definition</a> (CRD) to the application’s Kubernetes config [1]. This CRD specifies parameters for the SSL certificate: <em>name, common name, and others</em>. A microservice in the control plane watches CRDs and triggers some processing for SSL certificate generation [2]. Once the certificate is ready, the same control plane service sends it to the API server in a Kubernetes <a href=/docs/concepts/configuration/secret/>Secret</a> [3]. After that, the application containers access their certificates using Kubernetes <a href=/docs/concepts/storage/volumes/#secret>Secret VolumeMounts</a> [4]. You can see a working demo of this system in our <a href=https://github.com/box/error-reporting-with-kubernetes-events>example application</a> on GitHub.</p><p>The rest of this post covers the error scenarios in this “triggered” processing in the control plane. In particular, we are especially concerned with user input errors. Because the SSL certificate parameters come from the application’s config file in a CRD format, what should happen if there is an error in that CRD specification? Even a typo results in a failure of the SSL certificate creation. The error information is available in the control plane even though the root cause is most probably inside the application’s config file. The application owner does not have access to the control plane’s state or logs.</p><p>Providing the right diagnosis to the application owner so she can fix the mistake becomes a serious productivity problem at scale. Box’s rapid migration to microservices results in several new deployments every week. Numerous first time users, who do not know every detail of the infrastructure, need to succeed in deploying their services and troubleshooting problems easily. As the owners of the infrastructure, we do not want to be the bottleneck while reading the errors from the control plane logs and passing them on to application owners. If something in an owner’s configuration causes an error somewhere else, owners need a fully empowering diagnosis. This error data must flow automatically without any human involvement.</p><p>After considerable thought and experimentation, we found that <a href=https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/#event-v1-core>Kubernetes Events</a> work great to automatically communicate these kind of errors. If the error information is placed in a pod’s event stream, it shows up in kubectl describe output. Even beginner users can execute kubectl describe pod and obtain an error diagnosis.</p><p>We experimented with a status web page for the control plane service as an alternative to Kubernetes Events. We determined that the status page could update every time after processing an SSL certificate, and that application owners could probe the status page and get the diagnosis from there. After experimenting with a status page initially, we have seen that this does not work as effectively as the Kubernetes Events solution. The status page becomes a new interface to learn for the application owner, a new web address to remember, and one more context switch to a distinct tool during troubleshooting efforts. On the other hand, Kubernetes Events show up cleanly at the kubectl describe output, which is easily recognized by the developers.</p><p>Here is a simplified example showing how we used Kubernetes Events for error reporting across distinct services. We have open sourced a <a href=https://github.com/box/error-reporting-with-kubernetes-events>sample golang application</a> representative of the previously mentioned control plane service. It watches changes on CRDs and does input parameter checking. If an error is discovered, a Kubernetes Event is generated and the relevant pod’s event stream is updated.</p><p>The sample application executes this <a href=https://github.com/box/error-reporting-with-kubernetes-events/blob/master/cmd/controlplane/main.go#L201>code</a> to setup the Kubernetes Event generation:</p><pre><code>// eventRecorder returns an EventRecorder type that can be  
// used to post Events to different object's lifecycles.  
func eventRecorder(  
   kubeClient \*kubernetes.Clientset) (record.EventRecorder, error) {  
   eventBroadcaster := record.NewBroadcaster()  
   eventBroadcaster.StartLogging(glog.Infof)  
   eventBroadcaster.StartRecordingToSink(  
      &amp;typedcorev1.EventSinkImpl{  
         Interface: kubeClient.CoreV1().Events(&quot;&quot;)})  
   recorder := eventBroadcaster.NewRecorder(  
      scheme.Scheme,  
      v1.EventSource{Component: &quot;controlplane&quot;})  
   return recorder, nil  
}
</code></pre><p>After the one-time setup, the following <a href=https://github.com/box/error-reporting-with-kubernetes-events/blob/master/cmd/controlplane/main.go#L163>code</a> generates events affiliated with pods:</p><pre><code>ref, err := reference.GetReference(scheme.Scheme, &amp;pod)  
if err != nil {  
   glog.Fatalf(&quot;Could not get reference for pod %v: %v\n&quot;,  
      pod.Name, err)  
}  
recorder.Event(ref, v1.EventTypeWarning, &quot;pki ServiceName error&quot;,  
   fmt.Sprintf(&quot;ServiceName: %s in pki: %s is not found in&quot;+  
      &quot; allowedNames: %s&quot;, pki.Spec.ServiceName, pki.Name,  
      allowedNames))
</code></pre><p>Further implementation details can be understood by running the sample application.</p><p>As mentioned previously, here is the relevant kubectl describe output for the application owner.</p><pre><code>Events:  
  FirstSeen   LastSeen   Count   From         SubObjectPath   Type      Reason         Message  
  ---------   --------   -----   ----         -------------   --------   ------     
  ....  
  1d      1m      24   controlplane            Warning      pki ServiceName error   ServiceName: appp1 in pki: app1-pki is not found in allowedNames: [app1 app2]  
  ....  

</code></pre><p>We have demonstrated a practical use case with Kubernetes Events. The automated feedback to programmers in the case of configuration errors has significantly improved our troubleshooting efforts. In the future, we plan to use Kubernetes Events in various other applications under similar use cases. The recently created <a href=https://github.com/kubernetes/sample-controller>sample-controller</a> example also utilizes Kubernetes Events in a similar scenario. It is great to see there are more sample applications to guide the community. We are excited to continue exploring other use cases for Events and the rest of the Kubernetes API to make development easier for our engineers.</p><p><em>If you have a Kubernetes experience you’d like to share, <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>submit your story</a>. If you use Kubernetes in your organization and want to voice your experience more directly, consider joining the <a href=https://www.cncf.io/people/end-user-community/>CNCF End User Community</a> that Box and dozens of like-minded companies are part of.</em></p><p>Special thanks for Greg Lyons and Mohit Soni for their contributions.<br>Hakan Baba, Sr. Software Engineer, Box</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7e3c4ed0cfb0bab3da0672d198f0d0fd>Core Workloads API GA</h1><div class="td-byline mb-4"><time datetime=2018-01-15 class=text-muted>Monday, January 15, 2018</time></div><h2 id=daemonset-deployment-replicaset-and-statefulset-are-ga>DaemonSet, Deployment, ReplicaSet, and StatefulSet are GA</h2><p><strong><em>Editor’s Note: We’re happy to announce that the Core Workloads API is GA in Kubernetes 1.9! This blog post from Kenneth Owens reviews how Core Workloads got to GA from its origins, reveals changes in 1.9, and talks about what you can expect going forward.</em></strong></p><h2 id=in-the-beginning>In the Beginning …</h2><p>There were <a href=/docs/concepts/workloads/pods/pod-overview/>Pods</a>, tightly coupled containers that share resource requirements, networking, storage, and a lifecycle. Pods were useful, but, as it turns out, users wanted to seamlessly, reproducibly, and automatically create many identical replicas of the same Pod, so we created <a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationController</a>.</p><p>Replication was a step forward, but what users really needed was higher level orchestration of their replicated Pods. They wanted rolling updates, roll backs, and roll overs. So the OpenShift team created <a href=https://docs.openshift.org/latest/architecture/core_concepts/deployments.html#deployments-and-deployment-configurations>DeploymentConfig</a>. DeploymentConfigs were also useful, and OpenShift users were happy. In order to allow all OSS Kubernetes uses to share in the elation, and to take advantage of <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>set-based label selectors</a>, <a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> and <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> were added to the extensions/v1beta1 group version providing rolling updates, roll backs, and roll overs for all Kubernetes users.</p><p>That mostly solved the problem of orchestrating containerized 12 factor apps on Kubernetes, so the community turned its attention to a different problem. Replicating a Pod &lt;n> times isn’t the right hammer for every nail in your cluster. Sometimes, you need to run a Pod on every Node, or on a subset of Nodes (for example, shared side cars like log shippers and metrics collectors, Kubernetes add-ons, and Distributed File Systems). The state of the art was Pods combined with NodeSelectors, or static Pods, but this is unwieldy. After having grown used to the ease of automation provided by Deployments, users demanded the same features for this category of application, so <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> was added to extension/v1beta1 as well.</p><p>For a time, users were content, until they decided that Kubernetes needed to be able to orchestrate more than just 12 factor apps and cluster infrastructure. Whether your architecture is N-tier, service oriented, or micro-service oriented, your 12 factor apps depend on stateful workloads (for example, RDBMSs, distributed key value stores, and messaging queues) to provide services to end users and other applications. These stateful workloads can have availability and durability requirements that can only be achieved by distributed systems, and users were ready to use Kubernetes to orchestrate the entire stack.</p><p>While Deployments are great for stateless workloads, they don’t provide the right guarantees for the orchestration of distributed systems. These applications can require stable network identities, ordered, sequential deployment, updates, and deletion, and stable, durable storage. <a href=/docs/tasks/run-application/upgrade-pet-set-to-stateful-set/>PetSet</a> was added to the apps/v1beta1 group version to address this category of application. Unfortunately, <a href=https://github.com/kubernetes/kubernetes/issues/27430>we were less than thoughtful with its naming</a>, and, as we always strive to be an inclusive community, we renamed the kind to <a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>.</p><p>Finally, we were done.</p><p><img src=https://lh5.googleusercontent.com/0T36knExav8JAr41ict3EVOPOqaIJPMBQrOT2N5jehXw_12jEILD87tKW8BvaK2UCOtCHzS700Oki8Fxja3bF37J3eceanEBjbHpRsATBhC1y3P0mas7DvPeQjt6QmfYuNWDqZVl alt></p><p>...Or were we?</p><h2 id=kubernetes-1-8-and-apps-v1beta2>Kubernetes 1.8 and apps/v1beta2</h2><p>Pod, ReplicationController, ReplicaSet, Deployment, DaemonSet, and StatefulSet came to collectively be known as the core workloads API. We could finally orchestrate all of the things, but the API surface was spread across three groups, had many inconsistencies, and left users wondering about the stability of each of the core workloads kinds. It was time to stop adding new features and focus on consistency and stability.</p><p>Pod and ReplicationController were at GA stability, and even though you can run a workload in a Pod, it’s a nucleus primitive that belongs in core. As Deployments are the recommended way to manage your stateless apps, moving ReplicationController would serve no purpose. In Kubernetes 1.8, we moved all the other core workloads API kinds (Deployment, DaemonSet, ReplicaSet, and StatefulSet) to the apps/v1beta2 group version. This had the benefit of providing a better aggregation across the API surface, and allowing us to break backward compatibility to fix inconsistencies. Our plan was to promote this new surface to GA, wholesale and as is, when we were satisfied with its completeness. The modifications in this release, which are also implemented in apps/v1, are described below.</p><h3 id=selector-defaulting-deprecated>Selector Defaulting Deprecated</h3><p>In prior versions of the apps and extensions groups, label selectors of the core workloads API kinds were, when left unspecified, defaulted to a label selector generated from the kind’s template’s labels.</p><p>This was completely incompatible with strategic merge patch and kubectl apply. Moreover, we’ve found that defaulting the value of a field from the value of another field of the same object is an anti-pattern, in general, and particularly dangerous for the API objects used to orchestrate workloads.</p><h3 id=immutable-selectors>Immutable Selectors</h3><p>Selector mutation, while allowing for some use cases like promotable Deployment canaries, is not handled gracefully by our workload controllers, and we have always <a href=/docs/concepts/workloads/controllers/deployment/#label-selector-updates>strongly cautioned users against it</a>. To provide a consistent, usable, and stable API, selectors were made immutable for all kinds in the workloads API.</p><p>We believe that there are better ways to support features like promotable canaries and orchestrated Pod relabeling, but, if restricted selector mutation is a necessary feature for our users, we can relax immutability in the future without breaking backward compatibility.</p><p>The development of features like promotable canaries, orchestrated Pod relabeling, and restricted selector mutability is driven by demand signals from our users. If you are currently modifying the selectors of your core workload API objects, please tell us about your use case via a GitHub issue, or by participating in SIG apps.</p><h3 id=default-rolling-updates>Default Rolling Updates</h3><p>Prior to apps/v1beta2, some kinds defaulted their update strategy to something other than RollingUpdate (e.g. app/v1beta1/StatefulSet uses OnDelete by default). We wanted to be confident that RollingUpdate worked well prior to making it the default update strategy, and we couldn’t change the default behavior in released versions without breaking our promise with respect to backward compatibility. In apps/v1beta2 we enabled RollingUpdate for all core workloads kinds by default.</p><h3 id=createdby-annotation-deprecated>CreatedBy Annotation Deprecated</h3><p>The "kubernetes.io/created-by" was a legacy hold over from the days before garbage collection. Users should use an object’s ControllerRef from its ownerReferences to determine object ownership. We deprecated this feature in 1.8 and removed it in 1.9.</p><h3 id=scale-subresources>Scale Subresources</h3><p>A scale subresource was added to all of the applicable kinds in apps/v1beta2 (DaemonSet scales based on its node selector).</p><h2 id=kubernetes-1-9-and-apps-v1>Kubernetes 1.9 and apps/v1</h2><p>In Kubernetes 1.9, as planned, we promoted the entire core workloads API surface to GA in the apps/v1 group version. We made a few more changes to make the API consistent, but apps/v1 is mostly identical to apps/v1beta2. The reality is that most users have been treating the beta versions of the core workloads API as GA for some time now. Anyone who is still using ReplicationControllers and shying away from DaemonSets, Deployments, and StatefulSets, due to a perceived lack of stability, should plan migrate their workloads (where applicable) to apps/v1. The minor changes that were made during promotion are described below.</p><h3 id=garbage-collection-defaults-to-delete>Garbage Collection Defaults to Delete</h3><p>Prior to apps/v1 the default garbage collection policy for Pods in a DaemonSet, Deployment, ReplicaSet, or StatefulSet, was to orphan the Pods. That is, if you deleted one of these kinds, the Pods that they owned would not be deleted automatically unless cascading deletion was explicitly specified. If you use kubectl, you probably didn’t notice this, as these kinds are scaled to zero prior to deletion. In apps/v1 all core worloads API objects will now, by default, be deleted when their owner is deleted. For most users, this change is transparent.<br>Status Conditions</p><p>Prior to apps/v1 only Deployment and ReplicaSet had Conditions in their Status objects. For consistency's sake, either all of the objects or none of them should have conditions. After some debate, we decided that Conditions are useful, and we added Conditions to StatefulSetStatus and DaemonSetStatus. The StatefulSet and DaemonSet controllers currently don’t populate them, but we may choose communicate conditions to clients, via this mechanism, in the future.</p><h3 id=scale-subresource-migrated-to-autoscale-v1>Scale Subresource Migrated to autoscale/v1</h3><p>We originally added a scale subresource to the apps group. This was the wrong direction for integration with the autoscaling, and, at some point, we would like to use custom metrics to autoscale StatefulSets. So the apps/v1 group version uses the autoscaling/v1 scale subresource.</p><h2 id=migration-and-deprecation>Migration and Deprecation</h2><p>The question most you’re probably asking now is, “What’s my migration path onto apps/v1 and how soon should I plan on migrating?” All of the group versions prior to apps/v1 are deprecated as of Kubernetes 1.9, and all new code should be developed against apps/v1, but, as discussed above, many of our users treat extensions/v1beta1 as if it were GA. We realize this, and the minimum support timelines in our <a href=/docs/reference/deprecation-policy/>deprecation policy</a> are just that, minimums.</p><p>In future releases, before completely removing any of the group versions, we will disable them by default in the API Server. At this point, you will still be able to use the group version, but you will have to explicitly enable it. We will also provide utilities to upgrade the storage version of the API objects to apps/v1. Remember, all of the versions of the core workloads kinds are bidirectionally convertible. If you want to manually update your core workloads API objects now, you can use <a href=/docs/reference/generated/kubectl/kubectl-commands#convert>kubectl convert</a> to convert manifests between group versions.</p><h2 id=what-s-next>What’s Next?</h2><p>The core workloads API surface is stable, but it’s still software, and software is never complete. We often add features to stable APIs to support new use cases, and we will likely do so for the core workloads API as well. GA stability means that any new features that we do add will be strictly backward compatible with the existing API surface. From this point forward, nothing we do will break our backwards compatibility guarantees. If you’re looking to participate in the evolution of this portion of the API, please feel free to get involved in <a href=https://github.com/kubernetes/kubernetes>GitHub</a> or to participate in <a href=https://github.com/kubernetes/community/tree/master/sig-apps>SIG Apps</a>.</p><p>--Kenneth Owens, Software Engineer, Google</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e75b26684359faadd1d61c5d0c6cbe40>Introducing client-go version 6</h1><div class="td-byline mb-4"><time datetime=2018-01-12 class=text-muted>Friday, January 12, 2018</time></div><p>The Kubernetes API server <a href=https://blog.openshift.com/tag/api-server/>exposes a REST interface</a> consumable by any client. <a href=https://github.com/kubernetes/client-go>client-go</a> is the official client library for the Go programming language. It is used both internally by Kubernetes itself (for example, inside kubectl) as well as by <a href="https://github.com/search?q=k8s.io%2Fclient-go&type=Code&utf8=%E2%9C%93">numerous external consumers</a>:operators like the <a href=https://github.com/coreos/etcd-operator>etcd-operator</a> or <a href=https://github.com/coreos/prometheus-operator>prometheus-operator;</a>higher level frameworks like <a href=https://github.com/kubeless/kubeless>KubeLess</a> and <a href=https://openshift.io/>OpenShift</a>; and many more.</p><p>The version 6 update to client-go adds support for Kubernetes 1.9, allowing access to the latest Kubernetes features. While the <a href=https://github.com/kubernetes/client-go/blob/master/CHANGELOG.md>changelog</a> contains all the gory details, this blog post highlights the most prominent changes and intends to guide on how to upgrade from version 5.</p><p>This blog post is one of a number of efforts to make client-go more accessible to third party consumers. Easier access is a joint effort by a number of people from numerous companies, all meeting in the #client-go-docs channel of the <a href=http://slack.k8s.io/>Kubernetes Slack</a>. We are happy to hear feedback and ideas for further improvement, and of course appreciate anybody who wants to contribute.</p><h2 id=api-group-changes>API group changes</h2><p>The following API group promotions are part of Kubernetes 1.9:</p><ul><li>Workload objects (Deployments, DaemonSets, ReplicaSets, and StatefulSets) have been <a href=/docs/reference/workloads-18-19/>promoted to the apps/v1 API group in Kubernetes 1.9</a>. client-go follows this transition and allows developers to use the latest version by importing the k8s.io/api/apps/v1 package instead of k8s.io/api/apps/v1beta1 and by using Clientset.AppsV1().</li><li>Admission Webhook Registration has been promoted to the admissionregistration.k8s.io/v1beta1 API group in Kubernetes 1.9. The former ExternalAdmissionHookConfiguration type has been replaced by the incompatible ValidatingWebhookConfiguration and MutatingWebhookConfiguration types. Moreover, the webhook admission payload type AdmissionReview in admission.k8s.io has been promoted to v1beta1. Note that versioned objects are now passed to webhooks. Refer to the admission webhook <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>documentation</a> for details.</li></ul><h2 id=validation-for-customresources>Validation for CustomResources</h2><p>In Kubernetes 1.8 we introduced CustomResourceDefinitions (CRD) <a href=/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation>pre-persistence schema validation</a> as an alpha feature. With 1.9, the feature got promoted to beta and will be enabled by default. As a client-go user, you will find the API types at k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1beta1.</p><p>The <a href=https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject>OpenAPI v3 schema</a> can be defined in the CRD spec as:</p><pre><code>
apiVersion: apiextensions.k8s.io/v1beta1  
kind: CustomResourceDefinition  
metadata: ...  
spec:  
  ...  
  validation:  
    openAPIV3Schema:  
      properties:  
        spec:  
          properties:  
            version:  
                type: string  
                enum:  
                - &quot;v1.0.0&quot;  
                - &quot;v1.0.1&quot;  
            replicas:  
                type: integer  
                minimum: 1  
                maximum: 10

</code></pre><p>The schema in the above CRD applies following validations for the instance:</p><ol><li>spec.version must be a string and must be either “v1.0.0” or “v1.0.1”.</li><li>spec.replicas must be an integer and must have a minimum value of 1 and a maximum value of 10.
A CustomResource with invalid values for spec.version (v1.0.2) and spec.replicas (15) will be rejected:</li></ol><pre><code>
apiVersion: mygroup.example.com/v1  
kind: App  
metadata:  
  name: example-app  
spec:  
  version: &quot;v1.0.2&quot;  
  replicas: 15

</code></pre><pre><code>$ kubectl create -f app.yaml

The App &quot;example-app&quot; is invalid: []: Invalid value: map[string]interface {}{&quot;apiVersion&quot;:&quot;mygroup.example.com/v1&quot;, &quot;kind&quot;:&quot;App&quot;, &quot;metadata&quot;:map[string]interface {}{&quot;creationTimestamp&quot;:&quot;2017-08-31T20:52:54Z&quot;, &quot;uid&quot;:&quot;5c674651-8e8e-11e7-86ad-f0761cb232d1&quot;, &quot;clusterName&quot;:&quot;&quot;, &quot;name&quot;:&quot;example-app&quot;, &quot;namespace&quot;:&quot;default&quot;, &quot;deletionTimestamp&quot;:interface {}(nil), &quot;deletionGracePeriodSeconds&quot;:(\*int64)(nil)}, &quot;spec&quot;:map[string]interface {}{&quot;replicas&quot;:15, &quot;version&quot;:&quot;v1.0.2&quot;}}:
validation failure list:  
spec.replicas in body should be less than or equal to 10  
spec.version in body should be one of [v1.0.0 v1.0.1]
</code></pre><p>Note that with <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>Admission Webhooks</a>, Kubernetes 1.9 provides another beta feature to validate objects before they are created or updated. Starting with 1.9, these webhooks also allow mutation of objects (for example, to set defaults or to inject values). Of course, webhooks work with CRDs as well. Moreover, webhooks can be used to implement validations that are not easily expressible with CRD validation. Note that webhooks are harder to implement than CRD validation, so for many purposes, CRD validation is the right tool.</p><h2 id=creating-namespaced-informers>Creating namespaced informers</h2><p>Often objects in one namespace or only with certain labels are to be processed in a controller. Informers <a href=https://github.com/kubernetes/kubernetes/pull/54660>now allow</a> you to tweak the ListOptions used to query the API server to list and watch objects. Uninitialized objects (for consumption by <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-initializers>initializers</a>) can be made visible by setting IncludeUnitialized to true. All this can be done using the new NewFilteredSharedInformerFactory constructor for shared informers:</p><pre><code>
import “k8s.io/client-go/informers”
...  
sharedInformers := informers.NewFilteredSharedInformerFactory(  
 client,  
 30\*time.Minute,   
 “some-namespace”,  
 func(opt \*metav1.ListOptions) {  
  opt.LabelSelector = “foo=bar”  
 },  
)  
</code></pre><p>Note that the corresponding lister will only know about the objects matching the namespace and the given ListOptions. Note that the same restrictions apply for a List or Watch call on a client.</p><p>This <a href=https://github.com/jetstack/cert-manager/blob/b978faa28c9f0fb0414b5d7293fab7bde65bde76/cmd/controller/app/controller.go#L123>production code example</a> of a cert-manager demonstrates how namespace informers can be used in real code.</p><h2 id=polymorphic-scale-client>Polymorphic scale client</h2><p>Historically, only types in the extensions API group would work with autogenerated Scale clients. Furthermore, different API groups use different Scale types for their /scale subresources. To remedy these issues, k8s.io/client-go/scale provides a <a href=https://github.com/kubernetes/client-go/tree/master/scale>polymorphic scale client</a> to scale different resources in different API groups in a coherent way:</p><pre><code>
import (


apimeta &quot;k8s.io/apimachinery/pkg/api/meta&quot;

 discocache &quot;k8s.io/client-go/discovery/cached&quot;  
 &quot;k8s.io/client-go/discovery&quot;

&quot;k8s.io/client-go/dynamic&quot;

“k8s.io/client-go/scale”  
)

...

cachedDiscovery := discocache.NewMemCacheClient(client.Discovery())  
restMapper := discovery.NewDeferredDiscoveryRESTMapper(

cachedDiscovery,

apimeta.InterfacesForUnstructured,

)  
scaleKindResolver := scale.NewDiscoveryScaleKindResolver(

client.Discovery(),

)  
scaleClient, err := scale.NewForConfig(

client, restMapper,

dynamic.LegacyAPIPathResolverFunc,

scaleKindResolver,

)
scale, err := scaleClient.Scales(&quot;default&quot;).Get(groupResource, &quot;foo&quot;)

</code></pre><p>The returned scale object is generic and is exposed as the autoscaling/v1.Scale object. It is backed by an internal Scale type, with conversions defined to and from all the special Scale types in the API groups supporting scaling. We planto <a href=https://github.com/kubernetes/kubernetes/pull/55168>extend this to CustomResources in 1.10</a>.</p><p>If you’re implementing support for the scale subresource, we recommend that you expose the autoscaling/v1.Scale object.</p><h2 id=type-safe-deepcopy>Type-safe DeepCopy</h2><p>Deeply copying an object formerly required a call to Scheme.Copy(Object) with the notable disadvantage of losing type safety. A typical piece of code from client-go version 5 required type casting:</p><pre><code>
newObj, err := runtime.NewScheme().Copy(node)


if err != nil {

    return fmt.Errorf(&quot;failed to copy node %v: %s”, node, err)

}


newNode, ok := newObj.(\*v1.Node)

if !ok {

    return fmt.Errorf(&quot;failed to type-assert node %v&quot;, newObj)


}

</code></pre><p>Thanks to <a href=https://github.com/kubernetes/code-generator>k8s.io/code-generator</a>, Copy has now been replaced by a type-safe DeepCopy method living on each object, allowing you to simplify code significantly both in terms of volume and API error surface:</p><p>newNode := node.DeepCopy()</p><p>No error handling is necessary: this call never fails. If and only if the node is nil does DeepCopy() return nil.</p><p>To copy runtime.Objects there is an additional DeepCopyObject() method in the runtime.Object interface.</p><p>With the old method gone for good, clients need to update their copy invocations accordingly.</p><h2 id=code-generation-and-customresources>Code generation and CustomResources</h2><p>Using client-go’s dynamic client to access CustomResources is discouraged and superseded by type-safe code using the generators in <a href=https://github.com/kubernetes/code-generator>k8s.io/code-generator</a>. Check out the <a href=https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/>Deep Dive on the Open Shift blog</a> to learn about using code generation with client-go.</p><h3 id=comment-blocks>Comment Blocks</h3><p>You can now place tags in the comment block just above a type or function, or in the second block above. There is no distinction anymore between these two comment blocks. This used to a be a source of <a href=https://github.com/kubernetes/kubernetes/issues/53893>subtle errors when using the generators</a>:</p><pre><code>// second block above  
// +k8s:some-tag  

// first block above  
// +k8s:another-tag  
type Foo struct {}
</code></pre><h3 id=custom-client-methods>Custom Client Methods</h3><p>You can now use extended tag definitions to create custom verbs . This lets you expand beyond the verbs defined by HTTP. This opens the door to higher levels of customization.</p><p>For example, this block leads to the generation of the method UpdateScale(s *autoscaling.Scale) (*autoscaling.Scale, error):</p><pre><code>// genclient:method=UpdateScale,verb=update,subresource=scale,input=k8s.io/kubernetes/pkg/apis/autoscaling.Scale,result=k8s.io/kubernetes/pkg/apis/autoscaling.Scale
</code></pre><h3 id=resolving-golang-naming-conflicts>Resolving Golang Naming Conflicts</h3><p>In more complex API groups it’s possible for Kinds, the group name, the Go package name, and the Go group alias name to conflict. This was not handled correctly prior to 1.9. The following tags resolve naming conflicts and make the generated code prettier:</p><pre><code>// +groupName=example2.example.com  
// +groupGoName=SecondExample
</code></pre><p>These are usually <a href=https://github.com/kubernetes/code-generator/blob/release-1.9/_examples/crd/apis/example2/v1/doc.go#L18>in the doc.go file of an API package</a>. The first is used as the CustomResource group name when RESTfully speaking to the API server using HTTP. The second is used in the generated Golang code (for example, in the clientset) to access the group version:</p><p>clientset.SecondExampleV1()</p><p>It’s finally possible to have dots in Go package names. In this section’s example, you would put the groupName snippet into the pkg/apis/example2.example.com directory of your project.</p><h2 id=example-projects>Example projects</h2><p>Kubernetes 1.9 includes a number of example projects which can serve as a blueprint for your own projects:</p><ul><li><a href=https://github.com/kubernetes/sample-apiserver>k8s.io/sample-apiserver</a> is a simple user-provided API server that is integrated into a cluster via <a href=/docs/concepts/api-extension/apiserver-aggregation/>API aggregation</a>.</li><li><a href=https://github.com/kubernetes/sample-controller>k8s.io/sample-controller</a> is a full-featured <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md>controller</a> (also called an operator) with shared informers and a workqueue to process created, changed or deleted objects. It is based on CustomResourceDefinitions and uses <a href=https://github.com/kubernetes/code-generator>k8s.io/code-generator</a> to generate deepcopy functions, typed clientsets, informers, and listers.</li></ul><h2 id=vendoring>Vendoring</h2><p>In order to update from the previous version 5 to version 6 of client-go, the library itself as well as certain third-party dependencies must be updated. Previously, this process had been tedious due to the fact that a lot of code got refactored or relocated within the existing package layout across releases. Fortunately, far less code had to move in the latest version, which should ease the upgrade procedure for most users.</p><h3 id=state-of-the-published-repositories>State of the published repositories</h3><p>In the past <a href=https://github.com/kubernetes/client-go>k8s.io/client-go</a>, <a href=https://github.com/kubernetes/api>k8s.io/api</a>, and <a href=https://github.com/kubernetes/apimachinery>k8s.io/apimachinery</a> were updated infrequently. Tags (for example, v4.0.0) were created quite some time after the Kubernetes releases. With the 1.9 release we resumed running a nightly bot that updates all the repositories for public consumption, even before manual tagging. This includes the branches:</p><ul><li>master</li><li>release-1.8 / release-5.0</li><li>release-1.9 / release-6.0
Kubernetes tags (for example, v1.9.1-beta1) are also applied automatically to the published repositories, prefixed with kubernetes- (for example, kubernetes-1.9.1-beta1).</li></ul><p>These tags have limited test coverage, but can be used by early adopters of client-go and the other libraries. Moreover, they help to vendor the correct version of <a href=https://github.com/kubernetes/api>k8s.io/api</a> and <a href=https://github.com/kubernetes/apimachinery>k8s.io/apimachinery</a>. Note that we only create a v6.0.3-like semantic versioning tag on <a href=https://github.com/kubernetes/client-go>k8s.io/client-go</a>. The corresponding tag for k8s.io/api and k8s.io/apimachinery is kubernetes-1.9.3.</p><p>Also note that only these tags correspond to tested releases of Kubernetes. If you depend on the release branch, e.g., release-1.9, your client is running on unreleased Kubernetes code.</p><h3 id=state-of-vendoring-of-client-go>State of vendoring of client-go</h3><p>In general, the list of which dependencies to vendor is automatically generated and written to the file Godeps/Godeps.json. Only the revisions listed there are tested. This means especially that we do not and cannot test the code-base against master branches of our dependencies. This puts us in the following situation depending on the used vendoring tool:</p><ul><li><a href=https://github.com/tools/godep>godep</a> reads Godeps/Godeps.json by running godep restore from k8s.io/client-go in your GOPATH. Then use godep save to vendor in your project. godep will choose the correct versions from your GOPATH.</li><li><a href=https://github.com/Masterminds/glide>glide</a> reads Godeps/Godeps.json automatically from its dependencies including from k8s.io/client-go, both on init and on update. Hence, glide should be mostly automatic as long as there are no conflicts.</li><li><a href=https://github.com/golang/dep>dep</a> does not currently respect Godeps/Godeps.json in a consistent way, especially not on updates. It is crucial to specify client-go dependencies manually as constraints or overrides, also for non k8s.io/* dependencies. Without those, dep simply chooses the dependency master branches, which can cause problems as they are updated frequently.</li><li>The Kubernetes and golang/dep community are aware of the problems [<a href=https://github.com/golang/dep/issues/1124>issue #1124</a>, <a href=https://github.com/golang/dep/issues/1236>issue #1236</a>] and <a href=https://github.com/kubernetes-dep-experiment/client-go>are working together on solutions</a>. Until then special care must be taken.
Please see client-go’s <a href=https://github.com/kubernetes/client-go/blob/master/INSTALL.md>INSTALL.md</a> for more details.</li></ul><h3 id=updating-dependencies-golang-dep>Updating dependencies – golang/dep</h3><p>Even with the deficiencies of golang/dep today, dep is slowly becoming the de-facto standard in the Go ecosystem. With the necessary care and the awareness of the missing features, dep can be (and is!) used successfully. Here’s a demonstration of how to update a project with client-go 5 to the latest version 6 using dep:</p><p>(If you are still running client-go version 4 and want to play it safe by not skipping a release, now is a good time to check out <a href=https://medium.com/@andy.goldstein/upgrading-kubernetes-client-go-from-v4-to-v5-bbd5025fe381>this excellent blog post</a> describing how to upgrade to version 5, put together by our friends at Heptio.)</p><p>Before starting, it is important to understand that client-go depends on two other Kubernetes projects: <a href=https://github.com/kubernetes/apimachinery>k8s.io/apimachinery</a> and <a href=https://github.com/kubernetes/api>k8s.io/api</a>. In addition, if you are using CRDs, you probably also depend on <a href=https://github.com/kubernetes/apiextensions-apiserver>k8s.io/apiextensions-apiserver</a> for the CRD client. The first exposes lower-level API mechanics (such as schemes, serialization, and type conversion), the second holds API definitions, and the third provides APIs related to CustomResourceDefinitions. In order for client-go to operate correctly, it needs to have its companion libraries vendored in correspondingly matching versions. Each library repository provides a branch named release-<em>&lt;version></em> where <em>&lt;version></em> refers to a particular Kubernetes version; for client-go version 6, it is imperative to refer to the <em>release</em>-1.9 branch on each repository.</p><p>Assuming the latest version 5 patch release of client-go being vendored through dep, the Gopkg.toml manifest file should look something like this (possibly using branches instead of versions):</p><pre><code>




[[constraint]]


  name = &quot;k8s.io/api&quot;

  version = &quot;kubernetes-1.8.1&quot;


[[constraint]]

  name = &quot;k8s.io/apimachinery&quot;

  version = &quot;kubernetes-1.8.1&quot;


[[constraint]]

  name = &quot;k8s.io/apiextensions-apiserver&quot;

  version = &quot;kubernetes-1.8.1&quot;


[[constraint]]

  name = &quot;k8s.io/client-go&quot;




  version = &quot;5.0.1&quot;

</code></pre><p>Note that some of the libraries could be missing if they are not actually needed by the client.</p><p>Upgrading to client-go version 6 means bumping the version and tag identifiers as following ( <strong>emphasis</strong> given):</p><pre><code>




[constraint]]


  name = &quot;k8s.io/api&quot;

  version = &quot;kubernetes-1.9.0&quot;


[[constraint]]

  name = &quot;k8s.io/apimachinery&quot;

  version = &quot;kubernetes-1.9.0&quot;


[[constraint]]

  name = &quot;k8s.io/apiextensions-apiserver&quot;

  version = &quot;kubernetes-1.9.0&quot;


[[constraint]]

  name = &quot;k8s.io/client-go&quot;




  version = &quot;6.0.0&quot;



</code></pre><p>The result of the upgrade can be found <a href=https://github.com/ncdc/client-go-4-to-5/tree/v5-to-v6>here</a>.</p><p>A note of caution: dep cannot capture the complete set of dependencies in a reliable and reproducible fashion as described above. This means that for a 100% future-proof project you have to add constraints (or even overrides) to many other packages listed in client-go’s Godeps/Godeps.json. Be prepared to add them if something breaks. We are working with the golang/dep community to make this an easier and more smooth experience.</p><p>Finally, we need to tell dep to upgrade to the specified versions by executing dep ensure. If everything goes well, the output of the command invocation should be empty, with the only indication that it was successful being a number of updated files inside the vendor folder.</p><p>If you are using CRDs, you probably also use code-generation. The following block for Gopkg.toml will add the required code-generation packages to your project:</p><pre><code>
required = [  
  &quot;k8s.io/code-generator/cmd/client-gen&quot;,  
  &quot;k8s.io/code-generator/cmd/conversion-gen&quot;,  
  &quot;k8s.io/code-generator/cmd/deepcopy-gen&quot;,  
  &quot;k8s.io/code-generator/cmd/defaulter-gen&quot;,  
  &quot;k8s.io/code-generator/cmd/informer-gen&quot;,  
  &quot;k8s.io/code-generator/cmd/lister-gen&quot;,  
]


[[constraint]]

  branch = &quot;kubernetes-1.9.0&quot;


  name = &quot;k8s.io/code-generator&quot;

</code></pre><p>Whether you would also like to prune unneeded packages (such as test files) through dep or commit the changes into the VCS at this point is up to you -- but from an upgrade perspective, you should now be ready to harness all the fancy new features that Kubernetes 1.9 brings through client-go.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e69ea2d567ef1a27cb6eae6d90305e92>Extensible Admission is Beta</h1><div class="td-byline mb-4"><time datetime=2018-01-11 class=text-muted>Thursday, January 11, 2018</time></div><p>In this post we review a feature, available in the Kubernetes API server, that allows you to implement arbitrary control decisions and which has matured considerably in Kubernetes 1.9.</p><p>The admission stage of API server processing is one of the most powerful tools for securing a Kubernetes cluster by restricting the objects that can be created, but it has always been limited to compiled code. In 1.9, we promoted webhooks for admission to beta, allowing you to leverage admission from outside the API server process.</p><h2 id=what-is-admission>What is Admission?</h2><p><a href=/docs/reference/access-authn-authz/admission-controllers/#what-are-they>Admission</a> is the phase of <a href=https://blog.openshift.com/kubernetes-deep-dive-api-server-part-1/>handling an API server request</a> that happens before a resource is persisted, but after authorization. Admission gets access to the same information as authorization (user, URL, etc) and the complete body of an API request (for most requests).</p><p><a href=https://2.bp.blogspot.com/-p8WGg2BATsY/WlfywbD_tAI/AAAAAAAAAJw/mDqZV0dB4_Y0gXXQp_1tQ7CtMRSd6lHVwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-01-11%2Bat%2B3.22.07%2BPM.png><img src=https://2.bp.blogspot.com/-p8WGg2BATsY/WlfywbD_tAI/AAAAAAAAAJw/mDqZV0dB4_Y0gXXQp_1tQ7CtMRSd6lHVwCK4BGAYYCw/s640/Screen%2BShot%2B2018-01-11%2Bat%2B3.22.07%2BPM.png alt></a></p><p>The admission phase is composed of individual plugins, each of which are narrowly focused and have semantic knowledge of what they are inspecting. Examples include: PodNodeSelector (influences scheduling decisions), PodSecurityPolicy (prevents escalating containers), and ResourceQuota (enforces resource allocation per namespace).</p><p>Admission is split into two phases:</p><ol><li>Mutation, which allows modification of the body content itself as well as rejection of an API request.</li><li>Validation, which allows introspection queries and rejection of an API request.
An admission plugin can be in both phases, but all mutation happens before validation.</li></ol><h3 id=mutation>Mutation</h3><p>The mutation phase of admission allows modification of the resource content before it is persisted. Because the same field can be mutated multiple times while in the admission chain, the order of the admission plugins in the mutation matters.</p><p>One example of a mutating admission plugin is the <code>PodNodeSelector</code> plugin, which uses an annotation on a namespace <code>namespace.annotations[“scheduler.alpha.kubernetes.io/node-selector”]</code> to find a label selector and add it to the <code>pod.spec.nodeselector</code> field. This positively restricts which nodes the pods in a particular namespace can land on, as opposed to taints, which provide negative restriction (also with an admission plugin).</p><h3 id=validation>Validation</h3><p>The validation phase of admission allows the enforcement of invariants on particular API resources. The validation phase runs after all mutators finish to ensure that the resource isn’t going to change again.</p><p>One example of a validation admission plugin is also the <code>PodNodeSelector</code> plugin, which ensures that all pods’ <code>spec.nodeSelector</code> fields are constrained by the node selector restrictions on the namespace. Even if a mutating admission plugin tries to change the <code>spec.nodeSelector</code> field after the PodNodeSelector runs in the mutating chain, the PodNodeSelector in the validating chain prevents the API resource from being created because it fails validation.</p><h2 id=what-are-admission-webhooks>What are admission webhooks?</h2><p>Admission webhooks allow a Kubernetes installer or a cluster-admin to add mutating and validating admission plugins to the admission chain of <code>kube-apiserver</code> as well as any extensions apiserver based on k8s.io/apiserver 1.9, like <a href=https://github.com/kubernetes/metrics>metrics</a>, <a href=https://github.com/kubernetes-incubator/service-catalog>service-catalog</a>, or <a href=https://github.com/openshift/kube-projects>kube-projects</a>, without recompiling them. Both kinds of admission webhooks run at the end of their respective chains and have the same powers and limitations as compiled admission plugins.</p><h3 id=what-are-they-good-for>What are they good for?</h3><p>Webhook admission plugins allow for mutation and validation of any resource on any API server, so the possible applications are vast. Some common use-cases include:</p><ol><li>Mutation of resources like pods. Istio has talked about doing this to inject side-car containers into pods. You could also write a plugin which forcefully resolves image tags into image SHAs.</li><li>Name restrictions. On multi-tenant systems, reserving namespaces has emerged as a use-case.</li><li>Complex CustomResource validation. Because the entire object is visible, a clever admission plugin can perform complex validation on dependent fields (A requires B) and even external resources (compare to LimitRanges).</li><li>Security response. If you forced image tags into image SHAs, you could write an admission plugin that prevents certain SHAs from running.</li></ol><h3 id=registration>Registration</h3><p>Webhook admission plugins of both types are registered in the API, and all API servers (kube-apiserver and all extension API servers) share a common config for them. During the registration process, a webhook admission plugin describes:</p><ol><li>How to connect to the webhook admission server</li><li>How to verify the webhook admission server (Is it really the server I expect?)</li><li>Where to send the data at that server (which URL path)</li><li>Which resources and which HTTP verbs it will handle</li><li>What an API server should do on connection failures (for example, if the admission webhook server goes down)</li></ol><pre><code>1 apiVersion: admissionregistration.k8s.io/v1beta1  
2 kind: ValidatingWebhookConfiguration  
3 metadata:  
4   name: namespacereservations.admission.online.openshift.io  
5 webhooks:  
6 - name: namespacereservations.admission.online.openshift.io  
7   clientConfig:  
8     service:  
9       namespace: default  
10      name: kubernetes  
11     path: /apis/admission.online.openshift.io/v1alpha1/namespacereservations  
12    caBundle: KUBE\_CA\_HERE  
13  rules:  
14  - operations:  
15    - CREATE  
16    apiGroups:  
17    - &quot;&quot;  
18    apiVersions:  
19    - &quot;\*&quot;  
20    resources:  
21    - namespaces  
22  failurePolicy: Fail
</code></pre><p>Line 6: <code>name</code> - the name for the webhook itself. For mutating webhooks, these are sorted to provide ordering.<br>Line 7: <code>clientConfig</code> - provides information about how to connect to, trust, and send data to the webhook admission server.<br>Line 13: <code>rules</code> - describe when an API server should call this admission plugin. In this case, only for creates of namespaces. You can specify any resource here so specifying creates of <code>serviceinstances.servicecatalog.k8s.io</code> is also legal.<br>Line 22: <code>failurePolicy</code> - says what to do if the webhook admission server is unavailable. Choices are “Ignore” (fail open) or “Fail” (fail closed). Failing open makes for unpredictable behavior for all clients.</p><h3 id=authentication-and-trust>Authentication and trust</h3><p>Because webhook admission plugins have a lot of power (remember, they get to see the API resource content of any request sent to them and might modify them for mutating plugins), it is important to consider:</p><ul><li>How individual API servers verify their connection to the webhook admission server</li><li>How the webhook admission server authenticates precisely which API server is contacting it</li><li>Whether that particular API server has authorization to make the request
There are three major categories of connection:</li></ul><ol><li>From kube-apiserver or extension-apiservers to externally hosted admission webhooks (webhooks not hosted in the cluster)</li><li>From kube-apiserver to self-hosted admission webhooks</li><li>From extension-apiservers to self-hosted admission webhooks
To support these categories, the webhook admission plugins accept a kubeconfig file which describes how to connect to individual servers. For interacting with externally hosted admission webhooks, there is really no alternative to configuring that file manually since the authentication/authorization and access paths are owned by the server you’re hooking to.</li></ol><p>For the self-hosted category, a cleverly built webhook admission server and topology can take advantage of the safe defaulting built into the admission plugin and have a secure, portable, zero-config topology that works from any API server.</p><h3 id=simple-secure-portable-zero-config-topology>Simple, secure, portable, zero-config topology</h3><p>If you build your webhook admission server to also be an extension API server, it becomes possible to aggregate it as a normal API server. This has a number of advantages:</p><ul><li>Your webhook becomes available like any other API under default kube-apiserver service <code>kubernetes.default.svc</code> (e.g. <a href=https://kubernetes.default.svc/apis/admission.example.com/v1/mymutatingadmissionreviews>https://kubernetes.default.svc/apis/admission.example.com/v1/mymutatingadmissionreviews</a>). Among other benefits, you can test using <code>kubectl</code>.</li><li>Your webhook automatically (without any config) makes use of the in-cluster authentication and authorization provided by kube-apiserver. You can restrict access to your webhook with normal RBAC rules.</li><li>Your extension API servers and kube-apiserver automatically (without any config) make use of their in-cluster credentials to communicate with the webhook.</li><li>Extension API servers do not leak their service account token to your webhook because they go through kube-apiserver, which is a secure front proxy.</li></ul><p><img src=https://lh6.googleusercontent.com/FeXoJLmbhf5exSBQu6Wxd2sIEqzkKPbRA_iv6T2QmJbhRsO4FyPtgAAbHdAmuTrE0jVEUzftfxcPndN8ACzstfsX9XTFdQFrioS1srvYgVP3l99R6x-vvd3RfBA4eWttaKRWj6iA alt><br><em>Source: <a href=https://drive.google.com/a/redhat.com/file/d/12nC9S2fWCbeX_P8nrmL6NgOSIha4HDNp>https://drive.google.com/a/redhat.com/file/d/12nC9S2fWCbeX_P8nrmL6NgOSIha4HDNp</a></em></p><p>In short: a secure topology makes use of all security mechanisms of API server aggregation and additionally requires no additional configuration.</p><p>Other topologies are possible but require additional manual configuration as well as a lot of effort to create a secure setup, especially when extension API servers like service catalog come into play. The topology above is zero-config and portable to every Kubernetes cluster.</p><h3 id=how-do-i-write-a-webhook-admission-server>How do I write a webhook admission server?</h3><p>Writing a full server complete with authentication and authorization can be intimidating. To make it easier, there are projects based on Kubernetes 1.9 that provide a library for building your webhook admission server in 200 lines or less. Take a look at the <a href=https://github.com/openshift/generic-admission-server>generic-admission-apiserver</a> and the <a href=https://github.com/openshift/kubernetes-namespace-reservation>kubernetes-namespace-reservation</a> projects for the library and an example of how to build your own secure and portable webhook admission server.</p><p>With the admission webhooks introduced in 1.9 we’ve made Kubernetes even more adaptable to your needs. We hope this work, driven by both Red Hat and Google, will enable many more workloads and support ecosystem components. (Istio is one example.) Now is a good time to give it a try!</p><p>If you’re interested in giving feedback or contributing to this area, join us in the <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API machinery</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9566f2f64212ff7827bef718c0f48163>Introducing Container Storage Interface (CSI) Alpha for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2018-01-10 class=text-muted>Wednesday, January 10, 2018</time></div><p>One of the key differentiators for Kubernetes has been a powerful <a href=/docs/concepts/storage/volumes/>volume plugin system</a> that enables many different types of storage systems to:</p><ol><li>Automatically create storage when required.</li><li>Make storage available to containers wherever they’re scheduled.</li><li>Automatically delete the storage when no longer needed.
Adding support for new storage systems to Kubernetes, however, has been challenging.</li></ol><p>Kubernetes 1.9 introduces an <a href=https://github.com/kubernetes/features/issues/178>alpha implementation of the Container Storage Interface (CSI)</a> which makes installing new volume plugins as easy as deploying a pod. It also enables third-party storage providers to develop solutions without the need to add to the core Kubernetes codebase.</p><p>Because the feature is alpha in 1.9, it must be explicitly enabled. Alpha features are not recommended for production usage, but are a good indication of the direction the project is headed (in this case, towards a more extensible and standards based Kubernetes storage ecosystem).</p><h3 id=why-kubernetes-csi>Why Kubernetes CSI?</h3><p>Kubernetes volume plugins are currently “in-tree”, meaning they’re linked, compiled, built, and shipped with the core kubernetes binaries. Adding support for a new storage system to Kubernetes (a volume plugin) requires checking code into the core Kubernetes repository. But aligning with the Kubernetes release process is painful for many plugin developers.</p><p>The existing <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md>Flex Volume plugin</a> attempted to address this pain by exposing an exec based API for external volume plugins. Although it enables third party storage vendors to write drivers out-of-tree, in order to deploy the third party driver files it requires access to the root filesystem of node and master machines.</p><p>In addition to being difficult to deploy, Flex did not address the pain of plugin dependencies: Volume plugins tend to have many external requirements (on mount and filesystem tools, for example). These dependencies are assumed to be available on the underlying host OS which is often not the case (and installing them requires access to the root filesystem of node machine).</p><p>CSI addresses all of these issues by enabling storage plugins to be developed out-of-tree, containerized, deployed via standard Kubernetes primitives, and consumed through the Kubernetes storage primitives users know and love (PersistentVolumeClaims, PersistentVolumes, StorageClasses).</p><h3 id=what-is-csi>What is CSI?</h3><p>The goal of CSI is to establish a standardized mechanism for Container Orchestration Systems (COs) to expose arbitrary storage systems to their containerized workloads. The CSI specification emerged from cooperation between community members from various Container Orchestration Systems (COs)--including Kubernetes, Mesos, Docker, and Cloud Foundry. The specification is developed, independent of Kubernetes, and maintained at <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>https://github.com/container-storage-interface/spec/blob/master/spec.md</a>.</p><p>Kubernetes v1.9 exposes an alpha implementation of the CSI specification enabling CSI compatible volume drivers to be deployed on Kubernetes and consumed by Kubernetes workloads.</p><h3 id=how-do-i-deploy-a-csi-driver-on-a-kubernetes-cluster>How do I deploy a CSI driver on a Kubernetes Cluster?</h3><p>CSI plugin authors will provide their own instructions for deploying their plugin on Kubernetes.</p><h3 id=how-do-i-use-a-csi-volume>How do I use a CSI Volume?</h3><p>Assuming a CSI storage plugin is already deployed on your cluster, you can use it through the familiar Kubernetes storage primitives: PersistentVolumeClaims, PersistentVolumes, and StorageClasses.</p><p>CSI is an alpha feature in Kubernetes v1.9. To enable it, set the following flags:</p><pre><code>CSI is an alpha feature in Kubernetes v1.9. To enable it, set the following flags:

API server binary:
--feature-gates=CSIPersistentVolume=true
--runtime-config=storage.k8s.io/v1alpha1=true
API server binary and kubelet binaries:
--feature-gates=MountPropagation=true
--allow-privileged=true
</code></pre><h3 id=dynamic-provisioning>Dynamic Provisioning</h3><p>You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a StorageClass pointing to the CSI plugin.</p><p>The following StorageClass, for example, enables dynamic creation of “fast-storage” volumes by a CSI volume plugin called “com.example.team/csi-driver”.</p><pre><code>kind: StorageClass

apiVersion: storage.k8s.io/v1

metadata:

  name: fast-storage

provisioner: com.example.team/csi-driver

parameters:

  type: pd-ssd
</code></pre><p>To trigger dynamic provisioning, create a PersistentVolumeClaim object. The following PersistentVolumeClaim, for example, triggers dynamic provisioning using the StorageClass above.</p><pre><code>apiVersion: v1

kind: PersistentVolumeClaim

metadata:

  name: my-request-for-storage

spec:

  accessModes:

  - ReadWriteOnce

  resources:

    requests:

      storage: 5Gi

  storageClassName: fast-storage
</code></pre><p>When volume provisioning is invoked, the parameter “type: pd-ssd” is passed to the CSI plugin “com.example.team/csi-driver” via a “CreateVolume” call. In response, the external volume plugin provisions a new volume and then automatically create a PersistentVolume object to represent the new volume. Kubernetes then binds the new PersistentVolume object to the PersistentVolumeClaim, making it ready to use.</p><p>If the “fast-storage” StorageClass is marked default, there is no need to include the storageClassName in the PersistentVolumeClaim, it will be used by default.</p><h3 id=pre-provisioned-volumes>Pre-Provisioned Volumes</h3><p>You can always expose a pre-existing volume in Kubernetes by manually creating a PersistentVolume object to represent the existing volume. The following PersistentVolume, for example, exposes a volume with the name “existingVolumeName” belonging to a CSI storage plugin called “com.example.team/csi-driver”.</p><pre><code>apiVersion: v1

kind: PersistentVolume

metadata:

  name: my-manually-created-pv

spec:

  capacity:

    storage: 5Gi

  accessModes:

    - ReadWriteOnce

  persistentVolumeReclaimPolicy: Retain

  csi:

    driver: com.example.team/csi-driver

    volumeHandle: existingVolumeName

    readOnly: false
</code></pre><h3 id=attaching-and-mounting>Attaching and Mounting</h3><p>You can reference a PersistentVolumeClaim that is bound to a CSI volume in any pod or pod template.</p><pre><code>kind: Pod

apiVersion: v1

metadata:

  name: my-pod

spec:

  containers:

    - name: my-frontend

      image: dockerfile/nginx

      volumeMounts:

      - mountPath: &quot;/var/www/html&quot;

        name: my-csi-volume

  volumes:

    - name: my-csi-volume

      persistentVolumeClaim:

        claimName: my-request-for-storage
</code></pre><p>When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (ControllerPublishVolume, NodePublishVolume, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.</p><p>For more details please see the CSI implementation <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md>design doc</a> and <a href=https://github.com/kubernetes-csi/docs/wiki/Setup>documentation</a>.</p><h3 id=how-do-i-create-a-csi-driver>How do I create a CSI driver?</h3><p>Kubernetes is as minimally prescriptive on the packaging and deployment of a CSI Volume Driver as possible. The minimum requirements for deploying a CSI Volume Driver on Kubernetes are documented <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers>here</a>.</p><p>The minimum requirements document also contains a <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes>section</a> outlining the suggested mechanism for deploying an arbitrary containerized CSI driver on Kubernetes. This mechanism can be used by a Storage Provider to simplify deployment of containerized CSI compatible volume drivers on Kubernetes.</p><p>As part of this recommended deployment process, the Kubernetes team provides the following sidecar (helper) containers:</p><ul><li><p><a href=https://github.com/kubernetes-csi/external-attacher>external-attacher</a></p><ul><li>Sidecar container that watches Kubernetes VolumeAttachment objects and triggers ControllerPublish and ControllerUnpublish operations against a CSI endpoint.</li></ul></li><li><p><a href=https://github.com/kubernetes-csi/external-provisioner>external-provisioner</a></p><ul><li>Sidecar container that watches Kubernetes PersistentVolumeClaim objects and triggers CreateVolume and DeleteVolume operations against a CSI endpoint.</li></ul></li><li><p><a href=https://github.com/kubernetes-csi/driver-registrar>driver-registrar</a></p><ul><li>Sidecar container that registers the CSI driver with kubelet (in the future), and adds the drivers custom NodeId (retrieved via GetNodeID call against the CSI endpoint) to an annotation on the Kubernetes Node API Object</li></ul></li></ul><p>Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.</p><h3 id=where-can-i-find-csi-drivers>Where can I find CSI drivers?</h3><p>CSI drivers are developed and maintained by third-parties. You can find example CSI drivers <a href=https://github.com/kubernetes-csi/drivers>here</a>, but these are provided purely for illustrative purposes, and are not intended to be used for production workloads.</p><h3 id=what-about-flex>What about Flex?</h3><p>The <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md>Flex Volume plugin</a> exists as an exec based mechanism to create “out-of-tree” volume plugins. Although it has some drawbacks (mentioned above), the Flex volume plugin coexists with the new CSI Volume plugin. SIG Storage will continue to maintain the Flex API so that existing third-party Flex drivers (already deployed in production clusters) continue to work. In the future, new volume features will only be added to CSI, not Flex.</p><h3 id=what-will-happen-to-the-in-tree-volume-plugins>What will happen to the in-tree volume plugins?</h3><p>Once CSI reaches stability, we plan to migrate most of the in-tree volume plugins to CSI. Stay tuned for more details as the Kubernetes CSI implementation approaches stable.</p><h3 id=what-are-the-limitations-of-alpha>What are the limitations of alpha?</h3><p>The alpha implementation of CSI has the following limitations:</p><ul><li>The credential fields in CreateVolume, NodePublishVolume, and ControllerPublishVolume calls are not supported.</li><li>Block volumes are not supported; only file.</li><li>Specifying filesystems is not supported, and defaults to ext4.</li><li>CSI drivers must be deployed with the provided “external-attacher,” even if they don’t implement “ControllerPublishVolume”.</li><li>Kubernetes scheduler topology awareness is not supported for CSI volumes: in short, sharing information about where a volume is provisioned (zone, regions, etc.) to allow k8s scheduler to make smarter scheduling decisions.</li></ul><h3 id=what-s-next>What’s next?</h3><p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI implementation to beta in either 1.10 or 1.11.</p><h3 id=how-do-i-get-involved>How Do I Get Involved?</h3><p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. A huge thank you to Vladimir Vivien (<a href=https://github.com/vladimirvivien>vladimirvivien</a>), Jan Šafránek (<a href=https://github.com/jsafrane>jsafrane</a>), Chakravarthy Nelluri (<a href=https://github.com/chakri-nelluri>chakri-nelluri</a>), Bradley Childs (<a href=https://github.com/childsb>childsb</a>), Luis Pabón (<a href=https://github.com/lpabon>lpabon</a>), and Saad Ali (<a href=https://github.com/saad-ali>saad-ali</a>) for their tireless efforts in bringing CSI to life in Kubernetes.</p><p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special-Interest-Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f018685bed3dd1eaad90e61436e60e10>Kubernetes v1.9 releases beta support for Windows Server Containers</h1><div class="td-byline mb-4"><time datetime=2018-01-09 class=text-muted>Tuesday, January 09, 2018</time></div><p>With the release of Kubernetes v1.9, our mission of ensuring Kubernetes works well everywhere and for everyone takes a great step forward. We’ve advanced support for Windows Server to beta along with continued feature and functional advancements on both the Kubernetes and Windows platforms. SIG-Windows has been working since March of 2016 to open the door for many Windows-specific applications and workloads to run on Kubernetes, significantly expanding the implementation scenarios and the enterprise reach of Kubernetes.</p><p>Enterprises of all sizes have made significant investments in .NET and Windows based applications. Many enterprise portfolios today contain .NET and Windows, with Gartner claiming that <a href=http://www.gartner.com/document/3446217>80%</a> of enterprise apps run on Windows. According to StackOverflow Insights, 40% of professional developers use the .NET programming languages (including .NET Core).</p><p>But why is all this information important? It means that enterprises have both legacy and new born-in-the-cloud (microservice) applications that utilize a wide array of programming frameworks. There is a big push in the industry to modernize existing/legacy applications to containers, using an approach similar to “lift and shift”. Modernizing existing applications into containers also provides added flexibility for new functionality to be introduced in additional Windows or Linux containers. Containers are becoming the de facto standard for packaging, deploying, and managing both existing and microservice applications. IT organizations are looking for an easier and homogenous way to orchestrate and manage containers across their Linux and Windows environments. Kubernetes v1.9 now offers beta support for Windows Server containers, making it the clear choice for orchestrating containers of any kind.</p><h3 id=features>Features</h3><p>Alpha support for Windows Server containers in Kubernetes was great for proof-of-concept projects and visualizing the road map for support of Windows in Kubernetes. The alpha release had significant drawbacks, however, and lacked many features, especially in networking. SIG-Windows, Microsoft, Cloudbase Solutions, Apprenda, and other community members banded together to create a comprehensive beta release, enabling Kubernetes users to start evaluating and using Windows.</p><p>Some key feature improvements for Windows Server containers on Kubernetes include:</p><ul><li>Improved support for pods! Multiple Windows Server containers in a pod can now share the network namespace using network compartments in Windows Server. This feature brings the concept of a pod to parity with Linux-based containers</li><li>Reduced network complexity by using a single network endpoint per pod</li><li>Kernel-Based load-balancing using the Virtual Filtering Platform (VFP) Hyper-v Switch Extension (analogous to Linux iptables)</li><li>Container Runtime Interface (CRI) pod and node level statistics. Windows Server containers can now be profiled for Horizontal Pod Autoscaling using performance metrics gathered from the pod and the node</li><li>Support for kubeadm commands to add Windows Server nodes to a Kubernetes environment. Kubeadm simplifies the provisioning of a Kubernetes cluster, and with the support for Windows Server, you can use a single tool to deploy Kubernetes in your infrastructure</li><li>Support for ConfigMaps, Secrets, and Volumes. These are key features that allow you to separate, and in some cases secure, the configuration of the containers from the implementation
The crown jewels of Kubernetes 1.9 Windows support, however, are the networking enhancements. With the release of Windows Server 1709, Microsoft has enabled key networking capabilities in the operating system and the Windows Host Networking Service (HNS) that paved the way to produce a number of CNI plugins that work with Windows Server containers in Kubernetes. The Layer-3 routed and network overlay plugins that are supported with Kubernetes 1.9 are listed below:</li></ul><ol><li>Upstream L3 Routing - IP routes configured in upstream ToR</li><li>Host-Gateway - IP routes configured on each host</li><li>Open vSwitch (OVS) & Open Virtual Network (OVN) with Overlay - Supports STT and Geneve tunneling types
You can read more about each of their <a href=/docs/getting-started-guides/windows/>configuration, setup, and runtime capabilities</a> to make an informed selection for your networking stack in Kubernetes.</li></ol><p>Even though you have to continue running the Kubernetes Control Plane and Master Components in Linux, you are now able to introduce Windows Server as a Node in Kubernetes. As a community, this is a huge milestone and achievement. We will now start seeing .NET, .NET Core, ASP.NET, IIS, Windows Services, Windows executables and many more windows-based applications in Kubernetes.</p><h3 id=what-s-coming-next>What’s coming next</h3><p>A lot of work went into this beta release, but the community realizes there are more areas of investment needed before we can release Windows support as GA (General Availability) for production workloads. Some keys areas of focus for the first two quarters of 2018 include:</p><ol><li>Continue to make progress in the area of networking. Additional CNI plugins are under development and nearing completion</li></ol><ul><li>Overlay - win-overlay (vxlan or IP-in-IP encapsulation using Flannel) </li><li>Win-l2bridge (host-gateway) </li><li>OVN using cloud networking - without overlays</li><li>Support for Kubernetes network policies in ovn-kubernetes</li><li>Support for Hyper-V Isolation</li><li>Support for StatefulSet functionality for stateful applications</li><li>Produce installation artifacts and documentation that work on any infrastructure and across many public cloud providers like Microsoft Azure, Google Cloud, and Amazon AWS</li><li>Continuous Integration/Continuous Delivery (CI/CD) infrastructure for SIG-Windows</li><li>Scalability and Performance testing
Even though we have not committed to a timeline for GA, SIG-Windows estimates a GA release in the first half of 2018.</li></ul><h3 id=get-involved>Get Involved</h3><p>As we continue to make progress towards General Availability of this feature in Kubernetes, we welcome you to get involved, contribute code, provide feedback, deploy Windows Server containers to your Kubernetes cluster, or simply join our community.</p><ul><li>If you want to get started on deploying Windows Server containers in Kubernetes, read our getting started guide at <a href=/docs/getting-started-guides/windows/>/docs/getting-started-guides/windows/</a></li><li>We meet every other Tuesday at 12:30 Eastern Standard Time (EST) at <a href=https://zoom.us/my/sigwindows>https://zoom.us/my/sigwindows</a>. All our meetings are recorded on youtube and referenced at <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4">https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4</a></li><li>Chat with us on Slack at <a href=https://kubernetes.slack.com/messages/sig-windows>https://kubernetes.slack.com/messages/sig-windows</a></li><li>Find us on GitHub at <a href=https://github.com/kubernetes/community/tree/master/sig-windows>https://github.com/kubernetes/community/tree/master/sig-windows</a></li></ul><p>Thank you,</p><p>Michael Michael (@michmike77)<br>SIG-Windows Lead<br>Senior Director of Product Management, Apprenda</p></div><div class=td-content style=page-break-before:always><h1 id=pg-50e09dc12a57b620dfc2e36758a63903>Five Days of Kubernetes 1.9</h1><div class="td-byline mb-4"><time datetime=2018-01-08 class=text-muted>Monday, January 08, 2018</time></div><p>Kubernetes 1.9 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.</p><p>The community has tallied around 32,300 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 90,700 commits across all repos and 7,800 commits across all repos for v1.8.0 to v1.9.0 alone.</p><p>With the help of our growing community of 1,400 plus contributors, we issued more than 4,490 PRs and pushed more than 7,800 commits to deliver Kubernetes 1.9 with many notable updates, including enhancements for the workloads and stateful application support areas. This all points to increased extensibility and standards-based Kubernetes ecosystem.</p><p>While many improvements have been contributed, we highlight key features in this series of in-depth posts listed below. <a href=https://twitter.com/kubernetesio>Follow along</a> and see what’s new and improved with workloads, Windows support and more.</p><p><strong>Day 1:</strong> 5 Days of Kubernetes 1.9<br><strong>Day 2:</strong> Windows and Docker support for Kubernetes (beta)<br><strong>Day 3:</strong> Storage, CSI framework (alpha)<br><strong>Day 4:</strong>  Web Hook and Mission Critical, Dynamic Admission Control<br><strong>Day 5:</strong> Introducing client-go version 6<br><strong>Day 6:</strong> Workloads API</p><p><strong>Connect</strong></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2f6487a6fb6a258f8dd34903ea98ed36>Introducing Kubeflow - A Composable, Portable, Scalable ML Stack Built for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-12-21 class=text-muted>Thursday, December 21, 2017</time></div><p><strong><em>Today’s post is by David Aronchick and Jeremy Lewi, a PM and Engineer on the Kubeflow project, a new open source GitHub repo dedicated to making using machine learning (ML) stacks on Kubernetes easy, fast and extensible.</em></strong></p><h2 id=kubernetes-and-machine-learning>Kubernetes and Machine Learning</h2><p>Kubernetes has quickly become the hybrid solution for deploying complicated workloads anywhere. While it started with just stateless services, customers have begun to move complex workloads to the platform, taking advantage of rich APIs, reliability and performance provided by Kubernetes. One of the fastest growing use cases is to use Kubernetes as the deployment platform of choice for machine learning.</p><p>Building any production-ready machine learning system involves various components, often mixing vendors and hand-rolled solutions. Connecting and managing these services for even moderately sophisticated setups introduces huge barriers of complexity in adopting machine learning. Infrastructure engineers will often spend a significant amount of time manually tweaking deployments and hand rolling solutions before a single model can be tested.</p><p>Worse, these deployments are so tied to the clusters they have been deployed to that these stacks are immobile, meaning that moving a model from a laptop to a highly scalable cloud cluster is effectively impossible without significant re-architecture. All these differences add up to wasted effort and create opportunities to introduce bugs at each transition.</p><h2 id=introducing-kubeflow>Introducing Kubeflow</h2><p>To address these concerns, we’re announcing the creation of the Kubeflow project, a new open source GitHub repo dedicated to making using ML stacks on Kubernetes easy, fast and extensible. This repository contains:</p><ul><li>JupyterHub to create & manage interactive Jupyter notebooks</li><li>A Tensorflow <a href=/docs/concepts/api-extension/custom-resources/>Custom Resource</a> (CRD) that can be configured to use CPUs or GPUs, and adjusted to the size of a cluster with a single setting</li><li>A TF Serving container
Because this solution relies on Kubernetes, it runs wherever Kubernetes runs. Just spin up a cluster and go!</li></ul><h2 id=using-kubeflow>Using Kubeflow</h2><p>Let's suppose you are working with two different Kubernetes clusters: a local <a href=https://github.com/kubernetes/minikube>minikube</a> cluster; and a <a href="https://docs.google.com/forms/d/1JNnoUe1_3xZvAogAi16DwH6AjF2eu08ggED24OGO7Xc/viewform?edit_requested=true">GKE cluster with GPUs</a>; and that you have two <a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts>kubectl contexts</a> defined named minikube and gke.</p><p>First we need to initialize our <a href=https://github.com/ksonnet>ksonnet</a> application and install the Kubeflow packages. (To use ksonnet, you must first install it on your operating system - the instructions for doing so are <a href=https://github.com/ksonnet/ksonnet>here</a>)</p><pre><code>     ks init my-kubeflow  
     cd my-kubeflow  
     ks registry add kubeflow \  
     github.com/google/kubeflow/tree/master/kubeflow  
     ks pkg install kubeflow/core  
     ks pkg install kubeflow/tf-serving  
     ks pkg install kubeflow/tf-job  
     ks generate core kubeflow-core --name=kubeflow-core
</code></pre><p>We can now define <a href=https://ksonnet.io/docs/concepts#environment>environments</a> corresponding to our two clusters.</p><pre><code>     kubectl config use-context minikube  
     ks env add minikube  

     kubectl config use-context gke  
     ks env add gke  
</code></pre><p>And we’re done! Now just create the environments on your cluster. First, on minikube:</p><pre><code>     ks apply minikube -c kubeflow-core  
</code></pre><p>And to create it on our multi-node GKE cluster for quicker training:</p><pre><code>     ks apply gke -c kubeflow-core  
</code></pre><p>By making it easy to deploy the same rich ML stack everywhere, the drift and rewriting between these environments is kept to a minimum.</p><p>To access either deployments, you can execute the following command:</p><pre><code>     kubectl port-forward tf-hub-0 8100:8000  
</code></pre><p>and then open up http://127.0.0.1:8100 to access JupyterHub. To change the environment used by kubectl, use either of these commands:</p><pre><code>     # To access minikube  
     kubectl config use-context minikube  

     # To access GKE  
     kubectl config use-context gke  
</code></pre><p>When you execute apply you are launching on K8s</p><ul><li>JupyterHub for launching and managing Jupyter notebooks on K8s</li><li>A <a href=https://github.com/tensorflow/k8s>TF CRD</a></li></ul><p>Let's suppose you want to submit a training job. Kubeflow provides ksonnet <a href=https://ksonnet.io/docs/concepts#prototype>prototypes</a> that make it easy to define <a href=https://ksonnet.io/docs/concepts#component>components</a>. The tf-job prototype makes it easy to create a job for your code but for this example, we'll use the tf-cnn prototype which runs <a href=https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks>TensorFlow's CNN benchmark</a>.</p><p>To submit a training job, you first generate a new job from a prototype:</p><pre><code>     ks generate tf-cnn cnn --name=cnn  
</code></pre><p>By default the tf-cnn prototype uses 1 worker and no GPUs which is perfect for our minikube cluster so we can just submit it.</p><pre><code>     ks apply minikube -c cnn
</code></pre><p>On GKE, we’ll want to tweak the prototype to take advantage of the multiple nodes and GPUs. First, let’s list all the parameters available:</p><pre><code>     # To see a list of parameters  
     ks prototype list tf-job  
</code></pre><p>Now let’s adjust the parameters to take advantage of GPUs and access to multiple nodes.</p><pre><code>     ks param set --env=gke cnn num\_gpus 1  
     ks param set --env=gke cnn num\_workers 1  

     ks apply gke -c cnn  
</code></pre><p>Note how we set those parameters so they are used only when you deploy to GKE. Your minikube parameters are unchanged!</p><p>After training, you <a href=https://www.tensorflow.org/serving/serving_basic>export your model</a> to a serving location.</p><p>Kubeflow also includes a serving package as well. In a separate example, we trained a standard Inception model, and stored the trained model in a bucket we’ve created called ‘gs://kubeflow-models’ with the path ‘/inception’.</p><p>To deploy a the trained model for serving, execute the following:</p><pre><code>     ks generate tf-serving inception --name=inception  
     ---namespace=default --model\_path=gs://kubeflow-models/inception  
     ks apply gke -c inception  
</code></pre><p>This highlights one more option in Kubeflow - the ability to pass in inputs based on your deployment. This command creates a tf-serving service on the GKE cluster, and makes it available to your application.</p><p>For more information about of deploying and monitoring TensorFlow training jobs and TensorFlow models please refer to the <a href=https://github.com/google/kubeflow/blob/master/user_guide.md>user guide</a>.</p><h2 id=kubeflow-ksonnet>Kubeflow + ksonnet</h2><p>One choice we want to call out is the use of the ksonnet project. We think working with multiple environments (dev, test, prod) will be the norm for most Kubeflow users. By making environments a first class concept, ksonnet makes it easy for Kubeflow users to easily move their workloads between their different environments.</p><p>Particularly now that <a href=https://blog.heptio.com/ksonnet-intro-43f6183a97a6>Helm is integrating ksonnet</a> with the next version of their platform, we felt like it was the perfect choice for us. More information about ksonnet can be found in the ksonnet <a href=https://ksonnet.io/>docs</a>.</p><p>We also want to thank the team at <a href=https://heptio.com/>Heptio</a> for expediting features critical to Kubeflow's use of ksonnet.</p><h2 id=what-s-next>What’s Next?</h2><p>We are in the midst of building out a community effort right now, and we would love your help! We’ve already been collaborating with many teams - <a href=https://caicloud.io/article_detail/5a3b58fce928ca1c69e1aa70>CaiCloud</a>, <a href=https://blog.openshift.com/machine-learning-openshift-kubernetes/>Red Hat & OpenShift</a>, <a href=https://tutorials.ubuntu.com/tutorial/get-started-kubeflow>Canonical</a>, <a href=https://www.weave.works/blog/kubeflow-and-weave-cloud>Weaveworks</a>, <a href=http://container-solutions.com/tensorflow-on-kubernetes-kubeflow/>Container Solutions</a> and many others. <a href=https://coreos.com/>CoreOS</a>, for example, is already seeing the promise of Kubeflow:</p><p>“The Kubeflow project was a needed advancement to make it significantly easier to set up and productionize machine learning workloads on Kubernetes, and we anticipate that it will greatly expand the opportunity for even more enterprises to embrace the platform. We look forward to working with the project members in providing tight integration of Kubeflow with Tectonic, the enterprise Kubernetes platform.” -- Reza Shafii, VP of product, CoreOS</p><p>If you’d like to try out Kubeflow right now right in your browser, we’ve partnered with <a href=https://www.katacoda.com/>Katacoda</a> to make it super easy. You can try it <a href=https://www.katacoda.com/kubeflow>here</a>!</p><p>And we’re just getting started! We would love for you to help. How you might ask? Well…</p><ul><li>Please join the<a href=https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg>slack channel</a></li><li>Please join the<a href=https://groups.google.com/forum/#!forum/kubeflow-discuss>kubeflow-discuss</a> email list</li><li>Please subscribe to the<a href=http://twitter.com/kubeflow>Kubeflow twitter</a> account</li><li>Please download and run kubeflow, and submit bugs!
Thank you for your support so far, we could not be more excited!</li></ul><p><em>Jeremy Lewi & David Aronchick</em>
Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bb3da9db31fdb4f015c6930857e2df92>Kubernetes 1.9: Apps Workloads GA and Expanded Ecosystem</h1><div class="td-byline mb-4"><time datetime=2017-12-15 class=text-muted>Friday, December 15, 2017</time></div><p>We’re pleased to announce the delivery of Kubernetes 1.9, our fourth and final release this year.</p><p>Today’s release continues the evolution of an increasingly rich feature set, more robust stability, and even greater community contributions. As the fourth release of the year, it gives us an opportunity to look back at the progress made in key areas. Particularly notable is the advancement of the Apps Workloads API to stable. This removes any reservations potential adopters might have had about the functional stability required to run mission-critical workloads. Another big milestone is the beta release of Windows support, which opens the door for many Windows-specific applications and workloads to run in Kubernetes, significantly expanding the implementation scenarios and enterprise readiness of Kubernetes.</p><h2 id=workloads-api-ga>Workloads API GA</h2><p>We’re excited to announce General Availability (GA) of the <a href=/docs/reference/workloads-18-19/>apps/v1 Workloads API</a>, which is now enabled by default. The Apps Workloads API groups the DaemonSet, Deployment, ReplicaSet, and StatefulSet APIs together to form the foundation for long-running stateless and stateful workloads in Kubernetes. Note that the Batch Workloads API (Job and CronJob) is not part of this effort and will have a separate path to GA stability.</p><p>Deployment and ReplicaSet, two of the most commonly used objects in Kubernetes, are now stabilized after more than a year of real-world use and feedback. <a href=https://github.com/kubernetes/community/tree/master/sig-apps>SIG Apps</a> has applied the lessons from this process to all four resource kinds over the last several release cycles, enabling DaemonSet and StatefulSet to join this graduation. The v1 (GA) designation indicates production hardening and readiness, and comes with the guarantee of long-term backwards compatibility.</p><h2 id=windows-support-beta>Windows Support (beta)</h2><p>Kubernetes was originally developed for Linux systems, but as our users are realizing the benefits of container orchestration at scale, we are seeing demand for Kubernetes to run Windows workloads. Work to support Windows Server in Kubernetes began in earnest about 12 months ago. <a href=https://github.com/kubernetes/community/tree/master/sig-windows>SIG-Windows</a>has now promoted this feature to beta status, which means that we can evaluate it for <a href=/docs/getting-started-guides/windows/>usage</a>.</p><h2 id=storage-enhancements>Storage Enhancements</h2><p>From the first release, Kubernetes has supported multiple options for persistent data storage, including commonly-used NFS or iSCSI, along with native support for storage solutions from the major public and private cloud providers. As the project and ecosystem grow, more and more storage options have become available for Kubernetes. Adding volume plugins for new storage systems, however, has been a challenge.</p><p>Container Storage Interface (CSI) is a cross-industry standards initiative that aims to lower the barrier for cloud native storage development and ensure compatibility. <a href=https://github.com/kubernetes/community/tree/master/sig-storage>SIG-Storage</a> and the <a href=https://github.com/container-storage-interface/community>CSI Community</a> are collaborating to deliver a single interface for provisioning, attaching, and mounting storage compatible with Kubernetes.</p><p>Kubernetes 1.9 introduces an <a href=https://github.com/kubernetes/features/issues/178>alpha implementation</a> of the Container Storage Interface (CSI), which will make installing new volume plugins as easy as deploying a pod, and enable third-party storage providers to develop their solutions without the need to add to the core Kubernetes codebase.</p><p>Because the feature is alpha in 1.9, it must be explicitly enabled and is not recommended for production usage, but it indicates the roadmap working toward a more extensible and standards-based Kubernetes storage ecosystem.</p><h2 id=additional-features>Additional Features</h2><p>Custom Resource Definition (CRD) Validation, now graduating to beta and enabled by default, helps CRD authors give clear and immediate feedback for invalid objects</p><p>SIG Node hardware accelerator moves to alpha, enabling GPUs and consequently machine learning and other high performance workloads</p><p>CoreDNS alpha makes it possible to install CoreDNS with standard tools</p><p>IPVS mode for kube-proxy goes beta, providing better scalability and performance for large clusters</p><p>Each Special Interest Group (SIG) in the community continues to deliver the most requested user features for their area. For a complete list, please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v190>release notes</a>.</p><h2 id=availability>Availability</h2><p>Kubernetes 1.9 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.9.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=/docs/tutorials/kubernetes-basics/>interactive tutorials</a>. </p><h2 id=release-team>Release team</h2><p>This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the <a href=https://github.com/kubernetes/features/blob/master/release-1.9/release_team.md>release team</a> led by Anthony Yeh, Software Engineer at Google. The 14 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process has become an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. </p><h2 id=project-velocity>Project Velocity</h2><p>The CNCF has embarked on an ambitious project to visualize the myriad contributions that go into the project. <a href=https://devstats.k8s.io/>K8s DevStats</a> illustrates the breakdown of contributions from major company contributors. Open issues remained relatively stable over the course of the release, while forks rose approximately 20%, as did individuals starring the various project repositories. Approver volume has risen slightly since the last release, but a lull is commonplace during the last quarter of the year. With 75,000+ comments, Kubernetes remains one of the most actively discussed projects on GitHub.</p><h2 id=user-highlights>User highlights</h2><p>According to the l<a href=https://www.cncf.io/blog/2017/12/06/cloud-native-technologies-scaling-production-applications>atest survey conducted by CNCF</a>, 61 percent of organizations are evaluating and 83 percent are using Kubernetes in production. Example of user stories from the community include:</p><p>BlaBlaCar, the world’s largest long distance carpooling community connects 40 million members across 22 countries. The company has about 3,000 pods, with <a href=https://kubernetes.io/case-studies/blablacar/>1,200 of them running on Kubernetes</a>, leading to improved website availability for customers.</p><p>Pokémon GO, the popular free-to-play, location-based augmented reality game developed by Niantic for iOS and Android devices, has its application logic running on Google Container Engine powered by Kubernetes. This was the <a href=https://cloudplatform.googleblog.com/2016/09/bringing-Pokemon-GO-to-life-on-Google-Cloud.html>largest Kubernetes deployment</a> ever on Google Container Engine.</p><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community. </p><h2 id=ecosystem-updates>Ecosystem updates</h2><p>Announced on November 13, the <a href=https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/>Certified Kubernetes Conformance Program</a> ensures that Certified Kubernetes™ products deliver consistency and portability. Thirty-two Certified Kubernetes Distributions and Platforms are <a href=https://kubernetes.io/partners/#dist>now available</a>. Development of the certification program involved close collaboration between CNCF and the rest of the Kubernetes community, especially the Testing and Architecture Special Interest Groups (SIGs). The Kubernetes Architecture SIG is the final arbiter of the definition of API conformance for the program. The program also includes strong guarantees that commercial providers of Kubernetes will continue to release new versions to ensure that customers can take advantage of the rapid pace of ongoing development.</p><p>CNCF also offers <a href=https://www.cncf.io/certification/training/>online training</a> that teaches the skills needed to create and configure a real-world Kubernetes cluster.</p><h2 id=kubecon>KubeCon</h2><p>For recorded sessions from the largest Kubernetes gathering, <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america>KubeCon + CloudNativeCon</a> in Austin from December 6-8, 2017, visit <a href=https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA>YouTube/CNCF</a>. The premiere Kubernetes event will be back May 2-4, 2018 in Copenhagen and will feature technical sessions, case studies, developer deep dives, salons and more! <a href=http://events.linuxfoundation.org/events/kubecon-and-cloudnativecon-europe/program/cfpguide>CFP</a> closes January 12, 2018. </p><h2 id=webinar>Webinar</h2><p>Join members of the Kubernetes 1.9 release team on <strong>January 9th from 10am-11am PT</strong> to learn about the major features in this release as they demo some of the highlights in the areas of Windows and Docker support, storage, admission control, and the workloads API. <a href=https://zoom.us/webinar/register/WN_oVjQMwyzQFOmWsfVzDsa2A>Register here</a>.</p><h2 id=get-involved>Get involved:</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and through the channels below.</p><p>Thank you for your continued feedback and support.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a5aff75686a87220001cbe27bcda2cad>Using eBPF in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-12-07 class=text-muted>Thursday, December 07, 2017</time></div><h2 id=introduction>Introduction</h2><p>Kubernetes provides a high-level API and a set of components that hides almost all of the intricate and—to some of us—interesting details of what happens at the systems level. Application developers are not required to have knowledge of the machines' IP tables, cgroups, namespaces, seccomp, or, nowadays, even the <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes>container runtime</a> that their application runs on top of. But underneath, Kubernetes and the technologies upon which it relies (for example, the container runtime) heavily leverage core Linux functionalities.</p><p>This article focuses on a core Linux functionality increasingly used in networking, security and auditing, and tracing and monitoring tools. This functionality is called <a href=http://man7.org/linux/man-pages/man2/bpf.2.html>extended Berkeley Packet Filter</a> (eBPF)</p><p><strong>Note:</strong> <em>In this article we use both acronyms: eBPF and BPF. The former is used for the extended BPF functionality, and the latter for "classic" BPF functionality.</em></p><h2 id=what-is-bpf>What is BPF?</h2><p>BPF is a mini-VM residing in the Linux kernel that runs BPF programs. Before running, BPF programs are loaded with the <a href=http://man7.org/linux/man-pages/man2/bpf.2.html>bpf()</a> syscall and are validated for safety: checking for loops, code size, etc. BPF programs are attached to kernel objects and executed when events happen on those objects—for example, when a network interface emits a packet.</p><h2 id=bpf-superpowers>BPF Superpowers</h2><p>BPF programs are event-driven by definition, an incredibly powerful concept, and executes code in the kernel when an event occurs. <a href=http://www.brendangregg.com/bio.html>Netflix's Brendan Gregg</a> refers to BPF as a <a href=http://www.brendangregg.com/blog/2016-03-05/linux-bpf-superpowers.html>Linux superpower</a>.</p><h2 id=the-e-in-ebpf>The 'e' in eBPF</h2><p>Traditionally, BPF could only be attached to sockets for socket filtering. BPF’s first use case was in <code>tcpdump</code>. When you run <code>tcpdump</code> the filter is compiled into a BPF program and attached to a raw <code>AF_PACKET</code> socket in order to print out filtered packets.</p><p>But over the years, eBPF added the ability to attach to <a href=https://github.com/torvalds/linux/blob/v4.14/include/uapi/linux/bpf.h#L117-L133>other kernel objects</a>. In addition to socket filtering, some supported attach points are:</p><ul><li>Kprobes (and userspace equivalents uprobes)</li><li>Tracepoints</li><li>Network schedulers or qdiscs for classification or action (tc)</li><li>XDP (eXpress Data Path)
This and other, newer features like in-kernel helper functions and shared data-structures (maps) that can be used to communicate with user space, extend BPF’s capabilities.</li></ul><h2 id=existing-use-cases-of-ebpf-with-kubernetes>Existing Use Cases of eBPF with Kubernetes</h2><p>Several open-source Kubernetes tools already use eBPF and many use cases warrant a closer look, especially in areas such as networking, monitoring and security tools.</p><h2 id=dynamic-network-control-and-visibility-with-cilium>Dynamic Network Control and Visibility with Cilium</h2><p><a href=https://github.com/cilium/cilium>Cilium</a> is a networking project that makes heavy use of eBPF superpowers to route and filter network traffic for container-based systems. By using eBPF, Cilium can dynamically generate and apply rules—even at the device level with XDP—without making changes to the Linux kernel itself.</p><p>The Cilium Agent runs on each host. Instead of managing IP tables, it translates network policy definitions to BPF programs that are loaded into the kernel and attached to a container's virtual ethernet device. These programs are executed—rules applied—on each packet that is sent or received.</p><p>This diagram shows how the Cilium project works:</p><p><img src=https://lh4.googleusercontent.com/Xe8qee5yYsJton2NHFLOhHevxdbpCHHPPgttOLP18ZWtoUJp9ChFKtKJiTxqNFn8zQPRJu4BdtG7xc24vlGkD2gtfbkCuHq_eU3Tx6z2m6ld4iYGEZv-MsSCcJ3jAcJO2HkMc_d_ alt></p><p>Depending on what network rules are applied, BPF programs may be attached with <a href=http://man7.org/linux/man-pages/man8/tc.8.html>tc</a> or <a href=https://www.iovisor.org/technology/xdp>XDP</a>. By using XDP, Cilium can attach the BPF programs at the lowest possible point, which is also the most performant point in the networking software stack.</p><p>If you'd like to learn more about how Cilium uses eBPF, take a look at the project's <a href=http://cilium.readthedocs.io/en/latest/bpf/>BPF and XDP reference guide</a>.</p><h2 id=tracking-tcp-connections-in-weave-scope>Tracking TCP Connections in Weave Scope</h2><p><a href=https://github.com/weaveworks/scope>Weave Scope</a> is a tool for monitoring, visualizing and interacting with container-based systems. For our purposes, we'll focus on how Weave Scope gets the TCP connections.</p><p><img src=https://lh6.googleusercontent.com/47C76UqCrrDr5O8wand6jESyFzx1SP4SQ_jVWiAhN5ctAEefz9e0orgmu0Q_2681QhcxJDfMQbn3HcRZYZN_QiPjKfXMo5Kt6XuXPjRGAoc_j2x7yC_9Un5JIoVt1Aa-DCHl-DUu alt></p><p>Weave Scope employs an agent that runs on each node of a cluster. The agent monitors the system, generates a report and sends it to the app server. The app server compiles the reports it receives and presents the results in the Weave Scope UI.</p><p>To accurately draw connections between containers, the agent attaches a BPF program to kprobes that track socket events: opening and closing connections. The BPF program, <a href=https://github.com/weaveworks/tcptracer-bpf>tcptracer-bpf</a>, is compiled into an ELF object file and loaded using <a href=https://github.com/iovisor/gobpf>gobpf</a>.</p><p>(As a side note, Weave Scope also has a plugin that make use of eBPF: <a href=https://github.com/weaveworks-plugins/scope-http-statistics>HTTP statistics</a>.)</p><p>To learn more about how this works and why it's done this way, read <a href=https://www.weave.works/blog/improving-performance-reliability-weave-scope-ebpf/>this extensive post</a> that the <a href=https://kinvolk.io/>Kinvolk</a> team wrote for the <a href=https://www.weave.works/blog/>Weaveworks Blog</a>. You can also watch <a href="https://www.youtube.com/watch?v=uTTFUpT0Sfw&list=PLWYdJViL9Eio5o5j4Uth_-Mt0FPrYXNwx">a recent talk</a> about the topic.</p><h2 id=limiting-syscalls-with-seccomp-bpf>Limiting syscalls with seccomp-bpf</h2><p>Linux has more than 300 system calls (read, write, open, close, etc.) available for use—or misuse. Most applications only need a small subset of syscalls to function properly. <a href=https://en.wikipedia.org/wiki/Seccomp>seccomp</a> is a Linux security facility used to limit the set of syscalls that an application can use, thereby limiting potential misuse.</p><p>The original implementation of seccomp was highly restrictive. Once applied, if an application attempted to do anything beyond reading and writing to files it had already opened, seccomp sent a <code>SIGKILL</code> signal.</p><p><a href=https://blog.yadutaf.fr/2014/05/29/introduction-to-seccomp-bpf-linux-syscall-filter/>seccomp-bpf</a> enables more complex filters and a wider range of actions. Seccomp-bpf, also known as seccomp mode 2, allows for applying custom filters in the form of BPF programs. When the BPF program is loaded, the filter is applied to each syscall and the appropriate action is taken (Allow, Kill, Trap, etc.).</p><p>seccomp-bpf is widely used in Kubernetes tools and exposed in Kubernetes itself. For example, seccomp-bpf is used in Docker to apply custom <a href=https://docs.docker.com/engine/security/seccomp/>seccomp security profiles</a>, in rkt to apply <a href=https://github.com/rkt/rkt/blob/5fadf0f1f444cdfde40d57e1d199b6dd6371594c/Documentation/seccomp-guide.md>seccomp isolators</a>, and in Kubernetes itself in its <a href=/docs/tasks/configure-pod-container/security-context/>Security Context</a>.</p><p>But in all of these cases the use of BPF is hidden behind <a href=https://github.com/seccomp/libseccomp>libseccomp</a>. Behind the scenes, libseccomp generates BPF code from rules provided to it. Once generated, the BPF program is loaded and the rules applied.</p><h2 id=potential-use-cases-for-ebpf-with-kubernetes>Potential Use Cases for eBPF with Kubernetes</h2><p>eBPF is a relatively new Linux technology. As such, there are many uses that remain unexplored. eBPF itself is also evolving: new features are being added in eBPF that will enable new use cases that aren’t currently possible. In the following sections, we're going to look at some of these that have only recently become possible and ones on the horizon. Our hope is that these features will be leveraged by open source tooling.</p><h2 id=pod-and-container-level-network-statistics>Pod and container level network statistics</h2><p>BPF socket filtering is nothing new, but BPF socket filtering per cgroup is. Introduced in Linux 4.10, <a href=https://lwn.net/Articles/698073/>cgroup-bpf</a> allows attaching eBPF programs to cgroups. Once attached, the program is executed for all packets entering or exiting any process in the cgroup.</p><p>A <a href=http://man7.org/linux/man-pages/man7/cgroups.7.html>cgroup</a> is, amongst other things, a hierarchical grouping of processes. In Kubernetes, this grouping is found at the container level. One idea for making use of cgroup-bpf, is to install BPF programs that collect detailed per-pod and/or per-container network statistics.</p><p>Generally, such statistics are collected by periodically checking the relevant file in Linux's <code>/sys</code> directory or using Netlink. By using BPF programs attached to cgroups for this, we can get much more detailed statistics: for example, how many packets/bytes on tcp port 443, or how many packets/bytes from IP 10.2.3.4. In general, because BPF programs have a kernel context, they can safely and efficiently deliver more detailed information to user space.</p><p>To explore the idea, the Kinvolk team implemented a proof-of-concept: <a href=https://github.com/kinvolk/cgnet>https://github.com/kinvolk/cgnet</a>. This project attaches a BPF program to each cgroup and exports the information to <a href=https://prometheus.io/>Prometheus</a>.</p><p>There are of course other interesting possibilities, like doing actual packet filtering. But the obstacle currently standing in the way of this is having cgroup v2 support—required by cgroup-bpf—in <a href=https://github.com/opencontainers/runc/issues/654>Docker</a> and Kubernetes.</p><h2 id=application-applied-lsm>Application-applied LSM</h2><p><a href=https://en.wikipedia.org/wiki/Linux_Security_Modules>Linux Security Modules</a> (LSM) implements a generic framework for security policies in the Linux kernel. <a href=https://wiki.centos.org/HowTos/SELinux>SELinux</a> and <a href=https://wiki.ubuntu.com/AppArmor>AppArmor</a> are examples of these. Both of these implement rules at a system-global scope, placing the onus on the administrator to configure the security policies.</p><p><a href=https://landlock.io/>Landlock</a> is another LSM under development that would co-exist with SELinux and AppArmor. An initial patchset has been submitted to the Linux kernel and is in an early stage of development. The main difference with other LSMs is that Landlock is designed to allow unprivileged applications to build their own sandbox, effectively restricting themselves instead of using a global configuration. With Landlock, an application can load a BPF program and have it executed when the process performs a specific action. For example, when the application opens a file with the open() system call, the kernel will execute the BPF program, and, depending on what the BPF program returns, the action will be accepted or denied.</p><p>In some ways, it is similar to seccomp-bpf: using a BPF program, seccomp-bpf allows unprivileged processes to restrict what system calls they can perform. Landlock will be more powerful and provide more flexibility. Consider the following system call:</p><pre><code>C  
fd = open(“myfile.txt”, O\_RDWR);

</code></pre><p>The first argument is a “char *”, a pointer to a memory address, such as <code>0xab004718</code>.</p><p>With seccomp, a BPF program only has access to the parameters of the syscall but cannot dereference the pointers, making it impossible to make security decisions based on a file. seccomp also uses classic BPF, meaning it cannot make use of eBPF maps, the mechanism for interfacing with user space. This restriction means security policies cannot be changed in seccomp-bpf based on a configuration in an eBPF map.</p><p>BPF programs with Landlock don’t receive the arguments of the syscalls but a reference to a kernel object. In the example above, this means it will have a reference to the file, so it does not need to dereference a pointer, consider relative paths, or perform chroots.</p><h2 id=use-case-landlock-in-kubernetes-based-serverless-frameworks>Use Case: Landlock in Kubernetes-based serverless frameworks</h2><p>In Kubernetes, the unit of deployment is a pod. Pods and containers are the main unit of isolation. In serverless frameworks, however, the main unit of deployment is a function. Ideally, the unit of deployment equals the unit of isolation. This puts serverless frameworks like <a href=https://github.com/kubeless/kubeless>Kubeless</a> or <a href=https://github.com/openfaas/faas>OpenFaaS</a> into a predicament: optimize for unit of isolation or deployment?</p><p>To achieve the best possible isolation, each function call would have to happen in its own container—ut what's good for isolation is not always good for performance. Inversely, if we run function calls within the same container, we increase the likelihood of collisions.</p><p>By using Landlock, we could isolate function calls from each other within the same container, making a temporary file created by one function call inaccessible to the next function call, for example. Integration between Landlock and technologies like Kubernetes-based serverless frameworks would be a ripe area for further exploration.</p><h2 id=auditing-kubectl-exec-with-ebpf>Auditing kubectl-exec with eBPF</h2><p>In Kubernetes 1.7 the <a href=/docs/tasks/debug-application-cluster/audit/>audit proposal</a> started making its way in. It's currently pre-stable with plans to be stable in the 1.10 release. As the name implies, it allows administrators to log and audit events that take place in a Kubernetes cluster.</p><p>While these events log Kubernetes events, they don't currently provide the level of visibility that some may require. For example, while we can see that someone has used <code>kubectl exec</code> to enter a container, we are not able to see what commands were executed in that session. With eBPF one can attach a BPF program that would record any commands executed in the <code>kubectl exec</code> session and pass those commands to a user-space program that logs those events. We could then play that session back and know the exact sequence of events that took place.</p><h2 id=learn-more-about-ebpf>Learn more about eBPF</h2><p>If you're interested in learning more about eBPF, here are some resources:</p><ul><li><p>A comprehensive <a href=https://qmonnet.github.io/whirl-offload/2016/09/01/dive-into-bpf/>reading list about eBPF</a> for doing just that</p></li><li><p><a href=https://github.com/iovisor/bcc>BCC</a> (BPF Compiler Collection) provides tools for working with eBPF as well as many example tools making use of BCC.</p></li><li><p>Some videos</p><ul><li><a href="https://www.youtube.com/watch?v=JRFNIKUROPE">BPF: Tracing and More</a> by Brendan Gregg</li><li><a href="https://www.youtube.com/watch?v=CcGtDMm1SJA">Cilium - Container Security and Networking Using BPF and XDP</a> by Thomas Graf</li><li><a href="https://www.youtube.com/watch?v=T3Wcuj8fy5o">Using BPF in Kubernetes</a> by Alban Crequy</li></ul></li></ul><h2 id=conclusion>Conclusion</h2><p>We are just starting to see the Linux superpowers of eBPF being put to use in Kubernetes tools and technologies. We will undoubtedly see increased use of eBPF. What we have highlighted here is just a taste of what you might expect in the future. What will be really exciting is seeing how these technologies will be used in ways that we have not yet thought about. Stay tuned!</p><p>The Kinvolk team will be hanging out at the Kinvolk booth at KubeCon in Austin. Come by to talk to us about all things, Kubernetes, Linux, container runtimes and yeah, eBPF.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e8fa47a93b8e7ae8336cb369f1710f7d>PaddlePaddle Fluid: Elastic Deep Learning on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-12-06 class=text-muted>Wednesday, December 06, 2017</time></div><p><em>Editor's note: Today's post is a joint post from the deep learning team at Baidu and the etcd team at CoreOS.</em></p><h2 id=paddlepaddle-fluid-elastic-deep-learning-on-kubernetes>PaddlePaddle Fluid: Elastic Deep Learning on Kubernetes</h2><p>Two open source communities—PaddlePaddle, the deep learning framework originated in Baidu, and Kubernetes®, the most famous containerized application scheduler—are announcing the Elastic Deep Learning (EDL) feature in PaddlePaddle’s new release codenamed Fluid.</p><p>Fluid EDL includes a <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md>Kubernetes controller</a>, <a href=https://github.com/PaddlePaddle/cloud/tree/develop/doc/edl/experiment#auto-scaling-experiment><em>PaddlePaddle auto-scaler</em></a>, which changes the number of processes of distributed jobs according to the idle hardware resource in the cluster, and a new fault-tolerable architecture as described in the <a href=https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/cluster_train/README.md>PaddlePaddle design doc</a>.</p><p>Industrial deep learning requires significant computation power. Research labs and companies often build GPU clusters managed by SLURM, MPI, or SGE. These clusters either run a submitted job if it requires less than the idle resource, or pend the job for an unpredictably long time. This approach has its drawbacks: in an example with 99 available nodes and a submitted job that requires 100, the job has to wait without using any of the available nodes. Fluid works with Kubernetes to power elastic deep learning jobs, which often lack optimal resources, by helping to expose potential algorithmic problems as early as possible.</p><p>Another challenge is that industrial users tend to run deep learning jobs as a subset stage of the complete data pipeline, including the web server and log collector. Such general-purpose clusters require priority-based elastic scheduling. This makes it possible to run more processes in the web server job and less in deep learning during periods of high web traffic, then prioritize deep learning when web traffic is low. Fluid talks to Kubernetes' API server to understand the global picture and orchestrate the number of processes affiliated with various jobs.</p><p>In both scenarios, PaddlePaddle jobs are tolerant to a process spikes and decreases. We achieved this by implementing the new design, which introduces a master process in addition to the old PaddlePaddle architecture as described in a <a href=https://kubernetes.io/blog/2017/02/run-deep-learning-with-paddlepaddle-on-kubernetes>previous blog post</a>. In the new design, as long as there are three processes left in a job, it continues. In extreme cases where all processes are killed, the job can be restored and resume.</p><p>We tested Fluid EDL for two use cases: 1) the Kubernetes cluster runs only PaddlePaddle jobs; and 2) the cluster runs PaddlePaddle and Nginx jobs.</p><p>In the first test, we started up to 20 PaddlePaddle jobs one by one with a 10-second interval. Each job has 60 trainers and 10 parameter server processes, and will last for hours. We repeated the experiment 20 times: 10 with FluidEDL turned off and 10 with FluidEDL turned on. In Figure one, solid lines correspond to the first 10 experiments and dotted lines the rest. In the upper part of the figure, we see that the number of pending jobs increments monotonically without EDL. However, when EDL is turned on, resources are evenly distributed to all jobs. Fluid EDL kills some existing processes to make room for new jobs and jobs coming in at a later point in time. In both cases, the cluster is equally utilized (see lower part of figure).</p><p>| <a href=https://1.bp.blogspot.com/-sp_sVZvhMbU/WiYgXMLQKuI/AAAAAAAAAIM/uc_3iT9BZmAtQGiGGSErgueHK71uWMBCACEwYBhgL/s1600/figure-1.png><img src=https://1.bp.blogspot.com/-sp_sVZvhMbU/WiYgXMLQKuI/AAAAAAAAAIM/uc_3iT9BZmAtQGiGGSErgueHK71uWMBCACEwYBhgL/s640/figure-1.png alt></a> |
| <em>Figure 1. Fluid EDL evenly distributes resource among jobs.</em><br>|</p><p>In the second test, each experiment ran 400 Nginx pods, which has higher priority than the six PaddlePaddle jobs. Initially, each PaddlePaddle job had 15 trainers and 10 parameter servers. We killed 100 Nginx pods every 90 seconds until 100 left, and then we started to increase the number of Nginx jobs by 100 every 90 seconds. The upper part of Figure 2 shows this process. The middle of the diagram shows that Fluid EDL automatically started some PaddlePaddle processes by decreasing Nginx pods, and killed PaddlePaddle processes by increasing Nginx pods later on. As a result, the cluster maintains around 90% utilization as shown in the bottom of the figure. When Fluid EDL was turned off, there were no PaddlePaddle processes autoincrement, and the utilization fluctuated with the varying number of Nginx pods.</p><p>| <a href=https://4.bp.blogspot.com/-gOMFfnaygSU/WiYgXO_KJ0I/AAAAAAAAAII/lMLjTGNGYhsovwKornCzMZBhEdMdPI5HACLcBGAs/s1600/figure-2.png><img src=https://4.bp.blogspot.com/-gOMFfnaygSU/WiYgXO_KJ0I/AAAAAAAAAII/lMLjTGNGYhsovwKornCzMZBhEdMdPI5HACLcBGAs/s640/figure-2.png alt></a> |
| <em>Figure 2. Fluid changes PaddlePaddle processes with the change of Nginx processes.</em> |</p><p>We continue to work on FluidEDL and welcome comments and contributions. Visit the <a href=https://github.com/PaddlePaddle/cloud>PaddlePaddle repo</a>, where you can find the <a href=https://github.com/PaddlePaddle/cloud/tree/develop/doc/design>design doc</a>, a <a href=https://github.com/PaddlePaddle/cloud/blob/develop/doc/autoscale/example/autoscale.md>simple tutorial</a>, and <a href=https://github.com/PaddlePaddle/cloud/tree/develop/doc/edl/experiment>experiment details</a>.</p><ul><li><p>Xu Yan (Baidu Research)</p></li><li><p>Helin Wang (Baidu Research)</p></li><li><p>Yi Wu (Baidu Research)</p></li><li><p>Xi Chen (Baidu Research)</p></li><li><p>Weibao Gong (Baidu Research)</p></li><li><p>Xiang Li (CoreOS)</p></li><li><p>Yi Wang (Baidu Research)</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bb3ab16fad15d53ca096effeae4cc253>Autoscaling in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-11-17 class=text-muted>Friday, November 17, 2017</time></div><p>Kubernetes allows developers to automatically adjust cluster sizes and the number of pod replicas based on current traffic and load. These adjustments reduce the amount of unused nodes, saving money and resources. In this talk, Marcin Wielgus of Google walks you through the current state of pod and node autoscaling in Kubernetes: .how it works, and how to use it, including best practices for deployments in production applications.</p><p>Enjoyed this talk? Join us for more exciting sessions on scaling and automating your Kubernetes clusters at KubeCon in Austin on December 6-8. <a href="https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006">Register Now</a></p><p>Be sure to check out <a href=http://sched.co/CU64>Automating and Testing Production Ready Kubernetes Clusters in the Public Cloud</a> by Ron Lipke, Senior Developer, Platform as a Service, Gannet/USA Today Network.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b848899277d49228b42ed8a172d8bd98>Certified Kubernetes Conformance Program: Launch Celebration Round Up</h1><div class="td-byline mb-4"><time datetime=2017-11-16 class=text-muted>Thursday, November 16, 2017</time></div><p><a href=https://1.bp.blogspot.com/-YasPeoIh8tA/Wg28rH4dzXI/AAAAAAAAAHg/Hfk2dnUoav4XMefGyjzMWdJMZbu1QJFagCK4BGAYYCw/s1600/certified_kubernetes_color.png><img src=https://1.bp.blogspot.com/-YasPeoIh8tA/Wg28rH4dzXI/AAAAAAAAAHg/Hfk2dnUoav4XMefGyjzMWdJMZbu1QJFagCK4BGAYYCw/s200/certified_kubernetes_color.png alt></a>This week the CNCFⓇ <a href=https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/>certified the first group</a> of KubernetesⓇ offerings under the <a href=https://www.cncf.io/certification/software-conformance/>Certified Kubernetes Conformance Program</a>. These first certifications follow a <a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>beta phase</a> during which we invited participants to submit conformance results. The community response was overwhelming: CNCF certified offerings from 32 vendors!</p><p>The new Certified Kubernetes Conformance Program gives enterprise organizations the confidence that workloads running on any Certified Kubernetes distribution or platform will work correctly on other Certified Kubernetes distributions or platforms. A Certified Kubernetes product guarantees that the complete Kubernetes API functions as specified, so users can rely on a seamless, stable experience.</p><p>Here’s what the world had to say about the Certified Kubernetes Conformance Program.</p><p>Press coverage:</p><ul><li><p><a href=https://venturebeat.com/2017/11/13/ibm-google-microsoft-and-33-more-partner-to-ensure-kubernetes-workload-portability/>IBM, Google, Microsoft, and 33 more partner to ensure Kubernetes workload portability</a>, VentureBeat.</p></li><li><p><a href=https://techcrunch.com/2017/11/13/the-cncf-just-got-36-companies-to-agree-to-a-kubernetes-certification-standard/>The CNCF just got 36 companies to agree to a Kubernetes certification standard</a>, TechCrunch</p></li><li><p><a href=https://thenewstack.io/cncf-introduces-c/>CNCF Ensures Kubernetes Interoperability with a New Cert Program</a>, The New Stack</p></li><li><p><a href=https://sdtimes.com/cloud-native-launches-certified-kubernetes-program/>Cloud Native launches Certified Kubernetes program</a>, SD Times</p></li><li><p><a href=https://www.rcrwireless.com/20171113/interoperability-standards-congealing-via-certified-kubernetes-program-tag27>CNCF offers Kubernetes Software Conformance Certification program</a> RCR Wireless </p></li><li><p><a href=https://virtualizationreview.com/articles/2017/11/13/new-kubernetes-certification-program-announced.aspx>New Kubernetes Certification Program Announced</a> Virtualization Review </p></li><li><p><a href=https://www.geekwire.com/2017/key-cloud-computing-group-launches-interoperability-certification-kubernetes/>Key Cloud Computing Group Launches Interoperability Certification for Kubernetes</a> GeekWire</p></li><li><p><a href=https://betanews.com/2017/11/13/kubernetes-certification-cloud/>New Kubernetes Certification Program Helps Deliver Consistency in the Cloud</a> BetaNews</p></li><li><p>​<a href=http://www.zdnet.com/article/kubernetes-vendors-agree-on-standardization/>Kubernetes vendors agree on standardization</a>, ZDNet</p></li><li><p><a href=https://www.sdxcentral.com/articles/news/cncf-kubernetes-conformance-program-provides-seal-of-approval/2017/11/>CNCF Launches Kubernetes Conformance Certification Program</a>, sdxcentral
Community blog round-up:</p></li><li><p><a href=https://cloudplatform.googleblog.com/2017/11/introducing-Certified-Kubernetes-and-Google-Kubernetes-Engine.html>Introducing Certified Kubernetes (and Google Kubernetes Engine!)</a>, Google</p></li><li><p><a href=https://blog.heptio.com/certified-kubernetes-a-key-step-forward-for-the-open-source-ecosystem-1f845df65898>Certified Kubernetes: A key step forward for the open source ecosystem</a>, Heptio</p></li><li><p><a href=https://developer.ibm.com/code/2017/11/13/is-kubernetes-crossing-the-chasm-yes/>Is Kubernetes “crossing the chasm”? Yes!</a>, IBM</p></li><li><p><a href=https://coreos.com/blog/coreos-tectonic-is-certified-kubernetes>CoreOS Tectonic is Certified Kubernetes</a>, CoreOS</p></li><li><p><a href=https://blogs.vmware.com/cloudnative/2017/11/13/vmware-pivotal-container-service-achieves-kubernetes-certification/>VMware Pivotal Container Service Achieves Kubernetes Certification</a>, VMWare Pivotal</p></li><li><p><a href=http://www.huaweicloud.com/en-us/news/1510655878651.html>Huawei Cloud Container Engine gained first wave of Certificated Kubernetes Qualification</a>, Huawei</p></li><li><p><a href=https://cloudfoundry.org/cloud-foundry-container-runtime-gets-kubernetes-certified/>Cloud Foundry Container Runtime Gets Kubernetes-Certified</a>, Cloud Foundry</p></li><li><p><a href=https://www.suse.com/communities/blog/suse-caas-platform-2-certified-kubernetes-conformance-software-certification/>SUSE CaaS Platform 2 certified under Kubernetes Conformance Software Certification</a>, SUSE</p></li><li><p><a href=https://mesosphere.com/blog/kubernetes-on-mesosphere-dcos-now-certified-by-cncf/>Kubernetes on Mesosphere DC/OS now certified by CNCF</a>, Mesosphere</p></li><li><p><a href=https://rancher.com/joining-k8s-conformance-program/>Rancher joins the CNCF Kubernetes Software Conformance Certification program</a>, Rancher</p></li><li><p><a href=https://blog.stackpoint.io/stackpointcloud-becomes-first-multi-cloud-cncf-certified-kubernetes-offering-3c7983a71c5f>StackPointCloud Becomes First Multi-Cloud CNCF Certified Kubernetes Offering</a>, StackPointCloud
Visit <a href=https://www.cncf.io/certification/software-conformance/>https://www.cncf.io/certification/software-conformance</a> for more information about the Certified Kubernetes Conformance Program, and learn how you can join a growing list of Certified Kubernetes providers.</p></li></ul><p><em>“Cloud Native Computing Foundation”, “CNCF” and “Kubernetes” are registered trademarks of The Linux Foundation in the United States and other countries. “Certified Kubernetes” and the Certified Kubernetes design are trademarks of The Linux Foundation in the United States and other countries.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-6e2f778b3b741a46a450bca4c9d0aee8>Kubernetes is Still Hard (for Developers)</h1><div class="td-byline mb-4"><time datetime=2017-11-15 class=text-muted>Wednesday, November 15, 2017</time></div><p>Kubernetes has made the Ops experience much easier, but how does the developer experience compare? Ops teams can deploy a Kubernetes cluster in a matter of minutes. But developers need to understand a host of new concepts before beginning to work with Kubernetes. This can be a tedious and manual process, but it doesn’t have to be. In this talk, <a href=https://twitter.com/michellenoorali>Michelle Noorali</a>, co-lead of SIG-Apps, reimagines the Kubernetes developer experience. She shares her top 3 tips for building a successful developer experience including:</p><ol><li>A framework for thinking about cloud native applications</li><li>An integrated experience for debugging and fine-tuning cloud native applicationsA way to get a cloud native application out the door quickly
Interested in learning how far the Kubernetes developer experience has come? Join us at KubeCon in Austin on December 6-8. <a href=https://goo.gl/TK9ET3>Register Now >></a></li></ol><p><a href=http://sched.co/CUCC>Check out Michelle’s keynote</a> to learn about exciting new updates from CNCF projects.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2605319ddc2ce7e9f0fcc32f893d4b82>Securing Software Supply Chain with Grafeas</h1><div class="td-byline mb-4"><time datetime=2017-11-03 class=text-muted>Friday, November 03, 2017</time></div><p><strong><em>Editor's note: This post is written by Kelsey Hightower, Staff Developer Advocate at Google, and Sandra Guo, Product Manager at Google.</em></strong></p><p>Kubernetes has evolved to support increasingly complex classes of applications, enabling the development of two major industry trends: hybrid cloud and microservices. With increasing complexity in production environments, customers—especially enterprises—are demanding better ways to manage their software supply chain with more centralized visibility and control over production deployments.</p><p>On October 12th, Google and partners <a href=https://cloudplatform.googleblog.com/2017/10/introducing-grafeas-open-source-api-.html>announced</a> Grafeas, an open source initiative to define a best practice for auditing and governing the modern software supply chain. With Grafeas (“scribe” in Greek), developers can plug in components of the CI/CD pipeline into a central source of truth for tracking and enforcing policies. Google is also working on <a href=https://github.com/Grafeas/Grafeas/blob/master/case-studies/binary-authorization.md>Kritis</a> (“judge” in Greek), allowing devOps teams to enforce deploy-time image policy using metadata and attestations stored in Grafeas.</p><p>Grafeas allows build, auditing and compliance tools to exchange comprehensive metadata on container images using a central API. This allows enforcing policies that provide central control over the software supply process.</p><p><a href=https://2.bp.blogspot.com/-TDD4slMA7gg/WfzDeKVLr2I/AAAAAAAAAGw/dhfWOrCMdmogSNhGr5RrA2ovr02K5nn8ACK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.13%2BPM.png><img src=https://2.bp.blogspot.com/-TDD4slMA7gg/WfzDeKVLr2I/AAAAAAAAAGw/dhfWOrCMdmogSNhGr5RrA2ovr02K5nn8ACK4BGAYYCw/s400/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.13%2BPM.png alt></a></p><h2 id=example-application-paymentprocessor>Example application: PaymentProcessor</h2><p>Let’s consider a simple application, <em>PaymentProcessor</em>, that retrieves, processes and updates payment info stored in a database. This application is made up of two containers: a standard ruby container and custom logic.</p><p>Due to the sensitive nature of the payment data, the developers and DevOps team really want to make sure that the code meets certain security and compliance requirements, with detailed records on the provenance of this code. There are CI/CD stages that validate the quality of the PaymentProcessor release, but there is no easy way to centrally view/manage this information:</p><p><a href=https://1.bp.blogspot.com/-WeI6zpGd42A/WfzDkkIonFI/AAAAAAAAAG4/wKUaNaXYvaQ-an9p4_9T9J3EQB_zHkRXwCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.23%2BPM.png><img src=https://1.bp.blogspot.com/-WeI6zpGd42A/WfzDkkIonFI/AAAAAAAAAG4/wKUaNaXYvaQ-an9p4_9T9J3EQB_zHkRXwCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.23%2BPM.png alt></a></p><h2 id=visibility-and-governance-over-the-paymentprocessor-code>Visibility and governance over the PaymentProcessor Code</h2><p>Grafeas provides an API for customers to centrally manage metadata created by various CI/CD components and enables deploy time policy enforcement through a Kritis implementation.</p><p><a href=https://4.bp.blogspot.com/-SRMfm5z606M/WfzDpHqlz-I/AAAAAAAAAHA/y2suaInhr9E0hU0u78PacBT_kZj2D7DKgCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.34%2BPM.png><img src=https://4.bp.blogspot.com/-SRMfm5z606M/WfzDpHqlz-I/AAAAAAAAAHA/y2suaInhr9E0hU0u78PacBT_kZj2D7DKgCK4BGAYYCw/s1600/Screen%2BShot%2B2017-11-03%2Bat%2B12.28.34%2BPM.png alt></a></p><p>Let’s consider a basic example of how Grafeas can provide deploy time control for the PaymentProcessor app using a demo verification pipeline.</p><p>Assume that a PaymentProcessor container image has been created and pushed to Google Container Registry. This example uses the gcr.io/exampleApp/PaymentProcessor container for testing. You as the QA engineer want to create an attestation certifying this image for production usage. Instead of trusting an image tag like 0.0.1, which can be reused and point to a different container image later, we can trust the image digest to ensure the attestation links to the full image contents.</p><p><strong>1. Set up the environment</strong></p><p>Generate a signing key:</p><pre><code>gpg --quick-generate-key --yes qa\_bob@example.com
</code></pre><p>Export the image signer's public key:</p><pre><code>gpg --armor --export image.signer@example.com \&gt; ${GPG\_KEY\_ID}.pub
</code></pre><p>Create the ‘qa’ AttestationAuthority note via the Grafeas API:</p><pre><code>curl -X POST \  
  &quot;http://127.0.0.1:8080/v1alpha1/projects/image-signing/notes?noteId=qa&quot; \  
  -d @note.json
</code></pre><p>Create the Kubernetes ConfigMap for admissions control and store the QA signer's public key:</p><pre><code>kubectl create configmap image-signature-webhook \  
  --from-file ${GPG\_KEY\_ID}.pub

kubectl get configmap image-signature-webhook -o yaml
</code></pre><p>Set up an admissions control webhook to require QA signature during deployment.</p><pre><code>kubectl apply -f kubernetes/image-signature-webhook.yaml
</code></pre><p><strong>2. Attempt to deploy an image without QA attestation</strong></p><p>Attempt to run the image in paymentProcessor.ymal before it is QA attested:</p><pre><code>kubectl apply -f pods/nginx.yaml

apiVersion: v1

kind: Pod

metadata:

  name: payment

spec:

  containers:

    - name: payment

      image: &quot;gcr.io/hightowerlabs/payment@sha256:aba48d60ba4410ec921f9d2e8169236c57660d121f9430dc9758d754eec8f887&quot;
</code></pre><p>Create the paymentProcessor pod:</p><pre><code>kubectl apply -f pods/paymentProcessor.yaml
</code></pre><p>Notice the paymentProcessor pod was not created and the following error was returned:</p><pre><code>The  &quot;&quot; is invalid: : No matched signatures for container image: gcr.io/hightowerlabs/payment@sha256:aba48d60ba4410ec921f9d2e8169236c57660d121f9430dc9758d754eec8f887
</code></pre><p><strong>3. Create an image signature</strong></p><p>Assume the image digest is stored in Image-digest.txt, sign the image digest:</p><pre><code>gpg -u qa\_bob@example.com \  
  --armor \  
  --clearsign \  
  --output=signature.gpg \  
  Image-digest.txt
</code></pre><p><strong>4. Upload the signature to the Grafeas API</strong></p><p>Generate a pgpSignedAttestation occurrence from the signature :</p><pre><code>cat \&gt; occurrence.json \&lt;\&lt;EOF  
{  
  &quot;resourceUrl&quot;: &quot;$(cat image-digest.txt)&quot;,  
  &quot;noteName&quot;: &quot;projects/image-signing/notes/qa&quot;,  
  &quot;attestation&quot;: {  
    &quot;pgpSignedAttestation&quot;: {  
       &quot;signature&quot;: &quot;$(cat signature.gpg)&quot;,  
       &quot;contentType&quot;: &quot;application/vnd.gcr.image.url.v1&quot;,  
       &quot;pgpKeyId&quot;: &quot;${GPG\_KEY\_ID}&quot;  
    }  
  }  
}  
EOF
</code></pre><p>Upload the attestation through the Grafeas API:</p><pre><code>curl -X POST \  
  'http://127.0.0.1:8080/v1alpha1/projects/image-signing/occurrences' \  
  -d @occurrence.json
</code></pre><p><strong>5. Verify QA attestation during a production deployment</strong></p><p>Attempt to run the image in paymentProcessor.ymal now that it has the correct attestation in the Grafeas API:</p><pre><code>kubectl apply -f pods/paymentProcessor.yaml

pod &quot;PaymentProcessor&quot; created
</code></pre><p>With the attestation added the pod will be created as the execution criteria are met.</p><p>For more detailed information, see this <a href=https://github.com/kelseyhightower/grafeas-tutorial>Grafeas tutorial</a>.</p><h2 id=summary>Summary</h2><p>The demo above showed how you can integrate your software supply chain with Grafeas and gain visibility and control over your production deployments. However, the demo verification pipeline by itself is not a full Kritis implementation. In addition to basic admission control, Kritis provides additional support for workflow enforcement, multi-authority signing, breakglass deployment and more. You can read the <a href=https://github.com/Grafeas/Grafeas/blob/master/case-studies/binary-authorization.md>Kritis whitepaper</a> for more details. The team is actively working on a full open-source implementation. We’d love your feedback!</p><p>In addition, a hosted alpha implementation of Kritis, called Binary Authorization, is available on Google Container Engine and will be available for broader consumption soon.</p><p>Google, JFrog, and other partners joined forces to create Grafeas based on our common experiences building secure, large, and complex microservice deployments for internal and enterprise customers. Grafeas is an industry-wide community effort.</p><p>To learn more about Grafeas and contribute to the project:</p><ul><li>Register for the JFrog-Google webinar [<a href=https://leap.jfrog.com/WN2017-ImplementingaSingleSourceofTruthinaHybridCloudWorld_RegistrationPage.html>here</a>]</li><li>Try Grafeas now and join the GitHub project: <a href=https://github.com/grafeas>https://github.com/grafeas</a></li><li>Try out the Grafeas demo and tutorial: <a href=https://github.com/kelseyhightower/grafeas-tutorial>https://github.com/kelseyhightower/grafeas-tutorial</a></li><li>Attend Shopify’s talks at <a href=https://kccncna17.sched.com/event/CU83/securing-shopifys-paas-on-gke-i-jonathan-pulsifer-shopify>KubeCon in December</a></li><li>Fill out [<a href=https://docs.google.com/forms/d/e/1FAIpQLSdr8kDTkAkml5f9TW_kzz06C0s0QuV_sWYzHC7NM90F5CZ2bQ/viewform>this form</a>] if you’re interested in learning more about our upcoming releases or talking to us about integrations</li><li>See <a href=https://grafeas.io/>grafeas.io</a> for documentation and examples
We hope you join us!<br>The Grafeas Team</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ce02ca3769c2c57ae67a61b96ef89b47>Containerd Brings More Container Runtime Options for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-11-02 class=text-muted>Thursday, November 02, 2017</time></div><p><strong><em>Editor's note: Today's post is by Lantao Liu, Software Engineer at Google, and Mike Brown, Open Source Developer Advocate at IBM.</em></strong></p><p>A <em>container runtime</em> is software that executes containers and manages container images on a node. Today, the most widely known container runtime is <a href=https://www.docker.com/>Docker</a>, but there are other container runtimes in the ecosystem, such as <a href=https://coreos.com/rkt/>rkt</a>, <a href=https://containerd.io/>containerd</a>, and <a href=https://linuxcontainers.org/lxd/>lxd</a>. Docker is by far the most common container runtime used in production Kubernetes environments, but Docker’s smaller offspring, containerd, may prove to be a better option. This post describes using containerd with Kubernetes.</p><p>Kubernetes 1.5 introduced an internal plugin API named <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes>Container Runtime Interface (CRI)</a> to provide easy access to different container runtimes. CRI enables Kubernetes to use a variety of container runtimes without the need to recompile. In theory, Kubernetes could use any container runtime that implements CRI to manage pods, containers and container images.</p><p>Over the past 6 months, engineers from Google, Docker, IBM, ZTE, and ZJU have worked to implement CRI for containerd. The project is called <a href=https://github.com/kubernetes-incubator/cri-containerd>cri-containerd</a>, which had its <a href=https://github.com/kubernetes-incubator/cri-containerd/releases/tag/v1.0.0-alpha.0>feature complete v1.0.0-alpha.0 release</a> on September 25, 2017. With cri-containerd, users can run Kubernetes clusters using containerd as the underlying runtime without Docker installed.</p><h2 id=containerd>containerd</h2><p><a href=https://containerd.io/>Containerd</a> is an <a href=https://www.opencontainers.org/>OCI</a> compliant core container runtime designed to be embedded into larger systems. It provides the minimum set of functionality to execute containers and manages images on a node. It was initiated by Docker Inc. and <a href=https://www.cncf.io/announcement/2017/03/29/containerd-joins-cloud-native-computing-foundation/>donated to CNCF</a> in March of 2017. The Docker engine itself is built on top of earlier versions of containerd, and will soon be updated to the newest version. Containerd is close to a feature complete stable release, with <a href=https://github.com/containerd/containerd/releases/tag/v1.0.0-beta.1>1.0.0-beta.1</a> available right now.</p><p>Containerd has a much smaller scope than Docker, provides a golang client API, and is more focused on being embeddable.The smaller scope results in a smaller codebase that’s easier to maintain and support over time, matching Kubernetes requirements as shown in the following table:</p><table><thead><tr><th></th><th>Containerd Scope (In/Out)</th><th>Kubernetes Requirement</th></tr></thead><tbody><tr><td>Container Lifecycle Management</td><td>In</td><td>Container Create/Start/Stop/Delete/List/Inspect (✔️)</td></tr><tr><td>Image Management</td><td>In</td><td>Pull/List/Inspect (✔️)</td></tr><tr><td>Networking</td><td>Out No concrete network solution. User can setup network namespace and put containers into it.</td><td>Kubernetes networking deals with pods, rather than containers, so container runtimes should not provide complex networking solutions that don't satisfy requirements. (✔️)</td></tr><tr><td>Volumes</td><td>Out, No volume management. User can setup host path, and mount it into container.</td><td>Kubernetes manages volumes. Container runtimes should not provide internal volume management that may conflict with Kubernetes. (✔️)</td></tr><tr><td>Persistent Container Logging</td><td>Out, No persistent container log. Container STDIO is provided as FIFOs, which can be redirected/decorated as is required.</td><td>Kubernetes has specific requirements for persistent container logs, such as format and path etc. Container runtimes should not  persist an unmanageable container log. (✔️)</td></tr><tr><td>Metrics</td><td>In Containerd provides container and snapshot metrics as part of the API.</td><td>Kubernetes expects container runtime to provide container metrics (CPU, Memory, writable layer size, etc.) and image filesystem usage (disk, inode usage, etc.). (✔️)</td></tr><tr><td>Overall, from a technical perspective, containerd is a very good alternative container runtime for Kubernetes.</td><td></td><td></td></tr></tbody></table><h2 id=cri-containerd>cri-containerd</h2><p><a href=https://github.com/kubernetes-incubator/cri-containerd>Cri-containerd</a> is exactly that: an implementation of CRI for containerd. It operates on the same node as the Kubelet and containerd. Layered between Kubernetes and containerd, cri-containerd handles all CRI service requests from the Kubelet and uses containerd to manage containers and container images. Cri-containerd manages these service requests in part by forming containerd service requests while adding sufficient additional function to support the CRI requirements.</p><p><img src=https://lh6.googleusercontent.com/4NGAPzwhkL0GTNjkAEFN9iWX_Wc0ZE-AZxAxEw4E5aOntuGmv764b3ZYQUyapSnP9BrlUs2rUyo5kiCrj5QuiMHw3-dz2vPUDma029Qt3tej9QABEHFSsOBsq6LjLfFhTBgMhAAc alt></p><p>Compared with the current Docker CRI implementation (<a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim>dockershim</a>), cri-containerd eliminates an extra hop in the stack, making the stack more stable and efficient.</p><h2 id=architecture>Architecture</h2><p>Cri-containerd uses containerd to manage the full container lifecycle and all container images. As also shown below, cri-containerd manages pod networking via <a href=https://github.com/containernetworking/cni>CNI</a> (another CNCF project).</p><p><img src=https://lh5.googleusercontent.com/sfkhKO3jiLZ9_TtPpxTsKxkbe1KHg1nrfqkbJYrjN2DbNQE_y31NJVSyDIXe0oQjSwVcQ4gFCyr1MZ9_V4GZuuiHwuU3Pq6ldpRhcRiiuTJaRVuezPK9KFLKovP8mQ6sXTYF_eru alt></p><p>Let’s use an example to demonstrate how cri-containerd works for the case when Kubelet creates a single-container pod:</p><ol><li>Kubelet calls cri-containerd, via the CRI runtime service API, to create a pod;</li><li>cri-containerd uses containerd to create and start a special <a href=https://www.ianlewis.org/en/almighty-pause-container>pause container</a> (the <em>sandbox container</em>) and put that container inside the pod’s cgroups and namespace (steps omitted for brevity);</li><li>cri-containerd configures the pod’s network namespace using CNI;</li><li>Kubelet subsequently calls cri-containerd, via the CRI image service API, to pull the application container image;</li><li>cri-containerd further uses containerd to pull the image if the image is not present on the node;</li><li>Kubelet then calls cri-containerd, via the CRI runtime service API, to create and start the application container inside the pod using the pulled container image;</li><li>cri-containerd finally calls containerd to create the application container, put it inside the pod’s cgroups and namespace, then to start the pod’s new application container.
After these steps, a pod and its corresponding application container is created and running.</li></ol><h2 id=status>Status</h2><p>Cri-containerd v1.0.0-alpha.0 was released on Sep. 25, 2017.</p><p>It is feature complete. All Kubernetes features are supported.</p><p>All <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md>CRI validation tests</a> have passed. (A CRI validation is a test framework for validating whether a CRI implementation meets all the requirements expected by Kubernetes.)</p><p>All regular <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md>node e2e tests</a> have passed. (The Kubernetes test framework for testing Kubernetes node level functionalities such as managing pods, mounting volumes etc.)</p><p>To learn more about the v1.0.0-alpha.0 release, see the <a href=https://github.com/kubernetes-incubator/cri-containerd/releases/tag/v1.0.0-alpha.0>project repository</a>.</p><h2 id=try-it-out>Try it Out</h2><p>For a multi-node cluster installer and bring up steps using ansible and kubeadm, see <a href=https://github.com/kubernetes-incubator/cri-containerd/blob/master/contrib/ansible/README.md>this repo link</a>.</p><p>For creating a cluster from scratch on Google Cloud, see <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kubernetes the Hard Way</a>.</p><p>For a custom installation from release tarball, see <a href=https://github.com/kubernetes-incubator/cri-containerd/blob/master/docs/installation.md>this repo link</a>.</p><p>For a installation with LinuxKit on a local VM, see <a href=https://github.com/linuxkit/linuxkit/tree/master/projects/kubernetes>this repo link</a>.</p><h2 id=next-steps>Next Steps</h2><p>We are focused on stability and usability improvements as our next steps.</p><ul><li><p>Stability:</p><ul><li>Set up a full set of Kubernetes integration test in the Kubernetes test infrastructure on various OS distros such as Ubuntu, COS (<a href=https://cloud.google.com/container-optimized-os/docs/>Container-Optimized OS</a>) etc.</li><li>Actively fix any test failures and other issues reported by users.</li></ul></li><li><p>Usability:</p><ul><li>Improve the user experience of <a href=https://github.com/kubernetes-incubator/cri-tools/blob/master/docs/crictl.md><em>crictl</em></a>. Crictl is a portable command line tool for all CRI container runtimes. The goal here is to make it easy to use for debug and development scenarios.</li><li>Integrate cri-containerd with <a href=/docs/getting-started-guides/gce/><em>kube-up.sh</em></a>, to help users bring up a production quality Kubernetes cluster using cri-containerd and containerd.</li><li>Improve our documentation for users and admins alike.</li></ul></li></ul><p>We plan to release our v1.0.0-beta.0 by the end of 2017.</p><h2 id=contribute>Contribute</h2><p>Cri-containerd is a Kubernetes incubator project located at <a href=https://github.com/kubernetes-incubator/cri-containerd>https://github.com/kubernetes-incubator/cri-containerd</a>. Any contributions in terms of ideas, issues, and/or fixes are welcome. The <a href=https://github.com/kubernetes-incubator/cri-containerd#getting-started-for-developers>getting started guide for developers</a> is a good place to start for contributors.</p><h2 id=community>Community</h2><p>Cri-containerd is developed and maintained by the Kubernetes SIG-Node community. We’d love to hear feedback from you. To join the community:</p><ul><li><a href=https://github.com/kubernetes/community/tree/master/sig-node>sig-node community site</a></li><li>Slack: #sig-node channel in Kubernetes (<a href=http://kubernetes.slack.com/>kubernetes.slack.com</a>)</li><li>Mailing List: <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-node>https://groups.google.com/forum/#!forum/kubernetes-sig-node</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a6359bde9cacd401a01c05bfedf54a5a>Kubernetes the Easy Way</h1><div class="td-byline mb-4"><time datetime=2017-11-01 class=text-muted>Wednesday, November 01, 2017</time></div><p><strong><em>Editor's note: Today's post is by Dan Garfield, VP of Marketing at Codefresh, on how to set up and easily deploy a Kubernetes cluster.</em></strong></p><p>Kelsey Hightower wrote an invaluable guide for Kubernetes called <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>Kubernetes the Hard Way</a>. It’s an awesome resource for those looking to understand the ins and outs of Kubernetes—but what if you want to put Kubernetes on easy mode? That’s something we’ve been working on together with Google Cloud. In this guide, we’ll show you how to get a cluster up and running, as well as how to actually deploy your code to that cluster and run it.</p><p>This is Kubernetes the easy way. </p><h2 id=what-we-ll-accomplish>What We’ll Accomplish</h2><ol><li>1.Set up a cluster</li><li>2.Deploy an application to the cluster</li><li>3.Automate deployment with rolling updates</li></ol><h2 id=prerequisites>Prerequisites</h2><ul><li>A containerized application</li><li>You can also use <a href=https://github.com/containers101/demochat>a demo app</a>.</li><li>A <a href="https://cloud.google.com/?utm_source=kubernetes.io&utm_medium=codefresh-easy-mode">Google Cloud Account</a> or a Kubernetes cluster on another provider</li><li>Everything after Cluster creation is identical with all providers.</li><li>A free account on <a href=https://codefresh.io/kubernetes-deploy/>Codefresh</a></li><li>Codefresh is a service that handles Kubernetes deployment configuration and automation. </li></ul><p>We made Codefresh free for open-source projects and offer 200 builds/mo free for private projects, to make adopting Kubernetes as easy as possible. Deploy as much as you like on as many clusters as you like. </p><h2 id=set-up-a-cluster>Set Up a Cluster</h2><ol><li>Create an account at <a href="https://cloud.google.com/?utm_source=kubernetes.io&utm_medium=codefresh-easy-mode">cloud.google.com</a> and log in.</li></ol><p><strong>Note:</strong> If you’re using a Cluster outside of Google Cloud, you can skip this step.</p><p>Google Container Engine is Google Cloud’s managed Kubernetes service. In our testing, it’s both powerful and easy to use.</p><p>If you’re new to the platform, you can get a $500 credit at the end of this process.</p><ol start=2><li>Open the menu and scroll down to <strong>Container Engine</strong>. Then select <strong>Container Clusters</strong>.</li></ol><p><img src=https://lh6.googleusercontent.com/dqvtK-xyGelr_LW3qlFiamYRrpiq633R68cKitrbCZPtDY_uLBF7R7_PGVNvWja24_mG74vDBzpXddYhbRNeyBGPbQ_yfCq367Zp7eJZoiJEWurFWdmJ0AJlNJJ9TzDivE-8Ak9E alt></p><ol start=3><li>Click <strong>Create cluster.</strong></li></ol><p>We’re done with step 1. In my experience it usually takes less than 5 minutes for a cluster to be created. </p><h2 id=deploy-an-application-to-kubernetes>Deploy an Application to Kubernetes</h2><p>First go to <a href=https://codefresh.io/kubernetes-deploy/>Codefresh and create an account using GitHub, Bitbucket, or Gitlab</a>. As mentioned previously, Codefresh is free for both open source and smaller private projects. We’ll use it to create the configuration Yaml necessary to deploy our application to Kubernetes. Then we'll deploy our application and automate the process to happen every time we commit code changes. Here are the steps:</p><ol><li>1.Create a Codefresh account</li><li>2.Connect to Google Cloud (or other cluster)</li><li>3.Add Cluster</li><li>4.Deploy static image</li><li>5.Build and deploy an image</li><li>6.Automate the process</li></ol><h3 id=connect-to-google-cloud>Connect to Google Cloud</h3><p>To connect your Clusters in Google Container Engine, go to <em>Account Settings > Integrations > Kubernetes</em> and click <strong>Authenticate</strong>. This prompts you to login with your Google credentials.</p><p>Once you log in, all of your clusters are available within Codefresh.</p><p><img src=https://lh4.googleusercontent.com/edYv9DtPymvBBN37KdjUUkhkA9Cy7tZmGMw5V94XEWkesGh9xlOn3O7f6MdsmzKlF75KPM908CXLd9i3bbJCfgZ4BpGy6WvL_l1ADu9tWSIdm9l_uUiB0lPLyvCk1d1FCu2fLc0f alt></p><h3 id=add-cluster>Add Cluster</h3><p>To add your cluster, click the down arrow, and then click <strong>add cluster</strong>, select the project and cluster name. You can now deploy images!</p><h3 id=optional-use-an-alternative-cluster>Optional: Use an Alternative Cluster</h3><p>To connect a non-GKE cluster we’ll need to add a token and certificate to Codefresh. Go to <em>Account Settings (bottom left) > Integrations > Kubernetes > Configure > Add Provider > Custom Providers</em>. Expand the dropdown and click <strong>Add Cluster</strong>.</p><p><img src=https://lh4.googleusercontent.com/UNXfErkrIV-eoyAi3DS9zkRm8Awk7wMTpIQZssrscKY6hehDo63jzvkBYAdgD3fXJXgcDApi4z5dHI5S99Nk6YbvUVUQU_6hC7qRZ-9Y828k-N86f23OOSG04CXvlTWDE9XDIWhd alt></p><p>Follow the instructions on how to generate the needed information and click Save. Your cluster now appears under the Kubernetes tab. </p><h3 id=deploy-static-image-to-kubernetes>Deploy Static Image to Kubernetes</h3><p>Now for the fun part! Codefresh provides an easily modifiable boilerplate that takes care of the heavy lifting of configuring Kubernetes for your application.</p><ol><li>Click on the <strong>Kubernetes</strong> tab: this shows a list of namespaces.</li></ol><p>Think of namespaces as acting a bit like VLANs on a Kubernetes cluster. Each namespace can contain all the services that need to talk to each other on a Kubernetes cluster. For now, we’ll just work off the default namespace (the easy way!).</p><ol start=2><li>Click <strong>Add Service</strong> and fill in the details.</li></ol><p>You can use the <a href=https://github.com/containers101/demochat>demo application I mentioned earlier</a> that has a Node.js frontend with a MongoDB.</p><p><img src=https://lh4.googleusercontent.com/YzQzEdIMwWt3lGR9Q4RTELvaB_fYYo2QKqkeXhfTCDnIVX4FBx_quYNgAbo6Wc_wpk0anl7Co3RDwDWnrOyibog9V9DISOZYQqiFE9T4ErlDYuqOGWiRw3-zk4p4WcURaOVg3Dkn alt></p><p>Here’s the info we need to pass:</p><p><strong>Cluster</strong> - This is the cluster we added earlier, our application will be deployed there.<br><strong>Namespace</strong> - We’ll use default for our namespace but you can create and use a new one if you’d prefer. Namespaces are discrete units for grouping all the services associated with an application.<br><strong>Service name</strong> - You can name the service whatever you like. Since we’re deploying Mongo, I’ll just name it mongo!<br><strong>Expose port</strong> - We don’t need to expose the port outside of our cluster so we won’t check the box for now but we will specify a port where other containers can talk to this service. Mongo’s default port is ‘27017’.<br><strong>Image</strong> - Mongo is a public image on Dockerhub, so I can reference it by name and tag, ‘mongo:latest’.<br><strong>Internal Ports</strong> - This is the port the mongo application listens on, in this case it’s ‘27017’ again.</p><p>We can ignore the other options for now.</p><ol start=3><li>Scroll down and click <strong>Deploy</strong>.</li></ol><p><img src=https://lh4.googleusercontent.com/4dhWXsf0BhyDDyB6XmmuCo2RCNztTPNuy36lYuzAHEYsFmKKkS6ibbKKo3sIqyQIYNTsTE6m5fjtlnEB0gmYoeQ40DZjwuSVyO4-pQKPjZflDT75NZ61aytXnEhFiAUHUDk9l1Wj alt></p><p>Boom! You’ve just deployed this image to Kubernetes. You can see by clicking on the status that the service, deployment, replicas, and pods are all configured and running. If you click Edit > Advanced, you can see and edit all the raw YAML files associated with this application, or copy them and put them into your repository for use on any cluster. </p><h3 id=build-and-deploy-an-image>Build and Deploy an Image</h3><p>To get the rest of our demo application up and running we need to build and deploy the Node.js portion of the application. To do that we’ll need to add our repository to Codefresh.</p><ol><li>Click on <em>Repositories > Add Repository</em>, then copy and paste the <a href=https://github.com/containers101/demochat>demochat repo url</a> (or use your own repo).</li></ol><p><img src=https://lh6.googleusercontent.com/Mbs04O7PFJ6yFRRmPo2PDs3MU5IyKq53jrgSB6Xcm1Ki8eStJacoRsPDqv5_m92E0Ki-r-hi_4nbaAqUKRXNE57-TJbmacM3vqrkwM-3ASuBGmmugGc-QkHgfQrRSuAzCP60bSzA alt></p><p>We have the option to use a dockerfile, or to use a template if we need help creating a dockerfile. In this case, the demochat repo already has a dockerfile so we’ll select that. Click through the next few screens until the image builds.</p><p>Once the build is finished the image is automatically saved inside of the Codefresh docker registry. You can also add any <a href=https://docs.codefresh.io/v1.0/docs/docker-registry>other registry to your account</a> and use that instead.</p><p>To deploy the image we’ll need</p><ul><li>a pull secret</li><li>the image name and registry</li><li>the ports that will be used</li></ul><h3 id=creating-the-pull-secret>Creating the Pull Secret</h3><p>The pull secret is a token that the Kubernetes cluster can use to access a private Docker registry. To create one, we’ll need to generate the token and save it to Codefresh.</p><ol><li><p>Click on <strong>User Settings</strong> (bottom left) and generate a new token.</p></li><li><p>Copy the token to your clipboard.</p></li></ol><p><img src=https://lh5.googleusercontent.com/fJxTvuK0b-ssLls87EgSccmpZoRk_KXTQdxOglvgKlPHlc6pr-yNBht4rKYyLcFF7SERS2czWLSh_YUNGOy7Q9UjQqlGNKJdmG1uyDpVr_IIx3BqsauxfXnIrEtQbdXKAOg-nfr3 alt></p><ol start=3><li><p>Go to <em>Account Settings > Integrations > Docker Registry > Add Registry</em> and select <strong>Codefresh Registry</strong>. Paste in your token and enter your username (entry is case sensitive). Your username must match your name displayed at the bottom left of the screen.</p></li><li><p>Test and save it.</p></li></ol><p>We’ll now be able to create our secret later on when we deploy our image.</p><h3 id=get-the-image-name>Get the image name</h3><ol><li>Click on <strong>Images</strong> and open the image you just built. Under <em>Comment</em> you’ll see the image name starting with r.cfcr.io.</li></ol><p><img src=https://lh5.googleusercontent.com/5XciQfEpUxYZp6Tuic3TWFOkXi5I_x-16i9yXkpAMn4BRC54Rh7Hic4yM5Feo6A65jArBQyXfIgexTZ9rp-mM6l9Rmu4fm3aeE48x98veKN4_39j3hkRVm8goLTaWX0U9KgJuYIi alt></p><ol start=2><li>Copy the image name; we’ll need to paste it in later.</li></ol><h3 id=deploy-the-private-image-to-kubernetes>Deploy the private image to Kubernetes</h3><p>We’re now ready to deploy the image we built.</p><ol><li>Go to the Kubernetes page and, like we did with mongo, click Add Service and fill out the page. Make sure to select the same namespace you used to deploy mongo earlier. </li></ol><p><img src=https://lh4.googleusercontent.com/reUappaYGmp27xL32HFg6OWHRZfw60o5fUxTII7jrUUmGN4lqNrEaPW8Dl5RHK-N4nOCSOTe-9A6Y0HIiSzPxyCceOzOmrNeTB_QGKRfyI5EnpTM7mT-neGsBwYx-zn4BETgN8Nz alt></p><p>Now let’s expose the port so we can access this application. This provisions an IP address and automatically configures ingress.</p><ol start=2><li>Click <strong>Deploy</strong> : your application will be up and running within a few seconds! The IP address may take longer to provision depending on your cluster location.</li></ol><p><img src=https://lh6.googleusercontent.com/nGiPsfscMpcvxfjqseEH5Ft2K3yzvT93ZW3vVJtg_QF3gN_-ndMnZ4Kpcz_WqIr76irCwaBFr7Du6mzVGYYgHxgZFdBNi3hWWW5UWFtnvhyEq2DDM8zCIEXKTo84gjGCOsvenp1r alt></p><p>From this view you can scale the replicas, see application status, and similar tasks.</p><ol start=3><li>Click on the IP address to view the running application.</li></ol><p><img src=https://lh4.googleusercontent.com/GmZYxhd4tgEJONm8MBIY_m1rfOH05_LxCwpnbrFk013pNEIMAcNGsuPqR5DfFevjbTYAKTRqj4aXhwxowXM5D7p5KjBLqZ0YyTP226Awl2BC6MdBXwfb3E-HEAZTI_MlEEkBu5oC alt></p><p>At this point you should have your entire application up and running! Not so bad huh? Now to automate deployment!</p><h3 id=automate-deployment-to-kubernetes>Automate Deployment to Kubernetes</h3><p>Every time we make a change to our application, we want to build a new image and deploy it to our cluster. We’ve already set up automated builds, but to automate deployment:</p><ol><li><p>Click on <strong>Repositories</strong> (top left).</p></li><li><p>Click on the pipeline for the demochat repo (the gear icon).</p></li></ol><p><img src=https://lh5.googleusercontent.com/dD_Dn5SgpSSqTfIeHep4QKhx6rM8zcTWQVR-wHwBWLMzeZ9vsueS320yOeH_nuaKSYlSwrSB3UhML0wcLYZeoCPtga9mvvpyShutYoVKNtZ16e9ZDvglHDiOqugXunkDstUPF_aV alt></p><ol start=3><li><p>It’s a good idea to run some tests before deploying. Under <em>Build and Unit Test</em>, add npm test for the unit test script.</p></li><li><p>Click <strong>Deploy Script</strong> and select <strong>Kubernetes (Beta)</strong>. Enter the information for the service you’ve already deployed.</p></li></ol><p><img src=https://lh5.googleusercontent.com/an0aib6sTTqZqhfjMjOGFcWRRcaQSjezjk9XHVxEsLr_6hWi0kslsgQR6D0gP3EiA8D4pON-BhmakaRhpFVCkH16F80jt2-EWAJki4i2u4fYRQSdumiihC5fDjUyOyC9rwm1QilT alt></p><p>You can see the option to use a deployment file from your repo, or to use the deployment file that you just generated.</p><ol start=5><li>Click <strong>Save</strong>.</li></ol><p>You’re done with deployment automation! Now whenever a change is made, the image will build, test, and deploy. </p><h2 id=conclusions>Conclusions</h2><p>We want to make it easy for every team, not just big enterprise teams, to adopt Kubernetes while preserving all of Kubernetes’ power and flexibility. At any point on the Kubernetes service screen you can switch to YAML to view all of the YAMLfiles generated by the configuration you performed in this walkthrough. You can tweak the file content, copy and paste them into local files, etc.</p><p>This walkthrough gives everyone a solid base to start with. When you’re ready, you can tweak the entities directly to specify the exact configuration you’d like.</p><p>We’d love your feedback! Please share with us on <a href=https://twitter.com/codefresh>Twitter</a>, or <a href=https://codefresh.io/contact-us/>reach out directly</a>.</p><h2 id=addendums>Addendums</h2><p><strong>Do you have a video to walk me through this?</strong> <a href="https://www.youtube.com/watch?v=oFwFuUxxFdI&list=PL8mgsmlx4BWV_j_L5oq-q8JdPnlJc3bUv">You bet</a>.</p><p><strong>Does this work with Helm Charts?</strong> Yes! We’re currently piloting Helm Charts with a limited set of users. Ping us if you’d like to try it early.</p><p><strong>Does this work with any Kubernetes cluster?</strong> It should work with any Kubernetes cluster and is tested for Kubernetes 1.5 forward.</p><p><strong>Can I deploy Codefresh in my own data center?</strong> Sure, Codefresh is built on top of Kubernetes using Helm Charts. Codefresh cloud is free for open source, and 200 builds/mo. Codefresh on prem is currently for enterprise users only.</p><p><strong>Won’t the database be wiped every time we update?</strong> Yes, in this case we skipped creating a persistent volume. It’s a bit more work to get the persistent volume configured, if you’d like, <a href=https://codefresh.io/contact-us/>feel free to reach out</a> and we’re happy to help!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ef6eae1d4faf2d7f58a8043ea4f40f28>Enforcing Network Policies in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-10-30 class=text-muted>Monday, October 30, 2017</time></div><p><em><strong>Editor's note: this post is part of a <a href=https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18>series of in-depth articles</a> on what's new in Kubernetes 1.8. Today’s post comes from Ahmet Alp Balkan, Software Engineer, Google.</strong></em></p><p>Kubernetes now offers functionality to enforce rules about which pods can communicate with each other using <a href=/docs/concepts/services-networking/network-policies/>network policies</a>. This feature is has become stable Kubernetes 1.7 and is ready to use with supported networking plugins. The Kubernetes 1.8 release has added better capabilities to this feature.</p><h2 id=network-policy-what-does-it-mean>Network policy: What does it mean?</h2><p>In a Kubernetes cluster configured with default settings, all pods can discover and communicate with each other without any restrictions. The new Kubernetes object type NetworkPolicy lets you allow and block traffic to pods.</p><p>If you’re running multiple applications in a Kubernetes cluster or sharing a cluster among multiple teams, it’s a security best practice to create firewalls that permit pods to talk to each other while blocking other network traffic. Networking policy corresponds to the Security Groups concepts in the Virtual Machines world.</p><h2 id=how-do-i-add-network-policy-to-my-cluster>How do I add Network Policy to my cluster?</h2><p>Networking Policies are implemented by networking plugins. These plugins typically install an overlay network in your cluster to enforce the Network Policies configured. A number of networking plugins, including <a href=/docs/tasks/configure-pod-container/calico-network-policy/>Calico</a>, <a href=/docs/tasks/configure-pod-container/romana-network-policy/>Romana</a> and <a href=/docs/tasks/configure-pod-container/weave-network-policy/>Weave Net</a>, support using Network Policies.</p><p>Google Container Engine (GKE) also provides beta support for <a href=https://cloud.google.com/container-engine/docs/network-policy>Network Policies</a> using the Calico networking plugin when you create clusters with the following command:</p><p>gcloud beta container clusters create --enable-network-policy</p><h2 id=heading></h2><h2 id=how-do-i-configure-a-network-policy>How do I configure a Network Policy?</h2><p>Once you install a networking plugin that implements Network Policies, you need to create a Kubernetes resource of type NetworkPolicy. This object describes two set of label-based pod selector fields, matching:</p><ol><li>a set of pods the network policy applies to (required)</li><li>a set of pods allowed access to each other (optional). If you omit this field, it matches to no pods; therefore, no pods are allowed. If you specify an empty pod selector, it matches to all pods; therefore, all pods are allowed.</li></ol><h2 id=example-restricting-traffic-to-a-pod>Example: restricting traffic to a pod</h2><p>The following example of a network policy blocks all in-cluster traffic to a set of web server pods, except the pods allowed by the policy configuration.</p><p><img src=https://lh4.googleusercontent.com/e8JzhKYICOzh44sHcedjt4IRRpw2zpFNbJ2UY83fBdWYCIvFVSlHJNmIwLzIHVxrScc2eNCyv37mm903TVT9VkMuHPxe_5Hk8CvJTqGsSK7WtEDCbn1Q25S-o_kHcEiKUUl1NV9g alt></p><p>To achieve this setup, create a NetworkPolicy with the following manifest:</p><pre><code>kind: NetworkPolicy

apiVersion: networking.k8s.io/v1

metadata:

  name: access-nginx

spec:

  podSelector:

    matchLabels:

      app: nginx

  ingress:

  - from:

    - podSelector:

        matchLabels:

          app: foo
</code></pre><p>Once you apply this configuration, only pods with label <strong>app: foo</strong> can talk to the pods with the label <strong>app: nginx</strong>. For a more detailed tutorial, see the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Kubernetes documentation</a>.</p><h2 id=example-restricting-traffic-between-all-pods-by-default>Example: restricting traffic between all pods by default</h2><p>If you specify the spec.podSelector field as empty, the set of pods the network policy matches to all pods in the namespace, blocking all traffic between pods by default. In this case, you must explicitly create network policies whitelisting all communication between the pods.</p><p><img src=https://lh6.googleusercontent.com/FYmu74F7fW7DabtzBd6PULsgzKz0WmCli2Sw0SW8zVr0U7m-P6eGvov0mZGv9ngxncGXJmPxzapL3yQXXSBKTHsI8zw5kh-2hqzK6fW7YuqU6X5ofb5ilbis2KUJ2HvF3IHXsMcK alt></p><p>You can enable a policy like this by applying the following manifest in your Kubernetes cluster:</p><pre><code>apiVersion: networking.k8s.io/v1

kind: NetworkPolicy

metadata:

  name: default-deny

spec:

  podSelector:
</code></pre><h2 id=other-network-policy-features>Other Network Policy features</h2><p>In addition to the previous examples, you can make the Network Policy API enforce more complicated rules:</p><ul><li>Egress network policies: Introduced in Kubernetes 1.8, you can restrict your workloads from establishing connections to resources outside specified IP ranges.</li><li>IP blocks support: In addition to using podSelector/namespaceSelector, you can specify IP ranges with CIDR blocks to allow/deny traffic in ingress or egress rules.</li><li>Cross-namespace policies: Using the ingress.namespaceSelector field, you can enforce Network Policies for particular or for all namespaces in the cluster. For example, you can create privileged/system namespaces that can communicate with pods even though the default policy is to block traffic.</li><li>Restricting traffic to port numbers: Using the ingress.ports field, you can specify port numbers for the policy to enforce. If you omit this field, the policy matches all ports by default. For example, you can use this to allow a monitoring pod to query only the monitoring port number of an application.</li><li>Multiple ingress rules on a single policy: Because spec.ingress field is an array, you can use the same NetworkPolicy object to give access to different ports using different pod selectors. For example, a NetworkPolicy can have one ingress rule giving pods with the kind: monitoring label access to port 9000, and another ingress rule for the label app: foo giving access to port 80, without creating an additional NetworkPolicy resource.</li></ul><h2 id=learn-more>Learn more</h2><ul><li>Read more: <a href=/docs/concepts/services-networking/network-policies/>Networking Policy documentation</a></li><li>Read more: <a href=https://ahmet.im/blog/kubernetes-network-policy/>Unofficial Network Policy Guide</a></li><li>Hands-on: <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare a Network Policy</a></li><li>Try: <a href=https://github.com/ahmetb/kubernetes-networkpolicy-tutorial>Network Policy Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c701c89bced6c8429688cf438865f315>Using RBAC, Generally Available in Kubernetes v1.8</h1><div class="td-byline mb-4"><time datetime=2017-10-28 class=text-muted>Saturday, October 28, 2017</time></div><p><strong><em>Editor's note: this post is part of a <a href=https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18>series of in-depth articles</a> on what's new in Kubernetes 1.8. Today’s post comes from Eric Chiang, software engineer, CoreOS, and SIG-Auth co-lead.</em></strong></p><p>Kubernetes 1.8 represents a significant milestone for the <a href=/docs/reference/access-authn-authz/rbac/>role-based access control (RBAC) authorizer</a>, which was promoted to GA in this release. RBAC is a mechanism for controlling access to the Kubernetes API, and since its <a href=https://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes>beta in 1.6</a>, many Kubernetes clusters and provisioning strategies have enabled it by default.</p><p>Going forward, we expect to see RBAC become a fundamental building block for securing Kubernetes clusters. This post explores using RBAC to manage user and application access to the Kubernetes API.</p><h2 id=granting-access-to-users>Granting access to users</h2><p>RBAC is configured using standard Kubernetes resources. Users can be bound to a set of roles (ClusterRoles and Roles) through bindings (ClusterRoleBindings and RoleBindings). Users start with no permissions and must explicitly be granted access by an administrator.</p><p>All Kubernetes clusters install a default set of ClusterRoles, representing common buckets users can be placed in. The “edit” role lets users perform basic actions like deploying pods; “view” lets a user observe non-sensitive resources; “admin” allows a user to administer a namespace; and “cluster-admin” grants access to administer a cluster.</p><pre><code>
$ kubectl get clusterroles

NAME            AGE

admin           40m

cluster-admin   40m

edit            40m

# ...


view            40m

</code></pre><p>ClusterRoleBindings grant a user, group, or service account a ClusterRole’s power across the entire cluster. Using kubectl, we can let a sample user “jane” perform basic actions in all namespaces by binding her to the “edit” ClusterRole:</p><pre><code>
$ kubectl create clusterrolebinding jane --clusterrole=edit --user=jane

$ kubectl get namespaces --as=jane

NAME          STATUS    AGE

default       Active    43m

kube-public   Active    43m

kube-system   Active    43m

$ kubectl auth can-i create deployments --namespace=dev --as=jane

yes

</code></pre><p>RoleBindings grant a ClusterRole’s power within a namespace, allowing administrators to manage a central list of ClusterRoles that are reused throughout the cluster. For example, as new resources are added to Kubernetes, the default ClusterRoles are updated to automatically grant the correct permissions to RoleBinding subjects within their namespace.</p><p>Next we’ll let the group “infra” modify resources in the “dev” namespace:</p><pre><code>
$ kubectl create rolebinding infra --clusterrole=edit --group=infra --namespace=dev

rolebinding &quot;infra&quot; created

</code></pre><p>Because we used a RoleBinding, these powers only apply within the RoleBinding’s namespace. In our case, a user in the “infra” group can view resources in the “dev” namespace but not in “prod”:</p><pre><code>
$ kubectl get deployments --as=dave --as-group=infra --namespace dev

No resources found.

$ kubectl get deployments --as=dave --as-group=infra --namespace prod

Error from server (Forbidden): deployments.extensions is forbidden: User &quot;dave&quot; cannot list deployments.extensions in the namespace &quot;prod&quot;.

</code></pre><h2 id=creating-custom-roles>Creating custom roles</h2><p>When the default ClusterRoles aren’t enough, it’s possible to create new roles that define a custom set of permissions. Since ClusterRoles are just regular API resources, they can be expressed as YAML or JSON manifests and applied using kubectl.</p><p>Each ClusterRole holds a list of permissions specifying “rules.” Rules are purely additive and allow specific HTTP verb to be performed on a set of resource. For example, the following ClusterRole holds the permissions to perform any action on "deployments”, “configmaps,” or “secrets”, and to view any “pod”:</p><pre><code>
kind: ClusterRole

apiVersion: rbac.authorization.k8s.io/v1

metadata:

 name: deployer

rules:

- apiGroups: [&quot;apps&quot;]

 resources: [&quot;deployments&quot;]

 verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;, &quot;update&quot;, &quot;patch&quot;]



- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group

 resources: [&quot;configmaps&quot;, &quot;secrets&quot;]

 verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;, &quot;update&quot;, &quot;patch&quot;]



- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group

 resources: [&quot;pods&quot;]

 verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]

</code></pre><p>Verbs correspond to the HTTP verb of the request, while the resource and API groups refer to the resource being referenced. Consider the following Ingress resource:</p><pre><code>
apiVersion: extensions/v1beta1

kind: Ingress

metadata:

 name: test-ingress

spec:

 backend:

   serviceName: testsvc

   servicePort: 80

</code></pre><p>To POST the resource, the user would need the following permissions:</p><pre><code>
rules:

- apiGroups: [&quot;extensions&quot;] # &quot;apiVersion&quot; without version

 resources: [&quot;ingresses&quot;]  # Plural of &quot;kind&quot;

 verbs: [&quot;create&quot;]         # &quot;POST&quot; maps to &quot;create&quot;

</code></pre><h2 id=roles-for-applications>Roles for applications</h2><p>When deploying containers that require access to the Kubernetes API, it’s good practice to ship an RBAC Role with your application manifests. Besides ensuring your app works on RBAC enabled clusters, this helps users audit what actions your app will perform on the cluster and consider their security implications.</p><p>A namespaced Role is usually more appropriate for an application, since apps are traditionally run inside a single namespace and the namespace's resources should be tied to the lifecycle of the app. However, Roles cannot grant access to non-namespaced resources (such as nodes) or across namespaces, so some apps may still require ClusterRoles.</p><p>The following Role allows a Prometheus instance to monitor and discover services, endpoints, and pods in the “dev” namespace:</p><pre><code>
kind: Role

metadata:

 name: prometheus-role

 namespace: dev

rules:

- apiGroups: [&quot;&quot;] # &quot;&quot; refers to the core API group

 Resources: [&quot;services&quot;, &quot;endpoints&quot;, &quot;pods&quot;]

 verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]

</code></pre><p>Containers running in a Kubernetes cluster receive service account credentials to talk to the Kubernetes API, and service accounts can be targeted by a RoleBinding. Pods normally run with the “default” service account, but it’s good practice to run each app with a unique service account so RoleBindings don’t unintentionally grant permissions to other apps.</p><p>To run a pod with a custom service account, create a ServiceAccount resource in the same namespace and specify the <code>serviceAccountName</code> field of the manifest.</p><pre><code>
apiVersion: apps/v1beta2 # Abbreviated, not a full manifest

kind: Deployment

metadata:

 name: prometheus-deployment

 namespace: dev

spec:

 replicas: 1

 template:

   spec:

     containers:

     - name: prometheus

       image: prom/prometheus:v1.8.0

       command: [&quot;prometheus&quot;, &quot;-config.file=/etc/prom/config.yml&quot;]

   # Run this pod using the &quot;prometheus-sa&quot; service account.

   serviceAccountName: prometheus-sa

---

apiVersion: v1

kind: ServiceAccount

metadata:

 name: prometheus-sa

 namespace: dev

</code></pre><h2 id=get-involved>Get involved</h2><p>Development of RBAC is a community effort organized through the <a href=https://github.com/kubernetes/community/blob/master/sig-auth/README.md>Auth Special Interest Group</a>, one of the <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>many SIGs</a> responsible for maintaining Kubernetes. A great way to get involved in the Kubernetes community is to join a SIG that aligns with your interests, provide feedback, and help with the roadmap.</p><h2 id=about-the-author>About the author</h2><p>Eric Chiang is a software engineer and technical lead of Kubernetes development at <a href="https://coreos.com/?utm_source=k8sblog&utm_medium=social&utm_campaign=organic">CoreOS</a>, the creator of Tectonic, the enterprise-ready Kubernetes platform. Eric co-leads Kubernetes SIG Auth and maintains several open source projects and libraries on behalf of CoreOS.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bec925973774692910e48225303890bc>It Takes a Village to Raise a Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-10-26 class=text-muted>Thursday, October 26, 2017</time></div><p><strong><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18>series of in-depth articles</a> on what's new in Kubernetes 1.8, written by Jaice Singer DuMars from Microsoft.</em></strong></p><p>Each time we release a new version of Kubernetes, it’s enthralling to see how the community responds to all of the hard work that went into it. Blogs on new or enhanced capabilities crop up all over the web like wildflowers in the spring. Talks, videos, webinars, and demos are not far behind. As soon as the community seems to take this all in, we turn around and add more to the mix. It’s a thrilling time to be a part of this project, and even more so, the movement. It’s not just software anymore.</p><p>When circumstances opened the door for me to lead the 1.8 release, I signed on despite a minor case of the butterflies. In a private conversation with another community member, they assured me that “being organized, following up, and knowing when to ask for help” were the keys to being a successful lead. That’s when I knew I could do it — and so I did.</p><p>From that point forward, I was wrapped in a patchwork quilt of community that magically appeared at just the right moments. The community’s commitment and earnest passion for quality, consistency, and accountability formed a bedrock from which the release itself was chiseled.</p><p>The 1.8 release team proved incredibly cohesive despite a late start. We approached even the most difficult situations with humor, diligence, and sincere curiosity. My experience leading large teams served me well, and underscored another difference about this release: it was more valuable for me to focus on leadership than diving into the technical weeds to solve every problem.</p><p>Also, the uplifting power of <a href=https://kubernetes.slack.com/archives/C2C40FMNF/p1506659664000090>emoji in Slack</a> cannot be overestimated.</p><p>An important inflection point is underway in the Kubernetes project. If you’ve taken a ride on a “startup rollercoaster,” this is a familiar story. You come up with an idea so crazy that it might work. You build it, get traction, and slowly clickity-clack up that first big hill. The view from the top is dizzying, as you’ve poured countless hours of life into something completely unknown. Once you go over the top of that hill, everything changes. Breakneck acceleration defines or destroys what has been built.</p><p>In my experience, that zero gravity point is where everyone in the company (or in this case, project) has to get serious about not only building something, but also maintaining it. Without a commitment to maintenance, things go awry really quickly. From codebases that resemble the Winchester Mystery House to epidemics of crashing production implementations, a fiery descent into chaos can happen quickly despite the outward appearance of success. Thankfully, the Kubernetes community seems to be riding our growth rollercoaster with increasing success at each release.</p><p>As software startups mature, there is a natural evolution reflected in the increasing distribution of labor. Explosive adoption means that full-time security, operations, quality, documentation, and project management staff become necessary to deliver stability, reliability, and extensibility. Also, you know things are getting serious when intentional architecture becomes necessary to ensure consistency over time.</p><p>Kubernetes has followed a similar path. In the absence of company departments or skill-specific teams, Special Interest Groups (SIGs) have organically formed around core project needs like storage, networking, API machinery, applications, and the operational lifecycle. As SIGs have proliferated, the Kubernetes governance model has crystallized around them, providing a framework for code ownership and shared responsibility. SIGs also help ensure the community is sustainable because success is often more about people than code.</p><p>At the Kubernetes <a href=https://github.com/kubernetes/community/tree/master/community/2017-events/05-leadership-summit>leadership summit</a> in June, a proposed SIG architecture was ratified with a unanimous vote, underscoring a stability theme that seemed to permeate every conversation in one way or another. The days of filling in major functionality gaps appear to be over, and a new era of feature depth has emerged in its place.</p><p>Another change is the move away from project-level release “feature themes” to SIG-level initiatives delivered in increments over the course of several releases. That’s an important shift: SIGs have a mission, and everything they deliver should ultimately serve that. As a community, we need to provide facilitation and support so SIGs are empowered to do their best work with minimal overhead and maximum transparency.</p><p>Wisely, the community also spotted the opportunity to provide safe mechanisms for innovation that are increasingly less dependent on the code in kubernetes/kubernetes. This in turn creates a flourishing habitat for experimentation without hampering overall velocity. The project can also address technical debt created during the initial ride up the rollercoaster. However, new mechanisms for innovation present an architectural challenge in defining what is and is not Kubernetes. SIG Architecture addresses the challenge of defining Kubernetes’ boundaries. It’s a work in progress that trends toward continuous improvement.</p><p>This can be a little overwhelming at the individual level. In reality, it’s not that much different from any other successful startup, save for the fact that authority does not come from a traditional org chart. It comes from SIGs, community technical leaders, the newly-formed steering committee, and ultimately you.</p><p>The Kubernetes release process provides a special opportunity to see everything that makes this project tick. I’ll tell you what I saw: people, working together, to do the best they can, in service to everyone who sets out on the cloud native journey.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8526f677b2724ff689dd611cabc2ca93>kubeadm v1.8 Released: Introducing Easy Upgrades for Kubernetes Clusters</h1><div class="td-byline mb-4"><time datetime=2017-10-25 class=text-muted>Wednesday, October 25, 2017</time></div><p><strong><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18>series of in-depth articles</a> on what's new in Kubernetes 1.8</em></strong></p><p>Since its debut in <a href=https://kubernetes.io/blog/2016/09/how-we-made-kubernetes-easy-to-install>September 2016</a>, the Cluster Lifecycle Special Interest Group (SIG) has established kubeadm as the easiest Kubernetes bootstrap method. Now, we’re releasing kubeadm v1.8.0 in tandem with the release of <a href=https://kubernetes.io/blog/2017/09/kubernetes-18-security-workloads-and>Kubernetes v1.8.0</a>. In this blog post, I’ll walk you through the changes we’ve made to kubeadm since the last update, the scope of kubeadm, and how you can contribute to this effort.</p><h2 id=security-first-kubeadm-v1-6-v1-7>Security first: kubeadm v1.6 & v1.7</h2><p>Previously, we discussed <a href=https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters>planned updates for kubeadm v1.6</a>. Our primary focus for v1.6 was security. We started enforcing role based access control (RBAC) as it graduated to beta, gave unique identities and locked-down privileges for different system components in the cluster, disabled the insecure <code>localhost:8080</code> API server port, started authorizing all API calls to the kubelets, and <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery.md>improved the token discovery</a> method used formerly in v1.5. Token discovery (aka Bootstrap Tokens) graduated to beta in v1.8.</p><p>In number of features, kubeadm v1.7.0 was a much smaller release compared to v1.6.0 and v1.8.0. The main additions were enforcing <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/kubelet-authorizer.md>the Node Authorizer</a>, which significantly reduces the attack surface for a Kubernetes cluster, and initial, limited upgrading support from v1.6 clusters.</p><h2 id=easier-upgrades-extensibility-and-stabilization-in-v1-8>Easier upgrades, extensibility, and stabilization in v1.8</h2><p>We had eight weeks between Kubernetes v1.7.0 and our stabilization period (code freeze) to implement new features and to stabilize the upcoming v1.8.0 release. Our goal for kubeadm v1.8.0 was to make it more extensible. We wanted to add a lot of new features and improvements in this cycle, and we succeeded.Upgrades along with better introspectability. The most important update in kubeadm v1.8.0 (and my favorite new feature) is <strong>one-command upgrades</strong> of the control plane. While v1.7.0 had the ability to upgrade clusters, the user experience was far from optimal, and the process was risky.</p><p>Now, you can easily check to see if your system can handle an upgrade by entering:</p><pre><code>$ kubeadm upgrade plan
</code></pre><p>This gives you information about which versions you can upgrade to, as well as the health of your cluster.</p><p>You can examine the effects an upgrade will have on your system by specifying the --dry-run flag. In previous versions of kubeadm, upgrades were essentially blind in that you could only make assumptions about how an upgrade would impact your cluster. With the new dry run feature, there is no more mystery. You can see exactly what applying an upgrade would do before applying it.</p><p>After checking to see how an upgrade will affect your cluster, you can apply the upgrade by typing:</p><pre><code>$ kubeadm upgrade apply v1.8.0
</code></pre><p>This is a much cleaner and safer way of performing an upgrade than the previous version. As with any type of upgrade or downgrade, it’s a good idea to backup your cluster first using your preferred solution.</p><h2 id=self-hosting>Self-hosting</h2><p>Self-hosting in this context refers to a specific way of setting up the control plane. The self-hosting concept was initially developed by CoreOS in their <a href=https://github.com/kubernetes-incubator/bootkube>bootkube</a> project. The long-term goal is to move this functionality (currently in an alpha stage) to the generic kubeadm toolbox. Self-hosting means that the control plane components, the API Server, Controller Manager and Scheduler are workloads themselves in the cluster they run. This means the control plane components can be managed using Kubernetes primitives, which has numerous advantages. For instance, leader-elected components like the scheduler and controller-manager will automatically be run on all masters when HA is implemented if they are run in a DaemonSet. Rolling upgrades in Kubernetes can be used for upgrades of the control plane components, and next to no extra code has to be written for that to work; it’s one of Kubernetes’ built-in primitives!</p><p>Self-hosting won’t be the default until v1.9.0, but users can easily test the feature in experimental clusters. If you test this feature, we’d love your feedback!</p><p>You can test out self-hosting by enabling its feature gate:</p><pre><code>$ kubeadm init --feature-gates=SelfHosting=true
</code></pre><h2 id=extensibility>Extensibility</h2><p>We’ve added some new extensibility features. You can delegate some tasks, like generating certificates or writing control plane arguments to kubeadm, but still drive the control plane bootstrap process yourself. Basically, you can let kubeadm do some parts and fill in yourself where you need customizations. Previously, you could only use kubeadm init to perform “the full meal deal.” The inclusion of the kubeadm alpha phase command supports our aim to make kubeadm more modular, letting you invoke atomic sub-steps of the bootstrap process.</p><p>In v1.8.0, kubeadm alpha phase is just that: an alpha preview. We hope that we can graduate the command to beta as kubeadm phase in v1.9.0. We can’t wait for feedback from the community on how to better improve this feature!</p><h2 id=improvements>Improvements</h2><p>Along with our new kubeadm features, we’ve also made improvements to existing ones. The Bootstrap Token feature that makes <code>kubeadm join</code> so short and sweet has graduated from alpha to beta and gained even more security features.</p><p>If you made customizations to your system in v1.6 or v1.7, you had to remember what those customizations were when you upgraded your cluster. No longer: beginning with v1.8.0, kubeadm uploads your configuration to a ConfigMap inside of the cluster, and later reads that configuration when upgrading for a seamless user experience.</p><p>The first certificate rotation feature has graduated to beta in v1.8, which is great to see. Thanks to the <a href=https://github.com/kubernetes/community/tree/master/sig-auth>Auth Special Interest Group</a>, the Kubernetes node component kubelet can now <a href=https://github.com/kubernetes/features/issues/266>rotate its client certificate</a> automatically. We expect this area to improve continuously, and will continue to be a part of this cross-SIG effort to easily rotate all certificates in any cluster.</p><p>Last but not least, kubeadm is more resilient now. kubeadm init will detect even more faulty environments earlier, and time out instead of waiting forever for the expected condition.</p><h2 id=the-scope-of-kubeadm>The scope of kubeadm</h2><p>As there are so many different end-to-end installers for Kubernetes, there is some fragmentation in the ecosystem. With each new release of Kubernetes, these installers naturally become more divergent. This can create problems down the line if users rely on installer-specific variations and hooks that aren’t standardized in any way. Our goal from the beginning has been to make kubeadm a building block for deploying Kubernetes clusters and to provide kubeadm init and kubeadm join as best-practice “fast paths” for new Kubernetes users. Ideally, using kubeadm as the basis of all deployments will make it easier to create conformant clusters.</p><p>kubeadm performs the actions necessary to get a minimum viable cluster up and running. It only cares about bootstrapping, not about provisioning machines, by design. Likewise, installing various nice-to-have addons by default like the <a href=https://github.com/kubernetes/dashboard>Kubernetes Dashboard</a>, some monitoring solution, cloud provider-specific addons, etc. is not in scope. Instead, we expect higher-level and more tailored tooling to be built on top of kubeadm, that installs the software the end user needs.</p><h2 id=v1-9-0-and-beyond>v1.9.0 and beyond</h2><p>What’s in store for the future of kubeadm?</p><h4 id=planned-features>Planned features</h4><p>We plan to address high availability (replicated etcd and multiple, redundant API servers and other control plane components) as an alpha feature in v1.9.0. This has been a regular request from our user base.</p><p>Also, we want to make self-hosting the default way to deploy your control plane: Kubernetes becomes much easier to manage if we can rely on Kubernetes' own tools to manage the cluster components.</p><h4 id=promoting-kubeadm-adoption-and-getting-involved>Promoting kubeadm adoption and getting involved</h4><p>The <a href=https://github.com/kubernetes/community/tree/master/wg-kubeadm-adoption>kubeadm adoption working group</a> is an ongoing effort between SIG Cluster Lifecycle and other parties in the Kubernetes ecosystem. This working group focuses on making kubeadm more extensible in order to promote adoption of it for other end-to-end installers in the community. Everyone is welcome to join. So far, we’re glad to announce that <a href=https://github.com/kubernetes-incubator/kubespray>kubespray</a> started using kubeadm under the hood, and gained new features at the same time! We’re excited to see others follow and make the ecosystem stronger.</p><p>kubeadm is a great way to learn about Kubernetes: it binds all of Kubernetes’ components together in a single package. To learn more about what kubeadm really does under the hood, <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.8.md>this document</a> describes kubeadm functions in v1.8.0.</p><p>If you want to get involved in these efforts, join SIG Cluster Lifecycle. We <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>meet on Zoom</a> once a week on Tuesdays at 16:00 UTC. For more information about what we talk about in our weekly meetings, <a href=https://docs.google.com/document/d/1deJYPIF4LmhGjDVaqrswErIrV7mtwJgovtLnPCDxP7U/edit#>check out our meeting notes</a>. Meetings are a great educational opportunity, even if you don’t want to jump in and present your own ideas right away. You can also sign up for our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>mailing list</a>, join our <a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>Slack channel,</a> <a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>o</a>r check out the <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP29D0nYgAGWt1ZFqS9Z7lw4&disable_polymer=true">video archive</a> of our past mee<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>t</a>i<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>n</a>g<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle>s</a>. Even if you’re only interested in watching the video calls initially, we’re excited to welcome you as a new member to SIG Cluster Lifecycle!</p><p>If you want to know what a kubeadm developer does at a given time in the Kubernetes release cycle, check out <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/release-cycle.md>this doc</a>. Finally, don’t hesitate to join if any of our upcoming projects are of interest to you!</p><p>Thank you,<br>Lucas Käldström<br>Kubernetes maintainer & SIG Cluster Lifecycle co-lead<br><a href="https://www.weave.works/?utm_source=k8&utm_medium=ww&utm_campaign=blog">Weaveworks</a> contractor</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bccec10413bd4f89b97d497368323e1b>Five Days of Kubernetes 1.8</h1><div class="td-byline mb-4"><time datetime=2017-10-24 class=text-muted>Tuesday, October 24, 2017</time></div><p>Kubernetes 1.8 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.</p><p>The community has tallied more than 66,000 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 120,000 commits across all repos and 17,839 commits across all repos for v1.7.0 to v1.8.0 alone.</p><p>With the help of our growing community of 1,400 plus contributors, we issued more than 3,000 PRs and pushed more than 5,000 commits to deliver Kubernetes 1.8 with significant security and workload support updates. This all points to increased stability, a result of our project-wide focus on maturing <a href=https://github.com/kubernetes/sig-release>process</a>, formalizing <a href=https://github.com/kubernetes/community/tree/master/sig-architecture>architecture</a>, and strengthening Kubernetes’ <a href=https://github.com/kubernetes/community/tree/master/community/elections/2017>governance model</a>.</p><p>While many improvements have been contributed, we highlight key features in this series of in-depth posts listed below. <a href=https://twitter.com/kubernetesio>Follow along</a> and see what’s new and improved with storage, security and more.</p><p><strong>Day 1:</strong> <a href=https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18>5 Days of Kubernetes 1.8</a><br><strong>Day 2:</strong> <a href=https://kubernetes.io/blog/2017/10/kubeadm-v18-released>kubeadm v1.8 Introduces Easy Upgrades for Kubernetes Clusters</a><br><strong>Day 3:</strong> <a href=https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes>Kubernetes v.1.8 Retrospective: It Takes a Village to Raise a Kubernetes</a>
<strong>Day 4:</strong> <a href=https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18>Using RBAC, Generally Available in Kubernetes v1.8</a><br><strong>Day 5:</strong> <a href=https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes>Enforcing Network Policies in Kubernetes</a></p><p><strong>Connect</strong></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-98f2cfc12ebe8b53a6367d81dee740f0>Introducing Software Certification for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-10-19 class=text-muted>Thursday, October 19, 2017</time></div><p><em><strong>Editor's Note: Today's post is by William Denniss, Product Manager, Google Cloud on the new Certified Kubernetes Conformance Program.</strong></em></p><p>Over the last three years, Kubernetes® has seen wide-scale adoption by a vibrant and diverse community of providers. In fact, there are now more than <a href="https://docs.google.com/spreadsheets/d/1LxSqBzjOxfGx3cmtZ4EbB_BGCxT_wlxW_xgHVVa23es/edit#gid=0">60</a> known Kubernetes platforms and distributions. From the start, one goal of Kubernetes has been consistency and portability.</p><p>In order to better serve this goal, today the Kubernetes community and the Cloud Native Computing Foundation® (CNCF®) announce the availability of the beta Certified Kubernetes Conformance Program. The Kubernetes conformance certification program gives users the confidence that when they use a Certified Kubernetes™ product, they can rely on a high level of common functionality. Certification provides Independent Software Vendors (ISVs) confidence that if their customer is using a Certified Kubernetes product, their software will behave as expected.</p><p>CNCF and the Kubernetes Community invites all vendors to <a href=https://github.com/cncf/k8s-conformance/blob/master/instructions.md>run the conformance test suite</a>, and submit conformance testing results for review and certification by the CNCF. When the program graduates to GA (generally available) later this year, all vendors receiving certification during the beta period will be listed in the launch announcement.</p><p>Just like Kubernetes itself, conformance certification is an evolving program managed by contributors in our community. Certification is versioned alongside Kubernetes, and certification requirements receive updates with each version of Kubernetes as features are added and the architecture changes. The Kubernetes community, through <a href=https://github.com/kubernetes/community/tree/master/sig-architecture>SIG Architecture</a>, controls changes and overseers what it means to be Certified Kubernetes. The <a href=https://github.com/kubernetes/community/tree/master/sig-testing>Testing SIG</a> works on the mechanics of conformance tests, while the <a href=https://github.com/cncf/k8s-conformance>Conformance Working Group</a> develops process and policy for the certification program.</p><p><img src=https://lh3.googleusercontent.com/-seEomiDY4syaWVbl0KT7k9fcJmylYK1n9_VANKyo5oIP5gH9MuIq_dcB_q3qvjE5YzOdM2HthMyc_wduC4xLmPStsb6Q6ASPBfOWi7ssGylfy1I7Pbd64THobytWa_1JX-pscH4 alt></p><p>Once the program moves to GA, certified products can proudly display the new Certified Kubernetes logo mark with stylized version information on their marketing materials. Certified products can also take advantage of a new combination trademark rule the CNCF adopted for Certified Kubernetes providers that keep their certification up to date.</p><p>Products must complete a recertification each year for the current or previous version of Kubernetes to remain certified. This ensures that when you see the Certified Kubernetes™ mark on a product, you’re not only getting something that’s proven conformant, but also contains the latest features and improvements from the community.</p><p>Visit <a href=https://github.com/cncf/k8s-conformance>https://github.com/cncf/k8s-conformance</a> for more information about the Certified Kubernetes Conformance Program, and learn how you can include your product in a growing list of Certified Kubernetes providers.</p><p><em>“Cloud Native Computing Foundation”, “CNCF” and “Kubernetes” are registered trademarks of The Linux Foundation in the United States and other countries. “Certified Kubernetes” and the Certified Kubernetes design are trademarks of The Linux Foundation in the United States and other countries.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c76b2587819b72e51c461cf5726f85e>Request Routing and Policy Management with the Istio Service Mesh</h1><div class="td-byline mb-4"><time datetime=2017-10-10 class=text-muted>Tuesday, October 10, 2017</time></div><p><strong><em>Editor's note: Today’s post by Frank Budinsky, Software Engineer, IBM, Andra Cismaru, Software Engineer, Google, and Israel Shalom, Product Manager, Google, is the second post in a three-part series on Istio. It offers a closer look at request routing and policy management.</em></strong></p><p>In a <a href=https://kubernetes.io/blog/2017/05/managing-microservices-with-istio-service-mesh>previous article</a>, we looked at a <a href=https://istio.io/docs/guides/bookinfo.html>simple application (Bookinfo)</a> that is composed of four separate microservices. The article showed how to deploy an application with Kubernetes and an Istio-enabled cluster without changing any application code. The article also outlined how to view Istio provided L7 metrics on the running services.</p><p>This article follows up by taking a deeper look at Istio using Bookinfo. Specifically, we’ll look at two more features of Istio: request routing and policy management.</p><h2 id=running-the-bookinfo-application>Running the Bookinfo Application</h2><p>As before, we run the v1 version of the Bookinfo application. After <a href=https://istio.io/docs/setup/kubernetes/quick-start.html>installing Istio</a> in our cluster, we start the app defined in <a href=https://raw.githubusercontent.com/istio/istio/master/samples/kubernetes-blog/bookinfo-v1.yaml>bookinfo-v1.yaml</a> using the following command:</p><pre><code>kubectl apply -f \&lt;(istioctl kube-inject -f bookinfo-v1.yaml)
</code></pre><p>We created an Ingress resource for the app:</p><pre><code>cat \&lt;\&lt;EOF | kubectl create -f -

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

name: bookinfo

annotations:

    kubernetes.io/ingress.class: &quot;istio&quot;

spec:

rules:

- http:

        paths:

        - path: /productpage

            backend:

                serviceName: productpage

                servicePort: 9080

        - path: /login

            backend:

                serviceName: productpage

                servicePort: 9080

        - path: /logout

            backend:

                serviceName: productpage

                servicePort: 9080

EOF
</code></pre><p>Then we retrieved the NodePort address of the Istio Ingress controller:</p><pre><code>export BOOKINFO\_URL=$(kubectl get po -n istio-system -l istio=ingress -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc -n istio-system istio-ingress -o jsonpath={.spec.ports[0].nodePort})
</code></pre><p>Finally, we pointed our browser to <a href=about:blank>http://$BOOKINFO_URL/productpage</a>, to see the running v1 application:</p><p><img src=https://lh3.googleusercontent.com/kGRJnhkf30FBOY2pyZzID90f_zxlyMUv43hEvfq70bcmYhKrGv2em2qph21k-ahlwfBthV3XQSf6CuUQXMlvgSlOUJr4W1ksDVXIvChEd6a5Y51lwepHmyQx2ksJgUpyTiEbZN11 alt></p><h2 id=http-request-routing>HTTP request routing</h2><p>Existing container orchestration platforms like Kubernetes, Mesos, and other microservice frameworks allow operators to control when a particular set of pods/VMs should receive traffic (e.g., by adding/removing specific labels). Unlike existing techniques, Istio decouples traffic flow and infrastructure scaling. This allows Istio to provide a variety of traffic management features that reside outside the application code, including dynamic HTTP <a href=https://istio.io/docs/concepts/traffic-management/#routing-rules>request routing</a> for A/B testing, canary releases, gradual rollouts, <a href=https://istio.io/docs/concepts/traffic-management/#network-resilience-and-testing>failure recovery</a> using timeouts, retries, circuit breakers, and <a href=https://istio.io/docs/concepts/traffic-management/fault-injection.html>fault injection</a> to test compatibility of failure recovery policies across services.</p><p>To demonstrate, we’ll deploy v2 of the <strong>reviews</strong> service and use Istio to make it visible only for a specific test user. We can create a Kubernetes deployment, reviews-v2, with <a href=https://raw.githubusercontent.com/istio/istio/master/samples/kubernetes-blog/bookinfo-reviews-v2.yaml>this YAML file</a>:</p><pre><code>apiVersion: extensions/v1beta1

kind: Deployment

metadata:

name: reviews-v2

spec:

replicas: 1

template:

    metadata:

        labels:

            app: reviews

            version: v2

    spec:

        containers:

        - name: reviews

            image: istio/examples-bookinfo-reviews-v2:0.2.3

            imagePullPolicy: IfNotPresent

            ports:

            - containerPort: 9080
</code></pre><p>From a Kubernetes perspective, the v2 deployment adds additional pods that the reviews service selector includes in the round-robin load balancing algorithm. This is also the default behavior for Istio.</p><p>Before we start reviews:v2, we’ll start the last of the four Bookinfo services, ratings, which is used by the v2 version to provide ratings stars corresponding to each review:</p><pre><code>kubectl apply -f \&lt;(istioctl kube-inject -f bookinfo-ratings.yaml)
</code></pre><p>If we were to start <strong>reviews:v2</strong> now, we would see browser responses alternating between v1 (reviews with no corresponding ratings) and v2 (review with black rating stars). This will not happen, however, because we’ll use Istio’s traffic management feature to control traffic.</p><p>With Istio, new versions don’t need to become visible based on the number of running pods. Version visibility is controlled instead by rules that specify the exact criteria. To demonstrate, we start by using Istio to specify that we want to send 100% of reviews traffic to v1 pods only.</p><p>Immediately setting a default rule <a href=https://github.com/istio/istio/blob/master/samples/bookinfo/kube/route-rule-all-v1.yaml>for every service</a> in the mesh is an Istio best practice. Doing so avoids accidental visibility of newer, potentially unstable versions. For the purpose of this demonstration, however, we’ll only do it for the reviews service:</p><pre><code>cat \&lt;\&lt;EOF | istioctl create -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

  name: reviews-default

spec:

  destination:

      name: reviews

  route:

  - labels:

          version: v1

      weight: 100

EOF
</code></pre><p>This command directs the service mesh to send 100% of traffic for the reviews service to pods with the label “version: v1”. With this rule in place, we can safely deploy the v2 version without exposing it.</p><pre><code>kubectl apply -f \&lt;(istioctl kube-inject -f bookinfo-reviews-v2.yaml)
</code></pre><p>Refreshing the Bookinfo web page confirms that nothing has changed.</p><p>At this point we have all kinds of options for how we might want to expose <strong>reviews:v2</strong>. If for example we wanted to do a simple canary test, we could send 10% of the traffic to v2 using a rule like this:</p><pre><code>apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

  name: reviews-default

spec:

  destination:

      name: reviews

  route:

  - labels:

          version: v2

      weight: 10

  - labels:

          version: v1

      weight: 90
</code></pre><p>A better approach for early testing of a service version is to instead restrict access to it much more specifically. To demonstrate, we’ll set a rule to only make reviews:v2 visible to a specific test user. We do this by setting a second, higher priority rule that will only be applied if the request matches a specific condition:</p><pre><code>cat \&lt;\&lt;EOF | istioctl create -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

name: reviews-test-v2

spec:

destination:

    name: reviews

precedence: 2

match:

    request:

        headers:

            cookie:

                regex: &quot;^(.\*?;)?(user=jason)(;.\*)?$&quot;

route:

- labels:

        version: v2

    weight: 100

EOF
</code></pre><p>Here we’re specifying that the request headers need to include a user cookie with value “tester” as the condition. If this rule is not matched, we fall back to the default routing rule for v1.</p><p>If we login to the Bookinfo UI with the user name “tester” (no password needed), we will now see version v2 of the application (each review includes 1-5 black rating stars). Every other user is unaffected by this change.</p><p><img src=https://lh5.googleusercontent.com/WLvX01Oja8R_cMb_AD91jIiF0bHW0nSJTRJ6Vt3Xz75MLzivZ5-ghHEZkdTJryhNXyTCUemF4OwxYn_96ntimOwyjABuZjaH3O2RsJyYQbqWoQgvSQktQd98t3T3Qe3KZSd20Cam alt></p><p>Once the v2 version has been thoroughly tested, we can use Istio to proceed with a canary test using the rule shown previously, or we can simply migrate all of the traffic from v1 to v2, optionally in a gradual fashion by using a sequence of rules with weights less than 100 (for example: 10, 20, 30, ... 100). This traffic control is independent of the number of pods implementing each version. If, for example, we had auto scaling in place, and high traffic volumes, we would likely see a corresponding scale up of v2 and scale down of v1 pods happening independently at the same time. For more about version routing with autoscaling, check out <a href=https://istio.io/blog/canary-deployments-using-istio.html>"Canary Deployments using Istio"</a>.</p><p>In our case, we’ll send all of the traffic to v2 with one command:</p><pre><code>cat \&lt;\&lt;EOF | istioctl replace -f -

apiVersion: config.istio.io/v1alpha2

kind: RouteRule

metadata:

  name: reviews-default

spec:

  destination:

      name: reviews

  route:

  - labels:

          version: v2

      weight: 100

EOF
</code></pre><p>We should also remove the special rule we created for the tester so that it doesn’t override any future rollouts we decide to do:</p><pre><code>istioctl delete routerule reviews-test-v2
</code></pre><p>In the Bookinfo UI, we’ll see that we are now exposing the v2 version of reviews to all users.</p><h2 id=policy-enforcement>Policy enforcement</h2><p>Istio provides policy enforcement functions, such as quotas, precondition checking, and access control. We can demonstrate Istio’s open and extensible framework for policies with an example: rate limiting.</p><p>Let’s pretend that the Bookinfo ratings service is an external paid service--for example, <a href=https://www.rottentomatoes.com/>Rotten Tomatoes®</a>--with a free quota of 1 request per second (req/sec). To make sure the application doesn’t exceed this limit, we’ll specify an Istio policy to cut off requests once the limit is reached. We’ll use one of Istio’s built-in policies for this purpose.</p><p>To set a 1 req/sec quota, we first configure a <strong>memquota</strong> handler with rate limits:</p><pre><code>cat \&lt;\&lt;EOF | istioctl create -f -

apiVersion: &quot;config.istio.io/v1alpha2&quot;

kind: memquota

metadata:

name: handler

namespace: default

spec:

quotas:

- name: requestcount.quota.default

    maxAmount: 5000

    validDuration: 1s

    overrides:

    - dimensions:

            destination: ratings

        maxAmount: 1

        validDuration: 1s

EOF
</code></pre><p>Then we create a <strong>quota</strong> instance that maps incoming attributes to quota dimensions, and create a <strong>rule</strong> that uses it with the <strong>memquota</strong> handler:</p><pre><code>cat \&lt;\&lt;EOF | istioctl create -f -

apiVersion: &quot;config.istio.io/v1alpha2&quot;

kind: quota

metadata:

name: requestcount

namespace: default

spec:

dimensions:

    source: source.labels[&quot;app&quot;] | source.service | &quot;unknown&quot;

    sourceVersion: source.labels[&quot;version&quot;] | &quot;unknown&quot;

    destination: destination.labels[&quot;app&quot;] | destination.service | &quot;unknown&quot;

    destinationVersion: destination.labels[&quot;version&quot;] | &quot;unknown&quot;

---

apiVersion: &quot;config.istio.io/v1alpha2&quot;

kind: rule

metadata:

name: quota

namespace: default

spec:

actions:

- handler: handler.memquota

    instances:

    - requestcount.quota

EOF
</code></pre><p>To see the rate limiting in action, we’ll generate some load on the application:</p><pre><code>wrk -t1 -c1 -d20s http://$BOOKINFO\_URL/productpage
</code></pre><p>In the web browser, we’ll notice that while the load generator is running (i.e., generating more than 1 req/sec), browser traffic is cut off. Instead of the black stars next to each review, the page now displays a message indicating that ratings are not currently available.</p><p>Stopping the load generator means the limit will no longer be exceeded: the black stars return when we refresh the page.</p><h2 id=summary>Summary</h2><p>We’ve shown you how to introduce advanced features like HTTP request routing and policy injection into a service mesh configured with Istio without restarting any of the services. This lets you develop and deploy without worrying about the ongoing management of the service mesh; service-wide policies can always be added later.</p><p>In the next and last installment of this series, we’ll focus on Istio’s security and authentication capabilities. We’ll discuss how to secure all interservice communications in a mesh, even against insiders with access to the network, without any changes to the application code or the deployment.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b7983d3e161ebbda5791cc9abf7d749d>Kubernetes Community Steering Committee Election Results</h1><div class="td-byline mb-4"><time datetime=2017-10-05 class=text-muted>Thursday, October 05, 2017</time></div><p>Beginning with the announcement of Kubernetes 1.0 at OSCON in 2015, there has been a concerted effort to share the power and burden of leadership across the Kubernetes community.</p><p>With the work of the Bootstrap Governance Committee, consisting of Brandon Philips, Brendan Burns, Brian Grant, Clayton Coleman, Joe Beda, Sarah Novotny and Tim Hockin - a cross section of long-time leaders representing 5 different companies with major investments of talent and effort in the Kubernetes Ecosystem - we wrote an initial <a href=https://github.com/kubernetes/steering/blob/master/charter.md>Steering Committee Charter</a> and launched a community wide election to seat a Kubernetes Steering Committee.</p><p>To quote from the Charter -</p><p><em>The initial role of the steering committee is to <strong>instantiate the formal process for Kubernetes governance</strong>. In addition to defining the initial governance process, the bootstrap committee strongly believes that <strong>it is important to provide a means for iterating</strong> the processes defined by the steering committee. We do not believe that we will get it right the first time, or possibly ever, and won’t even complete the governance development in a single shot. The role of the steering committee is to be a live, responsive body that can refactor and reform as necessary to adapt to a changing project and community.</em></p><p>This is our largest step yet toward making an implicit governance structure explicit. Kubernetes vision has been one of an inclusive and broad community seeking to build software which empowers our users with the portability of containers. The Steering Committee will be a strong leadership voice guiding the project toward success.</p><p>The Kubernetes Community is pleased to announce the results of the 2017 Steering Committee Elections. <strong>Please congratulate Aaron Crickenberger, Derek Carr, Michelle Noorali, Phillip Wittrock, Quinton Hoole and Timothy St. Clair</strong> , who will be joining the members of the Bootstrap Governance committee on the newly formed Kubernetes Steering Committee. Derek, Michelle, and Phillip will serve for 2 years. Aaron, Quinton, and Timothy will serve for 1 year.</p><p>This group will meet regularly in order to clarify and streamline the structure and operation of the project. Early work will include electing a representative to the CNCF Governing Board, evolving project processes, refining and documenting the vision and scope of the project, and chartering and delegating to more topical community groups.</p><p>Please see <a href=https://github.com/kubernetes/steering/blob/master/backlog.md>the full Steering Committee backlog</a> for more details.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5239569ee3ce5580a4696122578567c9>Kubernetes 1.8: Security, Workloads and Feature Depth</h1><div class="td-byline mb-4"><time datetime=2017-09-29 class=text-muted>Friday, September 29, 2017</time></div><p><em>Editor's note: today's post is by Aparna Sinha, Group Product Manager, Kubernetes, Google; Ihor Dvoretskyi, Developer Advocate, CNCF; Jaice Singer DuMars, Kubernetes Ambassador, Microsoft; and Caleb Miles, Technical Program Manager, CoreOS on the latest release of Kubernetes 1.8.</em></p><p>We’re pleased to announce the delivery of Kubernetes 1.8, our third release this year. Kubernetes 1.8 represents a snapshot of many exciting enhancements and refinements underway. In addition to functional improvements, we’re increasing project-wide focus on maturing <a href=https://github.com/kubernetes/sig-release>process</a>, formalizing <a href=https://github.com/kubernetes/community/tree/master/sig-architecture>architecture</a>, and strengthening Kubernetes’ <a href=https://github.com/kubernetes/community/tree/master/community/elections/2017>governance model</a>. The evolution of mature processes clearly signals that sustainability is a driving concern, and helps to ensure that Kubernetes is a viable and thriving project far into the future.</p><h2 id=spotlight-on-security>Spotlight on security</h2><p>Kubernetes 1.8 graduates support for <a href=https://en.wikipedia.org/wiki/Role-based_access_control>role based access control</a> (RBAC) to stable. RBAC allows cluster administrators to <a href=/docs/reference/access-authn-authz/rbac/>dynamically define roles</a> to enforce access policies through the Kubernetes API. Beta support for filtering outbound traffic through <a href=/docs/concepts/services-networking/network-policies/>network policies</a> augments existing support for filtering inbound traffic to a pod. RBAC and Network Policies are two powerful tools for enforcing organizational and regulatory security requirements within Kubernetes.</p><p>Transport Layer Security (TLS) <a href=/docs/admin/kubelet-tls-bootstrapping/>certificate rotation</a> for the Kubelet graduates to beta. Automatic certificate rotation eases secure cluster operation.</p><h2 id=spotlight-on-workload-support>Spotlight on workload support</h2><p>Kubernetes 1.8 promotes the core Workload APIs to beta with the apps/v1beta2 group and version. The beta contains the current version of Deployment, DaemonSet, ReplicaSet, and StatefulSet. The Workloads APIs provide a stable foundation for migrating existing workloads to Kubernetes as well as developing cloud native applications that target Kubernetes natively.</p><p>For those considering running Big Data workloads on Kubernetes, the Workloads API now enables <a href=https://apache-spark-on-k8s.github.io/userdocs/>native Kubernetes support</a> in Apache Spark.</p><p>Batch workloads, such as nightly ETL jobs, will benefit from the graduation of <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJobs</a> to beta.</p><p><a href=/docs/concepts/api-extension/custom-resources/>Custom Resource Definitions</a> (CRDs) remain in beta for Kubernetes 1.8. A CRD provides a powerful mechanism to extend Kubernetes with user-defined API objects. One use case for CRDs is the automation of complex stateful applications such as <a href=https://github.com/coreos/etcd-operator>key-value stores</a>, databases and <a href=https://rook.io/>storage engines</a> through the Operator Pattern. Expect continued enhancements to CRDs such as <a href=/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation>validation</a> as stabilization continues.</p><h2 id=spoilers-ahead>Spoilers ahead</h2><p>Volume snapshots, PV resizing, automatic taints, priority pods, kubectl plugins, oh my!</p><p>In addition to stabilizing existing functionality, Kubernetes 1.8 offers a number of alpha features that preview new functionality.</p><p>Each Special Interest Group (SIG) in the community continues to deliver the most requested user features for their area. For a complete list, please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v180>release notes</a>.</p><h4 id=availability>Availability</h4><p>Kubernetes 1.8 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.8.0>download on GitHub</a>. To get started with Kubernetes, check out these <a href=/docs/tutorials/kubernetes-basics/>interactive tutorials</a>.</p><h2 id=release-team>Release team</h2><p>The <a href=https://github.com/kubernetes/features/blob/master/release-1.8/release_team.md>Release team</a> for 1.8 was led by Jaice Singer DuMars, Kubernetes Ambassador at Microsoft, and was comprised of 14 individuals responsible for managing all aspects of the release, from documentation to testing, validation, and feature completeness.</p><p>As the Kubernetes community has grown, our release process has become an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem.</p><h2 id=user-highlights>User Highlights</h2><p>According to <a href=http://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100/>Redmonk</a>, 54 percent of Fortune 100 companies are running Kubernetes in some form with adoption coming from every sector across the world. Recent user stories from the community include:</p><ul><li>Ancestry.com currently holds 20 billion historical records and 90 million family trees, making it the largest consumer genomics DNA network in the world. With the move to Kubernetes, its deployment time for its Shaky Leaf icon service was <a href=https://kubernetes.io/case-studies/ancestry/>cut down from 50 minutes to 2 or 5 minutes</a>.</li><li>Wink, provider of smart home devices and apps, runs <a href=https://kubernetes.io/case-studies/wink/>80 percent of its workloads on a unified stack of Kubernetes-Docker-CoreOS</a>, allowing them to continually innovate and improve its products and services.</li><li>Pear Deck, a teacher communication app for students, ported their Heroku apps into Kubernetes, allowing them to deploy the exact same configuration in <a href=https://kubernetes.io/case-studies/peardeck/>lots of different clusters in 30 seconds</a>.</li><li>Buffer, social media management for agencies and marketers, has a remote team of 80 spread across a dozen different time zones. Kubernetes has provided the kind of <a href=https://kubernetes.io/case-studies/buffer/>liquid infrastructure</a> where a developer could create an app and deploy it and scale it horizontally as necessary.</li></ul><p>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community.</p><h2 id=ecosystem-updates>Ecosystem updates</h2><p>Announced on September 11, <a href=https://www.cncf.io/certification/kcsp/>Kubernetes Certified Service Providers</a> (KCSPs) are pre-qualified <a href=https://kubernetes.io/partners/#kcsp>organizations</a> with deep experience helping enterprises successfully adopt Kubernetes. Individual professionals can now <a href=https://www.cncf.io/certification/expert/>register</a> for the new Certified Kubernetes Administrator (CKA) program and exam, which requires passing an online, proctored, performance-based exam that tests one’s ability to solve multiple issues in a hands-on, command-line environment.<br>CNCF also offers <a href=https://www.cncf.io/certification/training/>online training</a> that teaches the skills needed to create and configure a real-world Kubernetes cluster.</p><h2 id=kubecon>KubeCon</h2><p>Join the community at <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america>KubeCon + CloudNativeCon</a> in Austin, December 6-8 for the largest Kubernetes gathering ever. The premiere Kubernetes event will feature technical sessions, case studies, developer deep dives, salons and more! A full schedule of events and speakers will be available <a href=http://events.linuxfoundation.org/events/kubecon-and-cloudnativecon-north-america/program/schedule>here</a> on September 28. Discounted <a href="https://www.regonline.com/registration/Checkin.aspx?EventID=1903774&_ga=2.224109086.464556664.1498490094-1623727562.1496428006">registration</a> ends October 6.</p><h2 id=open-source-summit-eu>Open Source Summit EU</h2><p>Ihor Dvoretskyi, Kubernetes 1.8 features release lead, will <a href=https://osseu17.sched.com/event/C4AA>present</a> new features and enhancements at Open Source Summit EU in Prague, October 23. Registration is <a href=http://events.linuxfoundation.org/events/open-source-summit-europe/attend/register>still open</a>.</p><h2 id=get-involved>Get involved</h2><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and through the channels below.</p><ul><li>Thank you for your continued feedback and support.</li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Chat with the community on <a href=http://slack.k8s.io/>Slack</a>.</li><li><a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your Kubernetes story.</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-daccff325ab87127da1590be1ccfbf9d>Kubernetes StatefulSets & DaemonSets Updates</h1><div class="td-byline mb-4"><time datetime=2017-09-27 class=text-muted>Wednesday, September 27, 2017</time></div><p>Editor's note: today's post is by Janet Kuo and Kenneth Owens, Software Engineers at Google.</p><p>This post talks about recent updates to the <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> and <a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a> API objects for Kubernetes. We explore these features using <a href=https://zookeeper.apache.org/>Apache ZooKeeper</a> and <a href=https://kafka.apache.org/>Apache Kafka</a> StatefulSets and a <a href=https://github.com/prometheus/node_exporter>Prometheus node exporter</a> DaemonSet.</p><p>In Kubernetes 1.6, we added the <a href=/docs/tasks/manage-daemon/update-daemon-set/>RollingUpdate</a> update strategy to the DaemonSet API Object. Configuring your DaemonSets with the RollingUpdate strategy causes the DaemonSet controller to perform automated rolling updates to the Pods in your DaemonSets when their spec.template are updated.</p><p>In Kubernetes 1.7, we enhanced the DaemonSet controller to track a history of revisions to the PodTemplateSpecs of DaemonSets. This allows the DaemonSet controller to roll back an update. We also added the <a href=/docs/concepts/workloads/controllers/statefulset/#update-strategies>RollingUpdate</a> strategy to the <a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a> API Object, and implemented revision history tracking for the StatefulSet controller. Additionally, we added the <a href=/docs/concepts/workloads/controllers/statefulset/#parallel-pod-management>Parallel</a> pod management policy to support stateful applications that require Pods with unique identities but not ordered Pod creation and termination.</p><h1 id=statefulset-rolling-update-and-pod-management-policy>StatefulSet rolling update and Pod management policy</h1><p>First, we’re going to demonstrate how to use StatefulSet rolling updates and Pod management policies by deploying a ZooKeeper ensemble and a Kafka cluster.</p><h2 id=prerequisites>Prerequisites</h2><p>To follow along, you’ll need to set up a Kubernetes 1.7 cluster with at least 3 schedulable nodes. Each node needs 1 CPU and 2 GiB of memory available. You will also need either a dynamic provisioner to allow the StatefulSet controller to provision 6 persistent volumes (PVs) with 10 GiB each, or you will need to manually provision the PVs prior to deploying the ZooKeeper ensemble or deploying the Kafka cluster.</p><h2 id=deploying-a-zookeeper-ensemble>Deploying a ZooKeeper ensemble</h2><p>Apache ZooKeeper is a strongly consistent, distributed system used by other distributed systems for cluster coordination and configuration management.</p><p>Note: You can create a ZooKeeper ensemble using this <a href=https://kow3ns.github.io/kubernetes-zookeeper/manifests/zookeeper_mini.yaml>zookeeper_mini.yaml</a> manifest. You can learn more about running a ZooKeeper ensemble on Kubernetes <a href=https://kow3ns.github.io/kubernetes-zookeeper/>here, as well as a</a> more in-depth explanation of <a href=https://kow3ns.github.io/kubernetes-zookeeper/manifests/>the manifest and its contents.</a></p><p>When you apply the manifest, you will see output like the following.</p><pre><code>$ kubectl apply -f zookeeper\_mini.yaml

service &quot;zk-hs&quot; created

service &quot;zk-cs&quot; created

poddisruptionbudget &quot;zk-pdb&quot; created

statefulset &quot;zk&quot; created
</code></pre><p>The manifest creates an ensemble of three ZooKeeper servers using a StatefulSet, zk; a Headless Service, zk-hs, to control the domain of the ensemble; a Service, zk-cs, that clients can use to connect to the ready ZooKeeper instances; and a PodDisruptionBugdet, zk-pdb, that allows for one planned disruption. (Note that while this ensemble is suitable for demonstration purposes, it isn’t sized correctly for production use.)</p><p>If you use kubectl get to watch Pod creation in another terminal you will see that, in contrast to the <a href=/docs/concepts/workloads/controllers/statefulset/#orderedready-pod-management>OrderedReady</a> strategy (the default policy that implements the full version of the StatefulSet guarantees), all of the Pods in the zk StatefulSet are created in parallel.</p><pre><code>$ kubectl get po -lapp=zk -w

NAME           READY         STATUS        RESTARTS     AGE


zk-0           0/1             Pending      0                   0s


zk-0           0/1             Pending     0                  0s


zk-1           0/1             Pending     0                  0s


zk-1           0/1             Pending     0                  0s


zk-0           0/1             ContainerCreating      0                  0s


zk-2           0/1             Pending      0                  0s


zk-1           0/1             ContainerCreating     0                  0s


zk-2           0/1             Pending      0                  0s


zk-2           0/1             ContainerCreating      0                  0s


zk-0           0/1             Running     0                  10s


zk-2           0/1             Running     0                  11s


zk-1           0/1             Running      0                  19s


zk-0           1/1             Running      0                  20s


zk-1           1/1             Running      0                  30s


zk-2           1/1             Running      0                  30s

</code></pre><p>This is because the zookeeper_mini.yaml manifest sets the podManagementPolicy of the StatefulSet to Parallel.</p><pre><code>apiVersion: apps/v1beta1  
kind: StatefulSet  
metadata:  
   name: zk  

spec:  
   serviceName: zk-hs  

   replicas: 3  

   updateStrategy:  

       type: RollingUpdate  

   podManagementPolicy: Parallel  

 ...
</code></pre><p>Many distributed systems, like ZooKeeper, do not require ordered creation and termination for their processes. You can use the Parallel Pod management policy to accelerate the creation and deletion of StatefulSets that manage these systems. Note that, when Parallel Pod management is used, the StatefulSet controller will not block when it fails to create a Pod. Ordered, sequential Pod creation and termination is performed when a StatefulSet’s podManagementPolicy is set to OrderedReady.</p><h2 id=deploying-a-kafka-cluster>Deploying a Kafka Cluster</h2><p>Apache Kafka is a popular distributed streaming platform. Kafka producers write data to partitioned topics which are stored, with a configurable replication factor, on a cluster of brokers. Consumers consume the produced data from the partitions stored on the brokers.</p><p>Note: Details of the manifests contents can be found <a href=https://kow3ns.github.io/kubernetes-kafka/manifests/>here</a>. You can learn more about running a Kafka cluster on Kubernetes <a href=https://kow3ns.github.io/kubernetes-kafka/>here</a>.</p><p>To create a cluster, you only need to download and apply the <a href=https://kow3ns.github.io/kubernetes-kafka/manifests/kafka_mini.yaml>kafka_mini.yaml</a> manifest. When you apply the manifest, you will see output like the following:</p><pre><code>$ kubectl apply -f kafka\_mini.yaml

service &quot;kafka-hs&quot; created

poddisruptionbudget &quot;kafka-pdb&quot; created

statefulset &quot;kafka&quot; created
</code></pre><p>The manifest creates a three broker cluster using the kafka StatefulSet, a Headless Service, kafka-hs, to control the domain of the brokers; and a PodDisruptionBudget, kafka-pdb, that allows for one planned disruption. The brokers are configured to use the ZooKeeper ensemble we created above by connecting through the zk-cs Service. As with the ZooKeeper ensemble deployed above, this Kafka cluster is fine for demonstration purposes, but it’s probably not sized correctly for production use.</p><p>If you watch Pod creation, you will notice that, like the ZooKeeper ensemble created above, the Kafka cluster uses the Parallel podManagementPolicy.</p><pre><code>$ kubectl get po -lapp=kafka -w

NAME           READY         STATUS        RESTARTS     AGE


kafka-0     0/1             Pending      0                   0s


kafka-0     0/1             Pending      0                  0s


kafka-1     0/1             Pending      0                  0s


kafka-1     0/1             Pending      0                  0s


kafka-2     0/1             Pending      0                  0s


kafka-0     0/1             ContainerCreating     0                  0s


kafka-2     0/1             Pending      0                  0s


kafka-1     0/1             ContainerCreating     0                  0s


kafka-1     0/1             Running     0                  11s


kafka-0     0/1             Running     0                  19s


kafka-1     1/1             Running     0                  23s


kafka-0     1/1             Running     0                  32s

</code></pre><h2 id=producing-and-consuming-data>Producing and consuming data</h2><p>You can use kubectl run to execute the kafka-topics.sh script to create a topic named test.</p><pre><code>$ kubectl run -ti --image=gcr.io/google\_containers/kubernetes-kafka:1.0-10.2.1 createtopic --restart=Never --rm -- kafka-topics.sh --create \

\&gt; --topic test \

\&gt; --zookeeper zk-cs.default.svc.cluster.local:2181 \

\&gt; --partitions 1 \

\&gt; --replication-factor 3
</code></pre><p>Now you can use kubectl run to execute the kafka-console-consumer.sh command to listen for messages.</p><pre><code>$ kubectl run -ti --image=gcr.io/google\_containers/kubnetes-kafka:1.0-10.2.1 consume --restart=Never --rm -- kafka-console-consumer.sh --topic test --bootstrap-server kafka-0.kafka-hs.default.svc.cluster.local:9093
</code></pre><p>In another terminal, you can run the kafka-console-producer.sh command.</p><pre><code>$kubectl run -ti --image=gcr.io/google\_containers/kubernetes-kafka:1.0-10.2.1 produce --restart=Never --rm \

\&gt;   -- kafka-console-producer.sh --topic test --broker-list kafka-0.kafka-hs.default.svc.cluster.local:9093,kafka-1.kafka-hs.default.svc.cluster.local:9093,kafka-2.kafka-hs.default.svc.cluster.local:9093

</code></pre><p>Output from the second terminal appears in the first terminal. If you continue to produce and consume messages while updating the cluster, you will notice that no messages are lost. You may see error messages as the leader for the partition changes when individual brokers are updated, but the client retries until the message is committed. This is due to the ordered, sequential nature of StatefulSet rolling updates which we will explore further in the next section.</p><p>Updating the Kafka cluster</p><p>StatefulSet updates are like DaemonSet updates in that they are both configured by setting the spec.updateStrategy of the corresponding API object. When the update strategy is set to OnDelete, the respective controllers will only create new Pods when a Pod in the StatefulSet or DaemonSet has been deleted. When the update strategy is set to RollingUpdate, the controllers will delete and recreate Pods when a modification is made to the spec.template field of a DaemonSet or StatefulSet. You can use rolling updates to change the configuration (via environment variables or command line parameters), resource requests, resource limits, container images, labels, and/or annotations of the Pods in a StatefulSet or DaemonSet. Note that all updates are destructive, always requiring that each Pod in the DaemonSet or StatefulSet be destroyed and recreated. StatefulSet rolling updates differ from DaemonSet rolling updates in that Pod termination and creation is ordered and sequential.</p><p>You can patch the kafka StatefulSet to reduce the CPU resource request to 250m.</p><pre><code>$ kubectl patch sts kafka --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/resources/requests/cpu&quot;, &quot;value&quot;:&quot;250m&quot;}]'

statefulset &quot;kafka&quot; patched
</code></pre><p>If you watch the status of the Pods in the StatefulSet, you will see that each Pod is deleted and recreated in reverse ordinal order (starting with the Pod with the largest ordinal and progressing to the smallest). The controller waits for each updated Pod to be running and ready before updating the subsequent Pod.</p><pre><code>$kubectl get po -lapp=kafka -w

NAME           READY         STATUS       RESTARTS     AGE


kafka-0     1/1             Running     0                   13m


kafka-1     1/1             Running     0                   13m


kafka-2     1/1             Running     0                   13m


kafka-2     1/1             Terminating     0                 14m


kafka-2     0/1             Terminating     0                 14m


kafka-2     0/1             Terminating     0                 14m


kafka-2     0/1             Terminating     0                 14m


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             ContainerCreating     0                 0s


kafka-2     0/1             Running     0                 10s


kafka-2     1/1             Running     0                 21s


kafka-1     1/1             Terminating     0                 14m


kafka-1     0/1             Terminating     0                 14m


kafka-1     0/1             Terminating     0                 14m


kafka-1     0/1             Terminating     0                 14m


kafka-1     0/1             Pending     0                 0s


kafka-1     0/1             Pending     0                 0s


kafka-1     0/1             ContainerCreating     0                 0s


kafka-1     0/1             Running     0                 11s


kafka-1     1/1             Running     0                 21s


kafka-0     1/1             Terminating     0                 14m


kafka-0     0/1             Terminating     0                 14m


kafka-0     0/1             Terminating     0                 14m


kafka-0     0/1             Terminating     0                 14m


kafka-0     0/1             Pending     0                 0s


kafka-0     0/1             Pending     0                 0s


kafka-0     0/1             ContainerCreating     0                 0s


kafka-0     0/1             Running     0                 10s


kafka-0     1/1             Running     0                 22s

</code></pre><p>Note that unplanned disruptions will not lead to unintentional updates during the update process. That is, the StatefulSet controller will always recreate the Pod at the correct version to ensure the ordering of the update is preserved. If a Pod is deleted, and if it has already been updated, it will be created from the updated version of the StatefulSet’s spec.template. If the Pod has not already been updated, it will be created from the previous version of the StatefulSet’s spec.template. We will explore this further in the following sections.</p><h2 id=staging-an-update>Staging an update</h2><p>Depending on how your organization handles deployments and configuration modifications, you may want or need to stage updates to a StatefulSet prior to allowing the roll out to progress. You can accomplish this by setting a partition for the RollingUpdate. When the StatefulSet controller detects a partition in the updateStrategy of a StatefulSet, it will only apply the updated version of the StatefulSet’s spec.template to Pods whose ordinal is greater than or equal to the value of the partition.</p><p>You can patch the kafka StatefulSet to add a partition to the RollingUpdate update strategy. If you set the partition to a number greater than or equal to the StatefulSet’s spec.replicas (as below), any subsequent updates you perform to the StatefulSet’s spec.template will be staged for roll out, but the StatefulSet controller will not start a rolling update.</p><pre><code>$ kubectl patch sts kafka -p '{&quot;spec&quot;:{&quot;updateStrategy&quot;:{&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:{&quot;partition&quot;:3}}}}'

statefulset &quot;kafka&quot; patched
</code></pre><p>If you patch the StatefulSet to set the requested CPU to 0.3, you will notice that none of the Pods are updated.</p><pre><code>$ kubectl patch sts kafka --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/resources/requests/cpu&quot;, &quot;value&quot;:&quot;0.3&quot;}]'

statefulset &quot;kafka&quot; patched
</code></pre><p>Even if you delete a Pod and wait for the StatefulSet controller to recreate it, you will notice that the Pod is recreated with current CPU request.</p><pre><code>$   kubectl delete po kafka-1


pod &quot;kafka-1&quot; deleted


$ kubectl get po kafka-1 -w

NAME           READY         STATUS                           RESTARTS     AGE


kafka-1     0/1             ContainerCreating     0                   10s


kafka-1     0/1             Running     0                 19s


kafka-1     1/1             Running     0                 21s



$ kubectl get po kafka-1 -o yaml

apiVersion: v1

kind: Pod

metadata:

   ...


       resources:


           requests:


               cpu: 250m


               memory: 1Gi

</code></pre><h2 id=rolling-out-a-canary>Rolling out a canary</h2><p>Often, we want to verify an image update or configuration change on a single instance of an application before rolling it out globally. If you modify the partition created above to be 2, the StatefulSet controller will roll out a <a href=http://whatis.techtarget.com/definition/canary-canary-testing>canary</a> that can be used to verify that the update is working as intended.</p><pre><code>$ kubectl patch sts kafka -p '{&quot;spec&quot;:{&quot;updateStrategy&quot;:{&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:{&quot;partition&quot;:2}}}}'

statefulset &quot;kafka&quot; patched
</code></pre><p>You can watch the StatefulSet controller update the kafka-2 Pod and pause after the update is complete.</p><pre><code>$   kubectl get po -lapp=kafka -w


NAME           READY         STATUS       RESTARTS     AGE


kafka-0     1/1             Running     0                   50m


kafka-1     1/1             Running     0                   10m


kafka-2     1/1             Running     0                   29s


kafka-2     1/1             Terminating     0                 34s


kafka-2     0/1             Terminating     0                 38s


kafka-2     0/1             Terminating     0                 39s


kafka-2     0/1             Terminating     0                 39s


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             Terminating     0                 20s


kafka-2     0/1             Terminating     0                 20s


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             Pending     0                 0s


kafka-2     0/1             ContainerCreating     0                 0s


kafka-2     0/1             Running     0                 19s


kafka-2     1/1             Running     0                 22s

</code></pre><h2 id=phased-roll-outs>Phased roll outs</h2><p>Similar to rolling out a canary, you can roll out updates based on a phased progression (e.g. linear, geometric, or exponential roll outs).</p><p>If you patch the kafka StatefulSet to set the partition to 1, the StatefulSet controller updates one more broker.</p><pre><code>$ kubectl patch sts kafka -p '{&quot;spec&quot;:{&quot;updateStrategy&quot;:{&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:{&quot;partition&quot;:1}}}}'

statefulset &quot;kafka&quot; patched
</code></pre><p>If you set it to 0, the StatefulSet controller updates the final broker and completes the update.</p><pre><code>$ kubectl patch sts kafka -p '{&quot;spec&quot;:{&quot;updateStrategy&quot;:{&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:{&quot;partition&quot;:0}}}}'

statefulset &quot;kafka&quot; patched
</code></pre><p>Note that you don’t have to decrement the partition by one. For a larger StatefulSet--for example, one with 100 replicas--you might use a progression more like 100, 99, 90, 50, 0. In this case, you would stage your update, deploy a canary, roll out to 10 instances, update fifty percent of the Pods, and then complete the update.</p><h2 id=cleaning-up>Cleaning up</h2><p>To delete the API Objects created above, you can use kubectl delete on the two manifests you used to create the ZooKeeper ensemble and the Kafka cluster.</p><pre><code>$ kubectl delete -f kafka\_mini.yaml

service &quot;kafka-hs&quot; deleted

poddisruptionbudget &quot;kafka-pdb&quot; deleted

Statefulset “kafka” deleted


$ kubectl delete -f zookeeper\_mini.yaml

service &quot;zk-hs&quot; deleted

service &quot;zk-cs&quot; deleted

poddisruptionbudget &quot;zk-pdb&quot; deleted

statefulset &quot;zk&quot; deleted
</code></pre><p>By design, the StatefulSet controller does not delete any persistent volume claims (PVCs): the PVCs created for the ZooKeeper ensemble and the Kafka cluster must be manually deleted. Depending on the storage reclamation policy of your cluster, you many also need to manually delete the backing PVs.</p><h1 id=daemonset-rolling-update-history-and-rollback>DaemonSet rolling update, history, and rollback</h1><p>In this section, we’re going to show you how to perform a rolling update on a DaemonSet, look at its history, and then perform a rollback after a bad rollout. We will use a DaemonSet to deploy a <a href=https://github.com/prometheus/node_exporter>Prometheus node exporter</a> on each Kubernetes node in the cluster. These node exporters export node metrics to the Prometheus monitoring system. For the sake of simplicity, we’ve omitted the installation of the <a href=https://github.com/prometheus/prometheus>Prometheus server</a> and the service for <a href=/docs/concepts/workloads/controllers/daemonset/#communicating-with-daemon-pods>communication with DaemonSet pods</a> from this blogpost.</p><h2 id=prerequisites-1>Prerequisites</h2><p>To follow along with this section of the blog, you need a working Kubernetes 1.7 cluster and kubectl version 1.7 or later. If you followed along with the first section, you can use the same cluster.</p><h2 id=daemonset-rolling-upfirst-prepare-the-node-exporter-daemonset-manifest-to-run-a-v0-13-prometheus-node-exporter-on-every-node-in-the-cluster>DaemonSet rolling upFirst, prepare the node exporter DaemonSet manifest to run a v0.13 Prometheus node exporter on every node in the cluster:</h2><pre><code>$ cat \&gt;\&gt; node-exporter-v0.13.yaml \&lt;\&lt;EOF

apiVersion: extensions/v1beta1  
kind: DaemonSet  
metadata:  
   name: node-exporter  

spec:  
   updateStrategy:  

       type: RollingUpdate  

   template:  

       metadata:  

           labels:  

               app: node-exporter  

           name: node-exporter  

       spec:  

           containers:  

           - image: prom/node-exporter:v0.13.0  

               name: node-exporter  

               ports:  

               - containerPort: 9100  

                   hostPort: 9100  

                   name: scrape  

           hostNetwork: true  

           hostPID: true


EOF
</code></pre><p>Note that you need to enable the DaemonSet rolling update feature by explicitly setting DaemonSet .spec.updateStrategy.type to RollingUpdate.</p><p>Apply the manifest to create the node exporter DaemonSet:</p><pre><code>$ kubectl apply -f node-exporter-v0.13.yaml --record

daemonset &quot;node-exporter&quot; created
</code></pre><p>Wait for the first DaemonSet rollout to complete:</p><pre><code>$ kubectl rollout status ds node-exporter  
daemon set &quot;node-exporter&quot; successfully rolled out
</code></pre><p>You should see each of your node runs one copy of the node exporter pod:</p><pre><code>$ kubectl get pods -l app=node-exporter -o wide
</code></pre><p>To perform a rolling update on the node exporter DaemonSet, prepare a manifest that includes the v0.14 Prometheus node exporter:</p><pre><code>$ cat node-exporter-v0.13.yaml ```  sed &quot;s/v0.13.0/v0.14.0/g&quot; \&gt; node-exporter-v0.14.yaml
</code></pre><p>Then apply the v0.14 node exporter DaemonSet:</p><pre><code>$ kubectl apply -f node-exporter-v0.14.yaml --record

daemonset &quot;node-exporter&quot; configured
</code></pre><p>Wait for the DaemonSet rolling update to complete:</p><pre><code>$ kubectl rollout status ds node-exporter

...

Waiting for rollout to finish: 3 out of 4 new pods have been updated...  
Waiting for rollout to finish: 3 of 4 updated pods are available...  
daemon set &quot;node-exporter&quot; successfully rolled out
</code></pre><p>We just triggered a DaemonSet rolling update by updating the DaemonSet template. By default, one old DaemonSet pod will be killed and one new DaemonSet pod will be created at a time.</p><p>Now we’ll cause a rollout to fail by updating the image to an invalid value:</p><pre><code>$ cat node-exporter-v0.13.yaml | sed &quot;s/v0.13.0/bad/g&quot; \&gt; node-exporter-bad.yaml


$ kubectl apply -f node-exporter-bad.yaml --record

daemonset &quot;node-exporter&quot; configured
</code></pre><p>Notice that the rollout never finishes:</p><pre><code>$ kubectl rollout status ds node-exporter   
Waiting for rollout to finish: 0 out of 4 new pods have been updated...  
Waiting for rollout to finish: 1 out of 4 new pods have been updated…

# Use ^C to exit
</code></pre><p>This behavior is expected. We mentioned earlier that a DaemonSet rolling update kills and creates one pod at a time. Because the new pod never becomes available, the rollout is halted, preventing the invalid specification from propagating to more than one node. StatefulSet rolling updates implement the same behavior with respect to failed deployments. Unsuccessful updates are blocked until it corrected via roll back or by rolling forward with a specification.</p><pre><code>$ kubectl get pods -l app=node-exporter

NAME                                   READY         STATUS                 RESTARTS     AGE


node-exporter-f2n14     0/1             ErrImagePull     0                   3m


...


# N = number of nodes

$ kubectl get ds node-exporter  
NAME                       DESIRED     CURRENT     READY         UP-TO-DATE     AVAILABLE     NODE SELECTOR     AGE  

node-exporter     N                 N                 N-1             1                       N                     \&lt;none\&gt;                   46m

</code></pre><h2 id=daemonset-history-rollbacks-and-rolling-forward>DaemonSet history, rollbacks, and rolling forward</h2><p>Next, perform a rollback. Take a look at the node exporter DaemonSet rollout history:</p><pre><code>$ kubectl rollout history ds node-exporter   
daemonsets &quot;node-exporter&quot;  
REVISION               CHANGE-CAUSE  

1                             kubectl apply --filename=node-exporter-v0.13.yaml --record=true  

2                             kubectl apply --filename=node-exporter-v0.14.yaml --record=true


3                             kubectl apply --filename=node-exporter-bad.yaml --record=true

</code></pre><p>Check the details of the revision you want to roll back to:</p><pre><code>$ kubectl rollout history ds node-exporter --revision=2  
daemonsets &quot;node-exporter&quot; with revision #2  
Pod Template:  
   Labels:             app=node-exporter  

   Containers:  

     node-exporter:  

       Image:           prom/node-exporter:v0.14.0  

       Port:             9100/TCP  

       Environment:               \&lt;none\&gt;  

       Mounts:         \&lt;none\&gt;  

   Volumes:           \&lt;none\&gt;

</code></pre><p>You can quickly roll back to any DaemonSet revision you found through kubectl rollout history:</p><pre><code># Roll back to the last revision

$ kubectl rollout undo ds node-exporter   
daemonset &quot;node-exporter&quot; rolled back


# Or use --to-revision to roll back to a specific revision

$ kubectl rollout undo ds node-exporter --to-revision=2  
daemonset &quot;node-exporter&quot; rolled back
</code></pre><p>A DaemonSet rollback is done by rolling forward. Therefore, after the rollback, DaemonSet revision 2 becomes revision 4 (current revision):</p><pre><code>$ kubectl rollout history ds node-exporter   
daemonsets &quot;node-exporter&quot;  
REVISION               CHANGE-CAUSE  

1                             kubectl apply --filename=node-exporter-v0.13.yaml --record=true  

3                             kubectl apply --filename=node-exporter-bad.yaml --record=true  

4                             kubectl apply --filename=node-exporter-v0.14.yaml --record=true

</code></pre><p>The node exporter DaemonSet is now healthy again:</p><pre><code>$ kubectl rollout status ds node-exporter  
daemon set &quot;node-exporter&quot; successfully rolled out


# N = number of nodes

$ kubectl get ds node-exporter

NAME                       DESIRED     CURRENT     READY         UP-TO-DATE     AVAILABLE     NODE SELECTOR     AGE  

node-exporter     N                 N                 N                 N                       N                     \&lt;none\&gt;                   46m

</code></pre><p>If current DaemonSet revision is specified while performing a rollback, the rollback is skipped:</p><pre><code>$ kubectl rollout undo ds node-exporter --to-revision=4  
daemonset &quot;node-exporter&quot; skipped rollback (current template already matches revision 4)
</code></pre><p>You will see this complaint from kubectl if the DaemonSet revision is not found:</p><pre><code>$ kubectl rollout undo ds node-exporter --to-revision=10  
error: unable to find specified revision 10 in history
</code></pre><p>Note that kubectl rollout history and kubectl rollout status support StatefulSets, too!</p><h2 id=cleaning-up-1>Cleaning up</h2><pre><code>$ kubectl delete ds node-exporter
</code></pre><h1 id=what-s-next-for-daemonset-and-statefulset>What’s next for DaemonSet and StatefulSet</h1><p>Rolling updates and roll backs close an important feature gap for DaemonSets and StatefulSets. As we plan for Kubernetes 1.8, we want to continue to focus on advancing the core controllers to GA. This likely means that some advanced feature requests (e.g. automatic roll back, infant mortality detection) will be deferred in favor of ensuring the consistency, usability, and stability of the core controllers. We welcome feedback and contributions, so please feel free to reach out on <a href=http://slack.k8s.io/>Slack</a>, to ask questions on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a>, or open issues or pull requests on <a href=https://github.com/kubernetes/kubernetes>GitHub</a>.</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7265277509164b163724b0537be1457b>Introducing the Resource Management Working Group</h1><div class="td-byline mb-4"><time datetime=2017-09-21 class=text-muted>Thursday, September 21, 2017</time></div><p><em><strong>Editor's note: today's post is by Jeremy Eder, Senior Principal Software Engineer at Red Hat, on the formation of the Resource Management Working Group</strong></em></p><h2 id=why-are-we-here>Why are we here?</h2><p>Kubernetes has evolved to support diverse and increasingly complex classes of applications. We can onboard and scale out modern, cloud-native web applications based on microservices, batch jobs, and stateful applications with persistent storage requirements.</p><p>However, there are still opportunities to improve Kubernetes; for example, the ability to run workloads that require specialized hardware or those that perform measurably better when hardware topology is taken into account. These conflicts can make it difficult for application classes (particularly in established verticals) to adopt Kubernetes.</p><p>We see an unprecedented opportunity here, with a high cost if it’s missed. The Kubernetes ecosystem must create a consumable path forward to the next generation of system architectures by catering to needs of as-yet unserviced workloads in meaningful ways. The Resource Management Working Group, along with other SIGs, must demonstrate the vision customers want to see, while enabling solutions to run well in a fully integrated, thoughtfully planned end-to-end stack.<br> <br>Kubernetes Working Groups are created when a particular challenge requires cross-SIG collaboration. The Resource Management Working Group, for example, works primarily with sig-node and sig-scheduling to drive support for additional resource management capabilities in Kubernetes. We make sure that key contributors from across SIGs are frequently consulted because working groups are not meant to make system-level decisions on behalf of any SIG.<br> <br>An example and key benefit of this is the working group’s relationship with sig-node.  We were able to ensure completion of several releases of node reliability work (complete in 1.6) before contemplating feature design on top. Those designs are use-case driven: research into technical requirements for a variety of workloads, then sorting based on measurable impact to the largest cross-section.</p><h2 id=target-workloads-and-use-cases>Target Workloads and Use-cases</h2><p>One of the working group’s key design tenets is that user experience must remain clean and portable, while still surfacing infrastructure capabilities that are required by businesses and applications.<br> <br>While not representing any commitment, we hope in the fullness of time that Kubernetes can optimally run financial services workloads, machine learning/training, grid schedulers, map-reduce, animation workloads, and more. As a use-case driven group, we account for potential application integration that can also facilitate an ecosystem of complementary independent software vendors to flourish on top of Kubernetes.</p><p><img src=https://lh6.googleusercontent.com/HFbnRmEIQZ43lBGRvUPZaPe-NGDoCoQVMglola-sZXdkUAbgiZiEB_ktbebPPMPY9D3p1tXj9toTjp_tZUjiQTHukl3ir_DE-_6yix0xWIr4-yJnrPA9zWBLzTFXBM0DhTURHLd6 alt=venn-kubernetes.png></p><h2 id=why-do-this>Why do this?</h2><p>Kubernetes covers generic web hosting capabilities very well, so why go through the effort of expanding workload coverage for Kubernetes at all? The fact is that workloads elegantly covered by Kubernetes today, only represent a fraction of the world’s compute usage. We have a tremendous opportunity to safely and methodically expand upon the set of workloads that can run optimally on Kubernetes.</p><p>To date, there’s demonstrable progress in the areas of expanded workload coverage:</p><ul><li>Stateful applications such as Zookeeper, etcd, MySQL, Cassandra, ElasticSearch</li><li>Jobs, such as timed events to process the day’s logs or any other batch processing</li><li>Machine Learning and compute-bound workload acceleration through Alpha GPU support
Collectively, the folks working on Kubernetes are hearing from their customers that we need to go further. Following the tremendous popularity of containers in 2014, industry rhetoric circled around a more modern, container-based, datacenter-level workload orchestrator as folks looked to plan their next architectures.</li></ul><p>As a consequence, we began advocating for increasing the scope of workloads covered by Kubernetes, from overall concepts to specific features. Our aim is to put control and choice in users hands, helping them move with confidence towards whatever infrastructure strategy they choose. In this advocacy, we quickly found a large group of like-minded companies interested in broadening the types of workloads that Kubernetes can orchestrate. And thus the working group was born.</p><h2 id=genesis-of-the-resource-management-working-group>Genesis of the Resource Management Working Group</h2><p>After extensive development/feature <a href="https://docs.google.com/document/d/1p7scsTPzPyouktBFTxu4RhRwW8yUn5Lj7VGY9SaOo-8/edit?ts=5824ee1f">discussions</a> during the Kubernetes Developer Summit 2016 after <a href=http://events.linuxfoundation.org/events/kubecon/program/schedule>CloudNativeCon | KubeCon Seattle</a>, we decided to <a href=https://groups.google.com/d/msg/kubernetes-dev/Sb0VlXOM8eQ/La3YCe2-CgAJ>formalize</a> our loosely organized group. In January 2017, the Kubernetes <em><a href=https://github.com/kubernetes/community/tree/master/wg-resource-management>Resource Management Working Group</a></em> was formed. This group (led by Derek Carr from Red Hat and Vishnu Kannan from Google) was originally cast as a temporary initiative to provide guidance back to sig-node and sig-scheduling (primarily). However, due to the cross-cutting nature of the goals within the working group, and the depth of <a href=https://docs.google.com/spreadsheets/d/1NWarIgtSLsq3izc5wOzV7ItdhDNRd-6oBVawmvs-LGw/edit>roadmap</a> quickly uncovered, the Resource Management Working Group became its own entity within the first few months.</p><p>Recently, Brian Grant from Google (@bgrant0607) posted the following image on his <a href=https://twitter.com/bgrant0607/status/862091393723842561>Twitter feed</a>. This image helps to explain the role of each SIG, and shows where the Resource Management Working Group fits into the overall project organization.</p><p><img src=https://lh4.googleusercontent.com/P9CFdgJK3pdaKkqefpYwoLHkaT--ntJQ0XZT5FbO5TlZtwnvepaO0eCOwxlUYKAsZqZFfOw78_6nEJfY89x3j1w_nHaVqUj7sBTpcAA4g80MoQy5-n3YU7GI8-IFwHUo85cy-rCc alt=C_bDdiWUAAAcB2y.jpg>{.big-img}</p><p>To help bootstrap this effort, the Resource Management Working Group had its first face-to-face kickoff meeting in May 2017. Thanks to Google for hosting!</p><p><img src=https://lh3.googleusercontent.com/eL16-GnX335XcVta2u8nt3UgtoJMGuo2Xfqj3SJ34slepm_xzl6G4WmcBtFIIiaw_gYi-h5FsMnXA8GCl3xqhZGy44Gt6GmB5Ajy4McCdANkFQUy26z02e5rZU88lN-NFO774GgE alt=20170502_100834.jpg></p><p>Folks from Intel, NVIDIA, Google, IBM, Red Hat. and Microsoft (among others) participated. <br>You can read the outcomes of that 3-day meeting <a href=https://docs.google.com/document/d/13_nk75eItkpbgZOt62In3jj0YuPbGPC_NnvSCHpgvUM/edit>here</a>.</p><p>The group’s prioritized list of features for increasing workload coverage on Kubernetes enumerated in the <a href=https://github.com/kubernetes/community/tree/master/wg-resource-management>charter</a> of the Resource Management Working group includes:</p><ul><li>Support for performance sensitive workloads (exclusive cores, cpu pinning strategies, NUMA)</li><li>Integrating new hardware devices (GPUs, FPGAs, Infiniband, etc.)</li><li>Improving resource isolation (local storage, hugepages, caches, etc.)</li><li>Improving Quality of Service (performance SLOs)</li><li>Performance benchmarking</li><li>APIs and extensions related to the features mentioned above
The discussions made it clear that there was tremendous overlap between needs for various workloads, and that we ought to de-duplicate requirements, and plumb generically.</li></ul><h2 id=workload-characteristics>Workload Characteristics</h2><p>The set of initially targeted use-cases share one or more of the following characteristics:</p><ul><li>Deterministic performance (address long tail latencies)</li><li>Isolation within a single node, as well as within groups of nodes sharing a control plane</li><li>Requirements on advanced hardware and/or software capabilities</li><li>Predictable, reproducible placement: applications need granular guarantees around placement 
The Resource Management Working Group is spearheading the feature design and development in support of these workload requirements. Our goal is to provide best practices and patterns for these scenarios.</li></ul><h2 id=initial-scope>Initial Scope</h2><p>In the months leading up to our recent face-to-face, we had discussed how to safely abstract resources in a way that retains portability and clean user experience, while still meeting application requirements. The working group came away with a multi-release <a href=https://docs.google.com/spreadsheets/d/1NWarIgtSLsq3izc5wOzV7ItdhDNRd-6oBVawmvs-LGw/edit>roadmap</a> that included 4 short- to mid-term targets with great overlap between target workloads:</p><ul><li><p><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md>Device Manager (Plugin) Proposal</a></p><ul><li>Kubernetes should provide access to hardware devices such as NICs, GPUs, FPGA, Infiniband and so on.</li></ul></li><li><p><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/cpu-manager.md>CPU Manager</a></p><ul><li>Kubernetes should provide a way for users to request static CPU assignment via the Guaranteed QoS tier. No support for NUMA in this phase.</li></ul></li><li><p><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/hugepages.md>HugePages support in Kubernetes</a></p><ul><li>Kubernetes should provide a way for users to consume huge pages of any size.</li></ul></li><li><p><a href=https://github.com/kubernetes/community/pull/782>Resource Class proposal</a></p><ul><li>Kubernetes should implement an abstraction layer (analogous to StorageClasses) for devices other than CPU and memory that allows a user to consume a resource in a portable way. For example, how can a pod request a GPU that has a minimum amount of memory?</li></ul></li></ul><h2 id=getting-involved-summary>Getting Involved & Summary</h2><p>Our charter document includes a <a href=https://github.com/kubernetes/community/tree/master/wg-resource-management#contact-us>Contact Us</a> section with links to our mailing list, Slack channel, and Zoom meetings. Recordings of previous meetings are uploaded to <a href=https://www.youtube.com/channel/UCyfvrmhAGcsFlJeGgZQvZ6g>Youtube</a>. We plan to discuss these topics and more at the 2017 Kubernetes Developer Summit at <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america>CloudNativeCon | KubeCon</a> in Austin. Please come and join one of our meetings (users, customers, software and hardware vendors are all welcome) and contribute to the working group!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d6b5dd1ab80a70cf585e5b6868705c22>Windows Networking at Parity with Linux for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-09-08 class=text-muted>Friday, September 08, 2017</time></div><p><em><strong>Editor's note: today's post is by Jason Messer, Principal PM Manager at Microsoft, on improvements to the Windows network stack to support the Kubernetes CNI model.</strong></em></p><p>Since I last blogged about <a href=https://blogs.technet.microsoft.com/networking/2017/04/04/windows-networking-for-kubernetes/>Kubernetes Networking for Windows</a> four months ago, the Windows Core Networking team has made tremendous progress in both the platform and open source Kubernetes projects. With the updates, Windows is now on par with Linux in terms of networking. Customers can now deploy mixed-OS, Kubernetes clusters in any environment including Azure, on-premises, and on 3rd-party cloud stacks with the same network primitives and topologies supported on Linux without any workarounds, “hacks”, or 3rd-party switch extensions.</p><p>"So what?", you may ask. There are multiple application and infrastructure-related reasons why these platform improvements make a substantial difference in the lives of developers and operations teams wanting to run Kubernetes. Read on to learn more!</p><h2 id=tightly-coupled-communication>Tightly-Coupled Communication</h2><p>These improvements enable tightly-coupled communication between multiple Windows Server containers (without Hyper-V isolation) within a single "<a href=/docs/concepts/workloads/pods/pod/>Pod</a>". Think of Pods as the scheduling unit for the Kubernetes cluster, inside of which, one or more application containers are co-located and able to share storage and networking resources. All containers within a Pod shared the same IP address and port range and are able to communicate with each other using localhost. This enables applications to easily leverage "helper" programs for tasks such as monitoring, configuration updates, log management, and proxies. Another way to think of a Pod is as a compute host with the app containers representing processes.</p><h2 id=simplified-network-topology>Simplified Network Topology</h2><p>We also simplified the network topology on Windows nodes in a Kubernetes cluster by reducing the number of endpoints required per container (or more generally, per pod) to one. Previously, Windows containers (pods) running in a Kubernetes cluster required two endpoints - one for external (internet) communication and a second for intra-cluster communication between between other nodes or pods in the cluster. This was due to the fact that external communication from containers attached to a host network with local scope (i.e. not publicly routable) required a NAT operation which could only be provided through the Windows NAT (WinNAT) component on the host. Intra-cluster communication required containers to be attached to a separate network with "global" (cluster-level) scope through a second endpoint. Recent platform improvements now enable NAT''ing to occur directly on a container endpoint which is implemented with the Microsoft Virtual Filtering Platform (VFP) Hyper-V switch extension. Now, both external and intra-cluster traffic can flow through a single endpoint.</p><h2 id=load-balancing-using-vfp-in-windows-kernel>Load-Balancing using VFP in Windows kernel</h2><p>Kubernetes worker nodes rely on the kube-proxy to load-balance ingress network traffic to Service IPs between pods in a cluster. Previous versions of Windows implemented the Kube-proxy's load-balancing through a user-space proxy. We recently added support for "Proxy mode: iptables" which is implemented using VFP in the Windows kernel so that any IP traffic can be load-balanced more efficiently by the Windows OS kernel. Users can also configure an external load balancer by specifying the externalIP parameter in a service definition. In addition to the aforementioned improvements, we have also added platform support for the following:</p><ul><li>Support for DNS search suffixes per container / Pod (Docker improvement - removes additional work previously done by kube-proxy to append DNS suffixes) </li><li>[Platform Support] 5-tuple rules for creating ACLs (Looking for help from community to integrate this with support for K8s Network Policy)</li></ul><p>Now that Windows Server has <a href=https://blogs.technet.microsoft.com/hybridcloud/2017/07/13/new-windows-server-preview-release-available-to-windows-insiders/>joined</a> the <a href=https://insider.windows.com/>Windows Insider Program</a>, customers and partners can take advantage of these new platform features today which accrue value to eagerly anticipated, new feature release later this year and new build after six months. The latest Windows Server insider <a href=https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewserver>build</a> now includes support for all of these platform improvements.</p><p>In addition to the platform improvements for Windows, the team submitted code (PRs) for CNI, kubelet, and kube-proxy with the goal of mainlining Windows support into the Kubernetes v1.8 release. These PRs remove previous work-arounds required on Windows for items such as user-mode proxy for internal load balancing, appending additional DNS suffixes to each Kube-DNS request, and a separate container endpoint for external (internet) connectivity.</p><ul><li><a href=https://github.com/kubernetes/kubernetes/pull/51063>https://github.com/kubernetes/kubernetes/pull/51063</a></li><li><a href=https://github.com/kubernetes/kubernetes/pull/51064>https://github.com/kubernetes/kubernetes/pull/51064</a></li></ul><p>These new platform features and work on kubelet and kube-proxy align with the CNI network model used by Kubernetes on Linux and simplify the deployment of a K8s cluster without additional configuration or custom (Azure) resource templates. To this end, we completed work on CNI network and IPAM plugins to create/remove endpoints and manage IP addresses. The CNI plugin works through kubelet to target the Windows Host Networking Service (HNS) APIs to create an 'l2bridge' network (analogous to macvlan on Linux) which is enforced by the VFP switch extension.</p><p>The 'l2bridge' network driver re-writes the MAC address of container network traffic on ingress and egress to use the container host's MAC address. This obviates the need for multiple MAC addresses (one per container running on the host) to be "learned" by the upstream network switch port to which the container host is connected. This preserves memory space in physical switch TCAM tables and relies on the Hyper-V virtual switch to do MAC address translation in the host to forward traffic to the correct container. IP addresses are managed by a default, Windows IPAM plug-in which requires that POD CIDR IPs be taken from the container host's network IP space.</p><p>The team demoed (<a href=https://files.slack.com/files-pri/T09NY5SBT-F6KTG30E8/download/sigwindows-2017-08-08.mp4>link</a> to video) these new platform features and open-source updates to the SIG-Windows group on 8/8. We are working with the community to merge the kubelet and kube-proxy PRs to mainline these changes in time for the Kubernetes v1.8 release due out this September. These capabilities can then be used on current Windows Server insider builds and the <a href=https://blogs.technet.microsoft.com/windowsserver/2017/08/24/sneak-peek-1-windows-server-version-1709/>Windows Server, version 1709</a>.</p><p>Soon after RTM, we will also introduce these improvements into the Azure Container Service (ACS) so that Windows worker nodes and the containers hosted are first-class, Azure VNet citizens. An Azure IPAM plugin for Windows CNI will enable these endpoints to directly attach to Azure VNets with network policies for Windows containers enforced the same way as VMs.</p><p>| Feature | Windows Server 2016 (In-Market) | Next Windows Server Feature Release, Semi-Annual Channel | Linux |
| Multiple Containers per Pod with shared network namespace (Compartment) | One Container per Pod | ✔ | ✔ |
| Single (Shared) Endpoint per Pod | Two endpoints: WinNAT (External) + Transparent (Intra-Cluster) | ✔ | ✔ |
| User-Mode, Load Balancing | ✔ | ✔ | ✔ |
| Kernel-Mode, Load Balancing | Not Supported | ✔ | ✔ |
| Support for DNS search suffixes per Pod (Docker update) | Kube-Proxy  added multiple DNS suffixes to each request | ✔ | ✔ |
| CNI Plugin Support | Not Supported | ✔ | ✔ |</p><p>The Kubernetes SIG Windows group meets bi-weekly on Tuesdays at 12:30 PM ET. To join or view notes from previous meetings, check out this <a href="https://docs.google.com/document/d/1Tjxzjjuy4SQsFSUVXZbvqVb64hjNAG5CQX8bK7Yda9w/edit#heading=h.kbz22d1yc431">document</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0694164705b83bd74e3c69392dc49e48>Kubernetes Meets High-Performance Computing</h1><div class="td-byline mb-4"><time datetime=2017-08-22 class=text-muted>Tuesday, August 22, 2017</time></div><p>Editor's note: today's post is by Robert Lalonde, general manager at Univa, on supporting mixed HPC and containerized applications  </p><p>Anyone who has worked with Docker can appreciate the enormous gains in efficiency achievable with containers. While Kubernetes excels at orchestrating containers, high-performance computing (HPC) applications can be tricky to deploy on Kubernetes.</p><p>In this post, I discuss some of the challenges of running HPC workloads with Kubernetes, explain how organizations approach these challenges today, and suggest an approach for supporting mixed workloads on a shared Kubernetes cluster. We will also provide information and links to a case study on a customer, IHME, showing how Kubernetes is extended to service their HPC workloads seamlessly while retaining scalability and interfaces familiar to HPC users.</p><h2 id=hpc-workloads-unique-challenges>HPC workloads unique challenges</h2><p>In Kubernetes, the base unit of scheduling is a Pod: one or more Docker containers scheduled to a cluster host. Kubernetes assumes that workloads are containers. While Kubernetes has the notion of <a href=/docs/concepts/workloads/controllers/cron-jobs/>Cron Jobs</a> and <a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>Jobs</a> that run to completion, applications deployed on Kubernetes are typically long-running services, like web servers, load balancers or data stores and while they are highly dynamic with pods coming and going, they differ greatly from HPC application patterns.</p><p>Traditional HPC applications often exhibit different characteristics:</p><ul><li>In financial or engineering simulations, a job may be comprised of tens of thousands of short-running tasks, demanding low-latency and high-throughput scheduling to complete a simulation in an acceptable amount of time.</li><li>A computational fluid dynamics (CFD) problem may execute in parallel across many hundred or even thousands of nodes using a message passing library to synchronize state. This requires specialized scheduling and job management features to allocate and launch such jobs and then to checkpoint, suspend/resume or backfill them.</li><li>Other HPC workloads may require specialized resources like GPUs or require access to limited software licenses. Organizations may enforce policies around what types of resources can be used by whom to ensure projects are adequately resourced and deadlines are met.</li></ul><p>HPC workload schedulers have evolved to support exactly these kinds of workloads. Examples include <a href=http://www.univa.com/products/>Univa Grid Engine</a>, <a href=https://www-03.ibm.com/systems/spectrum-computing/products/lsf/>IBM Spectrum LSF</a> and Altair’s <a href="http://www.pbsworks.com/PBSProduct.aspx?n=PBS-Professional&c=Overview-and-Capabilities">PBS Professional</a>. Sites managing HPC workloads have come to rely on capabilities like array jobs, configurable pre-emption, user, group or project based quotas and a variety of other features.</p><h2 id=blurring-the-lines-between-containers-and-hpc>Blurring the lines between containers and HPC</h2><p>HPC users believe containers are valuable for the same reasons as other organizations. Packaging logic in a container to make it portable, insulated from environmental dependencies, and easily exchanged with other containers clearly has value. However, making the switch to containers can be difficult.</p><p>HPC workloads are often integrated at the command line level. Rather than requiring coding, jobs are submitted to queues via the command line as binaries or simple shell scripts that act as wrappers. There are literally hundreds of engineering, scientific and analytic applications used by HPC sites that take this approach and have mature and certified integrations with popular workload schedulers.</p><p>While the notion of packaging a workload into a Docker container, publishing it to a registry, and submitting a YAML description of the workload is second nature to users of Kubernetes, this is foreign to most HPC users. An analyst running models in R, MATLAB or Stata simply wants to submit their simulation quickly, monitor their execution, and get a result as quickly as possible.</p><h2 id=existing-approaches>Existing approaches</h2><p>To deal with the challenges of migrating to containers, organizations running container and HPC workloads have several options:</p><ul><li>Maintain separate infrastructures</li></ul><p>For sites with sunk investments in HPC, this may be a preferred approach. Rather than disrupt existing environments, it may be easier to deploy new containerized applications on a separate cluster and leave the HPC environment alone. The challenge is that this comes at the cost of siloed clusters, increasing infrastructure and management cost.</p><ul><li>Run containerized workloads under an existing HPC workload manager</li></ul><p>For sites running traditional HPC workloads, another approach is to use existing job submission mechanisms to launch jobs that in turn instantiate Docker containers on one or more target hosts. Sites using this approach can introduce containerized workloads with minimal disruption to their environment. Leading HPC workload managers such as <a href=http://blogs.univa.com/2016/05/new-version-of-univa-grid-engine-now-supports-docker-containers/>Univa Grid Engine Container Edition</a> and <a href=http://blogs.univa.com/2016/05/new-version-of-univa-grid-engine-now-supports-docker-containers/>IBM Spectrum LSF</a> are adding native support for Docker containers. <a href=https://github.com/NERSC/shifter>Shifter</a> and <a href=http://singularity.lbl.gov/>Singularity</a> are important open source tools supporting this type of deployment also. While this is a good solution for sites with simple requirements that want to stick with their HPC scheduler, they will not have access to native Kubernetes features, and this may constrain flexibility in managing long-running services where Kubernetes excels.</p><ul><li>Use native job scheduling features in Kubernetes</li></ul><p>Sites less invested in existing HPC applications can use existing scheduling facilities in Kubernetes for <a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>jobs that run to completion</a>. While this is an option, it may be impractical for many HPC users. HPC applications are often either optimized towards massive throughput or large scale parallelism. In both cases startup and teardown latencies have a discriminating impact. Latencies that appear to be acceptable for containerized microservices today would render such applications unable to scale to the required levels.</p><p>All of these solutions involve tradeoffs. The first option doesn’t allow resources to be shared (increasing costs) and the second and third options require customers to pick a single scheduler, constraining future flexibility.</p><h2 id=mixed-workloads-on-kubernetes>Mixed workloads on Kubernetes</h2><p>A better approach is to support HPC and container workloads natively in the same shared environment. Ideally, users should see the environment appropriate to their workload or workflow type.</p><p>One approach to supporting mixed workloads is to allow Kubernetes and the HPC workload manager to co-exist on the same cluster, throttling resources to avoid conflicts. While simple, this means that neither workload manager can fully utilize the cluster.</p><p>Another approach is to use a peer scheduler that coordinates with the Kubernetes scheduler. Navops Command by Univa is a solution that takes this third approach, augmenting the functionality of the Kubernetes scheduler. Navops Command provides its own web interface and CLI and allows additional scheduling policies to be enabled on Kubernetes without impacting the operation of the Kubernetes scheduler and existing containerized applications. Navops Command plugs into the Kubernetes architecture via the 'schedulerName' attribute in the pod spec as a peer scheduler that workloads can choose to use instead of the Kubernetes stock scheduler as shown below.</p><p><img src=https://lh6.googleusercontent.com/nKTtfQVVmL4qBoSR0lBmBuLt8KOrVEyjn9YcAu7hrhhV-rwnxRY3p-Y5Qfddf7BI6u1KN85VKfeaaU74xDl-oDk5NzybdIxAp0SJ42x14gwzpmwLwjVy5nIng6K8Ih-bRDlOmA9j alt="Screen Shot 2017-08-15 at 9.15.45 AM.png"></p><p>With this approach, Kubernetes acts as a resource manager, making resources available to a separate HPC scheduler. Cluster administrators can use a visual interface to allocate resources based on policy or simply drag sliders via a web UI to allocate different proportions of the Kubernetes environment to non-container (HPC) workloads, and native Kubernetes applications and services.</p><p><img src=https://lh6.googleusercontent.com/wSBBl5d-YL4_UCYgvHpE_XzijtqftSi6PTHJLGfHr5nAxmTj945jQB-pMNIGLovWwKWGnEsPjCkCPrUMWZEs9UHnQPPDSWPEl-Gl76Yczd-Yn65pEE8mKC-Asj3zP5xyfZc-r2qU-YmmOyBhLQ alt></p><p>From a client perspective, the HPC scheduler runs as a service deployed in Kubernetes pods, operating just as it would on a bare metal cluster. Navops Command provides additional scheduling features including things like resource reservation, run-time quotas, workload preemption and more. This environment works equally well for on-premise, cloud-based or hybrid deployments.</p><h2 id=deploying-mixed-workloads-at-ihme>Deploying mixed workloads at IHME</h2><p>One client having success with mixed workloads is the Institute for Health Metrics & Evaluation (IHME), an independent health research center at the University of Washington. In support of their globally recognized Global Health Data Exchange (GHDx), IHME operates a significantly sized environment comprised of 500 nodes and 20,000 cores running a mix of analytic, HPC, and container-based applications on Kubernetes. <a href=http://navops.io/ihme-case-study.html>This case study</a> describes IHME’s success hosting existing HPC workloads on a shared Kubernetes cluster using Navops Command.</p><p><img src=https://lh5.googleusercontent.com/GJeP6e89r6drl72yzZM_OsZ81MYDp7Zm5xEFpItpmioian3lOp535H4jy1_eELKrzGMYr_wnjGwpK3Uku9dwg2-vqmMC1A1GrMtJc-PZR6GR6Z-fAZNJMEr_Uw3HqvWvi86mF_63XTozysaLpg alt></p><p>For sites deploying new clusters that want access to the rich capabilities in Kubernetes but need the flexibility to run non-containerized workloads, this approach is worth a look. It offers the opportunity for sites to share infrastructure between Kubernetes and HPC workloads without disrupting existing applications and businesses processes. It also allows them to migrate their HPC workloads to use Docker containers at their own pace.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6eeb81d6317e8723709eeca85651ebea>High Performance Networking with EC2 Virtual Private Clouds</h1><div class="td-byline mb-4"><time datetime=2017-08-11 class=text-muted>Friday, August 11, 2017</time></div><p>One of the most popular platforms for running Kubernetes is Amazon Web Services’ Elastic Compute Cloud (AWS EC2). With more than a decade of experience delivering IaaS, and expanding over time to include a rich set of services with easy to consume APIs, EC2 has captured developer mindshare and loyalty worldwide.</p><p>When it comes to networking, however, EC2 has some limits that hinder performance and make deploying Kubernetes clusters to production unnecessarily complex. The preview release of <a href=http://romana.io/>Romana v2.0</a>, a network and security automation solution for Cloud Native applications, includes features that address some well known network issues when running Kubernetes in EC2.</p><h2 id=traditional-vpc-networking-performance-roadblocks>Traditional VPC Networking Performance Roadblocks</h2><p>A Kubernetes pod network is separate from an Amazon Virtual Private Cloud (VPC) instance network; consequently, off-instance pod traffic needs a route to the destination pods. Fortunately, VPCs support setting these routes. When building a cluster network with the <a href=/docs/concepts/cluster-administration/network-plugins/#kubenet>kubenet</a> plugin, whenever new nodes are added, the AWS cloud provider will automatically add a VPC route to the pods running on that node.</p><p>Using kubenet to set routes provides native VPC network performance and visibility. However, since kubenet does not support more advanced network functions like network policy for pod traffic isolation, many users choose to run a Container Network Interface (CNI) provider on the back end.</p><p>Before Romana v2.0, all CNI network providers required an overlay when used across Availability Zones (AZs), leaving CNI users who want to deploy HA clusters unable to get the performance of native VPC networking.</p><p>Even users who don’t need advanced networking encounter restriction, since the VPC route tables support a maximum of 50 entries, which limits the size of a cluster to 50 nodes (or less, if some VPC routes are needed for other purposes). Until Romana v2.0, users also needed to run an overlay network to get around this limit.</p><p>Whether you were interested in advanced networking for traffic isolation or running large production HA clusters (or both), you were unable to get the performance and visibility of native VPC networking.</p><p><img src=https://ia601500.us.archive.org/12/items/hpc-ec2-vpc-2/hpn-ec2-vpc.png alt></p><h2 id=kubernetes-on-multi-segment-networks>Kubernetes on Multi-Segment Networks</h2><p>The way to avoid running out of VPC routes is to use them sparingly by making them forward pod traffic for multiple instances. From a networking perspective, what that means is that the VPC route needs to forward to a router, which can then forward traffic on to the final destination instance.</p><p><a href=http://romana.io/>Romana</a> is a CNI network provider that configures routes on the host to forward pod network traffic without an overlay. Since inter-node routes are installed on hosts, no VPC routes are necessary at all. However, when the VPC is split into subnets for an HA deployment across zones, VPC routes are necessary.</p><p>Fortunately, inter-node routes on hosts allows them to act as a network router and forward traffic inbound from another zone just as it would for traffic from local pods. This makes any Kubernetes node configured by Romana able to accept inbound pod traffic from other zones and forward it to the proper destination node on the subnet.</p><p>Because of this local routing function, top-level routes to pods on other instances on the subnet can be aggregated, collapsing the total number of routes necessary to as few as one per subnet. To avoid using a single instance to forward all traffic, more routes can be used to spread traffic across multiple instances, up to the maximum number of available routes (i.e. equivalent to kubenet).</p><p>The net result is that you can now build clusters of any size across AZs without an overlay. Romana clusters also support network policies for better security through network isolation.</p><h2 id=making-it-all-work>Making it All Work</h2><p>While the combination of aggregated routes and node forwarding on a subnet eliminates overlays and avoids the VPC 50 route limitation, it imposes certain requirements on the CNI provider. For example, hosts should be configured with inter-node routes only to other nodes in the same zone on the local subnet. Traffic to all other hosts must use the default route off host, then use the (aggregated) VPC route to forward traffic out of the zone. Also: when adding a new host, in order to maintain aggregated VPC routes, the CNI plugin needs to use IP addresses for pods that are reachable on the new host.</p><p>The latest release of Romana also addresses questions about how VPC routes are installed; what happens when a node that is forwarding traffic fails; how forwarding node failures are detected; and how routes get updated and the cluster recovers.</p><p>Romana v2.0 includes a new AWS route configuration function to set VPC routes. This is part of a new set of network advertising features that automate route configuration in L3 networks. Romana v2.0 includes topology-aware IP address management (IPAM) that enables VPC route aggregation to stay within the 50 route limit as described here, as well as new health checks to update VPC routes when a routing instance fails. For smaller clusters, Romana configures VPC routes as kubenet does, with a route to each instance, taking advantage of every available VPC route.</p><h2 id=native-vpc-networking-everywhere>Native VPC Networking Everywhere</h2><p>When using Romana v2.0, native VPC networking is now available for clusters of any size, with or without network policies and for HA production deployment split across multiple zones.</p><p><img src=https://archive.org/download/hpc-ec2-vpc-2/hpc-ec2-vpc-2.png alt></p><p>The preview release of Romana v2.0 is available <a href=http://romana.io/preview>here</a>. We welcome comments and feedback so we can make EC2 deployments of Kubernetes as fast and reliable as possible.</p><p>-- <em>Juergen Brendel and Chris Marino, co-founders of Pani Networks, sponsor of the Romana project</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-dc37213cd51941b1ae482892c4d1abef>Kompose Helps Developers Move Docker Compose Files to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-08-10 class=text-muted>Thursday, August 10, 2017</time></div><p><em>Editor's note: today's post is by Charlie Drage, Software Engineer at Red Hat giving an update about the Kubernetes project Kompose.</em></p><p>I'm pleased to announce that <a href=https://github.com/kubernetes/kompose>Kompose</a>, a conversion tool for developers to transition Docker Compose applications to Kubernetes, has graduated from the <a href=https://github.com/kubernetes/community/blob/master/incubator.md>Kubernetes Incubator</a> to become an official part of the project.</p><p>Since our first commit on June 27, 2016, Kompose has achieved 13 releases over 851 commits, gaining 21 contributors since the inception of the project. Our work started at Skippbox (now part of <a href=https://bitnami.com/>Bitnami</a>) and grew through contributions from Google and Red Hat.</p><p>The Kubernetes Incubator allowed contributors to get to know each other across companies, as well as collaborate effectively under guidance from Kubernetes contributors and maintainers. Our incubation led to the development and release of a new and useful tool for the Kubernetes ecosystem.</p><p>We’ve created a reliable, scalable Kubernetes environment from an initial Docker Compose file. We worked hard to convert as many keys as possible to their Kubernetes equivalent. Running a single command gets you up and running on Kubernetes: kompose up.</p><p>We couldn’t have done it without feedback and contributions from the community!</p><p>If you haven’t yet tried <a href=https://github.com/kubernetes/kompose>Kompose on GitHub</a> check it out!</p><p>Kubernetes guestbook</p><p>The go-to example for Kubernetes is the famous <a href=https://github.com/kubernetes/examples/blob/master/guestbook>guestbook</a>, which we use as a base for conversion.</p><p>Here is an example from the official <a href=https://kompose.io/>kompose.io</a> site, starting with a simple Docker Compose <a href=https://raw.githubusercontent.com/kubernetes/kompose/master/examples/docker-compose.yaml>file</a>).</p><p>First, we’ll retrieve the file:</p><pre><code>$ wget https://raw.githubusercontent.com/kubernetes/kompose/master/examples/docker-compose.yaml
</code></pre><p>You can test it out by first deploying to Docker Compose:</p><pre><code>$ docker-compose up -d

Creating network &quot;examples\_default&quot; with the default driver

Creating examples\_redis-slave\_1

Creating examples\_frontend\_1

Creating examples\_redis-master\_1
</code></pre><p>And when you’re ready to deploy to Kubernetes:</p><pre><code>$ kompose up


We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application.


If you need different kind of resources, use the kompose convert and kubectl create -f commands instead.


INFO Successfully created Service: redis          

INFO Successfully created Service: web            

INFO Successfully created Deployment: redis       

INFO Successfully created Deployment: web         


Your application has been deployed to Kubernetes. You can run kubectl get deployment,svc,pods,pvc for details
</code></pre><p>Check out <a href=https://github.com/kubernetes/kompose/tree/master/examples>other examples</a> of what Kompose can do.</p><p>Converting to alternative Kubernetes controllers</p><p>Kompose can also convert to specific Kubernetes controllers with the use of flags:</p><pre><code>$ kompose convert --help  

Usage:

  kompose convert [file] [flags]


Kubernetes Flags:

      --daemon-set               Generate a Kubernetes daemonset object

  -d, --deployment               Generate a Kubernetes deployment object

  -c, --chart                    Create a Helm chart for converted objects

      --replication-controller   Generate a Kubernetes replication controller object

…
</code></pre><p>For example, let’s convert our <a href=https://github.com/kubernetes/examples/blob/master/guestbook>guestbook</a> example to a DaemonSet:</p><pre><code>$ kompose convert --daemon-set

INFO Kubernetes file &quot;frontend-service.yaml&quot; created

INFO Kubernetes file &quot;redis-master-service.yaml&quot; created

INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created

INFO Kubernetes file &quot;frontend-daemonset.yaml&quot; created

INFO Kubernetes file &quot;redis-master-daemonset.yaml&quot; created

INFO Kubernetes file &quot;redis-slave-daemonset.yaml&quot; created
</code></pre><p>Key Kompose 1.0 features</p><p>With our graduation, comes the release of Kompose 1.0.0, here’s what’s new:</p><ul><li>Docker Compose Version 3: Kompose now supports Docker Compose Version 3. New keys such as ‘deploy’ now convert to their Kubernetes equivalent.</li><li>Docker Push and Build Support: When you supply a ‘build’ key within your <code>docker-compose.yaml</code> file, Kompose will automatically build and push the image to the respective Docker repository for Kubernetes to consume.</li><li>New Keys: With the addition of version 3 support, new keys such as pid and deploy are supported. For full details on what Kompose supports, view our <a href=http://kompose.io/conversion/>conversion document</a>.</li><li>Bug Fixes: In every release we fix any bugs related to edge-cases when converting. This release fixes issues relating to converting volumes with ‘./’ in the target name.</li></ul><p>What’s ahead?</p><p>As we continue development, we will strive to convert as many Docker Compose keys as possible for all future and current Docker Compose releases, converting each one to their Kubernetes equivalent. All future releases will be backwards-compatible.</p><ul><li><a href=https://github.com/kubernetes/kompose/blob/master/docs/installation.md>Install Kompose</a></li><li><a href=https://github.com/kubernetes/kompose/blob/master/docs/installation.md>Kompose Quick Start Guide</a></li><li><a href=http://kompose.io/>Kompose Web Site</a></li><li><a href=https://github.com/kubernetes/kompose/tree/master/docs>Kompose Documentation</a></li></ul><p>--Charlie Drage, Software Engineer, Red Hat</p><ul><li>Post questions (or answer questions) on<a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on<a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter<a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on<a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on<a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6e5fb5a8969c05484c12254b6d9f8832>Happy Second Birthday: A Kubernetes Retrospective</h1><div class="td-byline mb-4"><time datetime=2017-07-28 class=text-muted>Friday, July 28, 2017</time></div><p>As we do every July, we’re excited to celebrate Kubernetes 2nd birthday! In the two years since GA 1.0 launched as an open source project, <a href=/docs/whatisk8s/>Kubernetes</a> (abbreviated as K8s) has grown to become the highest velocity cloud-related project. With more than 2,611 diverse contributors, from independents to leading global companies, the project has had 50,685 commits in the last 12 months. Of the 54 million projects on GitHub, Kubernetes is in the top 5 for number of unique developers contributing code. It also has <a href=https://www.cncf.io/blog/2017/02/27/measuring-popularity-kubernetes-using-bigquery/>more pull requests and issue comments</a> than any other project on GitHub.  </p><p><img src=https://lh3.googleusercontent.com/ldb4PfuqammWmcPiFpMa48ALxD0kGrSre0WGMpuXKqAqnKhyWEmIcJXnQcAK2sdVCiE5cvw0H2FXtLt_dVihAk4b-XTA2HIQba3A0irnRaIHup4bhFUwPLSSFmw3zFk9ZOt61TKc alt="Screen Shot 2017-07-18 at 9.39.42 AM.png"></p><p>Figure 1: Kubernetes <a href=https://www.cncf.io/blog/2017/02/27/measuring-popularity-kubernetes-using-bigquery>Rankings</a></p><p>At the center of the community are <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> with members from different companies and organizations, all with a common interest in a specific topic. Given how fast Kubernetes is growing, SIGs help nurture and distribute leadership, while advancing new proposals, designs and release updates. Here's a look at the SIG building blocks supporting Kubernetes:</p><p><img src=https://lh3.googleusercontent.com/XkN1qNDVOvRTCnY1kGVadCl6IqtI-MzSBVjEDduUpqYgScn9VN5xvlp2EPfqhmJzZbqZv40vlxkV7y9DfU4Fq4CCwYJvbF17bJKM4UB59JR8lELWyLvkrsuI4rB51sw_omXqoAq- alt="Screen Shot 2017-07-18 at 11.32.07 AM.png"></p><p>Kubernetes has also earned the trust of many <a href=https://kubernetes.io/case-studies/>Fortune 500 companies</a> with deployments at Box, Comcast, Pearson, GolfNow, eBay, Ancestry.com and contributions from CoreOS, Fujitsu, Google, Huawei, Mirantis, Red Hat, Weaveworks and ZTE Company and others. Today, on the second anniversary of the Kubernetes 1.0 launch, we take a look back at some of the major accomplishments of the last year:</p><p>July 2016</p><ul><li>Kubernauts celebrated its <a href=https://kubernetes.io/blog/2016/07/happy-k8sbday-1>first anniversary</a> of the Kubernetes 1.0 launch with 20 <a href="https://twitter.com/search?q=k8sbday&src=typd">#k8sbday</a> parties hosted worldwide</li><li>Kubernetes <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>v1.3 release</a></li></ul><p>September 2016</p><ul><li>Kubernetes <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>v1.4 release</a></li><li>Launch of <a href=https://kubernetes.io/blog/2016/09/how-we-made-kubernetes-easy-to-install>kubeadm</a>, a tool that makes Kubernetes dramatically easier to install</li><li><a href=https://www.sdxcentral.com/articles/news/google-dealt-pokemon-go-traffic-50-times-beyond-expectations/2016/09/>Pokemon Go</a> - one of the largest installs of Kubernetes ever</li></ul><p>October 2016</p><ul><li>Introduced <a href=https://kubernetes.io/blog/2016/10/kubernetes-service-technology-partners-program>Kubernetes service partners program</a> and a redesigned <a href=https://kubernetes.io/partners/>partners page</a></li></ul><p>November 2016</p><ul><li>CloudNativeCon/KubeCon <a href=https://www.cncf.io/blog/2016/11/17/cloudnativeconkubecon-2016-wrap/>Seattle</a></li><li>Cloud Native Computing Foundation partners with The Linux Foundation to launch a <a href=https://www.cncf.io/blog/2016/11/08/cncf-partners-linux-foundation-launch-new-kubernetes-certification-training-managed-service-provider-program/>new Kubernetes certification, training and managed service provider program</a></li></ul><p>December 2016</p><ul><li>Kubernetes <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>v1.5 release</a></li></ul><p>January 2017</p><ul><li><a href=https://www.cncf.io/blog/2017/01/17/container-management-trends-kubernetes-moves-testing-production/>Survey</a> from CloudNativeCon + KubeCon Seattle showcases the maturation of Kubernetes deployment</li></ul><p>March 2017</p><ul><li>CloudNativeCon/KubeCon <a href=https://www.cncf.io/blog/2017/04/17/highlights-cloudnativecon-kubecon-europe-2017/>Europe</a></li><li>Kubernetes<a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>v1.6 release</a></li></ul><p>April 2017</p><ul><li>The <a href=https://www.battery.com/powered/boss-index-tracking-explosive-growth-open-source-software/>Battery Open Source Software (BOSS) Index</a> lists Kubernetes as #33 in the top 100 popular open-source software projects</li></ul><p>May 2017</p><ul><li><a href=https://www.cncf.io/blog/2017/05/04/cncf-brings-kubernetes-coredns-opentracing-prometheus-google-summer-code-2017/>Four Kubernetes projects</a> accepted to The <a href=https://developers.google.com/open-source/gsoc/>Google Summer of Code</a> (GSOC) 2017 program</li><li>Stutterstock and Kubernetes appear in <a href=https://blogs.wsj.com/cio/2017/05/26/shutterstock-ceo-says-new-business-plan-hinged-upon-total-overhaul-of-it/>The Wall Street Journal</a>: “On average we [Shutterstock] deploy 45 different releases into production a day using that framework. We use Docker, Kubernetes and Jenkins [to build and run containers and automate development,” said CTO Marty Brodbeck on the company’s IT overhaul and adoption of containerization.</li></ul><p>June 2017</p><ul><li>Kubernetes <a href=https://kubernetes.io/blog/2017/06/kubernetes-1-7-security-hardening-stateful-application-extensibility-updates>v1.7 release</a></li><li><a href=https://www.cncf.io/blog/2017/06/28/survey-shows-kubernetes-leading-orchestration-platform/>Survey</a> from CloudNativeCon + KubeCon Europe shows Kubernetes leading as the orchestration platform of choice</li><li>Kubernetes ranked <a href=https://github.com/cncf/velocity>#4</a> in the <a href=https://www.cncf.io/blog/2017/06/05/30-highest-velocity-open-source-projects/>30 highest velocity open source projects</a></li></ul><p><img src=https://lh5.googleusercontent.com/tN_M9v5pFyr3uzwAXTliSKofTGz9DUSMotLHWgy2vl2VSsfIfysagv7h5VRkMA5L9TsNBTMX4dWr-V3O1S9d3dw9IctSj4bAyzblXCAe4xjAhnNJEA3vjSq4Cw79SfoRWfnW-zYY alt></p><p>Figure 2: The 30 highest velocity open source projects. Source: <a href=https://github.com/cncf/velocity>https://github.com/cncf/velocity</a></p><p>July 2017</p><ul><li>Kubernauts celebrate the second anniversary of the Kubernetes 1.0 launch with <a href="https://twitter.com/search?q=k8sbday&src=typd">#k8sbday</a> parties worldwide!</li></ul><p>At the one year anniversary of the Kubernetes 1.0 launch, there were 130 Kubernetes-related Meetup groups. Today, there are more than <a href=http://www.meetup.com/topics/kubernetes/>322 Meetup groups</a> with 104,195 members. Local Meetups around the world joined the <a href="https://twitter.com/search?q=k8sbday&src=typd">#k8sbday</a> celebration! Take a look at some of the pictures from their celebrations. We hope you’ll join us at CloudNativeCon + <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america>KubeCon</a>, December 6- 8 in Austin, TX.</p><p><img src=https://lh5.googleusercontent.com/xTwKqYyvb-f7a_OYFhoNTdxh65zX8Q02u172jqLrif0VYm1KdKOGuK-HgIfO3I4o7VZeATa3E5TBLJbbM70xzxOzXFcy34p3OSWTnH_m5LliZKU134FAHMECqYZxDofOwD-h2CMR alt></p><p>Celebrating at the K8s birthday party in San Francisco</p><p><img src=https://lh3.googleusercontent.com/YBqyvEtkND8hHlflmGq7wNUQquk0le13TfRO7eFMiN9ecDPModhCCe6pBvP_BrjrLFRPsn9RpMiR4bal2PazNbqO_KlkKBEB6L2EaOU1LhJuv6bIsWW-mkr3ffGqTPfbJKjaqCQK alt=k8sbday-rtp.jpg-large></p><p>Celebrating in RTP, NC with a presentation from Jason McGee, VP and CTO, IBM Cloud Platform. Photo courtesy of <a href=https://twitter.com/FranklyBriana>@FranklyBriana</a></p><p><img src=https://lh6.googleusercontent.com/KtleeU4vkSvalQqiCj4tiDcDjAStZ17Ttxx_KTuKR-Mm2Og35bsZLYTttu_HoJyJxF5wAFTDrXOowGtfXiplCaZjh8_YKnO8gUDhtib79o3kf80VWmL0tV8nHrn_MMBOFLOS_8XZ alt=k8sbday-sg.jpg-large></p><p>The Kubernetes Singapore meetup celebrating with an intro to GKE. Photo courtesy of <a href=https://twitter.com/hunternield>@hunternield</a></p><p><img src=https://lh6.googleusercontent.com/JZ_moLy996JVn-ajABO0-T7g4B8IHDnhI4chHAp6so_9gyQsbApDWyk0SOMUWPeLSbNph50YQFjZcqQaWth5QckbMln6Jz3lpk1EjKRvba6rR_OZUgdU3fW6FNG-guiCTvixMOMV alt> <img src=https://lh3.googleusercontent.com/62TuMsCRMOXk8ly-MEy0yYmNY09zRBSWM6IgmU5BmcDnLvx2lEqsBdtEYsK_QV7GCGYF5XoY-mhLruxiBz0UZbq9CFzJ_twg5NuX5CoHHYEOzjTw3sp57NnS2eM-iMQLBC5dRdiC alt></p><p>New York celebrated with mini k8s cupcakes and a presentation on the history of cloud native from CNCF Executive Director, Dan Kohn. Photo courtesy of <a href=https://twitter.com/arieljatib>@arieljatib</a> and <a href="https://twitter.com/coreos?lang=en">@coreos</a></p><p><img src=https://lh6.googleusercontent.com/XomXc8LI79M1M5XLmZmXUpRYSOBj5HUwYTxOlgBKwoC00P8jWr-Aqam_c_IS9S69RWGt6hFc50BLHPQN41ZEV8Wsx4QRhym4hWGjlgDbLYMKJpsZ4CfGkLPh_rInZy5PrttqPmQJ alt> <img src=https://lh4.googleusercontent.com/HlNGI5bwJBNFf9q5UkqM2l0--ieJegbbnF5lqiVas6Rp5PzKJ-XHnEjUxWQnvE21WUKrp5HiKLi3VoiG-QFZFSPPTQ74rV9nGxC-d4xFwZzENGdOWUSCwSdzd4XcgGjyAGNgqi72 alt></p><p>Quebec City had custom k8s cupcakes too! Photo courtesy of <a href=https://twitter.com/zig_max>@zig_max</a><a href=https://twitter.com/zig_max></a></p><p><img src=https://lh6.googleusercontent.com/4JzZ0zCe98tfL5pMgTKDWPQo4HPfGq6WvArQgOU7FbwtjoiRIQiJWZmke7KDlL0m22jpEFmzYg92dJx6aXySiM37VKaOuf7sybdplRvw4F5gJlL9lP-lEO87m735Gd4QZYMiYB2Y alt></p><p>Beijing celebrated with custom k8s lollipops. Photo courtesy of <a href=https://twitter.com/maxwell9215>@maxwell9215</a></p><p><em>-- Sarah Novotny, Program Manager, Kubernetes Community </em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8e70c4f34c6fee1bd77b95674c6aab11>How Watson Health Cloud Deploys Applications with Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-07-14 class=text-muted>Friday, July 14, 2017</time></div><p>Today’s post is by <a href=https://www.linkedin.com/in/sandhyakapoor/>Sandhya Kapoor</a>, Senior Technologist, Watson Platform for Health, IBM</p><p>For more than a year, Watson Platform for Health at IBM deployed healthcare applications in virtual machines on our cloud platform. Because virtual machines had been a costly, heavyweight solution for us, we were interested to evaluate Kubernetes for our deployments.</p><p>Our design was to set up the application and data containers in the same namespace, along with the required agents using sidecars, to meet security and compliance requirements in the healthcare industry.</p><p>I was able to run more processes on a single physical server than I could using a virtual machine. Also, running our applications in containers ensured optimal usage of system resources.</p><p>To orchestrate container deployment, we are using <a href=https://cloud.ibm.com/containers-kubernetes/landing>IBM Cloud Kubernetes Service infrastructure</a>, a Kubernetes implementation by IBM for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure.</p><p>With Kubernetes, our developers can rapidly develop highly available applications by leveraging the power and flexibility of containers, and with integrated and secure volume service, we can store persistent data, share data between Kubernetes pods, and restore data when needed.</p><p>Here is a snapshot of Watson Care Manager, running inside a Kubernetes cluster:</p><p><img src=https://lh4.googleusercontent.com/LeKfLOkNldqReFh47f2AuFU42dhvKDwDxac_Psil_bdZWldKY80ZZi4Rv3n0--jq8Mqq9qRFVa1AbLIt9TIPLLRVmon4DaBsltFYbUJikrOp0qcavJQ9XHjRL-A1yvWR6mTNayBP alt></p><p><img src=https://lh3.googleusercontent.com/EU3DgtFKagWp5S0UpKj-wRgx8WK2nvQ2BG-4dGio57pGNj42A7Lip9IARBba34hIm84-_7zwWt6iImQE8beSqLxpzXm-2w_84M_X2IHQ7jvpWtIDMF81hmq6N4hGSxp6DQoFW5qX alt></p><p>Before deploying an app, a user must create a worker node cluster. I can create a cluster using the kubectl cli commands or create it from the <a href=https://cloud.ibm.com/>IBM Cloud</a> dashboard.</p><p>Our clusters consist of one or more physical or virtual machines, also known as worker nodes, that are loosely coupled, extensible, and centrally monitored and managed by the Kubernetes master. When we deploy a containerized app, the Kubernetes master decides where to deploy the app, taking into consideration the deployment requirements and available capacity in the cluster.</p><p>A user makes a request to Kubernetes to deploy the containers, specifying the number of replicas required for high availability. The Kubernetes scheduler decides where the <a href=/docs/concepts/workloads/pods/pod/>pods</a> (groups of one or more containers) will be scheduled and which worker nodes they will be deployed on, storing this information internally in Kubernetes and <a href=https://github.com/coreos/etcd#etcd>etcd</a>. The deployment of pods in worker nodes is updated based on load at runtime, optimizing the placement of pods in the cluster.</p><p>Kubelet running in each worker node regularly polls the kube API server. If there is new work to do, kubelet pulls the configuration information and takes action, for example, spinning off a new pod.</p><p>Process Flow:</p><p>| <img src=https://lh6.googleusercontent.com/jckmDLJIsy6m8Dxj6GZ6yv5vmQqrZXAi42eJz8iIefl2A87LXoRJUubCkSh05Ptaojt_faEFq4G6UMfZZYVOUiaEzt8Erp51xbyRWW_08qn9vvz-WvztBNlrG431YgI6880-ZULO alt> |
| UCD – IBM UrbanCode Deploy is a tool for automating application deployments through your environments. WH Cluster – Kubernetes worker node. |</p><p>Usage of GitLab in the Process Flow:</p><p>We stored all our artifacts in GitLab, which includes the Docker files that are required for creating the image, YAML files needed to create a pod, and the configuration files to make the Healthcare application run.</p><p>GitLab and Jenkins interaction in the Process Flow:</p><p>We use Jenkins for continuous integration and build automation to create/pull/retag the Docker image and push the image to a Docker registry in the cloud.</p><p>Basically, we have a Jenkins job configured to interact with GitLab project to get the latest artifacts and, based on requirements, it will either create a new Docker image from scratch by pulling the needed intermediate images from Docker/Bluemix repository or update the Docker image.</p><p>After the image is created/updated the Jenkins job pushes the image to a Bluemix repository to save the latest image to be pulled by UrbanCode Deploy (UCD) component.</p><p>Jenkins and UCD interaction in the Process Flow:</p><p>The Jenkins job is configured to use the UCD component and its respective application, application process, and the UCD environment to deploy the application. The Docker image version files that will be used by the UCD component are also passed via Jenkins job to the UCD component.</p><p>Usage of UCD in the Process Flow:</p><p>UCD is used for deployment and the end-to end deployment process is automated here. UCD component process involves the following steps:</p><ul><li>Download the required artifacts for deployment from the Gitlab.</li><li>Login to Bluemix and set the KUBECONFIG based on the Kubernetes cluster used for creating the pods.</li><li>Create the application pod in the cluster using kubectl create command.</li><li>If needed, run a rolling update to update the existing pod.</li></ul><p><img src=https://lh4.googleusercontent.com/laBRZK_ifwLXGkLL8fl0fZbUmm-HI4nC-tUNIFAy2wg4UHQT97reKyNOrNydYS8PmnhgqsBQctYCLTjJF12KR_uuVUdqiNx-B1OP1YrBwL2vi5SlEO9RSFQEbs-X6FoMHw0QK53A alt></p><p>Deploying the application in IBM Cloud Kubernetes Service:</p><p>Provision a cluster in IBM Cloud Kubernetes Service with &lt;x> worker nodes. Create Kubernetes controllers for deploying the containers in worker nodes, the IBM Cloud Kubernetes Service infrastructure pulls the Docker images from IBM Cloud Container Registry to create containers. We tried deploying an application container and running a logmet agent (see Reading and displaying logs using logmet container, below) inside the containers that forwards the application logs to an IBM Cloud logging service. As part of the process, YAML files are used to create a controller resource for the UrbanCode Deploy (UCD). UCD agent is deployed as a <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> controller, which is used to connect to the UCD server. The whole process of deployment of application happens in UCD. To support the application for public access, we created a service resource to interact between pods and access container services. For storage support, we created persistent volume claims and mounted the volume for the containers.</p><p>| <img src=https://lh6.googleusercontent.com/iFKlbBX8rjWTuygIfjImdxP8R7xXuvaaoDwldEIC3VRL03XIehxagz8uePpXllYMSxoyai5a6N-0NB4aTGK9fwwd8leFyfypxtbmaWBK-b2Kh9awcA76-_82F7ZZl7lgbf0gyFN7 alt> |
| UCD: IBM UrbanCode Deploy is a tool for automating application deployments through your environments. IBM Cloud Kubernetes Service: Kubernetes implementation of IBM. WH Docker Registry: Docker Private image registry. Common agent containers: We expect to configure our services to use the WHC mandatory agents. We deployed all ion containers. |</p><p>Reading and displaying logs using logmet container:</p><p>Logmet is a cloud logging service that helps to collect, store, and analyze an application’s log data. It also aggregates application and environment logs for consolidated application or environment insights and forwards them. Metrics are transmitted with collectd. We chose a model that runs a logmet agent process inside the container. The agent takes care of forwarding the logs to the cloud logging service configured in containers.</p><p>The application pod mounts the application logging directory to the storage space, which is created by persistent volume claim, and stores the logs, which are not lost even when the pod dies. Kibana is an open source data visualization plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster.</p><p><img src=https://lh3.googleusercontent.com/Fat60VoOQ6CBxHgAdva9Xwcu1X4coZFlld1eS7ZrB4MbTR9HbwyuXgQ6CncXxeZ_mWqWzpTatB7bOB199QCcCaY8905yAqzMO0-Rx4NNnYj94uXHEy_dwLbLVFQJvQTu8cGW8HSz alt></p><p>Exposing services with Ingress:</p><p><a href=/docs/concepts/services-networking/ingress/#ingress-controllers>Ingress controllers</a> are reverse proxies that expose services outside cluster through URLs. They act as an external HTTP load balancer that uses a unique public entry point to route requests to the application.</p><p>To expose our services to outside the cluster, we used Ingress. In IBM Cloud Kubernetes Service, if we create a paid cluster, an Ingress controller is automatically installed for us to use. We were able to access services through Ingress by creating a YAML resource file that specifies the service path.</p><p>–Sandhya Kapoor, Senior Technologist, Watson Platform for Health, IBM</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c7f4a9fd3148d9d2aacf9908e495a5c4>Kubernetes 1.7: Security Hardening, Stateful Application Updates and Extensibility</h1><div class="td-byline mb-4"><time datetime=2017-06-30 class=text-muted>Friday, June 30, 2017</time></div><p>Today we’re announcing Kubernetes 1.7, a milestone release that adds security, storage and extensibility features motivated by widespread production use of Kubernetes in the most demanding enterprise environments. </p><p>At-a-glance, security enhancements in this release include encrypted secrets, network policy for pod-to-pod communication, node authorizer to limit kubelet access and client / server TLS certificate rotation. </p><p>For those of you running scale-out databases on Kubernetes, this release has a major feature that adds automated updates to StatefulSets and enhances updates for DaemonSets. We are also announcing alpha support for local storage and a burst mode for scaling StatefulSets faster. </p><p>Also, for power users, API aggregation in this release allows user-provided apiservers to be served along with the rest of the Kubernetes API at runtime. Additional highlights include support for extensible admission controllers, pluggable cloud providers, and container runtime interface (CRI) enhancements.</p><p><strong>What’s New</strong><br>Security:</p><ul><li><a href=/docs/concepts/services-networking/network-policies/>The Network Policy API</a> is promoted to stable. Network policy, implemented through a network plug-in, allows users to set and enforce rules governing which pods can communicate with each other. </li><li><a href=/docs/reference/access-authn-authz/node/>Node authorizer</a> and admission control plugin are new additions that restrict kubelet’s access to secrets, pods and other objects based on its node.</li><li><a href=/docs/tasks/administer-cluster/encrypt-data/>Encryption for Secrets</a>, and other resources in etcd, is now available as alpha. </li><li><a href=/docs/admin/kubelet-tls-bootstrapping/>Kubelet TLS bootstrapping</a> now supports client and server certificate rotation.</li><li><a href=/docs/tasks/debug-application-cluster/audit/>Audit logs</a> stored by the API server are now more customizable and extensible with support for event filtering and webhooks. They also provide richer data for system audit.</li></ul><p>Stateful workloads:</p><ul><li><a href=/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets>StatefulSet Updates</a> is a new beta feature in 1.7, allowing automated updates of stateful applications such as Kafka, Zookeeper and etcd, using a range of update strategies including rolling updates.</li><li>StatefulSets also now support faster scaling and startup for applications that do not require ordering through <a href=/docs/concepts/workloads/controllers/statefulset/#pod-management-policies>Pod Management Policy</a>. This can be a major performance improvement. </li><li><a href=/docs/concepts/storage/volumes/#local>Local Storage</a> (alpha) was one of most frequently requested features for stateful applications. Users can now access local storage volumes through the standard PVC/PV interface and via StorageClasses in StatefulSets.</li><li>DaemonSets, which create one pod per node already have an update feature, and in 1.7 have added smart <a href=/docs/tasks/manage-daemon/rollback-daemon-set/>rollback and history</a> capability.</li><li>A new <a href=/docs/concepts/storage/volumes/#storageos>StorageOS Volume plugin</a> provides highly-available cluster-wide persistent volumes from local or attached node storage.</li></ul><p>Extensibility:</p><ul><li><p><a href=/docs/concepts/api-extension/apiserver-aggregation/>API aggregation</a> at runtime is the most powerful extensibility features in this release, allowing power users to add Kubernetes-style pre-built, 3rd party or user-created APIs to their cluster.</p></li><li><p><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Container Runtime Interface</a> (CRI) has been enhanced with New RPC calls to retrieve container metrics from the runtime. <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md>Validation tests for the CRI</a> have been published and Alpha integration with <a href=http://containerd.io/>containerd</a>, which supports basic pod lifecycle and image management is now available. Read our previous <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes>in-depth post introducing CRI</a>.</p></li></ul><p>Additional Features:</p><ul><li>Alpha support for <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>external admission controllers</a> is introduced, providing two options for adding custom business logic to the API server for modifying objects as they are created and validating policy. </li><li><a href=/docs/tasks/federation/set-up-placement-policies-federation/>Policy-based Federated Resource Placement</a> is introduced as Alpha providing placement policies for the federated clusters, based on custom requirements such as regulation, pricing or performance.</li></ul><p>Deprecation: </p><ul><li>Third Party Resource (TPR) has been replaced with Custom Resource Definitions (CRD) which provides a cleaner API, and resolves issues and corner cases that were raised during the beta period of TPR. If you use the TPR beta feature, you are encouraged to <a href=/docs/tasks/access-kubernetes-api/migrate-third-party-resource/>migrate</a>, as it is slated for removal by the community in Kubernetes 1.8.</li></ul><p>The above are a subset of the feature highlights in Kubernetes 1.7. For a complete list please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v170>release notes</a>.</p><p><strong>Adoption</strong><br>This release is possible thanks to our vast and open community. Together, we’ve already pushed more than 50,000 commits in just three years, and that’s only in the main Kubernetes repo. Additional extensions to Kubernetes are contributed in associated repos bringing overall stability to the project. This velocity makes Kubernetes one of the fastest growing open source projects -- ever. </p><p>Kubernetes adoption has been coming from every sector across the world. Recent user stories from the community include: </p><ul><li>GolfNow, a member of the NBC Sports Group, migrated their application to Kubernetes giving them better resource utilization and<a href=https://kubernetes.io/case-studies/golfnow>slashing their infrastructure costs in half</a>.</li><li>Bitmovin, provider of video infrastructure solutions, showed us how they’re using Kubernetes to do <a href=https://kubernetes.io/blog/2017/04/multi-stage-canary-deployments-with-kubernetes-in-the-cloud-onprem>multi-stage canary deployments</a> in the cloud and on-prem.</li><li>Ocado, world’s largest online supermarket, uses Kubernetes to create a distributed data center for their smart warehouses. Read about their full setup <a href=http://ocadotechnology.com/blog/creating-a-distributed-data-centre-architecture-using-kubernetes-and-containers/>here</a>.</li><li>Is Kubernetes helping your team? <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>Share your story</a> with the community. See our growing resource of user case studies and learn from great companies like <a href=https://kubernetes.io/case-studies/box>Box</a> that have adopted Kubernetes in their organization. </li></ul><p>Huge kudos and thanks go out to the Kubernetes 1.7 <a href=https://github.com/kubernetes/features/blob/master/release-1.7/release_team.md>release team</a>, led by Dawn Chen of Google. </p><p><strong>Availability</strong><br>Kubernetes 1.7 is available for <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.7.0>download on GitHub</a>. To get started with Kubernetes, try one of the these <a href=/docs/tutorials/kubernetes-basics/>interactive tutorials</a>. </p><p><strong>Get Involved</strong><br>Join the community at <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america>CloudNativeCon + KubeCon</a> in Austin Dec. 6-8 for the largest Kubernetes gathering ever. <a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-north-america/program/cfp>Speaking submissions</a> are open till August 21 and <a href="https://www.regonline.com/registration/Checkin.aspx?EventID=1903774&_ga=2.224109086.464556664.1498490094-1623727562.1496428006">discounted registration</a> ends October 6.</p><p>The simplest way to get involved is joining one of the many <a href=https://github.com/kubernetes/community/blob/master/sig-list.md>Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>, and these channels:</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Share your Kubernetes <a href=https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>story</a>. </li></ul><p>Many thanks to our vast community of contributors and supporters in making this and all releases possible.</p><p><em>-- Aparna Sinha, Group Product Manager, Kubernetes Google and Ihor Dvoretskyi, Program Manager, Kubernetes Mirantis</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-3673321caf498b340318f7e226fd10bf>Managing microservices with the Istio service mesh</h1><div class="td-byline mb-4"><time datetime=2017-05-31 class=text-muted>Wednesday, May 31, 2017</time></div><p><em>Today’s post is by the Istio team showing how you can get visibility, resiliency, security and control for your microservices in Kubernetes.</em></p><p>Services are at the core of modern software architecture. Deploying a series of modular, small (micro-)services rather than big monoliths gives developers the flexibility to work in different languages, technologies and release cadence across the system; resulting in higher productivity and velocity, especially for larger teams.</p><p>With the adoption of microservices, however, new problems emerge due to the sheer number of services that exist in a larger system. Problems that had to be solved once for a monolith, like security, load balancing, monitoring, and rate limiting need to be handled for each service.</p><p><strong>Kubernetes and Services</strong></p><p>Kubernetes supports a microservices architecture through the <a href=/docs/concepts/services-networking/service/>Service</a> construct. It allows developers to abstract away the functionality of a set of <a href=/docs/concepts/workloads/pods/pod/>Pods</a>, and expose it to other developers through a well-defined API. It allows adding a name to this level of abstraction and perform rudimentary L4 load balancing. But it doesn’t help with higher-level problems, such as L7 metrics, traffic splitting, rate limiting, circuit breaking, etc.</p><p><a href=https://istio.io/>Istio</a>, announced last week at GlueCon 2017, addresses these problems in a fundamental way through a service mesh framework. With Istio, developers can implement the core logic for the microservices, and let the framework take care of the rest – traffic management, discovery, service identity and security, and policy enforcement. Better yet, this can be also done for existing microservices without rewriting or recompiling any of their parts. Istio uses <a href=https://lyft.github.io/envoy/>Envoy</a> as its runtime proxy component and provides an <a href=https://istio.io/docs/concepts/policy-and-control/mixer.html>extensible intermediation layer</a> which allows global cross-cutting policy enforcement and telemetry collection.</p><p>The current release of Istio is targeted to Kubernetes users and is packaged in a way that you can install in a few lines and get visibility, resiliency, security and control for your microservices in Kubernetes out of the box.</p><p>In a series of blog posts, we'll look at a simple application that is composed of 4 separate microservices. We'll start by looking at how the application can be deployed using plain Kubernetes. We'll then deploy the exact same services into an Istio-enabled cluster without changing any of the application code -- and see how we can observe metrics.</p><p>In subsequent posts, we’ll focus on more advanced capabilities such as HTTP request routing, policy, identity and security management.</p><p><strong>Example Application: BookInfo</strong></p><p>We will use a simple application called BookInfo, that displays information, reviews and ratings for books in a store. The application is composed of four microservices written in different languages:</p><p><img src=https://lh6.googleusercontent.com/2l4VGkujZ2U_Ujuo55vTz08JBKhMVjNgQqlnX7DZHttDhJs_rKudWsXh6kU4JkwkKZETR7ljN70zAzhb__LqC0CondM_ps3h3viYGqxfvVcIYnFhbahEjXvGEZSmmEOET1oc7dRL alt="BookInfo-all (2).png">Since the container images for these microservices can all be found in Docker Hub, all we need to deploy this application in Kubernetes are the yaml configurations.</p><p>It’s worth noting that these services have no dependencies on Kubernetes and Istio, but make an interesting case study. Particularly, the multitude of services, languages and versions for the reviews service make it an interesting service mesh example. More information about this example can be found <a href=https://istio.io/docs/samples/bookinfo.html>here</a>.</p><p><strong>Running the Bookinfo Application in Kubernetes</strong></p><p>In this post we’ll focus on the v1 version of the app:</p><p><img src=https://lh4.googleusercontent.com/yD_ktHrTzgybi2DXdRWlrGD78rQgWvcGgDoWj0Pv5QtREPsmoz5pNDd2JI_MeiXx6kIS4QKy_Ved2hXsa68AqGpLftcWlPmYtew5DJqi6fNZrBHfVymjhDCGWgoHEIuzWaf9_doP alt="BookInfo-v1 (3).png"></p><p>Deploying it with Kubernetes is straightforward, no different than deploying any other services. Service and Deployment resources for the <strong>productpage</strong> microservice looks like this:</p><pre><code>apiVersion: v1

kind: Service

metadata:

name: productpage

labels:

  app: productpage

spec:

type: NodePort

ports:

- port: 9080

  name: http

selector:

  app: productpage

---

apiVersion: extensions/v1beta1

kind: Deployment

metadata:

name: productpage-v1

spec:

replicas: 1

template:

  metadata:

    labels:

      app: productpage

      track: stable

  spec:

    containers:

    - name: productpage

      image: istio/examples-bookinfo-productpage-v1

      imagePullPolicy: IfNotPresent

      ports:

      - containerPort: 9080
</code></pre><p>The other two services that we will need to deploy if we want to run the app are <strong>details</strong> and <strong>reviews-v1</strong>. We don’t need to deploy the <strong>ratings</strong> service at this time because v1 of the reviews service doesn’t use it. The remaining services follow essentially the same pattern as <strong>productpage</strong>. The yaml files for all services can be found <a href=https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml>here</a>.</p><p>To run the services as an ordinary Kubernetes app:</p><pre><code>kubectl apply -f bookinfo-v1.yaml
</code></pre><p>To access the application from outside the cluster we’ll need the NodePort address of the <strong>productpage</strong> service:</p><pre><code>export BOOKINFO\_URL=$(kubectl get po -l app=productpage -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc productpage -o jsonpath={.spec.ports[0].nodePort})
</code></pre><p>We can now point the browser to http://$BOOKINFO_URL/productpage, and see:</p><p><img src=https://lh3.googleusercontent.com/AP3bEJR9uqsXufk5kZqD4DaRUs9ynuybfM8KBJlv_sF0g6A8LRO606jr_Z8xL71TrKWt_OfTXDJCcISZGy6ucj4KVZVFPFT8NCOOf6PpEZ0XVKlw-fgRP0iJvaBKuZaH-dySdJZ- alt></p><p><strong>Running the Bookinfo Application with Istio</strong></p><p>Now that we’ve seen the app, we’ll adjust our deployment slightly to make it work with Istio. We first need to <a href=https://istio.io/docs/tasks/installing-istio.html>install Istio</a> in our cluster. To see all of the metrics and tracing features in action, we also install the optional Prometheus, Grafana, and Zipkin addons. We can now delete the previous app and start the Bookinfo app again using the exact same yaml file, this time with Istio:</p><pre><code>kubectl delete -f bookinfo-v1.yaml

kubectl apply -f \&lt;(istioctl kube-inject -f bookinfo-v1.yaml)
</code></pre><p>Notice that this time we use the istioctl kube-inject command to modify bookinfo-v1.yaml before creating the deployments. It injects the Envoy sidecar into the Kubernetes pods as documented <a href=https://istio.io/docs/reference/commands/istioctl.html#istioctl-kube-inject>here</a>. Consequently, all of the microservices are packaged with an Envoy sidecar that manages incoming and outgoing traffic for the service.</p><p>In the Istio service mesh we will not want to access the application <strong>productpage</strong> directly, as we did in plain Kubernetes. Instead, we want an Envoy sidecar in the request path so that we can use Istio’s management features (version routing, circuit breakers, policies, etc.) to control external calls to <strong>productpage</strong> , just like we can for internal requests. Istio’s Ingress controller is used for this purpose.</p><p>To use the Istio Ingress controller, we need to create a Kubernetes <a href=https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo-ingress.yaml>Ingress resource</a> for the app, annotated with kubernetes.io/ingress.class: "istio", like this:</p><pre><code>cat \&lt;\&lt;EOF  ``` kubectl create -f -

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

name: bookinfo

annotations:

  kubernetes.io/ingress.class: &quot;istio&quot;

spec:

rules:

- http:

    paths:

    - path: /productpage

      backend:

        serviceName: productpage

        servicePort: 9080

    - path: /login

      backend:

        serviceName: productpage

        servicePort: 9080

    - path: /logout

      backend:

        serviceName: productpage

        servicePort: 9080

EOF
</code></pre><p>The resulting deployment with Istio and v1 version of the bookinfo app looks like this:</p><p><img src=https://lh3.googleusercontent.com/4gc7Yp7vX3uQjcJ7UUBTSP8szTyIn_muB9Xn8UvS8UMJ2C-OQApiX1NObwuqS92hJ42KjkUKt3otGjPtOUQhkb_qlauJA3ezOOu8KH4VchOE8DcY4JvO0aXjaTnVX_ivgbWyGqka alt="BookInfo-v1-Istio (5).png"></p><p>This time we will access the app using the NodePort address of the Istio Ingress controller:</p><pre><code>export BOOKINFO\_URL=$(kubectl get po -l istio=ingress -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc istio-ingress -o jsonpath={.spec.ports[0].nodePort})
</code></pre><p>We can now load the page at http://$BOOKINFO_URL/productpage and once again see the running app -- there should be no difference from the previous deployment without Istio for the user.</p><p>However, now that the application is running in the Istio service mesh, we can immediately start to see some benefits.</p><p><strong>Metrics collection</strong></p><p>The first thing we get from Istio out-of-the-box is the collection of metrics in Prometheus. These metrics are generated by the Istio filter in Envoy, collected according to default rules (which can be customized), and then sent to Prometheus. The metrics can be visualized in the Istio dashboard in Grafana. Note that while Prometheus is the out-of-the-box default metrics backend, Istio allows you to plug in to others, as we’ll demonstrate in future blog posts.</p><p>To demonstrate, we'll start by running the following command to generate some load on the application:</p><pre><code>wrk -t1 -c1 -d20s http://$BOOKINFO\_URL/productpage
</code></pre><p>We obtain Grafana’s NodePort URL:</p><pre><code>export GRAFANA\_URL=$(kubectl get po -l app=grafana -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc grafana -o jsonpath={.spec.ports[0].nodePort})
</code></pre><p>We can now open a browser at http://$GRAFANA_URL/dashboard/db/istio-dashboard and examine the various performance metrics for each of the Bookinfo services:</p><p><img src=https://lh5.googleusercontent.com/yFBKYWEmNxs-8VLtlJIG4BV0dUzqrvCfhWLh2CrGHyTtH5dArQy-owua3vdMCSjkdjtk8E3ZmEz32EupRL28WHALLm9MqJwCJrs1N5yv8typUJiLS_ExsO-uleaZ3bgbPraC8lgi alt=istio-dashboard-k8s-blog.png></p><p><strong>Distributed tracing</strong> The next thing we get from Istio is call tracing with Zipkin. We obtain its NodePort URL:</p><pre><code>export ZIPKIN\_URL=$(kubectl get po -l app=zipkin -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc zipkin -o jsonpath={.spec.ports[0].nodePort})
</code></pre><p>We can now point a browser at http://$ZIPKIN_URL/ to see request trace spans through the Bookinfo services.</p><p><img src=https://lh4.googleusercontent.com/qfm6Jobqaw9J6sdeG93rXb9KYb39DoVKJ0fqKFQiyi5JVEfiypAbvAOBw8OTPOgnAnv3TzDEripkOw9xCJLrwbE7jJziU_tHoyS8CFeVrGG_X0Ut1oV0OyUCB8Xo4U8UGNgm-7Ve alt></p><p>Although the Envoy proxies send trace spans to Zipkin out-of-the-box, to leverage its full potential, applications need to be Zipkin aware and forward some headers to tie the individual spans together. See <a href=https://istio.io/docs/tasks/zipkin-tracing.html>zipkin-tracing</a> for details.</p><p><strong>Holistic view of the entire fleet</strong> The metrics that Istio provides are much more than just a convenience. They provide a consistent view of the service mesh, by generating uniform metrics throughout. We don’t have to worry about reconciling different types of metrics emitted by various runtime agents, or add arbitrary agents to gather metrics for legacy uninstrumented apps. We also no longer have to rely on the development process to properly instrument the application to generate metrics. The service mesh sees all the traffic, even into and out of legacy "black box" services, and generates metrics for all of it. <strong>Summary</strong> The demo above showed how in a few steps, we can launch Istio-backed services and observe L7 metrics on them. Over the next weeks we’ll follow on with demonstration of more Istio capabilities like policy management and HTTP request routing. Google, IBM and Lyft joined forces to create Istio based on our common experiences building and operating large and complex microservice deployments for internal and enterprise customers. Istio is an industry-wide community effort. We’ve been thrilled to see the enthusiasm from the industry partners and the insights they brought. As we take the next step and release Istio to the wild, we cannot wait to see what the broader community of contributors will bring to it. If you’re using or considering to use a microservices architecture on Kubernetes, we encourage you to give Istio a try, learn about it more at <a href=http://istio.io/>istio.io</a>, let us know what you think, or better yet, <a href=https://istio.io/community/><strong>join</strong></a> the developer community to help shape its future!</p><p><em>--On behalf of the Istio team. Frank Budinsky, Software Engineer at IBM, Andra Cismaru, Software Engineer and Israel Shalom, Product Manager at Google.</em></p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2e8ec9a30d9d2f99a45ec9a89161236>Draft: Kubernetes container development made easy</h1><div class="td-byline mb-4"><time datetime=2017-05-31 class=text-muted>Wednesday, May 31, 2017</time></div><p>_Today's post is by _<em>Brendan Burns, Director of Engineering at Microsoft Azure and Kubernetes co-founder.</em></p><p>About a month ago Microsoft announced the acquisition of Deis to expand our expertise in containers and Kubernetes. Today, I’m excited to announce a new open source project derived from this newly expanded Azure team: Draft.</p><p>While by now the strengths of Kubernetes for deploying and managing applications at scale are well understood. The process of developing a new application for Kubernetes is still too hard. It’s harder still if you are new to containers, Kubernetes, or developing cloud applications.</p><p>Draft fills this role. As its name implies it is a tool that helps you begin that first draft of a containerized application running in Kubernetes. When you first run the draft tool, it automatically discovers the code that you are working on and builds out the scaffolding to support containerizing your application. Using heuristics and a variety of pre-defined project templates draft will create an initial Dockerfile to containerize your application, as well as a Helm Chart to enable your application to be deployed and maintained in a Kubernetes cluster. Teams can even bring their own draft project templates to customize the scaffolding that is built by the tool.</p><p>But the value of draft extends beyond simply scaffolding in some files to help you create your application. Draft also deploys a server into your existing Kubernetes cluster that is automatically kept in sync with the code on your laptop. Whenever you make changes to your application, the draft daemon on your laptop synchronizes that code with the draft server in Kubernetes and a new container is built and deployed automatically without any user action required. Draft enables the “inner loop” development experience for the cloud.</p><p>Of course, as is the expectation with all infrastructure software today, Draft is available as an open source project, and it itself is in “draft” form :) We eagerly invite the community to come and play around with draft today, we think it’s pretty awesome, even in this early form. But we’re especially excited to see how we can develop a community around draft to make it even more powerful for all developers of containerized applications on Kubernetes.</p><p>To give you a sense for what Draft can do, here is an example drawn from the <a href=https://github.com/Azure/draft/blob/master/docs/getting-started.md>Getting Started</a> page in the <a href=https://github.com/Azure/draft>GitHub repository</a>.</p><p>There are multiple example applications included within the <a href=https://github.com/Azure/draft/blob/master/examples>examples directory</a>. For this walkthrough, we'll be using the <a href=https://github.com/Azure/draft/tree/master/examples/example-python>python example application</a> which uses <a href=http://flask.pocoo.org/>Flask</a> to provide a very simple Hello World webserver.</p><pre><code>$ cd examples/python
</code></pre><p><strong>Draft Create</strong></p><p>We need some "scaffolding" to deploy our app into a <a href=https://kubernetes.io/>Kubernetes</a> cluster. Draft can create a <a href=https://github.com/kubernetes/helm>Helm</a> chart, a Dockerfile and a draft.toml with draft create:</p><pre><code>$ draft create

--\&gt; Python app detected

--\&gt; Ready to sail

$ ls

Dockerfile  app.py  chart/  draft.toml  requirements.txt
</code></pre><p>The chart/ and Dockerfile assets created by Draft default to a basic Python configuration. This Dockerfile harnesses the <a href=https://hub.docker.com/_/python/>python:onbuild image</a>, which will install the dependencies in requirements.txt and copy the current directory into /usr/src/app. And to align with the service values in chart/values.yaml, this Dockerfile exposes port 80 from the container.</p><p>The draft.toml file contains basic configuration about the application like the name, which namespace it will be deployed to, and whether to deploy the app automatically when local files change.</p><pre><code>$ cat draft.toml  
[environments]  
 [environments.development]  
   name = &quot;tufted-lamb&quot;  
   namespace = &quot;default&quot;  
   watch = true  
   watch\_delay = 2
</code></pre><p><strong>Draft Up</strong></p><p>Now we're ready to deploy app.py to a Kubernetes cluster.</p><p>Draft handles these tasks with one draft up command:</p><ul><li>reads configuration from draft.toml</li><li>compresses the chart/ directory and the application directory as two separate tarballs</li><li>uploads the tarballs to draftd, the server-side component</li><li>draftd then builds the docker image and pushes the image to a registry</li><li>draftd instructs helm to install the Helm chart, referencing the Docker registry image just built</li></ul><p>With the watch option set to true, we can let this run in the background while we make changes later on…</p><pre><code>$ draft up  
--\&gt; Building Dockerfile  
Step 1 : FROM python:onbuild  
onbuild: Pulling from library/python  
...  
Successfully built 38f35b50162c  
--\&gt; Pushing docker.io/microsoft/tufted-lamb:5a3c633ae76c9bdb81b55f5d4a783398bf00658e  
The push refers to a repository [docker.io/microsoft/tufted-lamb]  
...  
5a3c633ae76c9bdb81b55f5d4a783398bf00658e: digest: sha256:9d9e9fdb8ee3139dd77a110fa2d2b87573c3ff5ec9c045db6009009d1c9ebf5b size: 16384  
--\&gt; Deploying to Kubernetes  
   Release &quot;tufted-lamb&quot; does not exist. Installing it now.  
--\&gt; Status: DEPLOYED  
--\&gt; Notes:  
    1. Get the application URL by running these commands:  
    NOTE: It may take a few minutes for the LoadBalancer IP to be available.  
          You can watch the status of by running 'kubectl get svc -w tufted-lamb-tufted-lamb'  
 export SERVICE\_IP=$(kubectl get svc --namespace default tufted-lamb-tufted-lamb -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
 echo http://$SERVICE\_IP:80  

Watching local files for changes...
</code></pre><p><strong>Interact with the Deployed App</strong></p><p>Using the handy output that follows successful deployment, we can now contact our app. Note that it may take a few minutes before the load balancer is provisioned by Kubernetes. Be patient!</p><pre><code>$ export SERVICE\_IP=$(kubectl get svc --namespace default tufted-lamb-tufted-lamb -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
$ curl [http://$SERVICE\_IP](http://%24service_ip/)
</code></pre><p>When we curl our app, we see our app in action! A beautiful "Hello World!" greets us.</p><p><strong>Update the App</strong></p><p>Now, let's change the "Hello, World!" output in app.py to output "Hello, Draft!" instead:</p><pre><code>$ cat \&lt;\&lt;EOF \&gt; app.py  
from flask import Flask  

app = Flask(\_\_name\_\_)  

@app.route(&quot;/&quot;)  
def hello():  
   return &quot;Hello, Draft!\n&quot;  

if \_\_name\_\_ == &quot;\_\_main\_\_&quot;:  
   app.run(host='0.0.0.0', port=8080)  
EOF
</code></pre><p><strong>Draft Up(grade)</strong></p><p>Now if we watch the terminal that we initially called draft up with, Draft will notice that there were changes made locally and call draft up again. Draft then determines that the Helm release already exists and will perform a helm upgrade rather than attempting another helm install:</p><pre><code>--\&gt; Building Dockerfile  
Step 1 : FROM python:onbuild  
...  
Successfully built 9c90b0445146  
--\&gt; Pushing docker.io/microsoft/tufted-lamb:f031eb675112e2c942369a10815850a0b8bf190e  
The push refers to a repository [docker.io/microsoft/tufted-lamb]  
...  
--\&gt; Deploying to Kubernetes  
--\&gt; Status: DEPLOYED  
--\&gt; Notes:  
    1. Get the application URL by running these commands:  
    NOTE: It may take a few minutes for the LoadBalancer IP to be available.  
          You can watch the status of by running 'kubectl get svc -w tufted-lamb-tufted-lamb'  
 export SERVICE\_IP=$(kubectl get svc --namespace default tufted-lamb-tufted-lamb -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
 echo [http://$SERVICE\_IP:80](http://%24service_ip/)
</code></pre><p>Now when we run curl http://$SERVICE_IP, our first app has been deployed and updated to our Kubernetes cluster via Draft!</p><p>We hope this gives you a sense for everything that Draft can do to streamline development for Kubernetes. Happy drafting!</p><p><em>--Brendan Burns, Director of Engineering, Microsoft Azure</em></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-aabe890b5b99e32986fd3915a6a4e63c>Kubernetes: a monitoring guide</h1><div class="td-byline mb-4"><time datetime=2017-05-19 class=text-muted>Friday, May 19, 2017</time></div><p><em>Today’s post is by Jean-Mathieu Saponaro, Research & Analytics Engineer at Datadog, discussing what Kubernetes changes for monitoring, and how you can prepare to properly monitor a containerized infrastructure orchestrated by Kubernetes.</em></p><p>Container technologies are taking the infrastructure world by storm. While containers solve or simplify infrastructure management processes, they also introduce significant complexity in terms of orchestration. That’s where Kubernetes comes to our rescue. Just like a conductor directs an orchestra, <a href=/docs/concepts/overview/what-is-kubernetes/>Kubernetes</a> oversees our ensemble of containers—starting, stopping, creating, and destroying them automatically to keep our applications humming along.</p><p>Kubernetes makes managing a containerized infrastructure much easier by creating levels of abstractions such as <a href=/docs/concepts/workloads/pods/pod/>pods</a> and <a href=/docs/concepts/services-networking/service/>services</a>. We no longer have to worry about where applications are running or if they have enough resources to work properly. But that doesn’t change the fact that, in order to ensure good performance, we need to monitor our applications, the containers running them, and Kubernetes itself.</p><p><strong>Rethinking monitoring for the Kubernetes era</strong></p><p>Just as containers have completely transformed how we think about running services on virtual machines, Kubernetes has changed the way we interact with containers. The good news is that with proper monitoring, the abstraction levels inherent to Kubernetes provide a comprehensive view of your infrastructure, even if the containers and applications are constantly moving. But Kubernetes monitoring requires us to rethink and reorient our strategies, since it differs from monitoring traditional hosts such as VMs or physical machines in several ways.</p><p><strong>Tags and labels become essential</strong><br>With containers and their orchestration completely managed by Kubernetes, labels are now the only way we have to interact with pods and containers. That’s why they are absolutely crucial for monitoring since all metrics and events will be sliced and diced using <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> across the different layers of your infrastructure. Defining your labels with a logical and easy-to-understand schema is essential so your metrics will be as useful as possible.</p><p><strong>There are now more components to monitor</strong></p><p><a href=https://lh5.googleusercontent.com/tN8tzKcXWAFWF0TD9u9UkTFJakHsrdjtRx56WiF75UYwMKu8teFyr6LpLGjpuOWSr52M-l3do5r3a6VWi6VwhRWuaquCpGty8ksI585D9YuCL3t7DAcItJUwW6mlrM2jUw_jVq6A><img src=https://lh5.googleusercontent.com/tN8tzKcXWAFWF0TD9u9UkTFJakHsrdjtRx56WiF75UYwMKu8teFyr6LpLGjpuOWSr52M-l3do5r3a6VWi6VwhRWuaquCpGty8ksI585D9YuCL3t7DAcItJUwW6mlrM2jUw_jVq6A alt></a>
In traditional, host-centric infrastructure, we were used to monitoring only two layers: applications and the hosts running them. Now with containers in the middle and Kubernetes itself needing to be monitored, there are four different components to monitor and collect metrics from.</p><p><strong>Applications are constantly moving</strong><br>Kubernetes schedules applications dynamically based on scheduling policy, so you don’t always know where applications are running. But they still need to be monitored. That’s why using a monitoring system or tool with service discovery is a must. It will automatically adapt metric collection to moving containers so applications can be continuously monitored without interruption.</p><p><strong>Be prepared for distributed clusters</strong></p><p>Kubernetes has the <a href=/docs/tasks/federation/federation-service-discovery/#hybrid-cloud-capabilities>ability</a> to distribute containerized applications across multiple data centers and potentially different cloud providers. That means metrics must be collected and aggregated among all these different sources. </p><p> </p><p>For more details about all these new monitoring challenges inherent to Kubernetes and how to overcome them, we recently published an <a href=https://www.datadoghq.com/blog/monitoring-kubernetes-era/>in-depth Kubernetes monitoring guide</a>. Part 1 of the series covers how to adapt your monitoring strategies to the Kubernetes era.</p><p><strong>Metrics to monitor</strong></p><p>Whether you use <a href=https://github.com/kubernetes/heapster>Heapster</a> data or a monitoring tool integrating with Kubernetes and its different APIs, there are several key types of metrics that need to be closely tracked:</p><ul><li><strong>Running pods</strong> and their <strong>deployments</strong></li><li>Usual <strong>resource metrics</strong> such as CPU, memory usage, and disk I/O</li><li><strong>Container-native <a href=https://www.datadoghq.com/blog/monitoring-kubernetes-performance-metrics/>metrics</a></strong></li><li>Application metrics for which a service discovery feature in your monitoring tool is essential </li></ul><p>All these metrics should be aggregated using Kubernetes labels and correlated with events from Kubernetes and container technologies.</p><p> </p><p><a href=https://www.datadoghq.com/blog/monitoring-kubernetes-performance-metrics/>Part 2</a> of our series on Kubernetes monitoring guides you through all the data that needs to be collected and tracked.</p><p><strong>Collecting these metrics</strong></p><p>Whether you want to track these key performance metrics by combining Heapster, a storage backend, and a graphing tool, or by integrating a monitoring tool with the different components of your infrastructure, <a href=https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/>Part 3</a>, about Kubernetes metric collection, has you covered.</p><p> </p><p><strong>Anchors aweigh!</strong></p><p>Using Kubernetes drastically simplifies container management. But it requires us to rethink our monitoring strategies on several fronts, and to make sure all the key metrics from the different components are properly collected, aggregated, and tracked. We hope our monitoring guide will help you to effectively monitor your Kubernetes clusters. <a href=https://github.com/DataDog/the-monitor>Feedback and suggestions</a> are more than welcome.</p><p> </p><p><em>--Jean-Mathieu Saponaro, Research & Analytics Engineer, Datadog</em></p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd6b46c25ade710315e209edb61a4474>Kubespray Ansible Playbooks foster Collaborative Kubernetes Ops</h1><div class="td-byline mb-4"><time datetime=2017-05-19 class=text-muted>Friday, May 19, 2017</time></div><p><em>Today’s guest post is by Rob Hirschfeld, co-founder of open infrastructure automation project, Digital Rebar and co-chair of the SIG Cluster Ops.  </em></p><p><strong>Why Kubespray?</strong></p><p>Making Kubernetes operationally strong is a widely held priority and I track many deployment efforts around the project. The <a href=https://github.com/kubernetes-incubator/kubespray>incubated Kubespray project</a> is of particular interest for me because it uses the popular Ansible toolset to build robust, upgradable clusters on both cloud and physical targets. I believe using tools familiar to operators grows our community.</p><p>We’re excited to see the breadth of platforms enabled by Kubespray and how well it handles a wide range of options like integrating Ceph for <a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a> persistence and Helm for easier application uploads. Those additions have allowed us to fully integrate the <a href=https://github.com/att-comdev/openstack-helm>OpenStack Helm charts</a> (<a href="https://www.youtube.com/watch?v=wZ0vMrdx4a4&list=PLXPBeIrpXjfjabMbwYyDULOX3kZmlxEXK&index=2">demo video</a>).</p><p>By working with the upstream source instead of creating different install scripts, we get the benefits of a larger community. This requires some extra development effort; however, we believe helping share operational practices makes the whole community stronger. That was also the motivation behind the <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-ops>SIG-Cluster Ops</a>.</p><p><strong>With Kubespray delivering robust installs, we can focus on broader operational concerns.</strong></p><p>For example, we can now drive parallel deployments, so it’s possible to fully exercise the options enabled by Kubespray simultaneously for development and testing.  </p><p>That’s helpful to built-test-destroy coordinated Kubernetes installs on CentOS, Red Hat and Ubuntu as part of an automation pipeline. We can also set up a full classroom environment from a single command using <a href=https://github.com/digitalrebar/digitalrebar>Digital Rebar’s</a> providers, tenants and cluster definition JSON.</p><p><strong>Let’s explore the classroom example:</strong></p><p>First, we define a <a href=https://github.com/digitalrebar/digitalrebar/blob/master/deploy/workloads/cluster/deploy-001.json>student cluster in JSON</a> like the snippet below</p><p>|
{</p><p> "attribs": {</p><p>   "k8s-version": "v1.6.0",</p><p>   "k8s-kube_network_plugin": "calico",</p><p>   "k8s-docker_version": "1.12"</p><p> },</p><p> "name": "cluster01",</p><p> "tenant": "cluster01",</p><p> "public_keys": {</p><p>   "cluster01": "ssh-rsa AAAAB..... <a href=mailto:user@example.com>user@example.com</a>"</p><p> },</p><p> "provider": {</p><p>   "name": "google-provider"</p><p> },</p><p> "nodes": [</p><p>   {</p><p>     "roles": ["etcd","k8s-addons", "k8s-master"],</p><p>     "count": 1</p><p>   },</p><p>   {</p><p>     "roles": ["k8s-worker"],</p><p>     "count": 3</p><p>   }</p><p> ]</p><p>}
|</p><p>Then we run the <a href=https://github.com/digitalrebar/digitalrebar/blob/master/deploy/workloads/multideploy.sh>Digital Rebar workloads Multideploy.sh</a> reference script which inspects the deployment files to pull out key information.  Basically, it automates the following steps:</p><p>|
rebar provider create {“name”:“google-provider”, [secret stuff]}</p><p>rebar tenants create {“name”:“cluster01”}</p><p>rebar deployments create [contents from cluster01 file]
|</p><p>The deployments create command will automatically request nodes from the provider. Since we’re using tenants and SSH key additions, each student only gets access to their own cluster. When we’re done, adding the --destroy flag will reverse the process for the nodes and deployments but leave the providers and tenants.</p><p><strong>We are invested in operational scripts like this example using Kubespray and Digital Rebar because if we cannot manage variation in a consistent way then we’re doomed to operational fragmentation.  </strong></p><p>I am excited to see and be part of the community progress towards enterprise-ready Kubernetes operations on both cloud and on-premises. That means I am seeing reasonable patterns emerge with sharable/reusable automation. I strongly recommend watching (or better, collaborating in) these efforts if you are deploying Kubernetes even at experimental scale. Being part of the community requires more upfront effort but returns dividends as you get the benefits of shared experience and improvement.</p><p><strong>When deploying at scale, how do you set up a system to be both repeatable and multi-platform without compromising scale or security?</strong></p><p>With Kubespray and Digital Rebar as a repeatable base, extensions get much faster and easier. Even better, using upstream directly allows improvements to be quickly cycled back into upstream. That means we’re closer to building a community focused on the operational side of Kubernetes with an <a href=https://rackn.com/sre>SRE mindset</a>.</p><p>If this is interesting, please engage with us in the <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-ops>Cluster Ops SIG</a>, <a href=https://github.com/kubernetes-incubator/kubespray>Kubespray</a> or <a href=http://rebar.digital/>Digital Rebar</a> communities. </p><p><em>-- Rob Hirschfeld, co-founder of RackN and co-chair of the Cluster Ops SIG</em></p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a13190552c5d0d7417c1f06a29226b51>Dancing at the Lip of a Volcano: The Kubernetes Security Process - Explained</h1><div class="td-byline mb-4"><time datetime=2017-05-18 class=text-muted>Thursday, May 18, 2017</time></div><p><em>Editor's note: Today’s post is by  <strong>Jess Frazelle of Google and Brandon Philips of CoreOS about the Kubernetes security disclosures and response policy.</strong>  </em></p><p>Software running on servers underpins ever growing amounts of the world's commerce, communications, and physical infrastructure. And nearly all of these systems are connected to the internet; which means vital security updates must be applied rapidly. As software developers and IT professionals, we often find ourselves dancing on the edge of a volcano: we may either fall into magma induced oblivion from a security vulnerability exploited before we can fix it, or we may slide off the side of the mountain because of an inadequate process to address security vulnerabilities. </p><p>The Kubernetes community believes that we can help teams restore their footing on this volcano with a foundation built on Kubernetes. And the bedrock of this foundation requires a process for quickly acknowledging, patching, and releasing security updates to an ever growing community of Kubernetes users. </p><p>With over 1,200 contributors and <a href=https://www.openhub.net/p/kubernetes>over a million lines of code</a>, each release of Kubernetes is a massive undertaking staffed by brave volunteer <a href=https://github.com/kubernetes/community/wiki>release managers</a>. These normal releases are fully transparent and the process happens in public. However, security releases must be handled differently to keep potential attackers in the dark until a fix is made available to users.</p><p>We drew inspiration from other open source projects in order to create the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-release/release.md><strong>Kubernetes security release process</strong></a>. Unlike a regularly scheduled release, a security release must be delivered on an accelerated schedule, and we created the <a href=https://git.k8s.io/security/security-release-process.md#product-security-committee-psc>Product Security Team</a> to handle this process.</p><p>This team quickly selects a lead to coordinate work and manage communication with the persons that disclosed the vulnerability and the Kubernetes community. The security release process also documents ways to measure vulnerability severity using the <a href=https://www.first.org/cvss/calculator/3.0>Common Vulnerability Scoring System (CVSS) Version 3.0 Calculator</a>. This calculation helps inform decisions on release cadence in the face of holidays or limited developer bandwidth. By making severity criteria transparent we are able to better set expectations and hit critical timelines during an incident where we strive to:</p><ul><li>Respond to the person or team who reported the vulnerability and staff a development team responsible for a fix within 24 hours</li><li>Disclose a forthcoming fix to users within 7 days of disclosure</li><li>Provide advance notice to vendors within 14 days of disclosure</li><li>Release a fix within 21 days of disclosure</li></ul><p>As we <a href=https://lwn.net/Articles/720215/>continue to harden Kubernetes</a>, the security release process will help ensure that Kubernetes remains a secure platform for internet scale computing. If you are interested in learning more about the security release process please watch the presentation from KubeCon Europe 2017 <a href="https://www.youtube.com/watch?v=sNjylW8FV9A">on YouTube</a> and follow along with the <a href=https://speakerdeck.com/philips/kubecon-eu-2017-dancing-on-the-edge-of-a-volcano>slides</a>. If you are interested in learning more about authentication and authorization in Kubernetes, along with the Kubernetes cluster security model, consider joining <a href=https://github.com/kubernetes/community/blob/master/sig-auth/README.md>Kubernetes SIG Auth</a>. We also hope to see you at security related presentations and panels at the next Kubernetes community event: <a href=https://coreos.com/fest/>CoreOS Fest 2017 in San Francisco on May 31 and June 1</a>.</p><p>As a thank you to the Kubernetes community, a special 25 percent discount to CoreOS Fest is available using k8s25code or via this special <a href="https://coreosfest17.eventbrite.com/?discount=k8s25code">25 percent off link</a> to register today for CoreOS Fest 2017. </p><p><em>--Brandon Philips of CoreOS and Jess Frazelle of Google</em></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f0c782c272f0f9e71f7dee809a947166>How Bitmovin is Doing Multi-Stage Canary Deployments with Kubernetes in the Cloud and On-Prem</h1><div class="td-byline mb-4"><time datetime=2017-04-21 class=text-muted>Friday, April 21, 2017</time></div><p><em>Editor's Note: Today’s post is by Daniel Hoelbling-Inzko, Infrastructure Architect at Bitmovin, a company that provides services that transcode digital video and audio to streaming formats, sharing insights about their use of Kubernetes.</em></p><p>Running a large scale video encoding infrastructure on multiple public clouds is tough. At <a href=http://bitmovin.com/>Bitmovin</a>, we have been doing it successfully for the last few years, but from an engineering perspective, it’s neither been enjoyable nor particularly fun.</p><p>So obviously, one of the main things that really sold us on using Kubernetes, was it’s common abstraction from the different supported cloud providers and the well thought out programming interface it provides. More importantly, the Kubernetes project did not settle for the lowest common denominator approach. Instead, they added the necessary abstract concepts that are required and useful to run containerized workloads in a cloud and then did all the hard work to map these concepts to the different cloud providers and their offerings.</p><p>The great stability, speed and operational reliability we saw in our early tests in mid-2016 made the migration to Kubernetes a no-brainer.</p><p>And, it didn’t hurt that the vision for scale the Kubernetes project has been pursuing is closely aligned with our own goals as a company. Aiming for >1,000 node clusters might be a lofty goal, but for a fast growing video company like ours, having your infrastructure aim to support future growth is essential. Also, after initial brainstorming for our new infrastructure, we immediately knew that we would be running a huge number of containers and having a system, with the expressed goal of working at global scale, was the perfect fit for us. Now with the recent <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale/>Kubernetes 1.6</a> release and its <a href=https://kubernetes.io/blog/2017/03/scalability-updates-in-kubernetes-1-6/>support for 5,000 node clusters</a>, we feel even more validated in our choice of a container orchestration system.</p><p>During the testing and migration phase of getting our infrastructure running on Kubernetes, we got quite familiar with the Kubernetes API and the whole ecosystem around it. So when we were looking at expanding our cloud video encoding offering for customers to use in their own datacenters or cloud environments, we quickly decided to leverage Kubernetes as our ubiquitous cloud operating system to base the solution on.</p><p>Just a few months later this effort has become our newest service offering: <a href=https://bitmovin.com/managed-on-premise-encoding/>Bitmovin Managed On-Premise encoding</a>. Since all Kubernetes clusters share the same API, adapting our cloud encoding service to also run on Kubernetes enabled us to deploy into our customer’s datacenter, regardless of the hardware infrastructure running underneath. With great tools from the community, like kube-up and turnkey solutions, like Google Container Engine, anyone can easily provision a new Kubernetes cluster, either within their own infrastructure or in their own cloud accounts.</p><p>To give us the maximum flexibility for customers that deploy to bare metal and might not have any custom cloud integrations for Kubernetes yet, we decided to base our solution solely on facilities that are available in any Kubernetes install and don’t require any integration into the surrounding infrastructure (it will even run inside <a href=https://github.com/kubernetes/minikube>Minikube</a>!). We don’t rely on Services of type LoadBalancer, primarily because enterprise IT is usually reluctant to open up ports to the open internet - and not every bare metal Kubernetes install supports externally provisioned load balancers out of the box. To avoid these issues, we deploy a BitmovinAgent that runs inside the Cluster and polls our API for new encoding jobs without requiring any network setup. This agent then uses the locally available Kubernetes credentials to start up new deployments that run the encoders on the available hardware through the Kubernetes API.</p><p>Even without having a full cloud integration available, the consistent scheduling, health checking and monitoring we get from using the Kubernetes API really enabled us to focus on making the encoder work inside a container rather than spending precious engineering resources on integrating a bunch of different hypervisors, machine provisioners and monitoring systems.</p><p><img src=https://lh3.googleusercontent.com/k825xk4UlrK1IxnhRGFsxi_g_Yu65hbneXTmo2-F_rmVngxm7ghLdhiYMrjbi3xCf74wPANxJPDdSO4ZQJu43SKjR-JzRGbvf3fWewZ2-pcmXl3Uf-86xt4gYKwblsRiQXkvt_rv alt>
<strong>Multi-Stage Canary Deployments</strong></p><p>Our first encounters with the Kubernetes API were not for the On-Premise encoding product. Building our containerized encoding workflow on Kubernetes was rather a decision we made after seeing how incredibly easy and powerful the Kubernetes platform proved during development and rollout of our Bitmovin API infrastructure. We migrated to Kubernetes around four months ago and it has enabled us to provide rapid development iterations to our service while meeting our requirements of downtime-free deployments and a stable development to production pipeline. To achieve this we came up with an architecture that runs almost a thousand containers and meets the following requirements we had laid out on day one:</p><ol><li>1.Zero downtime deployments for our customers</li><li>2.Continuous deployment to production on each git mainline push</li><li>3.High stability of deployed services for customers</li></ol><p>Obviously #2 and #3 are at odds with each other, if each merged feature gets deployed to production right away - how can we ensure these releases are bug-free and don’t have adverse side effects for our customers?</p><p>To overcome this oxymoron, we came up with a four-stage canary pipeline for each microservice where we simultaneously deploy to production and keep changes away from customers until the new build has proven to work reliably and correctly in the production environment.</p><p>Once a new build is pushed, we deploy it to an internal stage that’s only accessible for our internal tests and the integration test suite. Once the internal test suite passes, QA reports no issues, and we don’t detect any abnormal behavior, we push the new build to our free stage. This means that 5% of our free users would get randomly assigned to this new build. After some time in this stage the build gets promoted to the next stage that gets 5% of our paid users routed to it. Only once the build has successfully passed all 3 of these hurdles, does it get deployed to the production tier, where it will receive all traffic from our remaining users as well as our enterprise customers, which are not part of the paid bucket and never see their traffic routed to a canary track.</p><p><img src=https://lh3.googleusercontent.com/4iiw1O-Ik8KeLSMh8Ubk9j4wh3Npelqon-ZJ8joGeXqpFoZvi6won9vLOBLyAEuHcFkigKYXH_twCVKWvjxL-YEJRAFbLbLP7Ry8DTMIAVKmrlp7pBIEnM5bE-22I7eZD3NBoMeB alt></p><p>This setup makes us a pretty big Kubernetes installation by default, since all of our canary tiers are available at a minimum replication of 2. Since we are currently deploying around 30 microservices (and growing) to our clusters, it adds up to a minimum of 10 pods per service (8 application pods + minimum 2 HAProxy pods that do the canary routing). Although, in reality our preferred standard configuration is usually running 2 internal, 4 free, 4 others and 10 production pods alongside 4 HAProxy pods - totalling around 700 pods in total. This also means that we are running at least 150 services that provide a static ClusterIP to their underlying microservice canary tier.</p><p>A typical deployment looks like this:</p><p>| Services (ClusterIP) | Deployments | #Pods |
| account-service | account-service-haproxy | 4 |
| account-service-internal | account-service-internal-v1.18.0 | 2 |
| account-service-canary | account-service-canary-v1.17.0 | 4 |
| account-service-paid | account-service-paid-v1.15.0 | 4 |
| account-service-production | account-service-production-v1.15.0 | 10 |</p><p>An example service definition the production track will have the following label selectors:</p><pre><code>apiVersion: v1

kind: Service

metadata:

 name: account-service-production

 labels:

 app: account-service-production

 tier: service

 lb: private

spec:

 ports:

 - port: 8080

 name: http

 targetPort: 8080

 protocol: TCP

 selector:

 app: account-service

 tier: service

 track: production
</code></pre><p>In front of the Kubernetes services, load balancing the different canary versions of the service, lives a small cluster of HAProxy pods that get their haproxy.conf from the Kubernetes <a href=/docs/tasks/configure-pod-container/configmap/>ConfigMaps</a> that looks something like this:</p><pre><code>frontend http-in

 bind \*:80

 log 127.0.0.1 local2 debug


 acl traffic\_internal hdr(X-Traffic-Group) -m str -i INTERNAL

 acl traffic\_free  hdr(X-Traffic-Group) -m str -i FREE

 acl traffic\_enterprise hdr(X-Traffic-Group) -m str -i ENTERPRISE


 use\_backend internal if traffic\_internal

 use\_backend canary if traffic\_free

 use\_backend enterprise if traffic\_enterprise


 default\_backend paid


backend internal

 balance roundrobin

 server internal-lb  user-resource-service-internal:8080 resolvers dns check inter 2000

backend canary

 balance roundrobin

 server canary-lb    user-resource-service-canary:8080 resolvers dns check inter 2000 weight 5

 server production-lb user-resource-service-production:8080 resolvers dns check inter 2000 weight 95

backend paid

 balance roundrobin

 server canary-paid-lb user-resource-service-paid:8080 resolvers dns check inter 2000 weight 5

 server production-lb user-resource-service-production:8080 resolvers dns check inter 2000 weight 95

backend enterprise

 balance roundrobin

 server production-lb user-resource-service-production:8080 resolvers dns check inter 2000 weight 100
</code></pre><p>Each HAProxy will inspect a header that gets assigned by our API-Gateway called X-Traffic-Group that determines which bucket of customers this request belongs to. Based on that, a decision is made to hit either a canary deployment or the production deployment.</p><p>Obviously, at this scale, kubectl (while still our main day-to-day tool to work on the cluster) doesn’t really give us a good overview of whether everything is actually running as it’s supposed to and what is maybe over or under replicated.</p><p>Since we do blue/green deployments, we sometimes forget to shut down the old version after the new one comes up, so some services might be running over replicated and finding these issues in a soup of 25 deployments listed in kubectl is not trivial, to say the least.</p><p>So, having a container orchestrator like Kubernetes, that’s very API driven, was really a godsend for us, as it allowed us to write tools that take care of that.</p><p>We built tools that either run directly off kubectl (eg bash-scripts) or interact directly with the API and understand our special architecture to give us a quick overview of the system. These tools were mostly built in Go using the <a href=https://github.com/kubernetes/client-go>client-go</a> library.</p><p>One of these tools is worth highlighting, as it’s basically our only way to really see service health at a glance. It goes through all our Kubernetes services that have the tier: service selector and checks if the accompanying HAProxy deployment is available and all pods are running with 4 replicas. It also checks if the 4 services behind the HAProxys (internal, free, others and production) have at least 2 endpoints running. If any of these conditions are not met, we immediately get a notification in Slack and by email.</p><p>Managing this many pods with our previous orchestrator proved very unreliable and the overlay network frequently caused issues. Not so with Kubernetes - even doubling our current workload for test purposes worked flawlessly and in general, the cluster has been working like clockwork ever since we installed it.</p><p>Another advantage of switching over to Kubernetes was the availability of the kubernetes resource specifications, in addition to the API (which we used to write some internal tools for deployment). This enabled us to have a Git repo with all our Kubernetes specifications, where each track is generated off a common template and only contains placeholders for variable things like the canary track and the names.</p><p>All changes to the cluster have to go through tools that modify these resource specifications and get checked into git automatically so, whenever we see issues, we can debug what changes the infrastructure went through over time!</p><p>To summarize this post - by migrating our infrastructure to Kubernetes, Bitmovin is able to have:</p><ul><li>Zero downtime deployments, allowing our customers to encode 24/7 without interruption</li><li>Fast development to production cycles, enabling us to ship new features faster</li><li>Multiple levels of quality assurance and high confidence in production deployments</li><li>Ubiquitous abstractions across cloud architectures and on-premise deployments</li><li>Stable and reliable health-checking and scheduling of services</li><li>Custom tooling around our infrastructure to check and validate the system</li><li>History of deployments (resource specifications in git + custom tooling)</li></ul><p>We want to thank the Kubernetes community for the incredible job they have done with the project. The velocity at which the project moves is just breathtaking! Maintaining such a high level of quality and robustness in such a diverse environment is really astonishing.</p><p><em>--Daniel Hoelbling-Inzko, Infrastructure Architect, Bitmovin</em></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f1c179178bbf223f64c5892c367783ca>RBAC Support in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-04-06 class=text-muted>Thursday, April 06, 2017</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</em></p><p>One of the highlights of the <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6</a> release is the RBAC authorizer feature moving to <em>beta</em>. RBAC, Role-based access control, is an authorization mechanism for managing permissions around Kubernetes resources. RBAC allows configuration of flexible authorization policies that can be updated without cluster restarts.</p><p>The focus of this post is to highlight some of the interesting new capabilities and best practices.</p><p><strong>RBAC vs ABAC</strong></p><p>Currently there are several <a href=/docs/reference/access-authn-authz/authorization/>authorization mechanisms</a> available for use with Kubernetes. Authorizers are the mechanisms that decide who is permitted to make what changes to the cluster using the Kubernetes API. This affects things like kubectl, system components, and also certain applications that run in the cluster and manipulate the state of the cluster, like Jenkins with the Kubernetes plugin, or <a href=https://github.com/kubernetes/helm>Helm</a> that runs in the cluster and uses the Kubernetes API to install applications in the cluster. Out of the available authorization mechanisms, ABAC and RBAC are the mechanisms local to a Kubernetes cluster that allow configurable permissions policies.</p><p>ABAC, Attribute Based Access Control, is a powerful concept. However, as implemented in Kubernetes, ABAC is difficult to manage and understand. It requires ssh and root filesystem access on the master VM of the cluster to make authorization policy changes. For permission changes to take effect the cluster API server must be restarted.</p><p>RBAC permission policies are configured using kubectl or the Kubernetes API directly. Users can be authorized to make authorization policy changes using RBAC itself, making it possible to delegate resource management without giving away ssh access to the cluster master. RBAC policies map easily to the resources and operations used in the Kubernetes API.</p><p>Based on where the Kubernetes community is focusing their development efforts, going forward RBAC should be preferred over ABAC.</p><p><strong>Basic Concepts</strong></p><p>There are a few basic ideas behind RBAC that are foundational in understanding it. At its core, RBAC is a way of granting users granular access to <a href=/docs/api-reference/v1.6/>Kubernetes API resources</a>.</p><p><a href=https://1.bp.blogspot.com/-v6KLs1tT_xI/WOa0anGP4sI/AAAAAAAABBo/KIgYfp8PjusuykUVTfgu9-2uKj_wXo4lwCLcB/s1600/rbac1.png><img src=https://1.bp.blogspot.com/-v6KLs1tT_xI/WOa0anGP4sI/AAAAAAAABBo/KIgYfp8PjusuykUVTfgu9-2uKj_wXo4lwCLcB/s400/rbac1.png alt></a></p><p>The connection between user and resources is defined in RBAC using two objects.</p><p><strong>Roles</strong><br>A Role is a collection of permissions. For example, a role could be defined to include read permission on pods and list permission for pods. A ClusterRole is just like a Role, but can be used anywhere in the cluster.</p><p><strong>Role Bindings</strong><br>A RoleBinding maps a Role to a user or set of users, granting that Role's permissions to those users for resources in that namespace. A ClusterRoleBinding allows users to be granted a ClusterRole for authorization across the entire cluster.</p><p><a href=https://1.bp.blogspot.com/-ixDe91-cnqw/WOa0auxC0mI/AAAAAAAABBs/4LxVsr6shEgTYqUapt5QPISUeuTuztVwwCEw/s1600/rbac2.png><img src=https://1.bp.blogspot.com/-ixDe91-cnqw/WOa0auxC0mI/AAAAAAAABBs/4LxVsr6shEgTYqUapt5QPISUeuTuztVwwCEw/s640/rbac2.png alt></a></p><p>Additionally there are cluster roles and cluster role bindings to consider. Cluster roles and cluster role bindings function like roles and role bindings except they have wider scope. The exact differences and how cluster roles and cluster role bindings interact with roles and role bindings are covered in the <a href=/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding>Kubernetes documentation</a>.</p><p><strong>RBAC in Kubernetes</strong></p><p>RBAC is now deeply integrated into Kubernetes and used by the system components to grant the permissions necessary for them to function. <a href=/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings>System roles</a> are typically prefixed with system: so they can be easily recognized.</p><pre><code>➜  kubectl get clusterroles --namespace=kube-system

NAME                    KIND

admin ClusterRole.v1beta1.rbac.authorization.k8s.io

cluster-admin ClusterRole.v1beta1.rbac.authorization.k8s.io

edit ClusterRole.v1beta1.rbac.authorization.k8s.io

kubelet-api-admin ClusterRole.v1beta1.rbac.authorization.k8s.io

system:auth-delegator ClusterRole.v1beta1.rbac.authorization.k8s.io

system:basic-user ClusterRole.v1beta1.rbac.authorization.k8s.io

system:controller:attachdetach-controller ClusterRole.v1beta1.rbac.authorization.k8s.io

system:controller:certificate-controller ClusterRole.v1beta1.rbac.authorization.k8s.io

...
</code></pre><p>The RBAC system roles have been expanded to cover the necessary permissions for running a Kubernetes cluster with RBAC only.</p><p>During the permission translation from ABAC to RBAC, some of the permissions that were enabled by default in many deployments of ABAC authorized clusters were identified as unnecessarily broad and were <a href=/docs/reference/access-authn-authz/rbac/#upgrading-from-1-5>scoped down</a> in RBAC. The area most likely to impact workloads on a cluster is the permissions available to service accounts. With the permissive ABAC configuration, requests from a pod using the pod mounted token to authenticate to the API server have broad authorization. As a concrete example, the curl command at the end of this sequence will return a JSON formatted result when ABAC is enabled and an error when only RBAC is enabled.</p><pre><code>➜  kubectl run nginx --image=nginx:latest

➜  kubectl exec -it $(kubectl get pods -o jsonpath='{.items[0].metadata.name}') bash

➜  apt-get update &amp;&amp; apt-get install -y curl

➜  curl -ik \

 -H &quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; \

 https://kubernetes/api/v1/namespaces/default/pods
</code></pre><p>Any applications you run in your Kubernetes cluster that interact with the Kubernetes API have the potential to be affected by the permissions changes when transitioning from ABAC to RBAC.</p><p>To smooth the transition from ABAC to RBAC, you can create Kubernetes 1.6 clusters with both <a href=/docs/reference/access-authn-authz/rbac/#parallel-authorizers>ABAC and RBAC authorizers</a> enabled. When both ABAC and RBAC are enabled, authorization for a resource is granted if either authorization policy grants access. However, under that configuration the most permissive authorizer is used and it will not be possible to use RBAC to fully control permissions.</p><p>At this point, RBAC is complete enough that ABAC support should be considered deprecated going forward. It will still remain in Kubernetes for the foreseeable future but development attention is focused on RBAC.</p><p>Two different talks at the at the Google Cloud Next conference touched on RBAC related changes in Kubernetes 1.6, jump to the relevant parts <a href="https://www.youtube.com/watch?v=Cd4JU7qzYbE#t=8m01s">here</a> and <a href="https://www.youtube.com/watch?v=18P7cFc6nTU#t=41m06s">here</a>. For more detailed information about using RBAC in Kubernetes 1.6 read the full <a href=/docs/reference/access-authn-authz/rbac/>RBAC documentation</a>.</p><p><strong>Get Involved</strong></p><p>If you’d like to contribute or simply help provide feedback and drive the roadmap, <a href=https://github.com/kubernetes/community#kubernetes-community>join our community</a>. Specifically interested in security and RBAC related conversation, participate through one of these channels:</p><ul><li>Chat with us on the Kubernetes <a href=https://kubernetes.slack.com/messages/sig-auth/>Slack sig-auth channel</a></li><li>Join the biweekly <a href=https://github.com/kubernetes/community/blob/master/sig-auth/README.md>SIG-Auth meetings</a> on Wednesday at 11:00 AM PT</li></ul><p>Thanks for your support and contributions. Read more in-depth posts on what's new in Kubernetes 1.6 <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>here</a>.</p><p><em>-- Jacob Simpson, Greg Castle & CJ Cullen, Software Engineers at Google</em></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-60d2a1aa4dc581b6ce80903823f5829a>Configuring Private DNS Zones and Upstream Nameservers in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-04-04 class=text-muted>Tuesday, April 04, 2017</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</em></p><p>Many users have existing domain name zones that they would like to integrate into their Kubernetes DNS namespace. For example, hybrid-cloud users may want to resolve their internal “.corp” domain addresses within the cluster. Other users may have a zone populated by a non-Kubernetes service discovery system (like Consul). We’re pleased to announce that, in <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6</a>, <a href=/docs/concepts/services-networking/dns-pod-service/>kube-dns</a> adds support for configurable private DNS zones (often called “stub domains”) and external upstream DNS nameservers. In this blog post, we describe how to configure and use this feature.</p><p><strong>Default lookup flow</strong></p><p><a href=https://2.bp.blogspot.com/-Jj4r6bGt1f8/WORRugYMobI/AAAAAAAABBE/HXH-wBGqweQcJbyQA3bqnUtYeN5aOtE9ACEw/s1600/dns2.png><img src=https://2.bp.blogspot.com/-Jj4r6bGt1f8/WORRugYMobI/AAAAAAAABBE/HXH-wBGqweQcJbyQA3bqnUtYeN5aOtE9ACEw/s400/dns2.png alt></a></p><p>Kubernetes currently supports two DNS policies specified on a per-pod basis using the dnsPolicy flag: “Default” and “ClusterFirst”. If dnsPolicy is not explicitly specified, then “ClusterFirst” is used:</p><ul><li>If dnsPolicy is set to “Default”, then the name resolution configuration is inherited from the node the pods run on. Note: this feature cannot be used in conjunction with dnsPolicy: “Default”.</li><li>If dnsPolicy is set to “ClusterFirst”, then DNS queries will be sent to the kube-dns service. Queries for domains rooted in the configured cluster domain suffix (any address ending in “.cluster.local” in the example above) will be answered by the kube-dns service. All other queries (for example, <a href=http://www.kubernetes.io>www.kubernetes.io</a>) will be forwarded to the upstream nameserver inherited from the node.
Before this feature, it was common to introduce stub domains by replacing the upstream DNS with a custom resolver. However, this caused the custom resolver itself to become a critical path for DNS resolution, where issues with scalability and availability may cause the cluster to lose DNS functionality. This feature allows the user to introduce custom resolution without taking over the entire resolution path.</li></ul><p><strong>Customizing the DNS Flow</strong></p><p>Beginning in Kubernetes 1.6, cluster administrators can specify custom stub domains and upstream nameservers by providing a ConfigMap for kube-dns. For example, the configuration below inserts a single stub domain and two upstream nameservers. As specified, DNS requests with the “.acme.local” suffix will be forwarded to a DNS listening at 1.2.3.4. Additionally, Google Public DNS will serve upstream queries. See ConfigMap Configuration Notes at the end of this section for a few notes about the data format.</p><pre><code>apiVersion: v1

kind: ConfigMap

metadata:

  name: kube-dns

  namespace: kube-system

data:

  stubDomains: |

    {“acme.local”: [“1.2.3.4”]}

  upstreamNameservers: |

    [“8.8.8.8”, “8.8.4.4”]
</code></pre><p>The diagram below shows the flow of DNS queries specified in the configuration above. With the dnsPolicy set to “ClusterFirst” a DNS query is first sent to the DNS caching layer in kube-dns. From here, the suffix of the request is examined and then forwarded to the appropriate DNS. In this case, names with the cluster suffix (e.g.; “.cluster.local”) are sent to kube-dns. Names with the stub domain suffix (e.g.; “.acme.local”) will be sent to the configured custom resolver. Finally, requests that do not match any of those suffixes will be forwarded to the upstream DNS.</p><p><a href=https://1.bp.blogspot.com/-IeFx2Uuq_i0/WORRuQpxG_I/AAAAAAAABBA/g1P3ljd7YGYMShoHJnPRK1IfX5h3o9GvACEw/s1600/dns.png><img src=https://1.bp.blogspot.com/-IeFx2Uuq_i0/WORRuQpxG_I/AAAAAAAABBA/g1P3ljd7YGYMShoHJnPRK1IfX5h3o9GvACEw/s400/dns.png alt></a></p><p>Below is a table of example domain names and the destination of the queries for those domain names:</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Domain name</td><td>Server answering the query</td></tr><tr><td>kubernetes.default.svc.cluster.local</td><td>kube-dns</td></tr><tr><td>foo.acme.local</td><td>custom DNS (1.2.3.4)</td></tr><tr><td>widget.com</td><td>upstream DNS (one of 8.8.8.8, 8.8.4.4)</td></tr></tbody></table><p><strong>ConfigMap Configuration Notes</strong></p><ul><li><p>stubDomains (optional)</p><ul><li>Format: a JSON map using a DNS suffix key (e.g.; “acme.local”) and a value consisting of a JSON array of DNS IPs.</li><li>Note: The target nameserver may itself be a Kubernetes service. For instance, you can run your own copy of dnsmasq to export custom DNS names into the ClusterDNS namespace.</li></ul></li><li><p>upstreamNameservers (optional)</p><ul><li>Format: a JSON array of DNS IPs.</li><li>Note: If specified, then the values specified replace the nameservers taken by default from the node’s /etc/resolv.conf</li><li>Limits: a maximum of three upstream nameservers can be specified</li></ul></li></ul><p><strong>Example #1: Adding a Consul DNS Stub Domain</strong></p><p>In this example, the user has Consul DNS service discovery system they wish to integrate with kube-dns. The consul domain server is located at 10.150.0.1, and all consul names have the suffix “.consul.local”. To configure Kubernetes, the cluster administrator simply creates a ConfigMap object as shown below. Note: in this example, the cluster administrator did not wish to override the node’s upstream nameservers, so they didn’t need to specify the optional upstreamNameservers field.</p><pre><code>apiVersion: v1

kind: ConfigMap

metadata:

  name: kube-dns

  namespace: kube-system

data:

  stubDomains: |

    {“consul.local”: [“10.150.0.1”]}
</code></pre><p><strong>Example #2: Replacing the Upstream Nameservers</strong></p><p>In this example the cluster administrator wants to explicitly force all non-cluster DNS lookups to go through their own nameserver at 172.16.0.1. Again, this is easy to accomplish; they just need to create a ConfigMap with the upstreamNameservers field specifying the desired nameserver.</p><pre><code>apiVersion: v1

kind: ConfigMap

metadata:

  name: kube-dns

  namespace: kube-system

data:

  upstreamNameservers: |

    [“172.16.0.1”]




**Get involved**  

If you’d like to contribute or simply help provide feedback and drive the roadmap, [join our community](https://github.com/kubernetes/community#kubernetes-community). Specifically for network related conversations participate though one of these channels:  

- Chat with us on the Kubernetes [Slack network channel](https://kubernetes.slack.com/messages/sig-network/)
- Join our Special Interest Group, [SIG-Network](https://github.com/kubernetes/community/wiki/SIG-Network), which meets on Tuesdays at 14:00 PT
Thanks for your support and contributions. Read more in-depth posts on what's new in Kubernetes 1.6 [here](https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6).





_--Bowei Du, Software Engineer and Matthew DeLio, Product Manager, Google_  



- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Join the community portal for advocates on [K8sPort](http://k8sport.org/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Connect with the community on [Slack](http://slack.k8s.io/)
- [Download](http://get.k8s.io/) Kubernetes
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-8864feaccfcc300b9c4bc3b8d07ed0c0>Advanced Scheduling in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-03-31 class=text-muted>Friday, March 31, 2017</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</em></p><p>The Kubernetes scheduler’s default behavior works well for most cases -- for example, it ensures that pods are only placed on nodes that have sufficient free resources, it ties to spread pods from the same set (<a href=/docs/user-guide/replicasets/>ReplicaSet</a>, <a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>, etc.) across nodes, it tries to balance out the resource utilization of nodes, etc.</p><p>But sometimes you want to control how your pods are scheduled. For example, perhaps you want to ensure that certain pods only schedule on nodes with specialized hardware, or you want to co-locate services that communicate frequently, or you want to dedicate a set of nodes to a particular set of users. Ultimately, you know much more about how your applications should be scheduled and deployed than Kubernetes ever will. So <strong><a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6</a> offers four advanced scheduling features: node affinity/anti-affinity, taints and tolerations, pod affinity/anti-affinity, and custom schedulers</strong>. Each of these features are now in <em>beta</em> in Kubernetes 1.6.</p><p><strong>Node Affinity/Anti-Affinity</strong></p><p><a href=/docs/user-guide/node-selection/#node-affinity-beta-feature>Node Affinity/Anti-Affinity</a> is one way to set rules on which nodes are selected by the scheduler. This feature is a generalization of the <a href=/docs/user-guide/node-selection/#nodeselector>nodeSelector</a> feature which has been in Kubernetes since version 1.0. The rules are defined using the familiar concepts of custom labels on nodes and selectors specified in pods, and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.</p><p>Required rules must be met for a pod to schedule on a particular node. If no node matches the criteria (plus all of the other normal criteria, such as having enough free resources for the pod’s resource request), then the pod won’t be scheduled. Required rules are specified in the requiredDuringSchedulingIgnoredDuringExecution field of nodeAffinity.</p><p>For example, if we want to require scheduling on a node that is in the us-central1-a GCE zone of a multi-zone Kubernetes cluster, we can specify the following affinity rule as part of the Pod spec:</p><pre><code>  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: &quot;failure-domain.beta.kubernetes.io/zone&quot;
              operator: In
              values: [&quot;us-central1-a&quot;]
</code></pre><p>“IgnoredDuringExecution” means that the pod will still run if labels on a node change and affinity rules are no longer met. There are future plans to offer requiredDuringSchedulingRequiredDuringExecution which will evict pods from nodes as soon as they don’t satisfy the node affinity rule(s).</p><p>Preferred rules mean that if nodes match the rules, they will be chosen first, and only if no preferred nodes are available will non-preferred nodes be chosen. You can prefer instead of require that pods are deployed to us-central1-a by slightly changing the pod spec to use preferredDuringSchedulingIgnoredDuringExecution:</p><pre><code>  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: &quot;failure-domain.beta.kubernetes.io/zone&quot;
              operator: In
              values: [&quot;us-central1-a&quot;]
</code></pre><p>Node anti-affinity can be achieved by using negative operators. So for instance if we want our pods to avoid us-central1-a we can do this:</p><pre><code>  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: &quot;failure-domain.beta.kubernetes.io/zone&quot;
              operator: NotIn
              values: [&quot;us-central1-a&quot;]
</code></pre><p>Valid operators you can use are In, NotIn, Exists, DoesNotExist. Gt, and Lt.</p><p>Additional use cases for this feature are to restrict scheduling based on nodes’ hardware architecture, operating system version, or specialized hardware. Node affinity/anti-affinity is <em>beta</em> in Kubernetes 1.6.</p><p><strong>Taints and Tolerations</strong></p><p>A related feature is “<a href=/docs/user-guide/node-selection/#taints-and-toleations-beta-feature>taints and tolerations</a>,” which allows you to mark (“taint”) a node so that no pods can schedule onto it unless a pod explicitly “tolerates” the taint. Marking nodes instead of pods (as in node affinity/anti-affinity) is particularly useful for situations where most pods in the cluster should avoid scheduling onto the node. For example, you might want to mark your master node as schedulable only by Kubernetes system components, or dedicate a set of nodes to a particular group of users, or keep regular pods away from nodes that have special hardware so as to leave room for pods that need the special hardware.</p><p>The kubectl command allows you to set taints on nodes, for example:</p><pre><code>kubectl taint nodes node1 key=value:NoSchedule
</code></pre><p>creates a taint that marks the node as unschedulable by any pods that do not have a toleration for taint with key key, value value, and effect NoSchedule. (The other taint effects are PreferNoSchedule, which is the preferred version of NoSchedule, and NoExecute, which means any pods that are running on the node when the taint is applied will be evicted unless they tolerate the taint.) The toleration you would add to a PodSpec to have the corresponding pod tolerate this taint would look like this</p><pre><code>  tolerations:
  - key: &quot;key&quot;
    operator: &quot;Equal&quot;
    value: &quot;value&quot;
    effect: &quot;NoSchedule&quot;
</code></pre><p>In addition to moving taints and tolerations to <em>beta</em> in Kubernetes 1.6, we have introduced an <em>alpha</em> feature that uses taints and tolerations to allow you to customize how long a pod stays bound to a node when the node experiences a problem like a network partition instead of using the default five minutes. See <a href=/docs/user-guide/node-selection/#per-pod-configurable-eviction-behavior-when-there-are-node-problems-alpha-feature>this section</a> of the documentation for more details.</p><p><strong>Pod Affinity/Anti-Affinity</strong></p><p>Node affinity/anti-affinity allows you to constrain which nodes a pod can run on based on the nodes’ labels. But what if you want to specify rules about how pods should be placed relative to one another, for example to spread or pack pods within a service or relative to pods in other services? For that you can use <a href=/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature>pod affinity/anti-affinity</a>, which is also <em>beta</em> in Kubernetes 1.6.</p><p>Let’s look at an example. Say you have front-ends in service S1, and they communicate frequently with back-ends that are in service S2 (a “north-south” communication pattern). So you want these two services to be co-located in the same cloud provider zone, but you don’t want to have to choose the zone manually--if the zone fails, you want the pods to be rescheduled to another (single) zone. You can specify this with a pod affinity rule that looks like this (assuming you give the pods of this service a label “service=S2” and the pods of the other service a label “service=S1”):</p><pre><code>affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: service
            operator: In
            values: [“S1”]
        topologyKey: failure-domain.beta.kubernetes.io/zone
</code></pre><p>As with node affinity/anti-affinity, there is also a preferredDuringSchedulingIgnoredDuringExecution variant.</p><p>Pod affinity/anti-affinity is very flexible. Imagine you have profiled the performance of your services and found that containers from service S1 interfere with containers from service S2 when they share the same node, perhaps due to cache interference effects or saturating the network link. Or maybe due to security concerns you never want containers of S1 and S2 to share a node. To implement these rules, just make two changes to the snippet above -- change podAffinity to podAntiAffinity and change topologyKey to kubernetes.io/hostname.</p><p><strong>Custom Schedulers</strong></p><p>If the Kubernetes scheduler’s various features don’t give you enough control over the scheduling of your workloads, you can delegate responsibility for scheduling arbitrary subsets of pods to your own custom scheduler(s) that run(s) alongside, or instead of, the default Kubernetes scheduler. <a href=/docs/admin/multiple-schedulers/>Multiple schedulers</a> is <em>beta</em> in Kubernetes 1.6.</p><p>Each new pod is normally scheduled by the default scheduler. But if you provide the name of your own custom scheduler, the default scheduler will ignore that Pod and allow your scheduler to schedule the Pod to a node. Let’s look at an example.</p><p>Here we have a Pod where we specify the schedulerName field:</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  schedulerName: my-scheduler
  containers:
  - name: nginx
    image: nginx:1.10
</code></pre><p>If we create this Pod without deploying a custom scheduler, the default scheduler will ignore it and it will remain in a Pending state. So we need a custom scheduler that looks for, and schedules, pods whose schedulerName field is my-scheduler.</p><p>A custom scheduler can be written in any language and can be as simple or complex as you need. Here is a very simple example of a custom scheduler written in Bash that assigns a node randomly. Note that you need to run this along with kubectl proxy for it to work.</p><pre><code>#!/bin/bash

SERVER='localhost:8001'

while true;

do

    for PODNAME in $(kubectl --server $SERVER get pods -o json | jq '.items[] | select(.spec.schedulerName == &quot;my-scheduler&quot;) | select(.spec.nodeName == null) | .metadata.name' | tr -d '&quot;')

;

    do

        NODES=($(kubectl --server $SERVER get nodes -o json | jq '.items[].metadata.name' | tr -d '&quot;'))


        NUMNODES=${#NODES[@]}

        CHOSEN=${NODES[$[$RANDOM % $NUMNODES]]}

        curl --header &quot;Content-Type:application/json&quot; --request POST --data '{&quot;apiVersion&quot;:&quot;v1&quot;, &quot;kind&quot;: &quot;Binding&quot;, &quot;metadata&quot;: {&quot;name&quot;: &quot;'$PODNAME'&quot;}, &quot;target&quot;: {&quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;

: &quot;Node&quot;, &quot;name&quot;: &quot;'$CHOSEN'&quot;}}' http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

        echo &quot;Assigned $PODNAME to $CHOSEN&quot;

    done

    sleep 1

done
</code></pre><p><strong>Learn more</strong></p><p>The Kubernetes 1.6 <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v160>release notes</a> have more information about these features, including details about how to change your configurations if you are already using the alpha version of one or more of these features (this is required, as the move from alpha to beta is a breaking change for these features).</p><p><strong>Acknowledgements</strong></p><p>The features described here, both in their alpha and beta forms, were a true community effort, involving engineers from Google, Huawei, IBM, Red Hat and more.</p><p><strong>Get Involved</strong></p><p>Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>:</p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a> (room #sig-scheduling)</li></ul><p>Many thanks for your contributions.</p><p><em>--Ian Lewis, Developer Advocate, and David Oppenheimer, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-2c544fae5405bf09bdedea192e9755bb>Scalability updates in Kubernetes 1.6: 5,000 node and 150,000 pod clusters</h1><div class="td-byline mb-4"><time datetime=2017-03-30 class=text-muted>Thursday, March 30, 2017</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</em></p><p>Last summer we <a href=https://kubernetes.io/blog/2016/07/update-on-kubernetes-for-windows-server-containers/>shared</a> updates on Kubernetes scalability, since then we’ve been working hard and are proud to announce that <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6</a> can handle 5,000-node clusters with up to 150,000 pods. Moreover, those cluster have even better end-to-end pod startup time than the previous 2,000-node clusters in the 1.3 release; and latency of the API calls are within the one-second SLO.</p><p>In this blog post we review what metrics we monitor in our tests and describe our performance results from Kubernetes 1.6. We also discuss what changes we made to achieve the improvements, and our plans for upcoming releases in the area of system scalability.</p><p><strong>X-node clusters - what does it mean?</strong></p><p>Now that Kubernetes 1.6 is released, it is a good time to review what it means when we say we “support” X-node clusters. As described in detail in a <a href=https://kubernetes.io/blog/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12>previous blog post</a>, we currently have two performance-related <a href=https://en.wikipedia.org/wiki/Service_level_objective>Service Level Objectives (SLO)</a>:</p><ul><li><strong>API-responsiveness</strong> : 99% of all API calls return in less than 1s</li><li><strong>Pod startup time</strong> : 99% of pods and their containers (with pre-pulled images) start within 5s.
As before, it is possible to run larger deployments than the stated supported 5,000-node cluster (and users have), but performance may be degraded and it may not meet our strict SLO defined above.</li></ul><p>We are aware of the limited scope of these SLOs. There are many aspects of the system that they do not exercise. For example, we do not measure how soon a new pod that is part of a service will be reachable through the service IP address after the pod is started. If you are considering using large Kubernetes clusters and have performance requirements not covered by our SLOs, please contact the Kubernetes <a href=https://github.com/kubernetes/community/blob/master/sig-scalability/README.md>Scalability SIG</a> so we can help you understand whether Kubernetes is ready to handle your workload now.</p><p>The top scalability-related priority for upcoming Kubernetes releases is to enhance our definition of what it means to support X-node clusters by:</p><ul><li>refining currently existing SLOs</li><li>adding more SLOs (that will cover various areas of Kubernetes, including networking)</li></ul><p><strong>Kubernetes 1.6 performance metrics at scale</strong></p><p>So how does performance in large clusters look in Kubernetes 1.6? The following graph shows the end-to-end pod startup latency with 2000- and 5000-node clusters. For comparison, we also show the same metric from Kubernetes 1.3, which we published in our previous scalability blog post that described support for 2000-node clusters. As you can see, Kubernetes 1.6 has better pod startup latency with both 2000 and 5000 nodes compared to Kubernetes 1.3 with 2000 nodes [1].</p><p><img src=https://lh6.googleusercontent.com/LdjAOmsLGdxLNTo222uif1V0Eupoyaq6dY-leg1FBGkyQxUNt5ROjrFh_XzW27P7nP865FYUVwTOaUpDEnirdHSBKvh9xl8PsBNEFlVWpJUbnj0FEdLX4MywqbjwK9oc8avLRNAX alt title=Wykres></p><p>The next graph shows API response latency for a 5000-node Kubernetes 1.6 cluster. The latencies at all percentiles are less than 500ms, and even 90th percentile is less than about 100ms.</p><p><img src=https://lh6.googleusercontent.com/RFGwgw9hvRshHH11vrUxGwl-X8vXdCvyd8ETdWS9Ud5_OFpG4WctzZbCy2ad4Ao_neYaMMDz46Z2JCQUzRI1jdk6OABTFIOyvZysZpDCAfr7Ztj-EM7v25sfHxf6dOe59fncDnra alt title=Wykres></p><p><strong>How did we get here?</strong></p><p>Over the past nine months (since the last scalability blog post), there have been a huge number of performance and scalability related changes in Kubernetes. In this post we will focus on the two biggest ones and will briefly enumerate a few others.</p><p><strong>etcd v3</strong><br>In Kubernetes 1.6 we switched the default storage backend (key-value store where the whole cluster state is stored) from etcd v2 to <a href=https://coreos.com/etcd/docs/3.0.17/index.html>etcd v3</a>. The initial works towards this transition has been started during the 1.3 release cycle. You might wonder why it took us so long, given that:</p><ul><li><p>the first stable version of etcd supporting the v3 API <a href=https://coreos.com/blog/etcd3-a-new-etcd.html>was announced</a> on June 30, 2016</p></li><li><p>the new API was designed together with the Kubernetes team to support our needs (from both a feature and scalability perspective)</p></li><li><p>the integration of etcd v3 with Kubernetes had already mostly been finished when etcd v3 was announced (indeed CoreOS used Kubernetes as a proof-of-concept for the new etcd v3 API)
As it turns out, there were a lot of reasons. We will describe the most important ones below.</p></li><li><p>Changing storage in a backward incompatible way, as is in the case for the etcd v2 to v3 migration, is a big change, and thus one for which we needed a strong justification. We found this justification in September when we determined that we would not be able to scale to 5000-node clusters if we continued to use etcd v2 (<a href=https://github.com/kubernetes/kubernetes/issues/32361>kubernetes/32361</a> contains some discussion about it). In particular, what didn’t scale was the watch implementation in etcd v2. In a 5000-node cluster, we need to be able to send at least 500 watch events per second to a single watcher, which wasn’t possible in etcd v2.</p></li><li><p>Once we had the strong incentive to actually update to etcd v3, we started thoroughly testing it. As you might expect, we found some issues. There were some minor bugs in Kubernetes, and in addition we requested a performance improvement in etcd v3’s watch implementation (watch was the main bottleneck in etcd v2 for us). This led to the 3.0.10 etcd patch release.</p></li><li><p>Once those changes had been made, we were convinced that <em>new</em> Kubernetes clusters would work with etcd v3. But the large challenge of migrating <em>existing</em> clusters remained. For this we needed to automate the migration process, thoroughly test the underlying CoreOS etcd upgrade tool, and figure out a contingency plan for rolling back from v3 to v2.
But finally, we are confident that it should work.</p></li></ul><p><strong>Switching storage data format to protobuf</strong><br>In the Kubernetes 1.3 release, we enabled <a href=https://developers.google.com/protocol-buffers/>protobufs</a> as the data format for Kubernetes components to communicate with the API server (in addition to maintaining support for JSON). This gave us a huge performance improvement.</p><p>However, we were still using JSON as a format in which data was stored in etcd, even though technically we were ready to change that. The reason for delaying this migration was related to our plans to migrate to etcd v3. Now you are probably wondering how this change was depending on migration to etcd v3. The reason for it was that with etcd v2 we couldn’t really store data in binary format (to workaround it we were additionally base64-encoding the data), whereas with etcd v3 it just worked. So to simplify the transition to etcd v3 and avoid some non-trivial transformation of data stored in etcd during it, we decided to wait with switching storage data format to protobufs until migration to etcd v3 storage backend is done.</p><p><strong>Other optimizations</strong><br>We made tens of optimizations throughout the Kubernetes codebase during the last three releases, including:</p><ul><li>optimizing the scheduler (which resulted in 5-10x higher scheduling throughput)</li><li>switching all controllers to a new recommended design using shared informers, which reduced resource consumption of controller-manager - for reference see <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md>this document</a></li><li>optimizing individual operations in the API server (conversions, deep-copies, patch)</li><li>reducing memory allocation in the API server (which significantly impacts the latency of API calls)
We want to emphasize that the optimization work we have done during the last few releases, and indeed throughout the history of the project, is a joint effort by many different companies and individuals from the whole Kubernetes community.</li></ul><p><strong>What’s next?</strong></p><p>People frequently ask how far we are going to go in improving Kubernetes scalability. Currently we do not have plans to increase scalability beyond 5000-node clusters (within our SLOs) in the next few releases. If you need clusters larger than 5000 nodes, we recommend to use <a href=/docs/concepts/cluster-administration/federation/>federation</a> to aggregate multiple Kubernetes clusters.</p><p>However, that doesn’t mean we are going to stop working on scalability and performance. As we mentioned at the beginning of this post, our top priority is to refine our two existing SLOs and introduce new ones that will cover more parts of the system, e.g. networking. This effort has already started within the Scalability SIG. We have made significant progress on how we would like to define performance SLOs, and this work should be finished in the coming month.</p><p><strong>Join the effort</strong><br>If you are interested in scalability and performance, please join our community and help us shape Kubernetes. There are many ways to participate, including:</p><ul><li>Chat with us in the Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-scale/>scalability channel</a>: </li><li>Join our Special Interest Group, <a href=https://github.com/kubernetes/community/blob/master/sig-scalability/README.md>SIG-Scalability</a>, which meets every Thursday at 9:00 AM PST
Thanks for the support and contributions! Read more in-depth posts on what's new in Kubernetes 1.6 <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>here</a>.</li></ul><p><em>-- Wojciech Tyczynski, Software Engineer, Google</em></p><p>[1] We are investigating why 5000-node clusters have better startup time than 2000-node clusters. The current theory is that it is related to running 5000-node experiments using 64-core master and 2000-node experiments using 32-core master.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-048341ca6d1d0496267de51f0f5cb1a3>Dynamic Provisioning and Storage Classes in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-03-29 class=text-muted>Wednesday, March 29, 2017</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</em></p><p>Storage is a critical part of running stateful containers, and Kubernetes offers powerful primitives for managing it. Dynamic volume provisioning, a feature unique to Kubernetes, allows storage volumes to be created on-demand. Before dynamic provisioning, cluster administrators had to manually make calls to their cloud or storage provider to provision new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. With dynamic provisioning, these two steps are automated, eliminating the need for cluster administrators to pre-provision storage. Instead, the storage resources can be dynamically provisioned using the provisioner specified by the StorageClass object (see <a href=/docs/user-guide/persistent-volumes/index#storageclasses>user-guide</a>). StorageClasses are essentially blueprints that abstract away the underlying storage provider, as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).</p><p>StorageClasses use provisioners that are specific to the storage platform or cloud provider to give Kubernetes access to the physical media being used. Several storage provisioners are provided in-tree (see <a href=/docs/user-guide/persistent-volumes/index#provisioner>user-guide</a>), but additionally out-of-tree provisioners are now supported (see <a href=https://github.com/kubernetes-incubator/external-storage>kubernetes-incubator</a>).</p><p>In the <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6 release</a>, <strong>dynamic provisioning has been promoted to stable</strong> (having entered beta in 1.4). This is a big step forward in completing the Kubernetes storage automation vision, allowing cluster administrators to control how resources are provisioned and giving users the ability to focus more on their application. With all of these benefits, <strong>there are a few important user-facing changes (discussed below) that are important to understand before using Kubernetes 1.6</strong>.</p><p><strong>Storage Classes and How to Use them</strong></p><p>StorageClasses are the foundation of dynamic provisioning, allowing cluster administrators to define abstractions for the underlying storage platform. Users simply refer to a StorageClass by name in the PersistentVolumeClaim (PVC) using the “storageClassName” parameter.</p><p>In the following example, a PVC refers to a specific storage class named “gold”.</p><pre><code>apiVersion: v1

kind: PersistentVolumeClaim

metadata:

 name: mypvc

 namespace: testns

spec:

 accessModes:

 - ReadWriteOnce

 resources:

   requests:

     storage: 100Gi

 storageClassName: gold
</code></pre><p>In order to promote the usage of dynamic provisioning this feature permits the cluster administrator to specify a <strong>default</strong> StorageClass. When present, the user can create a PVC without having specifying a storageClassName, further reducing the user’s responsibility to be aware of the underlying storage provider. When using default StorageClasses, there are some operational subtleties to be aware of when creating PersistentVolumeClaims (PVCs). This is particularly important if you already have existing PersistentVolumes (PVs) that you want to re-use:</p><ul><li><p>PVs that are already “Bound” to PVCs will remain bound with the move to 1.6</p><ul><li>They will not have a StorageClass associated with them unless the user manually adds it</li><li>If PVs become “Available” (i.e.; if you delete a PVC and the corresponding PV is recycled), then they are subject to the following</li></ul></li><li><p>If storageClassName <strong>is not specified</strong> in the PVC, the <strong>default storage class will be used</strong> for provisioning.</p><ul><li>Existing, “Available”, PVs that do not have the default storage class label <strong>will not be considered</strong> for binding to the PVC</li></ul></li><li><p>If storageClassName <strong>is set to an empty string</strong> <strong>(‘’)</strong> in the PVC, no storage class will be used (i.e.; dynamic provisioning is disabled for this PVC)</p><ul><li>Existing, “Available”, PVs (that do not have a specified storageClassName) <strong>will be considered</strong> for binding to the PVC</li></ul></li><li><p>If storageClassName is set to a specific value, then the matching storage class will be used</p><ul><li>Existing, “Available”, PVs that have a matching storageClassName <strong>will be considered</strong> for binding to the PVC</li><li>If no corresponding storage class exists, the PVC will fail.
To reduce the burden of setting up default StorageClasses in a cluster, beginning with 1.6, Kubernetes installs (via the add-on manager) default storage classes for several cloud providers. To use these default StorageClasses, users <strong>do not</strong> need refer to them by name – that is, storageClassName need not be specified in the PVC.</li></ul></li></ul><p>The following table provides more detail on default storage classes pre-installed by cloud provider as well as the specific parameters used by these defaults.</p><table><thead><tr><th>Cloud Provider</th><th>Default StorageClass Name</th><th>Default Provisioner</th></tr></thead><tbody><tr><td>Amazon Web Services</td><td>gp2</td><td>aws-ebs</td></tr><tr><td>Microsoft Azure</td><td>standard</td><td>azure-disk</td></tr><tr><td>Google Cloud Platform</td><td>standard</td><td>gce-pd</td></tr><tr><td>OpenStack</td><td>standard</td><td>cinder</td></tr><tr><td>VMware vSphere</td><td>thin</td><td>vsphere-volume</td></tr></tbody></table><p>While these pre-installed default storage classes are chosen to be “reasonable” for most storage users, <a href=/docs/tasks/administer-cluster/change-default-storage-class>this guide</a> provides instructions on how to specify your own default.</p><p><strong>Dynamically Provisioned Volumes and the Reclaim Policy</strong></p><p>All PVs have a reclaim policy associated with them that dictates what happens to a PV once it becomes released from a claim (see <a href=/docs/user-guide/persistent-volumes/#reclaiming>user-guide</a>). Since the goal of dynamic provisioning is to completely automate the lifecycle of storage resources, the default reclaim policy for dynamically provisioned volumes is “delete”. This means that when a PersistentVolumeClaim (PVC) is released, the dynamically provisioned volume is de-provisioned (deleted) on the storage provider and the data is likely irretrievable. If this is not the desired behavior, the user must change the reclaim policy on the corresponding PersistentVolume (PV) object after the volume is provisioned.</p><p><strong>How do I change the reclaim policy on a dynamically provisioned volume?</strong></p><p>You can change the reclaim policy by editing the PV object and changing the “persistentVolumeReclaimPolicy” field to the desired value. For more information on various reclaim policies see <a href=/docs/user-guide/persistent-volumes/#reclaim-policy>user-guide</a>.</p><p><strong>FAQs</strong></p><p><strong>How do I use a default StorageClass?</strong></p><p>If your cluster has a default StorageClass that meets your needs, then all you need to do is create a PersistentVolumeClaim (PVC) and the default provisioner will take care of the rest – there is no need to specify the storageClassName:</p><pre><code>apiVersion: v1

kind: PersistentVolumeClaim

metadata:

 name: mypvc

 namespace: testns

spec:

 accessModes:

 - ReadWriteOnce

 resources:

   requests:

     storage: 100Gi
</code></pre><p><strong>Can I add my own storage classes?</strong><br>Yes. To add your own storage class, first determine which provisioners will work in your cluster. Then, create a StorageClass object with parameters customized to meet your needs (see user-guide for more detail). For many users, the easiest way to create the object is to write a yaml file and apply it with “kubectl create -f”. The following is an example of a StorageClass for Google Cloud Platform named “gold” that creates a “pd-ssd”. Since multiple classes can exist within a cluster, the administrator may leave the default enabled for most workloads (since it uses a “pd-standard”), with the “gold” class reserved for workloads that need extra performance.</p><pre><code>kind: StorageClass

apiVersion: storage.k8s.io/v1

metadata:

 name: gold

provisioner: kubernetes.io/gce-pd

parameters:

 type: pd-ssd
</code></pre><p><strong>How do I check if I have a default StorageClass Installed?</strong></p><p>You can use kubectl to check for StorageClass objects. In the example below there are two storage classes: “gold” and “standard”. The “gold” class is user-defined, and the “standard” class is installed by Kubernetes and is the default.</p><pre><code>$ kubectl get sc

NAME                 TYPE

gold                 kubernetes.io/gce-pd   

standard (default)   kubernetes.io/gce-pd
</code></pre><pre><code>$ kubectl describe storageclass standard

Name:     standard

IsDefaultClass: Yes

Annotations: storageclass.beta.kubernetes.io/is-default-class=true

Provisioner: kubernetes.io/gce-pd

Parameters: type=pd-standard

Events:         \&lt;none\&gt;
</code></pre><p><strong>Can I delete/turn off the default StorageClasses?</strong><br>You cannot delete the default storage class objects provided. Since they are installed as cluster addons, they will be recreated if they are deleted.</p><p>You can, however, disable the defaulting behavior by removing (or setting to false) the following annotation: storageclass.beta.kubernetes.io/is-default-class.</p><p>If there are no StorageClass objects marked with the default annotation, then PersistentVolumeClaim objects (without a StorageClass specified) will not trigger dynamic provisioning. They will, instead, fall back to the legacy behavior of binding to an available PersistentVolume object.</p><p><strong>Can I assign my existing PVs to a particular StorageClass?</strong><br>Yes, you can assign a StorageClass to an existing PV by editing the appropriate PV object and adding (or setting) the desired storageClassName field to it.</p><p><strong>What happens if I delete a PersistentVolumeClaim (PVC)?</strong><br>If the volume was dynamically provisioned, then the default reclaim policy is set to “delete”. This means that, by default, when the PVC is deleted, the underlying PV and storage asset will also be deleted. If you want to retain the data stored on the volume, then you must change the reclaim policy from “delete” to “retain” after the PV is provisioned.</p><p><em>--Saad Ali & Michelle Au, Software Engineers, and Matthew De Lio, Product Manager, Google</em></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f282dcdc94740ae673746535ed7c2f69>Five Days of Kubernetes 1.6</h1><div class="td-byline mb-4"><time datetime=2017-03-29 class=text-muted>Wednesday, March 29, 2017</time></div><p>With the help of our growing community of 1,110 plus contributors, we pushed around 5,000 commits to deliver <a href=https://kubernetes.io/blog/2017/03/kubernetes-1-6-multi-user-multi-workloads-at-scale>Kubernetes 1.6</a>, bringing focus on multi-user, multi-workloads at scale. While many improvements have been contributed, we selected few features to highlight in a series of in-depths posts listed below. </p><p>Follow along and read what’s new:</p><table><thead><tr><th></th><th>Five Days of Kubernetes</th></tr></thead><tbody><tr><td>Day 1</td><td><a href=https://kubernetes.io/blog/2017/03/dynamic-provisioning-and-storage-classes-kubernetes>Dynamic Provisioning and Storage Classes in Kubernetes Stable in 1.6</a></td></tr><tr><td>Day 2</td><td><a href=https://kubernetes.io/blog/2017/03/scalability-updates-in-kubernetes-1-6/>Scalability updates in Kubernetes 1.6</a></td></tr><tr><td>Day 3</td><td><a href=https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes>Advanced Scheduling in Kubernetes 1.6</a></td></tr><tr><td>Day 4</td><td><a href=https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes>Configuring Private DNS Zones and Upstream Nameservers in Kubernetes</a></td></tr><tr><td>Day 5</td><td><a href=https://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes>RBAC support in Kubernetes</a></td></tr></tbody></table><p><strong>Connect</strong></p><ul><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Join the community portal for advocates on <a href=http://k8sport.org/>K8sPort</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-becd4dd38c8bc7a17aff746a61683974>Kubernetes 1.6: Multi-user, Multi-workloads at Scale</h1><div class="td-byline mb-4"><time datetime=2017-03-28 class=text-muted>Tuesday, March 28, 2017</time></div><p>Today we’re announcing the release of Kubernetes 1.6.</p><p>In this release the community’s focus is on scale and automation, to help you deploy multiple workloads to multiple users on a cluster. We are announcing that 5,000 node clusters are supported. We moved dynamic storage provisioning to <em>stable</em>. Role-based access control (<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>), <a href=/docs/tutorials/federation/set-up-cluster-federation-kubefed/>kubefed</a>, <a href=/docs/getting-started-guides/kubeadm/>kubeadm</a>, and several scheduling features are moving to <em>beta</em>. We have also added intelligent defaults throughout to enable greater automation out of the box.</p><p><strong>What’s New</strong></p><p><strong>Scale and Federation</strong> : Large enterprise users looking for proof of at-scale performance will be pleased to know that Kubernetes’ stringent scalability <a href=https://kubernetes.io/blog/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12>SLO</a> now supports 5,000 node (150,000 pod) clusters. This 150% increase in total cluster size, powered by a new version of <a href=https://coreos.com/blog/etcd3-a-new-etcd.html>etcd v3</a> by CoreOS, is great news if you are deploying applications such as search or games which can grow to consume larger clusters.</p><p>For users who want to scale beyond 5,000 nodes or spread across multiple regions or clouds, <a href=/docs/concepts/cluster-administration/federation/>federation</a> lets you combine multiple Kubernetes clusters and address them through a single API endpoint. In this release, the <a href=https://kubernetes.io//docs/tutorials/federation/set-up-cluster-federation-kubefed>kubefed</a> command line utility graduated to <em>beta</em> - with improved support for on-premise clusters. kubefed now <a href=https://kubernetes.io//docs/tutorials/federation/set-up-cluster-federation-kubefed.md#kube-dns-configuration>automatically configures</a> kube-dns on joining clusters and can pass arguments to federated components.</p><p><strong>Security and Setup</strong> : Users concerned with security will find that <a href=/docs/reference/access-authn-authz/rbac>RBAC</a>, now <em>beta</em> adds a significant security benefit through more tightly scoped default roles for system components. The default RBAC policies in 1.6 grant scoped permissions to control-plane components, nodes, and controllers. RBAC allows cluster administrators to selectively grant particular users or service accounts fine-grained access to specific resources on a per-namespace basis. RBAC users upgrading from 1.5 to 1.6 should view the guidance <a href=/docs/reference/access-authn-authz/rbac#upgrading-from-1-5>here</a>. </p><p>Users looking for an easy way to provision a secure cluster on physical or cloud servers can use <a href=/docs/getting-started-guides/kubeadm/>kubeadm</a>, which is now <em>beta</em>. kubeadm has been enhanced with a set of command line flags and a base feature set that includes RBAC setup, use of the <a href=/docs/reference/access-authn-authz/bootstrap-tokens/>Bootstrap Token system</a> and an enhanced <a href=/docs/tasks/tls/managing-tls-in-a-cluster/>Certificates API</a>.</p><p><strong>Advanced Scheduling</strong> : This release adds a set of <a href=/docs/user-guide/node-selection/>powerful and versatile scheduling constructs</a> to give you greater control over how pods are scheduled, including rules to restrict pods to particular nodes in heterogeneous clusters, and rules to spread or pack pods across failure domains such as nodes, racks, and zones.</p><p><a href=/docs/user-guide/node-selection/#node-affinity-beta-feature>Node affinity/anti-affinity</a>, now in <em>beta</em>, allows you to restrict pods to schedule only on certain nodes based on node labels. Use built-in or custom node labels to select specific zones, hostnames, hardware architecture, operating system version, specialized hardware, etc. The scheduling rules can be required or preferred, depending on how strictly you want the scheduler to enforce them.</p><p>A related feature, called <a href=/docs/user-guide/node-selection/#taints-and-tolerations-beta-feature>taints and tolerations</a>, makes it possible to compactly represent rules for excluding pods from particular nodes. The feature, also now in <em>beta</em>, makes it easy, for example, to dedicate sets of nodes to particular sets of users, or to keep nodes that have special hardware available for pods that need the special hardware by excluding pods that don’t need it.</p><p>Sometimes you want to co-schedule services, or pods within a service, near each other topologically, for example to optimize North-South or East-West communication. Or you want to spread pods of a service for failure tolerance, or keep antagonistic pods separated, or ensure sole tenancy of nodes. <a href=/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature>Pod affinity and anti-affinity</a>, now in <em>beta</em>, enables such use cases by letting you set hard or soft requirements for spreading and packing pods relative to one another within arbitrary topologies (node, zone, etc.).</p><p>Lastly, for the ultimate in scheduling flexibility, you can run your own custom scheduler(s) alongside, or instead of, the default Kubernetes scheduler. Each scheduler is responsible for different sets of pods. <a href=/docs/admin/multiple-schedulers/>Multiple schedulers</a> is <em>beta</em> in this release. </p><p><strong>Dynamic Storage Provisioning</strong> : Users deploying stateful applications will benefit from the extensive storage automation capabilities in this release of Kubernetes.</p><p>Since its early days, Kubernetes has been able to automatically attach and detach storage, format disk, mount and unmount volumes per the pod spec, and do so seamlessly as pods move between nodes. In addition, the PersistentVolumeClaim (PVC) and PersistentVolume (PV) objects decouple the request for storage from the specific storage implementation, making the pod spec portable across a range of cloud and on-premise environments. In this release <a href=/docs/user-guide/persistent-volumes/#storageclasses>StorageClass</a> and <a href=/docs/user-guide/persistent-volumes/#dynamic>dynamic volume provisioning</a> are promoted to <em>stable</em>, completing the automation story by creating and deleting storage on demand, eliminating the need to pre-provision.</p><p>The design allows cluster administrators to define and expose multiple flavors of storage within a cluster, each with a custom set of parameters. End users can stop worrying about the complexity and nuances of how storage is provisioned, while still selecting from multiple storage options.</p><p>In 1.6 Kubernetes comes with a set of built-in defaults to completely automate the storage provisioning lifecycle, freeing you to work on your applications. Specifically, Kubernetes now pre-installs system-defined StorageClass objects for AWS, Azure, GCP, OpenStack and VMware vSphere <a href=/docs/tasks/administer-cluster/change-default-storage-class>by default</a>. This gives Kubernetes users on these providers the benefits of dynamic storage provisioning without having to manually setup StorageClass objects. This is a <a href=/docs/user-guide/persistent-volumes/index#class-1>change in the default behavior</a> of PVC objects on these clouds. Note that default behavior is that dynamically provisioned volumes are created with the “delete” <a href=/docs/user-guide/persistent-volumes#reclaim-policy>reclaim policy</a>. That means once the PVC is deleted, the dynamically provisioned volume is automatically deleted so users do not have the extra step of ‘cleaning up’.</p><p>In addition, we have expanded the range of storage supported overall including:</p><ul><li>ScaleIO Kubernetes <a href=/docs/user-guide/persistent-volumes/index#scaleio>Volume Plugin</a> enabling pods to seamlessly access and use data stored on ScaleIO volumes.</li><li>Portworx Kubernetes <a href=/docs/user-guide/persistent-volumes/index#portworx-volume>Volume Plugin</a> adding the capability to use Portworx as a storage provider for Kubernetes clusters. Portworx pools your server capacity and turns your servers or cloud instances into converged, highly available compute and storage nodes.</li><li>Support for NFSv3, NFSv4, and GlusterFS on clusters using the <a href=https://cloud.google.com/container-engine/docs/node-image-migration>COS node image</a> </li><li>Support for user-written/run dynamic PV provisioners. A golang library and examples can be found <a href=http://github.com/kubernetes-incubator/external-storage>here</a>.</li><li><em>Beta</em> support for <a href=/docs/user-guide/persistent-volumes/index.md#mount-options>mount options</a> in persistent volumes</li></ul><p><strong>Container Runtime Interface, etcd v3 and Daemon set updates</strong> : while users may not directly interact with the container runtime or the API server datastore, they are foundational components for user facing functionality in Kubernetes’. As such the community invests in expanding the capabilities of these and other system components.</p><ul><li>The Docker-CRI implementation is <em>beta</em> and is enabled by default in kubelet. <em>Alpha</em> support for other runtimes, <a href=https://github.com/kubernetes-incubator/cri-o/releases/tag/v0.1>cri-o</a>, <a href=https://github.com/kubernetes/frakti/releases/tag/v0.1>frakti</a>, <a href="https://github.com/coreos/rkt/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%2Fcri">rkt</a>, has also been implemented.</li><li>The default backend storage for the API server has been <a href=/docs/admin/upgrade-1-6/>upgraded</a> to use <a href=https://coreos.com/blog/etcd3-a-new-etcd.html>etcd v3</a> by default for new clusters. If you are upgrading from a 1.5 cluster, care should be taken to ensure continuity by planning a data migration window. </li><li>Node reliability is improved as Kubelet exposes an admin configurable <a href=https://kubernetes.io//docs/admin/node-allocatable.md#node-allocatable>Node Allocatable</a> feature to reserve compute resources for system daemons.</li><li><a href=/docs/tasks/manage-daemon/update-daemon-set>Daemon set updates</a> lets you perform rolling updates on a daemon set</li></ul><p><strong>Alpha features</strong> : this release was mostly focused on maturing functionality, however, a few alpha features were added to support the roadmap</p><ul><li><a href=/docs/concepts/overview/components#cloud-controller-manager>Out-of-tree cloud provider</a> support adds a new cloud-controller-manager binary that may be used for testing the new out-of-core cloud provider flow</li><li><a href=/docs/user-guide/node-selection/#per-pod-configurable-eviction-behavior-when-there-are-node-problems-alpha-feature>Per-pod-eviction</a> in case of node problems combined with tolerationSeconds, lets users tune the duration a pod stays bound to a node that is experiencing problems</li><li><a href=/docs/user-guide/pod-preset/>Pod Injection Policy</a> adds a new API resource PodPreset to inject information such as secrets, volumes, volume mounts, and environment variables into pods at creation time.</li><li><a href=/docs/user-guide/horizontal-pod-autoscaling/#support-for-custom-metrics>Custom metrics</a> support in the Horizontal Pod Autoscaler changed to use </li><li>Multiple Nvidia <a href=https://vishh.github.io/docs/user-guide/gpus/>GPU support</a> is introduced with the Docker runtime only</li></ul><p>These are just some of the highlights in our first release for the year. For a complete list please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v160>release notes</a>.</p><p><strong>Community</strong><br>This release is possible thanks to our vast and open community. Together, we’ve pushed nearly 5,000 commits by some 275 authors. To bring our many advocates together, the community has launched a new program called <a href=http://k8sport.org/>K8sPort</a>, an online hub where the community can participate in gamified challenges and get credit for their contributions. Read more about the program <a href=https://kubernetes.io/blog/2017/03/k8sport-engaging-the-kubernetes-community>here</a>.</p><p><strong>Release Process</strong></p><p>A big thanks goes out to the <a href=https://github.com/kubernetes/features/blob/master/release-1.6/release_team.md>release team</a> for 1.6 (lead by Dan Gillespie of CoreOS) for their work bringing the 1.6 release to light. This release team is an exemplar of the Kubernetes community’s commitment to community governance. Dan is the first non-Google release manager and he, along with the rest of the team, worked throughout the release (building on the 1.5 release manager, Saad Ali’s, great work) to uncover and document tribal knowledge, shine light on tools and processes that still require special permissions, and prioritize work to improve the Kubernetes release process. Many thanks to the team.</p><p><strong>User Adoption</strong></p><p>We’re continuing to see rapid adoption of Kubernetes in all sectors and sizes of businesses. Furthermore, adoption is coming from across the globe, from a startup in Tennessee, USA to a Fortune 500 company in China. </p><ul><li>JD.com, one of China's largest internet companies, uses Kubernetes in conjunction with their OpenStack deployment. They’ve move 20% of their applications thus far on Kubernetes and are already running 20,000 pods daily. Read more about their setup <a href=https://kubernetes.io/blog/2017/02/inside-jd-com-shift-to-kubernetes-from-openstack>here</a>. </li><li>Spire, a startup based in Tennessee, witnessed their public cloud provider experience an outage, but suffered zero downtime because Kubernetes was able to move their workloads to different zones. Read their full experience <a href=https://medium.com/spire-labs/mitigating-an-aws-instance-failure-with-the-magic-of-kubernetes-128a44d44c14>here</a>.</li></ul><blockquote><p><em>“With Kubernetes, there was never a moment of panic, just a sense of awe watching the automatic mitigation as it happened.”</em></p></blockquote><ul><li>Share your Kubernetes use case story with the community <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform>here</a>.</li></ul><p><strong>Availability</strong><br>Kubernetes 1.6 is available for download <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.6.0>here</a> on GitHub and via <a href=http://get.k8s.io/>get.k8s.io</a>. To get started with Kubernetes, try one of the these <a href=/docs/tutorials/kubernetes-basics/>interactive tutorials</a>. </p><p><strong>Get Involved</strong><br><a href=http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-europe>CloudNativeCon + KubeCon in Berlin</a> is this week March 29-30, 2017. We hope to get together with much of the community and share more there!</p><p>Share your voice at our weekly <a href=https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting>community meeting</a>: </p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li></ul><p>Many thanks for your contributions and advocacy!</p><p><em>-- Aparna Sinha, Senior Product Manager, Kubernetes, Google</em></p><p><em><strong>PS: read this <a href=https://kubernetes.io/blog/2017/03/five-days-of-kubernetes-1-6>series of in-depth articles</a> on what's new in Kubernetes 1.6</strong></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-51bc9cd5c5388c387daf2e05502b4226>The K8sPort: Engaging Kubernetes Community One Activity at a Time</h1><div class="td-byline mb-4"><time datetime=2017-03-24 class=text-muted>Friday, March 24, 2017</time></div><p><em>Editor's note: Today’s post is by Ryan Quackenbush, Advocacy Programs Manager at Apprenda, showing a new community portal for Kubernetes advocates: the K8sPort.</em></p><p>The <a href=http://k8sport.org/><strong>K8sPort</strong></a> is a hub designed to help you, the Kubernetes community, earn credit for the hard work you’re putting forth in making this one of the most successful open source projects ever. Back at KubeCon Seattle in November, I <a href=https://youtu.be/LwViH5eLoOI>presented</a> a lightning talk of a preview of K8sPort.</p><p>This hub, and our intentions in helping to drive this initiative in the community, grew out of a desire to help cultivate an engaged community of Kubernetes advocates. This is done through gamification in a community hub full of different activities called “challenges,” which are activities meant to help direct members of the community to attend various events and meetings, share and provide feedback on important content, answer questions posed on sites like Stack Overflow, and more.</p><p>By completing these challenges, you collect points and can redeem them for different types of rewards and experiences, examples of which include charitable donations, gift certificates, conference tickets and more. As advocates complete challenges and gain points, they’ll earn performance-related badges, move up in community tiers and participate in a fun community leaderboard.</p><p>My presentation at KubeCon, simply put, was a call for early signups. Those who’ve been piloting the program have, for the most part, had positive things to say about their experiences.</p><blockquote><p>I know I'm the only one playing with <a href=https://twitter.com/K8sPort>@K8sPort</a> but it may be the most important thing the <a href="https://twitter.com/hashtag/Kubernetes?src=hash">#Kubernetes</a> community has.</p><p>— Justin Garrison (@rothgar) <a href=https://twitter.com/rothgar/status/800941707558670336>November 22, 2016</a></p></blockquote><ul><li><em>“Great way of improving the community and documentation. The gamification of Kubernetes gave me more insight into the stack as well.”</em><ul><li>Jonas Kint, Devops Engineer at Showpad</li></ul></li><li>_“A great way to engage with the kubernetes project and also help the community. Fun stuff.” _<ul><li>Kevin Duane, Systems Engineer at The Walt Disney Company</li></ul></li><li><em>“K8sPort seems like an awesome idea for incentivising giving back to the community in a way that will hopefully cause more valuable help from more people than might usually be helping.”</em><ul><li>William Stewart, Site Reliability Engineer at Superbalist</li></ul></li></ul><p>Today I am pleased to announce that the <a href=https://www.cncf.io/>Cloud Native Computing Foundation</a> (CNCF) is making the K8sPort generally available to the entire contributing community! We’ve simplified the signup process by allowing would-be advocates to authenticate and register through the use of their existing GitHub accounts.</p><p><img src=https://lh4.googleusercontent.com/h9D3-poSxGMelrhvKE2PBX-_pXRJJZF4NfW8ShyxzOrQekZvgZuIlaphkg_35QPKGG-Z22dTcFymj48qO5nXQjuVussmThewiceMQ9Hr7bUm5YRaJhTpDuCU2kJKQjTyZXcslreH alt="Screen Shot 2017-03-22 at 10.59.04 AM.png"></p><p>If you’re a contributing member of the Kubernetes community and you have an active GitHub account tied to the Kubernetes repository at GitHub, you can authenticate using your GitHub credentials and gain access to the K8sPort.</p><p><img src=https://lh3.googleusercontent.com/dmg-Po-XlYHMFrij3GcryySkxw4Q0BaEKlWLeeWwKFr8nSmw55rbmpk0WWRiIWQZgcAbCNhomt1JUT0Ohntm3aXVwReXxgWkfjbJJtICILltePU9Zr70iNqBNfgsX26majAqW5r8 alt="Screen Shot 2017-03-22 at 12.48.08 PM.png">
Beyond the challenges that get posted regularly, community members will be recognized and compile points for things they’re already doing today. This will be accomplished through the K8sPort’s full integration with GitHub and the core Kubernetes repository. Once you authenticate, you’ll automatically begin earning points and recognition for various contributions -- including logging issues, making pull requests, code commits & more.</p><p>If you’re interested in joining the advocacy hub, please join us at <a href=http://k8sport.org/>k8sport.org</a>! We hope you’re as excited about what you see as we are to continue to build it and present it to you.</p><p>For a quick walkthrough on K8sPort authentication and the hub itself, see this quick demo, below.</p><p><em>--Ryan Quackenbush, Advocacy Programs Manager, Apprenda</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4fd7ad490e1c089f40a637090b05933e>Deploying PostgreSQL Clusters using StatefulSets</h1><div class="td-byline mb-4"><time datetime=2017-02-24 class=text-muted>Friday, February 24, 2017</time></div><p><em>Editor’s note: Today’s guest post is by Jeff McCormick, a developer at Crunchy Data, showing how to build a PostgreSQL cluster using the new Kubernetes StatefulSet feature.</em></p><p>In an earlier <a href=https://kubernetes.io/blog/2016/09/creating-postgresql-cluster-using-helm>post</a>, I described how to deploy a PostgreSQL cluster using <a href=https://github.com/kubernetes/helm>Helm</a>, a Kubernetes package manager. The following example provides the steps for building a PostgreSQL cluster using the new Kubernetes <a href=/docs/concepts/abstractions/controllers/statefulsets/>StatefulSets</a> feature.</p><p><strong>StatefulSets Example</strong></p><p><strong>Step 1</strong> - Create Kubernetes Environment</p><p>StatefulSets is a new feature implemented in <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5</a> (prior versions it was known as PetSets). As a result, running this example will require an environment based on Kubernetes 1.5.0 or above.</p><p>The example in this blog deploys on Centos7 using <a href=/docs/admin/kubeadm/>kubeadm</a>. Some instructions on what kubeadm provides and how to deploy a Kubernetes cluster is located <a href=http://linoxide.com/containers/setup-kubernetes-kubeadm-centos>here</a>.</p><p><strong>Step 2</strong> - Install NFS</p><p>The example in this blog uses NFS for the Persistent Volumes, but any shared file system would also work (ex: ceph, gluster).</p><p>The example script assumes your NFS server is running locally and your hostname resolves to a known IP address.</p><p>In summary, the steps used to get NFS working on a Centos 7 host are as follows:</p><pre><code>sudo setsebool -P virt\_use\_nfs 1

sudo yum -y install nfs-utils libnfsidmap

sudo systemctl enable rpcbind nfs-server

sudo systemctl start rpcbind nfs-server rpc-statd nfs-idmapd

sudo mkdir /nfsfileshare

sudo chmod 777 /nfsfileshare/

sudo vi /etc/exports

sudo exportfs -r
</code></pre><p>The /etc/exports file should contain a line similar to this one except with the applicable IP address specified:</p><pre><code>/nfsfileshare 192.168.122.9(rw,sync)
</code></pre><p>After these steps NFS should be running in the test environment.</p><p><strong>Step 3</strong> - Clone the Crunchy PostgreSQL Container Suite</p><p>The example used in this blog is found at in the Crunchy Containers GitHub repo <a href=https://github.com/CrunchyData/crunchy-containers.git>here</a>. Clone the Crunchy Containers repository to your test Kubernertes host and go to the example:</p><pre><code>cd $HOME

git clone https://github.com/CrunchyData/crunchy-containers.git

cd crunchy-containers/examples/kube/statefulset
</code></pre><p>Next, pull down the Crunchy PostgreSQL container image:</p><pre><code>docker pull crunchydata/crunchy-postgres:centos7-9.5-1.2.6
</code></pre><p><strong>Step 4</strong> - Run the Example</p><p>To begin, it is necessary to set a few of the environment variables used in the example:</p><pre><code>export BUILDBASE=$HOME/crunchy-containers

export CCP\_IMAGE\_TAG=centos7-9.5-1.2.6
</code></pre><p>BUILDBASE is where you cloned the repository and CCP_IMAGE_TAG is the container image version we want to use.</p><p>Next, run the example:</p><pre><code>./run.sh
</code></pre><p>That script will create several Kubernetes objects including:</p><ul><li>Persistent Volumes (pv1, pv2, pv3)</li><li>Persistent Volume Claim (pgset-pvc)</li><li>Service Account (pgset-sa)</li><li>Services (pgset, pgset-master, pgset-replica)</li><li>StatefulSet (pgset)</li><li>Pods (pgset-0, pgset-1)</li></ul><p>At this point, two pods will be running in the Kubernetes environment:</p><pre><code>$ kubectl get pod

NAME      READY     STATUS    RESTARTS   AGE

pgset-0   1/1       Running   0          2m

pgset-1   1/1       Running   1          2m
</code></pre><p>Immediately after the pods are created, the deployment will be as depicted below:</p><p><a href=https://lh5.googleusercontent.com/tGg-37a7SoVQR9Zn3R209iKbkegX5XqRQdRa5ZD6q-vpm1hWqtBxnhOBiGw2uHHkZ5lc_VBKrSEEP29BmAzoWc1xydV7G4I8kaQqVZoYOdRCvBf755Rxf9aj-pm7FhfmgECBW3gR><img src=https://lh5.googleusercontent.com/tGg-37a7SoVQR9Zn3R209iKbkegX5XqRQdRa5ZD6q-vpm1hWqtBxnhOBiGw2uHHkZ5lc_VBKrSEEP29BmAzoWc1xydV7G4I8kaQqVZoYOdRCvBf755Rxf9aj-pm7FhfmgECBW3gR alt></a></p><p><strong>Step 5</strong> - What Just Happened?</p><p>This example will deploy a StatefulSet, which in turn creates two pods.</p><p>The containers in those two pods run the PostgreSQL database. For a PostgreSQL cluster, we need one of the containers to assume the master role and the other containers to assume the replica role.</p><p>So, how do the containers determine who will be the master, and who will be the replica?</p><p>This is where the new StateSet mechanics come into play. The StateSet mechanics assign a unique ordinal value to each pod in the set.</p><p>The StatefulSets provided unique ordinal value always start with 0. During the initialization of the container, each container examines its assigned ordinal value. An ordinal value of 0 causes the container to assume the master role within the PostgreSQL cluster. For all other ordinal values, the container assumes a replica role. This is a very simple form of discovery made possible by the StatefulSet mechanics.</p><p>PostgreSQL replicas are configured to connect to the master database via a Service dedicated to the master database. In order to support this replication, the example creates a separate Service for each of the master role and the replica role. Once the replica has connected, the replica will begin replicating state from the master.</p><p>During the container initialization, a master container will use a <a href=/docs/user-guide/service-accounts/>Service Account</a> (pgset-sa) to change it’s container label value to match the master Service selector. Changing the label is important to enable traffic destined to the master database to reach the correct container within the Stateful Set. All other pods in the set assume the replica Service label by default.</p><p><strong>Step 6</strong> - Deployment Diagram</p><p>The example results in a deployment depicted below:</p><p><img src=https://lh3.googleusercontent.com/5NthdAnA243jN_gXVlwZsg74jkGgCwQZh1yq78-8E0L7wuDgpdqH_AaeUvQd9RtXIlOV0cAWv1P0a_2oeVJN8fHstf9Iev1c-swGIqojIw0pXrVuqAqpCF3M5hw6sdTmx_1-Bg27 alt></p><p>In this deployment, there is a Service for the master and a separate Service for the replica. The replica is connected to the master and replication of state has started.</p><p>The Crunchy PostgreSQL container supports other forms of cluster deployment, the style of deployment is dictated by setting the PG_MODE environment variable for the container. In the case of a StatefulSet deployment, that value is set to: PG_MODE=set</p><p>This environment variable is a hint to the container initialization logic as to the style of deployment we intend.</p><p><strong>Step 7</strong> - Testing the Example</p><p>The tests below assume that the psql client has been installed on the test system. If not, the psql client has been previously installed, it can be installed as follows:</p><pre><code>sudo yum -y install postgresql
</code></pre><p>In addition, the tests below assume that the tested environment DNS resolves to the Kube DNS and that the tested environment DNS search path is specified to match the applicable Kube namespace and domain. The master service is named pgset-master and the replica service is named pgset-replica.</p><p>Test the master as follows (the password is password):</p><pre><code>psql -h pgset-master -U postgres postgres -c 'table pg\_stat\_replication'
</code></pre><p>If things are working, the command above will return output indicating that a single replica is connecting to the master.</p><p>Next, test the replica as follows:</p><pre><code>psql -h pgset-replica -U postgres postgres  -c 'create table foo (id int)'
</code></pre><p>The command above should fail as the replica is <strong>read-only</strong> within a PostgreSQL cluster.</p><p>Next, scale up the set as follows:</p><pre><code>kubectl scale statefulset pgset --replicas=3
</code></pre><p>The command above should successfully create a new replica pod called <strong>pgset-2</strong> as depicted below:</p><p><img src=https://lh5.googleusercontent.com/w82XRPd9LqwgcoY3wJrilJEULxZyub6HLcFk332--1fd94-Vte4YlDFvspLM9syNCdT47PISJlEDo7jSPmiflFv-ZZKmrY6Jm6sJWMki0RfJigf6a6IEPNeyy1PJ_5Mhd4NW4rHm alt></p><p><strong>Step 8</strong> - Persistence Explained</p><p>Take a look at the persisted PostgreSQL data files on the resulting NFS mount path:</p><pre><code>$ ls -l /nfsfileshare/

total 12

drwx------ 20   26   26 4096 Jan 17 16:35 pgset-0

drwx------ 20   26   26 4096 Jan 17 16:35 pgset-1

drwx------ 20   26   26 4096 Jan 17 16:48 pgset-2
</code></pre><p>Each container in the stateful set binds to the single NFS Persistent Volume Claim (pgset-pvc) created in the example script.</p><p>Since NFS and the PVC can be shared, each pod can write to this NFS path.</p><p>The container is designed to create a subdirectory on that path using the pod host name for uniqueness.</p><p><strong>Conclusion</strong></p><p>StatefulSets is an exciting feature added to Kubernetes for container builders that are implementing clustering. The ordinal values assigned to the set provide a very simple mechanism to make clustering decisions when deploying a PostgreSQL cluster.</p><p><em>--Jeff McCormick, Developer, <a href=http://crunchydata.com/>Crunchy Data</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1a75a28dfa116a64ae5bb5f2f7999206>Containers as a Service, the foundation for next generation PaaS</h1><div class="td-byline mb-4"><time datetime=2017-02-21 class=text-muted>Tuesday, February 21, 2017</time></div><p><em>Today’s post is by Brendan Burns, Partner Architect, at Microsoft & Kubernetes co-founder.</em></p><p>Containers are revolutionizing the way that people build, package and deploy software. But what is often overlooked is how they are revolutionizing the way that people build the software that builds, packages and deploys software. (it’s ok if you have to read that sentence twice…) Today, and in a talk at <a href=https://tmt.knect365.com/container-world/>Container World</a> tomorrow, I’m taking a look at how container orchestrators like Kubernetes form the foundation for next generation platform as a service (PaaS). In particular, I’m interested in how cloud container as a service (CaaS) platforms like <a href=https://azure.microsoft.com/en-us/services/container-service/>Azure Container Service</a>, <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> and <a href=/docs/getting-started-guides/#hosted-solutions>others</a> are becoming the new infrastructure layer that PaaS is built upon.</p><p>To see this, it’s important to consider the set of services that have traditionally been provided by PaaS platforms:</p><ul><li>Source code and executable packaging and distribution</li><li>Reliable, zero-downtime rollout of software versions</li><li>Healing, auto-scaling, load balancing</li></ul><p>When you look at this list, it’s clear that most of these traditional “PaaS” roles have now been taken over by containers. The container image and container image build tooling has become the way to package up your application. <a href=/docs/user-guide/images/#using-a-private-registry>Container registries</a> have become the way to distribute your application across the world. Reliable software rollout is achieved using orchestrator concepts like <a href=/docs/user-guide/deployments/#what-is-a-deployment>Deployment</a> in Kubernetes, and service healing, auto-scaling and load-balancing are all properties of an application deployed in Kubernetes using <a href=/docs/user-guide/replicasets/#what-is-a-replicaset>ReplicaSets</a> and <a href=/docs/user-guide/services/>Services</a>.</p><p>What then is left for PaaS? Is PaaS going to be replaced by container as a service? I think the answer is “no.” The piece that is left for PaaS is the part that was always the most important part of PaaS in the first place, and that’s the opinionated developer experience. In addition to all of the generic parts of PaaS that I listed above, the most important part of a PaaS has always been the way in which the developer experience and application framework made developers more productive within the boundaries of the platform. PaaS enables developers to go from source code on their laptop to a world-wide scalable service in less than an hour. That’s hugely powerful. </p><p>However, in the world of traditional PaaS, the skills needed to build PaaS infrastructure itself, the software on which the user’s software ran, required very strong skills and experience with distributed systems. Consequently, PaaS tended to be built by distributed system engineers rather than experts in a particular vertical developer experience. This means that PaaS platforms tended towards general purpose infrastructure rather than targeting specific verticals. Recently, we have seen this start to change, first with PaaS targeted at mobile API backends, and later with PaaS targeting “function as a service”. However, these products were still built from the ground up on top of raw infrastructure.</p><p>More recently, we are starting to see these platforms build on top of container infrastructure. Taking for example “function as a service” there are at least two (and likely more) open source implementations of functions as a service that run on top of Kubernetes (<a href=https://github.com/fission/fission>fission</a> and <a href=https://github.com/funktionio/funktion/>funktion</a>). This trend will only continue. Building a platform as a service, on top of container as a service is easy enough that you could imagine giving it out as an undergraduate computer science assignment. This ease of development means that individual developers with specific expertise in a vertical (say software for running three-dimensional simulations) can and will build PaaS platforms targeted at that specific vertical experience. In turn, by targeting such a narrow experience, they will build an experience that fits that narrow vertical perfectly, making their solution a compelling one in that target market.</p><p>This then points to the other benefit of next generation PaaS being built on top of container as a service. It frees the developer from having to make an “all-in” choice on a particular PaaS platform. When layered on top of container as a service, the basic functionality (naming, discovery, packaging, etc) are all provided by the CaaS and thus common across multiple PaaS that happened to be deployed on top of that CaaS. This means that developers can mix and match, deploying multiple PaaS to the same container infrastructure, and choosing for each application the PaaS platform that best suits that particular platform. Also, importantly, they can choose to “drop down” to raw CaaS infrastructure if that is a better fit for their application. Freeing PaaS from providing the infrastructure layer, enables PaaS to diversify and target specific experiences without fear of being too narrow. The experiences become more targeted, more powerful, and yet by building on top of container as a service, more flexible as well.</p><p>Kubernetes is infrastructure for next generation applications, PaaS and more. Given this, I’m really excited by our <a href=https://azure.microsoft.com/en-us/blog/kubernetes-now-generally-available-on-azure-container-service/>announcement</a> today that Kubernetes on Azure Container Service has reached general availability. When you deploy your next generation application to Azure, whether on a PaaS or deployed directly onto Kubernetes itself (or both) you can deploy it onto a managed, supported Kubernetes cluster.</p><p>Furthermore, because we know that the world of PaaS and software development in general is a hybrid one, we’re excited to announce the preview availability of <a href=https://docs.microsoft.com/en-us/azure/container-service/container-service-kubernetes-walkthrough>Windows clusters in Azure Container Service</a>. We’re also working on <a href=https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/windows.md>hybrid clusters</a> in <a href=https://github.com/Azure/acs-engine>ACS-Engine</a> and expect to roll those out to general availability in the coming months.</p><p>I’m thrilled to see how containers and container as a service is changing the world of compute, I’m confident that we’re only scratching the surface of the transformation we’ll see in the coming months and years.</p><p><em>--<a href=https://twitter.com/brendandburns>Brendan Burns</a>, Partner Architect, at Microsoft and co-founder of Kubernetes</em></p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-28efa8d9359621bd8692cb07ad010bd6>Inside JD.com's Shift to Kubernetes from OpenStack</h1><div class="td-byline mb-4"><time datetime=2017-02-10 class=text-muted>Friday, February 10, 2017</time></div><p><em>Editor's note: Today’s post is by the Infrastructure Platform Department team at JD.com about their transition from OpenStack to Kubernetes. JD.com is one of China’s largest companies and the first Chinese Internet company to make the Global Fortune 500 list.</em></p><p><a href=https://upload.wikimedia.org/wikipedia/en/7/79/JD_logo.png><img src=https://upload.wikimedia.org/wikipedia/en/7/79/JD_logo.png alt></a></p><p><strong>History of cluster building</strong></p><p><strong>The era of physical machines (2004-2014)</strong></p><p>Before 2014, our company's applications were all deployed on the physical machine. In the age of physical machines, we needed to wait an average of one week for the allocation to application coming on-line. Due to the lack of isolation, applications would affected each other, resulting in a lot of potential risks. At that time, the average number of tomcat instances on each physical machine was no more than nine. The resource of physical machine was seriously wasted and the scheduling was inflexible. The time of application migration took hours due to the breakdown of physical machines. And the auto-scaling cannot be achieved. To enhance the efficiency of application deployment, we developed compilation-packaging, automatic deployment, log collection, resource monitoring and some other systems.</p><p><strong>Containerized era (2014-2016)</strong></p><p>The Infrastructure Platform Department (<a href=https://github.com/ipdcode>IPD</a>) led by Liu Haifeng--Chief Architect of JD.COM, sought a new resolution in the fall of 2014. Docker ran into our horizon. At that time, docker had been rising, but was slightly weak and lacked of experience in production environment. We had repeatedly tested docker. In addition, docker was customized to fix a couple of issues, such as system crash caused by device mapper and some Linux kernel bugs. We also added plenty of new features into docker, including disk speed limit, capacity management, and layer merging in image building and so on.</p><p>To manage the container cluster properly, we chose the architecture of OpenStack + Novadocker driver. Containers are managed as virtual machines. It is known as the first generation of JD container engine platform--JDOS1.0 (JD Datacenter Operating System). The main purpose of JDOS 1.0 is to containerize the infrastructure. All applications run in containers rather than physical machines since then. As for the operation and maintenance of applications, we took full advantage of existing tools. The time for developers to request computing resources in production environment reduced to several minutes rather than a week. After the pooling of computing resources, even the scaling of 1,000 containers would be finished in seconds. Application instances had been isolated from each other. Both the average deployment density of applications and the physical machine utilization had increased by three times, which brought great economic benefits.</p><p>We deployed clusters in each IDC and provided unified global APIs to support deployment across the IDC. There are 10,000 compute nodes at most and 4,000 at least in a single OpenStack distributed container cluster in our production environment. The first generation of container engine platform (JDOS 1.0) successfully supported the “6.18” and “11.11” promotional activities in both 2015 and 2016. There are already 150,000 running containers online by November 2016.</p><p><em>“6.18” and “11.11” are known as the two most popular online promotion of JD.COM, similar to the black Friday promotions. Fulfilled orders in November 11, 2016 reached 30 million. </em></p><p>In the practice of developing and promoting JDOS 1.0, applications were migrated directly from physical machines to containers. Essentially, JDOS 1.0 was an implementation of IaaS. Therefore, deployment of applications was still heavily dependent on compilation-packaging and automatic deployment tools. However, the practice of JDOS1.0 is very meaningful. Firstly, we successfully moved business into containers. Secondly, we have a deep understanding of container network and storage, and know how to polish them to the best. Finally, all the experiences lay a solid foundation for us to develop a brand new application container platform.</p><p><strong>New container engine platform (JDOS 2.0)</strong></p><p><strong>Platform architecture</strong></p><p>When JDOS 1.0 grew from 2,000 containers to 100,000, we launched a new container engine platform (JDOS 2.0). The goal of JDOS 2.0 is not just an infrastructure management platform, but also a container engine platform faced to applications. On the basic of JDOS 1.0 and Kubernetes, JDOS 2.0 integrates the storage and network of JDOS 1.0, gets through the process of CI/CD from the source to the image, and finally to the deployment. Also, JDOS 2.0 provides one-stop service such as log, monitor, troubleshooting, terminal and orchestration. The platform architecture of JDOS 2.0 is shown below.</p><p><img src=https://lh3.googleusercontent.com/Hs2DAPmZIbqGrWK8oZvBGBJzZbSwLHry7_go0PmCQFCoB_nEjidMOwD8pHlLjqXHqGXXu140RT4EXQq7OX-qgNHQci1G-0-nEavRxha-L02RFmR9WyKp3sHCuxY2qFWmnz0UIVfpfGL_ZLsQAA alt=D:\百度云同步盘\徐新坤-新人培训计划\docker\MAE\分享\arc.png></p><table><thead><tr><th>Function</th><th>Product</th></tr></thead><tbody><tr><td>Source Code Management</td><td>Gitlab</td></tr><tr><td>Container Tool</td><td>Docker</td></tr><tr><td>Container Networking</td><td>Cane</td></tr><tr><td>Container Engine</td><td>Kubernetes</td></tr><tr><td>Image Registry</td><td>Harbor</td></tr><tr><td>CI Tool</td><td>Jenkins</td></tr><tr><td>Log Management</td><td>Logstash + Elastic Search</td></tr><tr><td>Monitor</td><td>Prometheus</td></tr></tbody></table><p>In JDOS 2.0, we define two levels, system and application. A system consists of several applications and an application consists of several Pods which provide the same service. In general, a department can apply for one or more systems which directly corresponds to the namespace of Kubernetes. This means that the Pods of the same system will be in the same namespace.</p><p>Most of the JDOS 2.0 components (GitLab / Jenkins / Harbor / Logstash / Elastic Search / Prometheus) are also containerized and deployed on the Kubernetes platform.</p><p><strong>One Stop Solution</strong></p><p><img src=https://lh4.googleusercontent.com/s3hP_s27l4FiV1rR0dcJEg4dAZL9caJbk-kiDnZyfykt5ldXdcsfxlDEdneZJA9L8sPzOvxJI8WyZV0Cm1CI_b_oABKzBwazoEe86yCd9E75Dm0UBfRU2AgzYYzJ5ukxBIME977-mT9GA6XTGw alt=D:\百度云同步盘\徐新坤-新人培训计划\docker\MAE\分享\cicd.png></p><ol><li>1.JDOS 2.0 takes docker image as the core to implement continuous integration and continuous deployment.</li><li>2.Developer pushes code to git.</li><li>3.Git triggers the jenkins master to generate build job.</li><li>4.Jenkins master invokes Kubernetes to create jenkins slave Pod.</li><li>5.Jenkins slave pulls the source code, compiles and packs.</li><li>6.Jenkins slave sends the package and the Dockerfile to the image build node with docker.</li><li>7.The image build node builds the image.</li><li>8.The image build node pushes the image to the image registry Harbor.</li><li>9.User creates or updates app Pods in different zone.</li></ol><p>The docker image in JDOS 1.0 consisted primarily of the operating system and the runtime software stack of the application. So, the deployment of applications was still dependent on the auto-deployment and some other tools. While in JDOS 2.0, the deployment of the application is done during the image building. And the image contains the complete software stack, including App. With the image, we can achieve the goal of running applications as designed in any environment.</p><p><img src=https://lh4.googleusercontent.com/dL9knSIAFBdaOQvIGRt8wUntzPQnV7J0Y4O8osNwQhC2N3O2cPKDA3b64THn0sorPOXXIuldc_tXJMv1dcanhdKf1wk0MfKbxpv_BLeTxo5B1CehgSX66XHYx7BrAeiGt7qFulytO9W5K9JfXg alt=D:\百度云同步盘\徐新坤-新人培训计划\docker\MAE\分享\image.png></p><p><strong>Networking and External Service Load Balancing</strong></p><p>JDOS 2.0 takes the network solution of JDOS 1.0, which is implemented with the VLAN model of OpenStack Neutron. This solution enables highly efficient communication between containers, making it ideal for a cluster environment within a company. Each Pod occupies a port in Neutron, with a separate IP. Based on the Container Network Interface standard (<a href=https://github.com/containernetworking/cni>CNI</a>) standard, we have developed a new project Cane for integrating kubelet and Neutron.</p><p><img src=https://lh6.googleusercontent.com/KV37EdZE0MDzNllUVlvaQYOEgDiS72UmHwPs6o2jj7LB7gL0ptTjxDxfjA9Vi6X-2xTBwsxfLgo6iJnt1P2_C9KHwKYe8bniclL5UsFRhdw0g0Ylr7MAPTSg1a3LQsEtN7eLoNsnl8NENBDETQ alt=D:\百度云同步盘\徐新坤-新人培训计划\docker\MAE\分享\network.png></p><p>At the same time, Cane is also responsible for the management of LoadBalancer in Kubernetes service. When a LoadBalancer is created / deleted / modified, Cane will call the creating / removing / modifying interface of the lbaas service in Neutron. In addition, the Hades component in the Cane project provides an internal DNS resolution service for the Pods.</p><p><em>The source code of the Cane project is currently being finished and will be released on GitHub soon.</em></p><p><strong>Flexible Scheduling</strong></p><p><a href=https://lh6.googleusercontent.com/P6aA1V-ND_i0l6flYQ1TFvjq651FpUznfLRrL6VqmnMYLdP_WUhDDICq9J0d2gcIu16I0Bz2KLAJnfk4RQ1tv1MuKj_hfF6cLdh5JVktH1xFmbFnsNus3anpL7q5NK8WAS0JQFz6cNT32S72Yg><img src=https://lh6.googleusercontent.com/P6aA1V-ND_i0l6flYQ1TFvjq651FpUznfLRrL6VqmnMYLdP_WUhDDICq9J0d2gcIu16I0Bz2KLAJnfk4RQ1tv1MuKj_hfF6cLdh5JVktH1xFmbFnsNus3anpL7q5NK8WAS0JQFz6cNT32S72Yg alt=D:\百度云同步盘\徐新坤-新人培训计划\docker\MAE\分享\schedule.png></a>JDOS 2.0 accesses applications, including big data, web applications, deep learning and some other types, and takes more diverse and flexible scheduling approaches. In some IDCs, we experimentally mixed deployment of online tasks and offline tasks. Compared to JDOS 1.0, overall resource utilization increased by about 30%.</p><p><strong>Summary</strong></p><p>The rich functionality of Kubernetes allows us to pay more attention to the entire ecosystem of the platform, such as network performance, rather than the platform itself. In particular, the SREs highly appreciated the functionality of replication controller. With it, the scaling of the applications is achieved in several seconds. JDOS 2.0 now has accessed about 20% of the applications, and deployed 2 clusters with about 20,000 Pods running daily. We plan to access more applications of our company, to replace the current JDOS 1.0. And we are also glad to share our experience in this process with the community.</p><p>Thank you to all the contributors of Kubernetes and the other open source projects.</p><p><em>--Infrastructure Platform Department team at JD.com</em></p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f517e7895817589b238b5c59ced90adc>Run Deep Learning with PaddlePaddle on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-02-08 class=text-muted>Wednesday, February 08, 2017</time></div><p><em>Editor's note: Today's post is a joint post from the deep learning team at Baidu and the etcd team at CoreOS.</em></p><p>**<a href=https://3.bp.blogspot.com/-Mwn3FU9hffI/WJk8QBxA6SI/AAAAAAAAA8w/AS5QoMdPTN8bL9jnixlsCXzj1IfYerhRQCLcB/s1600/baidu_research_logo_rgb.png><img src=https://3.bp.blogspot.com/-Mwn3FU9hffI/WJk8QBxA6SI/AAAAAAAAA8w/AS5QoMdPTN8bL9jnixlsCXzj1IfYerhRQCLcB/s200/baidu_research_logo_rgb.png alt></a>**</p><p><strong>What is PaddlePaddle</strong></p><p>PaddlePaddle is an easy-to-use, efficient, flexible and scalable deep learning platform originally developed at Baidu for applying deep learning to Baidu products since 2014.</p><p>There have been more than 50 innovations created using PaddlePaddle supporting 15 Baidu products ranging from the search engine, online advertising, to Q&A and system security.</p><p>In September 2016, Baidu open sourced <a href=https://github.com/PaddlePaddle/Paddle>PaddlePaddle</a>, and it soon attracted many contributors from outside of Baidu.</p><p><strong>Why Run PaddlePaddle on Kubernetes</strong></p><p>PaddlePaddle is designed to be slim and independent of computing infrastructure. Users can run it on top of Hadoop, Spark, Mesos, Kubernetes and others.. We have a strong interest with Kubernetes because of its flexibility, efficiency and rich features.</p><p>While we are applying PaddlePaddle in various Baidu products, we noticed two main kinds of PaddlePaddle usage -- research and product. Research data does not change often, and the focus is fast experiments to reach the expected scientific measurement. Products data changes often. It usually comes from log messages generated from the Web services.</p><p>A successful deep learning project includes both the research and the data processing pipeline. There are many parameters to be tuned. A lot of engineers work on the different parts of the project simultaneously.</p><p>To ensure the project is easy to manage and utilize hardware resource efficiently, we want to run all parts of the project on the same infrastructure platform.</p><p>The platform should provide:</p><ul><li><p>fault-tolerance. It should abstract each stage of the pipeline as a service, which consists of many processes that provide high throughput and robustness through redundancy.</p></li><li><p>auto-scaling. In the daytime, there are usually many active users, the platform should scale out online services. While during nights, the platform should free some resources for deep learning experiments.</p></li><li><p>job packing and isolation. It should be able to assign a PaddlePaddle trainer process requiring the GPU, a web backend service requiring large memory, and a CephFS process requiring disk IOs to the same node to fully utilize its hardware.</p></li></ul><p>What we want is a platform which runs the deep learning system, the Web server (e.g., Nginx), the log collector (e.g., fluentd), the distributed queue service (e.g., Kafka), the log joiner and other data processors written using Storm, Spark, and Hadoop MapReduce on the same cluster. We want to run all jobs -- online and offline, production and experiments -- on the same cluster, so we could make full utilization of the cluster, as different kinds of jobs require different hardware resource.</p><p>We chose container based solutions since the overhead introduced by VMs is contradictory to our goal of efficiency and utilization.</p><p>Based on our research of different container based solutions, Kubernetes fits our requirement the best.</p><p><strong>Distributed Training on Kubernetes</strong></p><p>PaddlePaddle supports distributed training natively. There are two roles in a PaddlePaddle cluster: <strong>parameter server</strong> and <strong>trainer</strong>. Each parameter server process maintains a shard of the global model. Each trainer has its local copy of the model, and uses its local data to update the model. During the training process, trainers send model updates to parameter servers, parameter servers are responsible for aggregating these updates, so that trainers can synchronize their local copy with the global model.</p><p>| <img src=https://lh5.googleusercontent.com/e7udXH-Vv2SZ7YSo3YLtQEQI6VvWfPJMsYAkdad5ZJJ9mYBJ-Du3soR1pgwD80tD9ZMrUliuQU1UhnposxFsCJaKI4grRlFSTJFS0xi9HQXHsU-5-qkghOn0IRYy6cy-YzuHF6Eq alt> |
| Figure 1: Model is partitioned into two shards. Managed by two parameter servers respectively. |</p><p>Some other approaches use a set of parameter servers to collectively hold a very large model in the CPU memory space on multiple hosts. But in practice, it is not often that we have such big models, because it would be very inefficient to handle very large model due to the limitation of GPU memory. In our configuration, multiple parameter servers are mostly for fast communications. Suppose there is only one parameter server process working with all trainers, the parameter server would have to aggregate gradients from all trainers and becomes a bottleneck. In our experience, an experimentally efficient configuration includes the same number of trainers and parameter servers. And we usually run a pair of trainer and parameter server on the same node. In the following Kubernetes job configuration, we start a job that runs N Pods, and in each Pod there are a parameter server and a trainer process.</p><pre><code>yaml

apiVersion: batch/v1

kind: Job

metadata:

  name: PaddlePaddle-cluster-job

spec:

  parallelism: 3

  completions: 3

  template:

    metadata:

      name: PaddlePaddle-cluster-job

    spec:

      volumes:

      - name: jobpath

        hostPath:

          path: /home/admin/efs

      containers:

      - name: trainer

        image: your\_repo/paddle:mypaddle

        command: [&quot;bin/bash&quot;,  &quot;-c&quot;, &quot;/root/start.sh&quot;]

        env:

        - name: JOB\_NAME

          value: paddle-cluster-job

        - name: JOB\_PATH

          value: /home/jobpath

        - name: JOB\_NAMESPACE

          value: default

        volumeMounts:

        - name: jobpath

          mountPath: /home/jobpath

      restartPolicy: Never
</code></pre><p>We can see from the config that parallelism, completions are both set to 3. So this job will simultaneously start up 3 PaddlePaddle pods, and this job will be finished when all 3 pods finishes.</p><p>| <a href=https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/usage/k8s/src/start_paddle.py><img src=https://lh5.googleusercontent.com/cKVFdtLUnX7mtE76xRCAFaylVilAX6E0mBy17XTKOJwJQy6_rqF33v5lgeUjIpfN-2pT00OpD13mByawgOrjHpwGwJ8y99Vgoqridu1GklIkMnKysOE8jIUwvwfSySUgUDGkTkpz alt></a> |
|</p><p>Figure 2: Job A of three pods and Job B of one pod running on two nodes.
|</p><p>The entrypoint of each pod is <a href=https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/usage/k8s/src/k8s_train/start.sh>start.sh</a>. It downloads data from a storage service, so that trainers can read quickly from the pod-local disk space. After downloading completes, it runs a Python script, <a href=https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/usage/k8s/src/k8s_train/start_paddle.py>start_paddle.py</a>, which starts a parameter server, waits until parameter servers of all pods are ready to serve, and then starts the trainer process in the pod.</p><p>This waiting is necessary because each trainer needs to talk to all parameter servers, as shown in Figure. 1. Kubernetes <a href=/docs/api-reference/v1/operations/#_list_or_watch_objects_of_kind_pod>API</a> enables trainers to check the status of pods, so the Python script could wait until all parameter servers’ status change to "running" before it triggers the training process.</p><p>Currently, the mapping from data shards to pods/trainers is static. If we are going to run N trainers, we would need to partition the data into N shards, and statically assign each shard to a trainer. Again we rely on the Kubernetes API to enlist pods in a job so could we index pods / trainers from 1 to N. The i-th trainer would read the i-th data shard.</p><p>Training data is usually served on a distributed filesystem. In practice we use CephFS on our on-premise clusters and Amazon Elastic File System on AWS. If you are interested in building a Kubernetes cluster to run distributed PaddlePaddle training jobs, please follow <a href=https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/usage/k8s/k8s_aws_en.md>this tutorial</a>.</p><p><strong>What’s Next</strong></p><p>We are working on running PaddlePaddle with Kubernetes more smoothly.</p><p>As you might notice the current trainer scheduling fully relies on Kubernetes based on a static partition map. This approach is simple to start, but might cause a few efficiency problems.</p><p>First, slow or dead trainers block the entire job. There is no controlled preemption or rescheduling after the initial deployment. Second, the resource allocation is static. So if Kubernetes has more available resources than we anticipated, we have to manually change the resource requirements. This is tedious work, and is not aligned with our efficiency and utilization goal.</p><p>To solve the problems mentioned above, we will add a PaddlePaddle master that understands Kubernetes API, can dynamically add/remove resource capacity, and dispatches shards to trainers in a more dynamic manner. The PaddlePaddle master uses etcd as a fault-tolerant storage of the dynamic mapping from shards to trainers. Thus, even if the master crashes, the mapping is not lost. Kubernetes can restart the master and the job will keep running.</p><p>Another potential improvement is better PaddlePaddle job configuration. Our experience of having the same number of trainers and parameter servers was mostly collected from using special-purpose clusters. That strategy was observed performant on our clients' clusters that run only PaddlePaddle jobs. However, this strategy might not be optimal on general-purpose clusters that run many kinds of jobs.</p><p>PaddlePaddle trainers can utilize multiple GPUs to accelerate computations. GPU is not a first class resource in Kubernetes yet. We have to manage GPUs semi-manually. We would love to work with Kubernetes community to improve GPU support to ensure PaddlePaddle runs the best on Kubernetes.</p><p><em>--Yi Wang, <a href=http://research.baidu.com/>Baidu Research</a> and Xiang Li, <a href=https://coreos.com/>CoreOS</a></em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-44820bd994caefe115ad67f27663cb15>Highly Available Kubernetes Clusters</h1><div class="td-byline mb-4"><time datetime=2017-02-02 class=text-muted>Thursday, February 02, 2017</time></div><p>Today’s post shows how to set-up a reliable, highly available distributed Kubernetes cluster. The support for running such clusters on Google Compute Engine (GCE) was added as an alpha feature in <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5 release</a>.</p><p><strong>Motivation</strong></p><p>We will create a Highly Available Kubernetes cluster, with master replicas and worker nodes distributed among three zones of a region. Such setup will ensure that the cluster will continue operating during a zone failure.</p><p><strong>Setting Up HA cluster</strong></p><p>The following instructions apply to GCE. First, we will setup a cluster that will span over one zone (europe-west1-b), will contain one master and three worker nodes and will be HA-compatible (will allow adding more master replicas and more worker nodes in multiple zones in future). To implement this, we’ll export the following environment variables:</p><pre><code>$ export KUBERNETES\_PROVIDER=gce

$ export NUM\_NODES=3

$ export MULTIZONE=true

$ export ENABLE\_ETCD\_QUORUM\_READ=true
</code></pre><p>and run kube-up script (note that the entire cluster will be initially placed in zone europe-west1-b):</p><pre><code>$ KUBE\_GCE\_ZONE=europe-west1-b ./cluster/kube-up.sh
</code></pre><p>Now, we will add two additional pools of worker nodes, each of three nodes, in zones europe-west1-c and europe-west1-d (more details on adding pools of worker nodes can be find <a href=/docs/setup/multiple-zones/>here</a>):</p><pre><code>$ KUBE\_USE\_EXISTING\_MASTER=true KUBE\_GCE\_ZONE=europe-west1-c ./cluster/kube-up.sh


$ KUBE\_USE\_EXISTING\_MASTER=true KUBE\_GCE\_ZONE=europe-west1-d ./cluster/kube-up.sh
</code></pre><p>To complete setup of HA cluster, we will add two master replicase, one in zone europe-west1-c, the other in europe-west1-d:</p><pre><code>$ KUBE\_GCE\_ZONE=europe-west1-c KUBE\_REPLICATE\_EXISTING\_MASTER=true ./cluster/kube-up.sh


$ KUBE\_GCE\_ZONE=europe-west1-d KUBE\_REPLICATE\_EXISTING\_MASTER=true ./cluster/kube-up.sh
</code></pre><p>Note that adding the first replica will take longer (~15 minutes), as we need to reassign the IP of the master to the load balancer in front of replicas and wait for it to propagate (see <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/design/ha_master.md>design doc</a> for more details).</p><p><strong>Verifying in HA cluster works as intended</strong></p><p>We may now list all nodes present in the cluster:</p><pre><code>$ kubectl get nodes

NAME                      STATUS                AGE

kubernetes-master         Ready,SchedulingDisabled 48m

kubernetes-master-2d4     Ready,SchedulingDisabled 5m

kubernetes-master-85f     Ready,SchedulingDisabled 32s

kubernetes-minion-group-6s52 Ready                 39m

kubernetes-minion-group-cw8e Ready                 48m

kubernetes-minion-group-fw91 Ready                 48m

kubernetes-minion-group-h2kn Ready                 31m

kubernetes-minion-group-ietm Ready                 39m

kubernetes-minion-group-j6lf Ready                 31m

kubernetes-minion-group-soj7 Ready                 31m

kubernetes-minion-group-tj82 Ready                 39m

kubernetes-minion-group-vd96 Ready                 48m
</code></pre><p>As we can see, we have 3 master replicas (with disabled scheduling) and 9 worker nodes.</p><p>We will deploy a sample application (nginx server) to verify that our cluster is working correctly:</p><pre><code>$ kubectl run nginx --image=nginx --expose --port=80
</code></pre><p>After waiting for a while, we can verify that both the deployment and the service were correctly created and are running:</p><pre><code>$ kubectl get pods

NAME                READY STATUS RESTARTS AGE

...

nginx-3449338310-m7fjm 1/1 Running 0     4s

...


$ kubectl run -i --tty test-a --image=busybox /bin/sh

If you don't see a command prompt, try pressing enter.

# wget -q -O- http://nginx.default.svc.cluster.local

...

\&lt;title\&gt;Welcome to nginx!\&lt;/title\&gt;

...
</code></pre><p>Now, let’s simulate failure of one of master’s replicas by executing halt command on it (kubernetes-master-137, zone europe-west1-c):</p><pre><code>$ gcloud compute ssh kubernetes-master-2d4 --zone=europe-west1-c

...

$ sudo halt
</code></pre><p>After a while the master replica will be marked as NotReady:</p><pre><code>$ kubectl get nodes

NAME                      STATUS                   AGE

kubernetes-master         Ready,SchedulingDisabled 51m

kubernetes-master-2d4     NotReady,SchedulingDisabled 8m

kubernetes-master-85f     Ready,SchedulingDisabled 4m

...
</code></pre><p>However, the cluster is still operational. We may verify it by checking if our nginx server works correctly:</p><pre><code>$ kubectl run -i --tty test-b --image=busybox /bin/sh

If you don't see a command prompt, try pressing enter.

# wget -q -O- http://nginx.default.svc.cluster.local

...

\&lt;title\&gt;Welcome to nginx!\&lt;/title\&gt;

...
</code></pre><p>We may also run another nginx server:</p><pre><code>$ kubectl run nginx-next --image=nginx --expose --port=80
</code></pre><p>The new server should be also working correctly:</p><pre><code>$ kubectl run -i --tty test-c --image=busybox /bin/sh

If you don't see a command prompt, try pressing enter.

# wget -q -O- http://nginx-next.default.svc.cluster.local

...

\&lt;title\&gt;Welcome to nginx!\&lt;/title\&gt;

...
</code></pre><p>Let’s now reset the broken replica:</p><pre><code>$ gcloud compute instances start kubernetes-master-2d4 --zone=europe-west1-c
</code></pre><p>After a while, the replica should be re-attached to the cluster:</p><pre><code>$ kubectl get nodes

NAME                      STATUS                AGE

kubernetes-master         Ready,SchedulingDisabled 57m

kubernetes-master-2d4     Ready,SchedulingDisabled 13m

kubernetes-master-85f     Ready,SchedulingDisabled 9m

...
</code></pre><p><strong>Shutting down HA cluster</strong></p><p>To shutdown the cluster, we will first shut down master replicas in zones D and E:</p><pre><code>$ KUBE\_DELETE\_NODES=false KUBE\_GCE\_ZONE=europe-west1-c ./cluster/kube-down.sh


$ KUBE\_DELETE\_NODES=false KUBE\_GCE\_ZONE=europe-west1-d ./cluster/kube-down.sh
</code></pre><p>Note that the second removal of replica will take longer (~15 minutes), as we need to reassign the IP of the load balancer in front of replicas to the remaining master and wait for it to propagate (see <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/design/ha_master.md>design doc</a> for more details).</p><p>Then, we will remove the additional worker nodes from zones europe-west1-c and europe-west1-d:</p><pre><code>$ KUBE\_USE\_EXISTING\_MASTER=true KUBE\_GCE\_ZONE=europe-west1-c ./cluster/kube-down.sh


$ KUBE\_USE\_EXISTING\_MASTER=true KUBE\_GCE\_ZONE=europe-west1-d ./cluster/kube-down.sh
</code></pre><p>And finally, we will shutdown the remaining master with the last group of nodes (zone europe-west1-b):</p><pre><code>$ KUBE\_GCE\_ZONE=europe-west1-b ./cluster/kube-down.sh
</code></pre><p><strong>Conclusions</strong></p><p>We have shown how, by adding worker node pools and master replicas, a Highly Available Kubernetes cluster can be created. As of Kubernetes version 1.5.2, it is supported in kube-up/kube-down scripts for GCE (as alpha). Additionally, there is a support for HA cluster on AWS in kops scripts (see <a href=http://kubecloud.io/setup-ha-k8s-kops/>this article</a> for more details).</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul><p><em>--Jerzy Szczepkowski, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-3199113628dca07f5421996fe436e09f>Fission: Serverless Functions as a Service for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2017-01-30 class=text-muted>Monday, January 30, 2017</time></div><p><em>Editor's note: Today’s post is by Soam Vasani, Software Engineer at Platform9 Systems, talking about a new open source Serverless Function (FaaS) framework for Kubernetes.</em> </p><p><a href=https://github.com/fission/fission>Fission</a> is a Functions as a Service (FaaS) / Serverless function framework built on Kubernetes.</p><p>Fission allows you to easily create HTTP services on Kubernetes from functions. It works at the source level and abstracts away container images (in most cases). It also simplifies the Kubernetes learning curve, by enabling you to make useful services without knowing much about Kubernetes.</p><p>To use Fission, you simply create functions and add them with a CLI. You can associate functions with HTTP routes, Kubernetes events, or other triggers. Fission supports NodeJS and Python today.</p><p>Functions are invoked when their trigger fires, and they only consume CPU and memory when they're running. Idle functions don't consume any resources except storage.</p><p><strong>Why make a FaaS framework on Kubernetes?</strong></p><p>We think there's a need for a FaaS framework that can be run on diverse infrastructure, both in public clouds and on-premise infrastructure. Next, we had to decide whether to build it from scratch, or on top of an existing orchestration system. It was quickly clear that we shouldn't build it from scratch -- we would just end up having to re-invent cluster management, scheduling, network management, and lots more.</p><p>Kubernetes offered a powerful and flexible orchestration system with a comprehensive API backed by a strong and growing community. Building on it meant Fission could leave container orchestration functionality to Kubernetes, and focus on FaaS features.</p><p>The other reason we don't want a separate FaaS cluster is that FaaS works best in combination with other infrastructure. For example, it may be the right fit for a small REST API, but it needs to work with other services to store state. FaaS also works great as a mechanism for event handlers to handle notifications from storage, databases, and from Kubernetes itself. Kubernetes is a great platform for all these services to interoperate on top of.</p><p><strong>Deploying and Using Fission</strong></p><p>Fission can be installed with a <code>kubectl create</code> command: see the <a href=https://github.com/fission/fission#get-and-run-fission-minikube-or-local-cluster>project README</a> for instructions.</p><p>Here's how you’d write a "hello world" HTTP service:</p><pre><code>$ cat \&gt; hello.py

def main(context):

 &amp;nbsp;&amp;nbsp;&amp;nbsp;print &quot;Hello, world!&quot;


$ fission function create --name hello --env python --code hello.py --route /hello


$ curl http://\&lt;fission router\&gt;/hello

Hello, world!
</code></pre><p>Fission takes care of loading the function into a container, routing the request to it, and so on. We go into more details in the next section.</p><p><strong>How Fission Is Implemented on Kubernetes</strong></p><p>At its core, a FaaS framework must (1) turn functions into services and (2) manage the lifecycle of these services.</p><p>There are a number of ways to achieve these goals, and each comes with different tradeoffs. Should the framework operate at the source-level, or at the level of Docker images (or something in-between, like "buildpacks")? What's an acceptable amount of overhead the first time a function runs? Choices made here affect platform flexibility, ease of use, resource usage and costs, and of course, performance. </p><p><strong>Packaging, source code, and images</strong></p><p>One of our goals is to make Fission very easy to use for new users. We chose to operate<br>at the source level, so that users can avoid having to deal with container image building, pushing images to registries, managing registry credentials, image versioning, and so on.</p><p>However, container images are the most flexible way to package apps. A purely source-level interface wouldn't allow users to package binary dependencies, for example.</p><p>So, Fission goes with a hybrid approach -- container images that contain a dynamic loader for functions. This approach allows most users to use Fission purely at the source level, but enables them to customize the container image when needed.</p><p>These images, called "environment images" in Fission, contain the runtime for the language (such as NodeJS or Python), a set of commonly used dependencies and a dynamic loader for functions. If these dependencies are sufficient for the function the user is writing, no image rebuilding is needed. Otherwise, the list of dependencies can be modified, and the image rebuilt.</p><p>These environment images are the only language-specific parts of Fission. They present a uniform interface to the rest of the framework. This design allows Fission to be easily extended to more languages.</p><p><strong>Cold start performance</strong></p><p>One of the goals of the serverless functions is that functions use CPU/memory resources only when running. This optimizes the resource cost of functions, but it comes at the cost of some performance overhead when starting from idle (the "cold start" overhead).</p><p>Cold start overhead is important in many use cases. In particular, with functions used in an interactive use case -- like a web or mobile app, where a user is waiting for the action to complete -- several-second cold start latencies would be unacceptable.</p><p>To optimize cold start overheads, Fission keeps a running pool of containers for each environment. When a request for a function comes in, Fission doesn't have to deploy a new container -- it just chooses one that's already running, copies the function into the container, loads it dynamically, and routes the request to that instance. The overhead of this process takes on the order of 100msec for NodeJS and Python functions.</p><p><strong>How Fission works on Kubernetes</strong></p><p><img src=https://lh4.googleusercontent.com/ORaF9Kw0UGttnjKddHJkcu99udMUGqzQ4j9J5ONHi7kmpjpRSWe-VZv1eb6eF6TZgZRumDGfAaDM_ZWoj4TUavvlhBdf3j0-A2WNIjiAWYtMFw0yXxV6mGfh3zalgwckM0DBVpcw alt=fission-arch.png>
Fission is designed as a set of microservices. A Controller keeps track of functions, HTTP<br>routes, event triggers, and environment images. A Pool Manager manages pools of idle environment containers, the loading of functions into these containers, and the killing of function instances when they're idle. A Router receives HTTP requests and routes them to function instances, requesting an instance from the Pool Manager if necessary.</p><p>The controller serves the fission API. All the other components watch the controller for updates. The router is exposed as a Kubernetes Service of the LoadBalancer or NodePort type, depending on where the Kubernetes cluster is hosted.</p><p>When the router gets a request, it looks up a cache to see if this request already has a service it should be routed to. If not, it looks up the function to map the request to, and requests the poolmgr for an instance. The poolmgr has a pool of idle pods; it picks one, loads the function into it (by sending a request into a sidecar container in the pod), and returns the address of the pod to the router. The router  proxies over the request to this pod. The pod is cached for subsequent requests, and if it's been idle for a few minutes, it is killed.</p><p>(For now, Fission maps one function to one container; autoscaling to multiple instances is planned for a later release. Re-using function pods to host multiple functions is also planned, for cases where isolation isn't a requirement.)</p><p><strong>Use Cases for Fission</strong></p><p><strong>Bots, Webhooks, REST APIs </strong><br>Fission is a good framework to make small REST APIs, implement webhooks, and write chatbots for Slack or other services.</p><p>As an example of a simple REST API, we've made a small guestbook app that uses functions for reading and writing to guestbook, and works with a redis instance to keep track of state. You can find the app <a href=https://github.com/fission/fission/tree/master/examples/python/guestbook>in the Fission GitHub repo</a>.</p><p>The app contains two end points -- the GET endpoint lists out guestbook entries from redis and renders them into HTML. The POST endpoint adds a new entry to the guestbook list in redis. That’s all there is -- there’s no Dockerfile to manage, and updating the app is as simple as calling fission function update. </p><p><strong>Handling Kubernetes Events</strong><br>Fission also supports triggering functions based on Kubernetes watches. For example, you can setup a function to watch for all pods in a certain namespace matching a certain label. The function gets the serialized object and the watch event type (added/removed/updated) in its context.</p><p>These event handler functions could be used for simple monitoring -- for example, you could send a slack message whenever a new service is added to the cluster. There are also more complex use cases, such as writing a custom controller by watching Kubernetes' Third Party Resources.</p><p><strong>Status and Roadmap</strong></p><p>Fission is in early alpha for now (Jan 2017). It's not ready for production use just yet. We're looking for early adopters and feedback.</p><p>What's ahead for Fission? We're working on making FaaS on Kubernetes more convenient, easy to use and easier to integrate with. In the coming months we're working on adding support for unit testing, integration with Git, function monitoring and log aggregation. We're also working on integration with other sources of events.</p><p>Creating more language environments is also in the works. NodeJS and Python are supported today. Preliminary support for C# .NET has been contributed by Klavs Madsen.</p><p>You can find our current roadmap on our GitHub <a href=https://github.com/fission/fission/issues>issues</a> and <a href=https://github.com/fission/fission/projects>projects</a>.</p><p><strong>Get Involved</strong></p><p>Fission is open source and developed in the open by <a href=http://platform9.com/fission>Platform9 Systems</a>. Check us out on <a href=https://github.com/fission/fission>GitHub</a>, and join our slack channel if you’d like to chat with us. We're also on twitter at <a href=https://twitter.com/fissionio>@fissionio</a>.</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul><p><em>--Soam Vasani, Software Engineer, Platform9 Systems</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4b822a3b7c095f8374ab1a6d2eada556>Running MongoDB on Kubernetes with StatefulSets</h1><div class="td-byline mb-4"><time datetime=2017-01-30 class=text-muted>Monday, January 30, 2017</time></div><p><em>Editor's note: Today’s post is by Sandeep Dinesh, Developer Advocate, Google Cloud Platform, showing how to run a database in a container.</em></p><blockquote class="warning callout"><div><strong>Warning:</strong> This post is several years old. The code examples need changes to work on a current Kubernetes cluster.</div></blockquote><p>Conventional wisdom says you can’t run a database in a container. “Containers are stateless!” they say, and “databases are pointless without state!”</p><p>Of course, this is not true at all. At Google, everything runs in a container, including databases. You just need the right tools. <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5</a> includes the new <a href=/docs/concepts/abstractions/controllers/statefulsets/>StatefulSet</a> API object (in previous versions, StatefulSet was known as PetSet). With StatefulSets, Kubernetes makes it much easier to run stateful workloads such as databases.</p><p>If you’ve followed my previous posts, you know how to create a <a href=http://blog.sandeepdinesh.com/2015/07/running-mean-web-application-in-docker.html>MEAN Stack app with Docker</a>, then <a href=https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-kubernetes-149ca81c2b5d>migrate it to Kubernetes</a> to provide easier management and reliability, and <a href=https://medium.com/google-cloud/mongodb-replica-sets-with-kubernetes-d96606bd9474>create a MongoDB replica set</a> to provide redundancy and high availability.</p><p>While the replica set in my previous blog post worked, there were some annoying steps that you needed to follow. You had to manually create a disk, a ReplicationController, and a service for each replica. Scaling the set up and down meant managing all of these resources manually, which is an opportunity for error, and would put your stateful application at risk In the previous example, we created a Makefile to ease the management of these resources, but it would have been great if Kubernetes could just take care of all of this for us.</p><p>With StatefulSets, these headaches finally go away. You can create and manage your MongoDB replica set natively in Kubernetes, without the need for scripts and Makefiles. Let’s take a look how.</p><p><em>Note: StatefulSets are currently a beta resource. The <a href=https://github.com/cvallance/mongo-k8s-sidecar>sidecar container</a> used for auto-configuration is also unsupported.</em></p><p><strong>Prerequisites and Setup</strong></p><p>Before we get started, you’ll need a Kubernetes 1.5+ and the <a href=/docs/user-guide/prereqs/>Kubernetes command line tool</a>. If you want to follow along with this tutorial and use Google Cloud Platform, you also need the <a href=http://cloud.google.com/sdk>Google Cloud SDK</a>.</p><p>Once you have a <a href=https://console.cloud.google.com/projectcreate>Google Cloud project created</a> and have your Google Cloud SDK setup (hint: gcloud init), we can create our cluster.</p><p>To create a Kubernetes 1.5 cluster, run the following command:</p><pre><code>gcloud container clusters create &quot;test-cluster&quot;
</code></pre><p>This will make a three node Kubernetes cluster. Feel free to <a href=https://cloud.google.com/sdk/gcloud/reference/container/clusters/create>customize the command</a> as you see fit.</p><p>Then, authenticate into the cluster:</p><pre><code>gcloud container clusters get-credentials test-cluster
</code></pre><p><strong>Setting up the MongoDB replica set</strong></p><p>To set up the MongoDB replica set, you need three things: A <a href=/docs/user-guide/persistent-volumes/#storageclasses>StorageClass</a>, a <a href=/docs/user-guide/services/#headless-services>Headless Service</a>, and a <a href=/docs/concepts/abstractions/controllers/statefulsets/>StatefulSet</a>.</p><p>I’ve created the configuration files for these already, and you can clone the example from GitHub:</p><pre><code>git clone https://github.com/thesandlord/mongo-k8s-sidecar.git

cd /mongo-k8s-sidecar/example/StatefulSet/
</code></pre><p>To create the MongoDB replica set, run these two commands:</p><pre><code>kubectl apply -f googlecloud\_ssd.yaml

kubectl apply -f mongo-statefulset.yaml
</code></pre><p>That's it! With these two commands, you have launched all the components required to run an highly available and redundant MongoDB replica set.</p><p>At an high level, it looks something like this:</p><p><img src=https://lh4.googleusercontent.com/ohALxLD4Ugj5FCwWqgqZ4xP9al4lTgrPDc9HsgPWYRZRz_buuYK6LKSC7A5n98DdOO-Po3Zq77Yt43-QhTWdIaXqltHI7PX0zMXAXbpiilYgdowGZapG0lJ9lgubwBj1CwNHHtXA alt></p><p>Let’s examine each piece in more detail.</p><p><strong>StorageClass</strong></p><p>The storage class tells Kubernetes what kind of storage to use for the database nodes. You can set up many different types of StorageClasses in a ton of different environments. For example, if you run Kubernetes in your own datacenter, you can use <a href=https://www.gluster.org/>GlusterFS</a>. On GCP, your <a href=https://cloud.google.com/compute/docs/disks/>storage choices</a> are SSDs and hard disks. There are currently drivers for <a href=/docs/user-guide/persistent-volumes/#aws>AWS</a>, <a href=/docs/user-guide/persistent-volumes/#azure-disk>Azure</a>, <a href=/docs/user-guide/persistent-volumes/#gce>Google Cloud</a>, <a href=/docs/user-guide/persistent-volumes/#glusterfs>GlusterFS</a>, <a href=/docs/user-guide/persistent-volumes/#openstack-cinder>OpenStack Cinder</a>, <a href=/docs/user-guide/persistent-volumes/#vsphere>vSphere</a>, <a href=/docs/user-guide/persistent-volumes/#ceph-rbd>Ceph RBD</a>, and <a href=/docs/user-guide/persistent-volumes/#quobyte>Quobyte</a>.</p><p>The configuration for the StorageClass looks like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-ssd<span style=color:#bbb>
</span></code></pre></div><p>This configuration creates a new StorageClass called “fast” that is backed by SSD volumes. The StatefulSet can now request a volume, and the StorageClass will automatically create it!</p><p>Deploy this StorageClass:</p><pre><code>kubectl apply -f googlecloud\_ssd.yaml
</code></pre><p><strong>Headless Service</strong></p><p>Now you have created the Storage Class, you need to make a Headless Service. These are just like normal Kubernetes Services, except they don’t do any load balancing for you. When combined with StatefulSets, they can give you unique DNS addresses that let you directly access the pods! This is perfect for creating MongoDB replica sets, because our app needs to connect to all of the MongoDB nodes individually.</p><p>The configuration for the Headless Service looks like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>27017</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>27017</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span></code></pre></div><p>You can tell this is a Headless Service because the clusterIP is set to “None.” Other than that, it looks exactly the same as any normal Kubernetes Service.</p><p><strong>StatefulSet</strong></p><p>The pièce de résistance. The StatefulSet actually runs MongoDB and orchestrates everything together. StatefulSets differ from Kubernetes <a href=/docs/user-guide/replicasets/>ReplicaSets</a> (not to be confused with MongoDB replica sets!) in certain ways that makes them more suited for stateful applications. Unlike Kubernetes ReplicaSets, pods created under a StatefulSet have a few unique attributes. The name of the pod is not random, instead each pod gets an ordinal name. Combined with the Headless Service, this allows pods to have stable identification. In addition, pods are created one at a time instead of all at once, which can help when bootstrapping a stateful system. You can read more about StatefulSets in the <a href=/docs/concepts/abstractions/controllers/statefulsets/>documentation</a>.</p><p>Just like before, <a href=https://github.com/cvallance/mongo-k8s-sidecar>this “sidecar” container</a> will configure the MongoDB replica set automatically. A “sidecar” is a helper container which helps the main container do its work.</p><p>The configuration for the StatefulSet looks like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>environment</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;mongo&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>environment</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mongo<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- mongod<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:#b44>&#34;--replSet&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span>- rs0<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:#b44>&#34;--smallfiles&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:#b44>&#34;--noprealloc&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>27017</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo-persistent-storage<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/data/db<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo-sidecar<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>cvallance/mongo-k8s-sidecar<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>MONGO_SIDECAR_POD_LABELS<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;role=mongo,environment=test&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mongo-persistent-storage<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;fast&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;ReadWriteOnce&#34;</span>]<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>100Gi<span style=color:#bbb>
</span></code></pre></div><p>It’s a little long, but fairly straightforward.</p><p>The first second describes the StatefulSet object. Then, we move into the Metadata section, where you can specify labels and the number of replicas.</p><p>Next comes the pod spec. The terminationGracePeriodSeconds is used to gracefully shutdown the pod when you scale down the number of replicas, which is important for databases! Then the configurations for the two containers is shown. The first one runs MongoDB with command line flags that configure the replica set name. It also mounts the persistent storage volume to /data/db, the location where MongoDB saves its data. The second container runs the sidecar.</p><p>Finally, there is the volumeClaimTemplates. This is what talks to the StorageClass we created before to provision the volume. It will provision a 100 GB disk for each MongoDB replica.</p><p><strong>Using the MongoDB replica set</strong></p><p>At this point, you should have three pods created in your cluster. These correspond to the three nodes in your MongoDB replica set. You can see them with this command:</p><pre><code>kubectl get pods

NAME   READY STATUS RESTARTS AGE
mongo-0 2/2  Running 0     3m
mongo-1 2/2  Running 0     3m
mongo-2 2/2  Running 0     3m
</code></pre><p>Each pod in a StatefulSet backed by a Headless Service will have a stable DNS name. The template follows this format: &lt;pod-name>.&lt;service-name></p><p>This means the DNS names for the MongoDB replica set are:</p><pre><code>mongo-0.mongo
mongo-1.mongo
mongo-2.mongo
</code></pre><p>You can use these names directly in the <a href=http://docs.mongodb.com/manual/reference/connection-string>connection string URI</a> of your app.</p><p>In this case, the connection string URI would be:</p><pre><code>mongodb://mongo-0.mongo,mongo-1.mongo,mongo-2.mongo:27017/dbname\_?
</code></pre><p>That’s it!</p><p><strong>Scaling the MongoDB replica set</strong></p><p>A huge advantage of StatefulSets is that you can scale them just like Kubernetes ReplicaSets. If you want 5 MongoDB Nodes instead of 3, just run the scale command:</p><pre><code>kubectl scale --replicas=5 statefulset mongo
</code></pre><p>The sidecar container will automatically configure the new MongoDB nodes to join the replica set.</p><p>Include the two new nodes (mongo-3.mongo & mongo-4.mongo) in your connection string URI and you are good to go. Too easy!</p><p><strong>Cleaning Up</strong></p><p>To clean up the deployed resources, delete the StatefulSet, Headless Service, and the provisioned volumes.</p><p>Delete the StatefulSet:</p><pre><code>kubectl delete statefulset mongo
</code></pre><p>Delete the Service:</p><pre><code>kubectl delete svc mongo
</code></pre><p>Delete the Volumes:</p><pre><code>kubectl delete pvc -l role=mongo
</code></pre><p>Finally, you can delete the test cluster:</p><pre><code>gcloud container clusters delete &quot;test-cluster&quot;
</code></pre><p>Happy Hacking!</p><p>For more cool Kubernetes and Container blog posts, follow me on <a href=https://twitter.com/sandeepdinesh>Twitter</a> and <a href=https://medium.com/@SandeepDinesh>Medium</a>.</p><p><em>--Sandeep Dinesh, Developer Advocate, Google Cloud Platform.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-829a2195d95f150a53deb09e9bd1bc4f>How we run Kubernetes in Kubernetes aka Kubeception</h1><div class="td-byline mb-4"><time datetime=2017-01-20 class=text-muted>Friday, January 20, 2017</time></div><p><em>Editor's note: Today’s post is by the team at Giant Swarm, showing how they run Kubernetes in Kubernetes.</em></p><p><a href=https://giantswarm.io/>Giant Swarm</a>’s container infrastructure started out with the goal to be an easy way for developers to deploy containerized microservices. Our first generation was extensively using <a href=https://github.com/coreos/fleet>fleet</a> as a base layer for our infrastructure components as well as for scheduling user containers.</p><p>In order to give our users a more powerful way to manage their containers we introduced Kubernetes into our stack in early 2016. However, as we needed a quick way to flexibly spin up and manage different users’ Kubernetes clusters resiliently we kept the underlying fleet layer.</p><p>As we insist on running all our underlying infrastructure components in containers, fleet gave us the flexibility of using systemd unit files to define our infrastructure components declaratively. Our self-developed deployment tooling allowed us to deploy and manage the infrastructure without the need for imperative configuration management tools.</p><p>However, fleet is just a distributed init and not a complete scheduling and orchestration system. Next to a lot of work on our tooling, it required significant improvements in terms of communication between peers, its reconciliation loop, and stability that we had to work on. Also the uptake in Kubernetes usage would ensure that issues are found and fixed faster.</p><p>As we had made good experience with introducing Kubernetes on the user side and with recent developments like <a href=https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-Kubernetes>rktnetes</a> and <a href=https://github.com/stackanetes/stackanetes>stackanetes</a> it felt like time for us to also move our base layer to Kubernetes.</p><p><strong>Why Kubernetes in Kubernetes</strong></p><p>Now, you could ask, why would anyone want to run multiple Kubernetes clusters inside of a Kubernetes cluster? Are we crazy? The answer is advanced multi-tenancy use cases as well as operability and automation thereof.</p><p>Kubernetes comes with its own growing feature set for multi-tenancy use cases. However, we had the goal of offering our users a fully-managed Kubernetes without any limitations to the functionality they would get using any vanilla Kubernetes environment, including privileged access to the nodes. Further, in bigger enterprise scenarios a single Kubernetes cluster with its inbuilt isolation mechanisms is often not sufficient to satisfy compliance and security requirements. More advanced (firewalled) zoning or layered security concepts are tough to reproduce with a single installation. With namespace isolation both privileged access as well as firewalled zones can hardly be implemented without sidestepping security measures.</p><p>Now you could go and set up multiple completely separate (and federated) installations of Kubernetes. However, automating the deployment and management of these clusters would need additional tooling and complex monitoring setups. Further, we wanted to be able to spin clusters up and down on demand, scale them, update them, keep track of which clusters are available, and be able to assign them to organizations and teams flexibly. In fact this setup can be combined with a federation control plane to federate deployments to the clusters over one API endpoint.</p><p>And wouldn’t it be nice to have an API and frontend for that?</p><p><strong>Enter Giantnetes</strong></p><p>Based on the above requirements we set out to build what we call Giantnetes - or if you’re into movies, Kubeception. At the most basic abstraction it is an outer Kubernetes cluster (the actual Giantnetes), which is used to run and manage multiple completely isolated user Kubernetes clusters.</p><p><img src=https://lh6.googleusercontent.com/jWRQBd96sPwtiG6vE_4DPAvEWrRnXTWVfWE3O4_JeCXYzSaAZPpVQA-s5K8W-GTZdQBYeC-g3rS3LMB_vgz6h8-EVQps0JIcaxoeXI8T6aVOowWtWdxRB78b_K3bxzfvVWGb5cWM alt></p><p>The physical machines are bootstrapped by using our CoreOS Container Linux bootstrapping tool, <a href=https://github.com/giantswarm/mayu>Mayu</a>. The Giantnetes components themselves are self-hosted, i.e. a kubelet is in charge of automatically bootstrapping the components that reside in a manifests folder. You could call this the first level of Kubeception.</p><p>Once the Giantnetes cluster is running we use it to schedule the user Kubernetes clusters as well as our tooling for managing and securing them.</p><p>We chose Calico as the Giantnetes network plugin to ensure security, isolation, and the right performance for all the applications running on top of Giantnetes.</p><p>Then, to create the inner Kubernetes clusters, we initiate a few pods, which configure the network bridge, create certificates and tokens, and launch virtual machines for the future cluster. To do so, we use lightweight technologies such as KVM and qemu to provision CoreOS Container Linux VMs that become the nodes of an inner Kubernetes cluster. You could call this the second level of Kubeception. </p><p>Currently this means we are starting Pods with Docker containers that in turn start VMs with KVM and qemu. However, we are looking into doing this with <a href=https://github.com/coreos/rkt/blob/master/Documentation/running-kvm-stage1.md>rkt qemu-kvm</a>, which would result in using a rktnetes setup for our Giantnetes.</p><p><img src=https://lh3.googleusercontent.com/fl8PIu5NgS4vRmUDuAGzni3uW-5RTYD0U22rF6fXr_UBfta4cLhQa2CsRNvDrmc2TiIZDRairTDYpn8QiU3Cjf6m8v74vFENCy9MHa3MgvNNEvvcwrwOxhvMe-pNITCDpV41bWBc alt></p><p>The networking solution for the inner Kubernetes clusters has two levels. It on a combination of flannel’s server/client architecture model and Calico BGP. While a flannel client is used to create the network bridge between the VMs of each virtualized inner Kubernetes cluster, Calico is running inside the virtual machines to connect the different Kubernetes nodes and create a single network for the inner Kubernetes. By using Calico, we mimic the Giantnetes networking solution inside of each Kubernetes cluster and provide the primitives to secure and isolate workloads through the Kubernetes network policy API.</p><p>Regarding security, we aim for separating privileges as much as possible and making things auditable. Currently this means we use certificates to secure access to the clusters and encrypt communication between all the components that form a cluster is (i.e. VM to VM, Kubernetes components to each other, etcd master to Calico workers, etc). For this we create a PKI backend per cluster and then issue certificates per service in Vault on-demand. Every component uses a different certificate, thus, avoiding to expose the whole cluster if any of the components or nodes gets compromised. We further rotate the certificates on a regular basis.</p><p>For ensuring access to the API and to services of each inner Kubernetes cluster from the outside we run a multi-level HAproxy ingress controller setup in the Giantnetes that connects the Kubernetes VMs to hardware load balancers.</p><p><strong>Looking into Giantnetes with kubectl</strong></p><p>Let’s have a look at a minimal sample deployment of Giantnetes.</p><p><img src=https://lh6.googleusercontent.com/wX9sxvO2um5DeT-mjMpazRWZOvauARHLA2z5wRZC41d4V72nzNQORSxxRxq1dJxZ4Rvw3ji7_ThAntYv-iSUgZl_Eq3gSCNRRHafTuN5rdQ9eo1HwD64LP01GNsSL-SRMA5-RDGW alt="Screen Shot 2016-11-14 at 12.08.40 PM.png"></p><p>In the above example you see a user Kubernetes cluster <code>customera</code> running in VM-containers on top of Giantnetes. We currently use Jobs for the network and certificate setups.</p><p>Peeking inside the user cluster, you see the DNS pods and a helloworld running.</p><p><img src=https://lh3.googleusercontent.com/5o88zBSr5-JigMWvVnfN6nmMlKPtEt8-Gw5j_3Rq3QdsvLiHIVoOsow8WfgA5wd8WsA8M9C-MV4AdS04XDzLfzNR4T6ZXqPPAZc-Imbr-Um0B5QajGTtCqIwsMjsSAA9O-un3wvU alt="Screen Shot 2016-11-14 at 12.07.28 PM.png"></p><p>Each one of these user clusters can be scheduled and used independently. They can be spun up and down on-demand.</p><p><strong>Conclusion</strong></p><p>To sum up, we could show how Kubernetes is able to easily not only self-host but also flexibly schedule a multitude of inner Kubernetes clusters while ensuring higher isolation and security aspects. A highlight in this setup is the composability and automation of the installation and the robust coordination between the Kubernetes components. This allows us to easily create, destroy, and reschedule clusters on-demand without affecting users or compromising the security of the infrastructure. It further allows us to spin up clusters with varying sizes and configurations or even versions by just changing some arguments at cluster creation. </p><p>This setup is still in its early days and our roadmap is planning for improvements in many areas such as transparent upgrades, dynamic reconfiguration and scaling of clusters, performance improvements, and (even more) security. Furthermore, we are looking forward to improve on our setup by making use of the ever advancing state of Kubernetes operations tooling and upcoming features, such as Init Containers, Scheduled Jobs, Pod and Node affinity and anti-affinity, etc.</p><p>Most importantly, we are working on making the inner Kubernetes clusters a third party resource that can then be managed by a custom controller. The result would be much like the <a href=https://coreos.com/blog/introducing-operators.html>Operator concept by CoreOS</a>. And to ensure that the community at large can benefit from this project we will be open sourcing this in the near future.</p><p><em>-- Hector Fernandez, Software Engineer & Puja Abbassi, Developer Advocate, Giant Swarm</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-2d5204101b1ade13068e270bbda62e97>Scaling Kubernetes deployments with Policy-Based Networking</h1><div class="td-byline mb-4"><time datetime=2017-01-19 class=text-muted>Thursday, January 19, 2017</time></div><p><em>Editor's note: Today’s post is by Harmeet Sahni, Director of Product Management, at Nuage Networks, writing about their contributions to Kubernetes and insights on policy-based networking.  </em></p><p>Although it’s just been eighteen-months since Kubernetes 1.0 was released, we’ve seen Kubernetes emerge as the leading container orchestration platform for deploying distributed applications. One of the biggest reasons for this is the vibrant open source community that has developed around it. The large number of Kubernetes contributors come from diverse backgrounds means we, and the community of users, are assured that we are investing in an open platform. Companies like Google (Container Engine), Red Hat (OpenShift), and CoreOS (Tectonic) are developing their own commercial offerings based on Kubernetes. This is a good thing since it will lead to more standardization and offer choice to the users. </p><p><strong>Networking requirements for Kubernetes applications</strong></p><p>For companies deploying applications on Kubernetes, one of biggest questions is how to deploy and orchestrate containers at scale. They’re aware that the underlying infrastructure, including networking and storage, needs to support distributed applications. Software-defined networking (SDN) is a great fit for such applications because the flexibility and agility of the networking infrastructure can match that of the applications themselves. The networking requirements of such applications include:</p><ul><li>Network automation </li><li>Distributed load balancing and service discovery</li><li>Distributed security with fine-grained policies</li><li>QoS Policies</li><li>Scalable Real-time Monitoring</li><li>Hybrid application environments with Services spread across Containers, VMs and Bare Metal Servers</li><li>Service Insertion (e.g. firewalls)</li><li>Support for Private and Public Cloud deployments</li></ul><p><strong>Kubernetes Networking</strong></p><p>Kubernetes provides a core set of platform services exposed through <a href=/docs/api/>APIs</a>. The platform can be extended in several ways through the extensions API, plugins and labels. This has allowed a wide variety integrations and tools to be developed for Kubernetes. Kubernetes recognizes that the network in each deployment is going to be unique. Instead of trying to make the core system try to handle all those use cases, Kubernetes chose to make the network pluggable.</p><p>With <a href=http://www.nuagenetworks.net/>Nuage Networks</a> we provide a scalable policy-based SDN platform. The platform is managed by a Network Policy Engine that abstracts away the complexity associated with configuring the system. There is a separate SDN Controller that comes with a very rich routing feature set and is designed to scale horizontally. Nuage uses the open source <a href=http://www.openvswitch.org/>Open vSwitch (OVS)</a> for the data plane with some enhancements in the OVS user space. Just like Kubernetes, Nuage has embraced openness as a core tenet for its platform. Nuage provides open APIs that allow users to orchestrate their networks and integrate network services such as firewalls, load balancers, IPAM tools etc. Nuage is supported in a wide variety of cloud platforms like OpenStack and VMware as well as container platforms like Kubernetes and others.</p><p>The Nuage platform implements a Kubernetes <a href=/docs/admin/network-plugins/>network plugin</a> that creates VXLAN overlays to provide seamless policy-based networking between Kubernetes Pods and non-Kubernetes environments (VMs and bare metal servers). Each Pod is given an IP address from a network that belongs to a <a href=/docs/user-guide/namespaces/>Namespace</a> and is not tied to the Kubernetes node.</p><p>As cloud applications are built using microservices, the ability to control traffic among these microservices is a fundamental requirement. It is important to point out that these network policies also need to control traffic that is going to/coming from external networks and services. Nuage’s policy abstraction model makes it easy to declare fine-grained ingress/egress policies for applications. Kubernetes has a beta <a href=/docs/user-guide/networkpolicies/>Network Policy API</a> implemented using the Kubernetes Extensions API. Nuage implements this Network Policy API to address a wide variety of policy use cases such as:</p><ul><li>Kubernetes Namespace isolation</li><li>Inter-Namespace policies</li><li>Policies between groups of Pods (Policy Groups) for Pods in same or different Namespaces</li><li>Policies between Kubernetes Pods/Namespaces and external Networks/Services</li></ul><p><a href=https://3.bp.blogspot.com/-jJK65zh2wE8/WIE5o3HkXFI/AAAAAAAAA7U/QkoCoYnTWAEz60H0nyP4_wN0tVG3WVWAwCEw/s1600/k8spolicy.png><img src=https://3.bp.blogspot.com/-jJK65zh2wE8/WIE5o3HkXFI/AAAAAAAAA7U/QkoCoYnTWAEz60H0nyP4_wN0tVG3WVWAwCEw/s640/k8spolicy.png alt></a></p><p>A key question for users to consider is the scalability of the policy implementation. Some networking setups require creating access control list (ACL) entries telling Pods how they can interact with one another. In most cases, this eventually leads to an n-squared pileup of ACL entries. The Nuage platform avoids this problem and can quickly assign a policy that applies to a whole group of Pods. The Nuage platform implements these policies using a fully distributed stateful firewall based on OVS.</p><p>Being able to monitor the traffic flowing between Kubernetes Pods is very useful to both development and operations teams. The Nuage platform’s real-time analytics engine enables visibility and security monitoring for Kubernetes applications. Users can get a visual representation of the traffic flows between groups of Pods, making it easy to see how the network policies are taking effect. Users can also get a rich set of traffic and policy statistics. Further, users can set alerts to be triggered based on policy event thresholds.</p><p><a href=https://4.bp.blogspot.com/-5VjajIIvq-A/WIE5qN2nsNI/AAAAAAAAA7U/mMfMQpeFvH85MHNbohJifEnW658l3w1agCEw/s1600/k8spolicy2.png><img src=https://4.bp.blogspot.com/-5VjajIIvq-A/WIE5qN2nsNI/AAAAAAAAA7U/mMfMQpeFvH85MHNbohJifEnW658l3w1agCEw/s640/k8spolicy2.png alt></a></p><p><strong>Conclusion</strong></p><p>Even though we started working on our integration with Kubernetes over a year ago, it feels we are just getting started. We have always felt that this is a truly open community and we want to be an integral part of it. You can find out more about our Kubernetes integration on our <a href=https://github.com/nuagenetworks/nuage-kubernetes>GitHub page</a>.</p><p><em>--Harmeet Sahni, Director of Product Management, Nuage Networks</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-372ad7968dfb46477d8c96369e9219d9>A Stronger Foundation for Creating and Managing Kubernetes Clusters</h1><div class="td-byline mb-4"><time datetime=2017-01-12 class=text-muted>Thursday, January 12, 2017</time></div><p><em>Editor's note: Today’s post is by Lucas Käldström an independent Kubernetes maintainer and SIG-Cluster-Lifecycle member, sharing what the group has been building and what’s upcoming. </em></p><p>Last time you heard from us was in September, when we announced <a href=https://kubernetes.io/blog/2016/09/how-we-made-kubernetes-easy-to-install>kubeadm</a>. The work on making kubeadm a first-class citizen in the Kubernetes ecosystem has continued and evolved. Some of us also met before KubeCon and had a very productive meeting where we talked about what the scopes for our SIG, kubeadm, and kops are. </p><p><strong>Continuing to Define SIG-Cluster-Lifecycle</strong></p><p><strong>What is the scope for kubeadm?</strong><br>We want kubeadm to be a common set of building blocks for all Kubernetes deployments; the piece that provides secure and recommended ways to bootstrap Kubernetes. Since there is no one true way to setup Kubernetes, kubeadm will support more than one method for each phase. We want to identify the phases every deployment of Kubernetes has in common and make configurable and easy-to-use kubeadm commands for those phases. If your organization, for example, requires that you distribute the certificates in the cluster manually or in a custom way, skip using kubeadm just for that phase. We aim to keep kubeadm usable for all other phases in that case. We want you to be able to pick which things you want kubeadm to do and let you do the rest yourself.</p><p>Therefore, the scope for kubeadm is to be easily extendable, modular and very easy to use. Right now, with this v1.5 release we have, kubeadm can only do the “full meal deal” for you. In future versions that will change as kubeadm becomes more componentized, while still leaving the opportunity to do everything for you. But kubeadm will still only handle the bootstrapping of Kubernetes; it won’t ever handle provisioning of machines for you since that can be done in many more ways. In addition, we want kubeadm to work everywhere, even on multiple architectures, therefore we built in <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>multi-architecture support</a> from the beginning.</p><p><strong>What is the scope for kops?</strong><br>The scope for <a href=https://github.com/kubernetes/kops>kops</a> is to automate full cluster operations: installation, reconfiguration of your cluster, upgrading kubernetes, and eventual cluster deletion. kops has a rich configuration model based on the Kubernetes API Machinery, so you can easily customize some parameters to your needs. kops (unlike kubeadm) handles provisioning of resources for you. kops aims to be the ultimate out-of-the-box experience on AWS (and perhaps other providers in the future). In the future kops will be adopting more and more of kubeadm for the bootstrapping phases that exist. This will move some of the complexity inside kops to a central place in the form of kubeadm.</p><p><strong>What is the scope for SIG-Cluster-Lifecycle?</strong><br>The <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG-Cluster-Lifecycle</a> actively tries to simplify the Kubernetes installation and management story. This is accomplished by modifying Kubernetes itself in many cases, and factoring out common tasks. We are also trying to address common problems in the cluster lifecycle (like the name says!). We maintain and are responsible for kubeadm and kops. We discuss problems with the current way to bootstrap clusters on AWS (and beyond) and try to make it easier. We hangout on Slack in the <a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a> and #kubeadm channels. <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>We meet and discuss</a> current topics once a week on Zoom. Feel free to come and say hi! Also, don’t be shy to <a href=https://github.com/kubernetes/kubeadm/issues>contribute</a>; we’d love your comments and insight!</p><p><strong>Looking forward to v1.6</strong></p><p>Our goals for v1.6 are centered around refactoring, stabilization and security. </p><p>First and foremost, we want to get kubeadm and its composable configuration experience to beta. We will refactor kubeadm so each phase in the bootstrap process is invokable separately. We want to bring the TLS Bootstrap API, the Certificates API and the ComponentConfig API to beta, and to get kops (and other tools) using them. </p><p>We will also graduate the token discovery we’re using now (aka. the gcr.io/google_containers/kube-discovery:1.0 image) to beta by adding a new controller to the controller manager: the <a href=https://github.com/kubernetes/kubernetes/pull/36101>BootstrapSigner</a>. Using tokens managed as Secrets, that controller will sign the contents (a kubeconfig file) of a well known ConfigMap in a new kube-public namespace. This object will be available to unauthenticated users in order to enable a secure bootstrap with a simple and short shared token.You can read the full proposal <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery.md>here</a>.</p><p>In addition to making it possible to invoke phases separately, we will also add a new phase for bringing up the control plane in a self-hosted mode (as opposed to the current static pod technique). The self-hosted technique was developed by CoreOS in the form of <a href=https://github.com/kubernetes-incubator/bootkube>bootkube</a>, and will now be incorporated as an alternative into an official Kubernetes product. Thanks to CoreOS for pushing that paradigm forward! This will be done by first setting up a temporary control plane with static pods, injecting the Deployments, ConfigMaps and DaemonSets as necessary, and lastly turning down the temporary control plane. For now, etcd will still be in a static pod by default. </p><p>We are supporting self hosting, initially, because we want to support doing patch release upgrades with kubeadm. It should be easy to upgrade from v1.6.2 to v1.6.4 for instance. We consider the built-in upgrade support a critical capability for a real cluster lifecycle tool. It will still be possible to upgrade without self-hosting but it will require more manual work.</p><p>On the stabilization front, we want to start running kubeadm e2e tests. In this v1.5 timeframe, we added unit tests and we will continue to increase that coverage. We want to expand this to per-PR e2e tests as well that spin up a cluster with <em>kubeadm init</em> and <em>kubeadm join</em>; runs some kubeadm-specific tests and optionally the Conformance test suite.</p><p>Finally, on the security front, we also want to kubeadm to be as secure as possible by default. We look to enable RBAC for v1.6, lock down what kubelet and built-in services like kube-dns and kube-proxy can do, and maybe create specific user accounts that have different permissions.</p><p>Regarding releasing, we want to have the official kubeadm v1.6 binary in the kubernetes v1.6 tarball. This means syncing our release with the official one. More details on what we’ve done so far can be found <a href=https://groups.google.com/d/msg/kubernetes-sig-cluster-lifecycle/P2oh5iHWBsA/ePeoil78BAAJ>here</a>. As it becomes possible, we aim to move the kubeadm code out to the kubernetes/kubeadm repo (This is blocked on some Kubernetes code-specific infrastructure issues that may take some time to resolve.)</p><p>Nice-to-haves for v1.6 would include an official CoreOS Container Linux installer container that does what the debs/rpms are doing for Ubuntu/CentOS. In general, it would be nice to extend the distro support. We also want to adopt <a href=https://github.com/kubernetes/kubernetes/pull/29459>Kubelet Dynamic Settings</a> so configuration passed to kubeadm init flows down to nodes automatically (it requires manual configuration currently). We want it to be possible to test Kubernetes from HEAD by using kubeadm.</p><p><strong>Through 2017 and beyond</strong></p><p>Apart from everything mentioned above, we want kubeadm to simply be a production grade (GA) tool you can use for bootstrapping a Kubernetes cluster. We want HA/multi-master to be much easier to achieve generally than it is now across platforms (though kops makes this easy on AWS today!). We want cloud providers to be out-of-tree and installable separately. <em>kubectl apply -f my-cloud-provider-here.yaml</em> should just work. The documentation should be more robust and should go deeper. Container Runtime Interface (CRI) and Federation should work well with kubeadm. Outdated getting started guides should be removed so new users aren’t mislead.</p><p><strong>Refactoring the cloud provider integration plugins</strong><br>Right now, the cloud provider integrations are built into the controller-manager, the kubelet and the API Server. This combined with the ever-growing interest for Kubernetes makes it unmaintainable to have the cloud provider integrations compiled into the core. Features that are clearly vendor-specific should not be a part of the core Kubernetes project, rather available as an addon from third party vendors. Everything cloud-specific should be moved into one controller, or a few if there’s need. This controller will be maintained by a third-party (usually the company behind the integration) and will implement cloud-specific features. This migration from in-core to out-of-core is disruptive yes, but it has very good side effects: leaner core, making it possible for more than the seven existing clouds to be integrated with Kubernetes and much easier installation. For example, you could run the cloud controller binary in a Deployment and install it with <em>kubectl apply</em> easily.</p><p>The plan for v1.6 is to make it possible to:</p><ul><li>Create and run out-of-core cloud provider integration controllers</li><li>Ship a new and temporary binary in the Kubernetes release: the cloud-controller-manager. This binary will include the seven existing cloud providers and will serve as a way of validating, testing and migrating to the new flow.
In a future release (v1.9 is proposed), the <code>--cloud-provider</code> flag will stop working, and the temporary cloud-controller-manager binary won’t be shipped anymore. Instead, a repository called something like kubernetes/cloud-providers will serve as a place for officially-validated cloud providers to evolve and exist, but all providers there will be independent to each other. (issue <a href=https://github.com/kubernetes/kubernetes/issues/2770>#2770</a>; proposal <a href=https://github.com/kubernetes/community/pull/128>#128</a>; code <a href=https://github.com/kubernetes/kubernetes/pull/34273>#3473</a>.)</li></ul><p><strong>Changelogs from v1.4 to v1.5</strong></p><p><strong>kubeadm</strong>  </p><p>v1.5 is a stabilization release for kubeadm. We’ve worked on making kubeadm more user-friendly, transparent and stable. Some new features have been added making it more configurable.</p><p>Here’s a very short extract of what’s changed:</p><ul><li>Made the <em>console output</em> of kubeadm cleaner and more <em>user-friendly</em> <a href=https://github.com/kubernetes/kubernetes/pull/37568>#37568</a></li><li>Implemented <em>kubeadm reset</em> and to drain and cleanup a node <a href=https://github.com/kubernetes/kubernetes/pull/34807>#34807</a> and <a href=https://github.com/kubernetes/kubernetes/pull/37831>#37831</a></li><li><em>Preflight checks</em> implementation that fails fast if the environment is invalid <a href=https://github.com/kubernetes/kubernetes/pull/34341>#34341</a> and <a href=https://github.com/kubernetes/kubernetes/pull/36334>#36334</a></li><li><em>kubectl logs</em> and <em>kubectl exec</em> can now be used with kubeadm <a href=https://github.com/kubernetes/kubernetes/pull/37568>#37568</a></li><li>and a lot of other improvements, please read the full <a href=https://github.com/kubernetes/kubeadm/blob/master/CHANGELOG.md>changelog</a>.</li></ul><p><strong>kops</strong></p><p>Here’s a short extract of what’s changed:</p><ul><li>Support for CNI network plugins (Weave, Calico, Kope.io)</li><li>Fully private deployments, where nodes and masters do not have public IPs</li><li>Improved rolling update of clusters, in particular of HA clusters</li><li>OS support for CentOS / RHEL / Ubuntu along with Debian, and support for sysdig & perf tools</li></ul><p>Go and check out the <a href=https://github.com/kubernetes/kops/releases>kops releases page</a> in order to get information about the latest and greatest kops release.</p><p><strong>Summary</strong></p><p>In short, we're excited on the roadmap ahead in bringing a lot of these improvements to you in the coming releases. Which we hope will make the experience to start much easier and lead to increased adoption of Kubernetes.</p><p>Thank you for all the feedback and contributions. I hope this has given you some insight in what we’re doing and encouraged you to join us at our meetings to say hi!</p><p><em>-- <a href=https://twitter.com/kubernetesonarm>Lucas Käldström</a>, Independent Kubernetes maintainer and SIG-Cluster-Lifecycle member</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-24b3af7cfba57bc76db8e0c014a09c5b>Kubernetes UX Survey Infographic</h1><div class="td-byline mb-4"><time datetime=2017-01-09 class=text-muted>Monday, January 09, 2017</time></div><p><em>Editor's note: Today’s post is by Dan Romlein, UX Designer at Apprenda and member of the SIG-UI, sharing UX survey results from the Kubernetes community. </em></p><p>The following infographic summarizes the findings of a survey that the team behind <a href=https://github.com/kubernetes/dashboard>Dashboard</a>, the official web UI for Kubernetes, sent during KubeCon in November 2016. Following the KubeCon launch of the survey, it was promoted on Twitter and various Slack channels over a two week period and generated over 100 responses. We’re delighted with the data it provides us to now make feature and roadmap decisions more in-line with the needs of you, our users.</p><p><strong>Satisfaction with Dashboard</strong></p><p><a href=https://1.bp.blogspot.com/-aSAimiXhbkw/WHPgEveTIzI/AAAAAAAAA5s/BMa-6jVzW4Ir-JExg-njJJge2tQg6QSOwCLcB/s1600/satisfaction-with-dashboard.png><img src=https://1.bp.blogspot.com/-aSAimiXhbkw/WHPgEveTIzI/AAAAAAAAA5s/BMa-6jVzW4Ir-JExg-njJJge2tQg6QSOwCLcB/s640/satisfaction-with-dashboard.png alt></a></p><p>Less than a year old, Dashboard is still very early in its development and we realize it has a long way to go, but it was encouraging to hear it’s tracking on the axis of MVP and even with its basic feature set is adding value for people. Respondents indicated that they like how quickly the Dashboard project is moving forward and the activity level of its contributors. Specific appreciation was given for the value Dashboard brings to first-time Kubernetes users and encouraging exploration. Frustration voiced around Dashboard centered on its limited capabilities: notably, the lack of RBAC and limited visualization of cluster objects and their relationships.</p><p><strong>Respondent Demographics</strong></p><p><a href=https://2.bp.blogspot.com/-f4lRiYxQ6Pg/WHPggSKpt7I/AAAAAAAAA5w/uThW4NAPiokHJ_Av721SRN4FThd2THAIQCLcB/s1600/respondent-demographics.png><img src=https://2.bp.blogspot.com/-f4lRiYxQ6Pg/WHPggSKpt7I/AAAAAAAAA5w/uThW4NAPiokHJ_Av721SRN4FThd2THAIQCLcB/s640/respondent-demographics.png alt></a></p><p><strong>Kubernetes Usage</strong></p><p><a href=https://4.bp.blogspot.com/-iQD8MEPL7nA/WHPgEensPbI/AAAAAAAAA5o/nRAVMQpcxmM9llFJyC-pVD16emtagnxgwCEw/s1600/kubernetes-usage.png><img src=https://4.bp.blogspot.com/-iQD8MEPL7nA/WHPgEensPbI/AAAAAAAAA5o/nRAVMQpcxmM9llFJyC-pVD16emtagnxgwCEw/s640/kubernetes-usage.png alt></a></p><p>People are using Dashboard in production, which is fantastic; it’s that setting that the team is focused on optimizing for.</p><p><strong>Feature Priority</strong></p><p><a href=https://1.bp.blogspot.com/-gGKQKRwgOto/WHPgEdVMqQI/AAAAAAAAA5k/MiTVQtKLuHkAMmSjpvAsmiBezAdQV4zCwCEw/s1600/feature-priority.png><img src=https://1.bp.blogspot.com/-gGKQKRwgOto/WHPgEdVMqQI/AAAAAAAAA5k/MiTVQtKLuHkAMmSjpvAsmiBezAdQV4zCwCEw/s640/feature-priority.png alt></a></p><p>In building Dashboard, we want to continually make alignments between the needs of Kubernetes users and our product. Feature areas have intentionally been kept as high-level as possible, so that UX designers on the Dashboard team can creatively transform those use cases into specific features. While there’s nothing wrong with “<a href=http://www.goodreads.com/quotes/15297-if-i-had-asked-people-what-they-wanted-they-would>faster horses</a>”, we want to make sure we’re creating an environment for the best possible innovation to flourish.</p><p>Troubleshooting & Debugging as a strong frontrunner in requested feature area is consistent with the <a href=http://static.lwy.io/img/kubernetes_dashboard_infographic.png>previous KubeCon survey</a>, and this is now our top area of investment. Currently in-progress is the ability to be able to exec into a Pod, and next up will be providing aggregated logs views across objects. One of a UI’s strengths over a CLI is its ability to show things, and the troubleshooting and debugging feature area is a prime application of this capability.</p><p>In addition to a continued ongoing investment in troubleshooting and debugging functionality, the other focus of the Dashboard team’s efforts currently is RBAC / IAM within Dashboard. Though #4 on the ranking of feature areas, In various conversations at KubeCon and the days following, this emerged as a top-requested feature of Dashboard, and the one people were most passionate about. This is a deal-breaker for many companies, and we’re confident its enablement will open many doors for Dashboard’s use in production.</p><p><strong>Conclusion</strong></p><p>It’s invaluable to have data from Kubernetes users on how they’re putting Dashboard to use and how well it’s serving their needs. If you missed the survey response window but still have something you’d like to share, we’d love to connect with you and hear feedback or answer questions: </p><ul><li>Email us at the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-ui>SIG-UI mailing list</a></li><li>Chat with us on the Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-ui/>#SIG-UI channel</a></li><li>Join our weekly meetings at 4PM CEST. See the <a href="https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&ctz=Europe/Warsaw">SIG-UI calendar</a> for details.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2df10d56069a485f6d0ee66040e4ede2>Kubernetes supports OpenAPI</h1><div class="td-byline mb-4"><time datetime=2016-12-23 class=text-muted>Friday, December 23, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/12/five-days-of-kubernetes-1-5/>series of in-depth articles</a> on what's new in Kubernetes 1.5</em></p><p><a href=https://www.openapis.org/>OpenAPI</a> allows API providers to define their operations and models, and enables developers to automate their tools and generate their favorite language’s client to talk to that API server. Kubernetes has supported swagger 1.2 (older version of OpenAPI spec) for a while, but the spec was incomplete and invalid, making it hard to generate tools/clients based on it.</p><p>In Kubernetes 1.4, we introduced alpha support for the OpenAPI spec (formerly known as swagger 2.0 before it was donated to the <a href=https://www.openapis.org/about>Open API Initiative</a>) by upgrading the current models and operations. Beginning in <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5</a>, the support for the OpenAPI spec has been completed by auto-generating the spec directly from Kubernetes source, which will keep the spec--and documentation--completely in sync with future changes in operations/models.</p><p>The new spec enables us to have better API documentation and we have even introduced a supported <a href=https://github.com/kubernetes-incubator/client-python>python client</a>.</p><p>The spec is modular, divided by GroupVersion: this is future-proof, since we intend to allow separate GroupVersions to be served out of separate API servers.</p><p>The structure of spec is explained in detail in <a href=https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md>OpenAPI spec definition</a>. We used <a href=https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#tag-object>operation’s tags</a> to separate each GroupVersion and filled as much information as we can about paths/operations and models. For a specific operation, all parameters, method of call, and responses are documented.</p><p>For example, OpenAPI spec for reading a pod information is:</p><pre><code>{

...  
  &quot;paths&quot;: {

&quot;/api/v1/namespaces/{namespace}/pods/{name}&quot;: {  
    &quot;get&quot;: {  
     &quot;description&quot;: &quot;read the specified Pod&quot;,  
     &quot;consumes&quot;: [  
      &quot;\*/\*&quot;  
     ],  
     &quot;produces&quot;: [  
      &quot;application/json&quot;,  
      &quot;application/yaml&quot;,  
      &quot;application/vnd.kubernetes.protobuf&quot;  
     ],  
     &quot;schemes&quot;: [  
      &quot;https&quot;  
     ],  
     &quot;tags&quot;: [  
      &quot;core\_v1&quot;  
     ],  
     &quot;operationId&quot;: &quot;readCoreV1NamespacedPod&quot;,  
     &quot;parameters&quot;: [  
      {  
       &quot;uniqueItems&quot;: true,  
       &quot;type&quot;: &quot;boolean&quot;,  
       &quot;description&quot;: &quot;Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'.&quot;,  
       &quot;name&quot;: &quot;exact&quot;,  
       &quot;in&quot;: &quot;query&quot;  
      },  
      {  
       &quot;uniqueItems&quot;: true,  
       &quot;type&quot;: &quot;boolean&quot;,  
       &quot;description&quot;: &quot;Should this value be exported.  Export strips fields that a user can not specify.&quot;,  
       &quot;name&quot;: &quot;export&quot;,  
       &quot;in&quot;: &quot;query&quot;  
      }  
     ],  
     &quot;responses&quot;: {  
      &quot;200&quot;: {  
       &quot;description&quot;: &quot;OK&quot;,  
       &quot;schema&quot;: {  
        &quot;$ref&quot;: &quot;#/definitions/v1.Pod&quot;  
       }  
      },  
      &quot;401&quot;: {  
       &quot;description&quot;: &quot;Unauthorized&quot;  
      }  
     }  
    },

…

}

…
</code></pre><p>Using this information and the URL of <code>kube-apiserver</code>, one should be able to make the call to the given url (/api/v1/namespaces/{namespace}/pods/{name}) with parameters such as <code>name</code>, <code>exact</code>, <code>export</code>, etc. to get pod’s information. Client libraries generators would also use this information to create an API function call for reading pod’s information. For example, <a href=https://github.com/kubernetes-incubator/client-python>python client</a> makes it easy to call this operation like this:</p><pre><code>from kubernetes import client

ret = client.CoreV1Api().read\_namespaced\_pod(name=&quot;pods\_name&quot;, namespace=&quot;default&quot;)
</code></pre><p>A simplified version of generated read_namespaced_pod, can be found <a href=https://gist.github.com/mbohlool/d5ec1dace27ef90cf742555c05480146>here</a>.</p><p>Swagger-codegen document generator would also be able to create documentation using the same information:</p><pre><code>GET /api/v1/namespaces/{namespace}/pods/{name}

(readCoreV1NamespacedPod)

read the specified Pod

Path parameters

name (required)

Path Parameter — name of the Pod

namespace (required)

Path Parameter — object name and auth scope, such as for teams and projects

Consumes

This API call consumes the following media types via the Content-Type request header:

-
\*/\*


Query parameters

pretty (optional)

Query Parameter — If 'true', then the output is pretty printed.

exact (optional)

Query Parameter — Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'.

export (optional)

Query Parameter — Should this value be exported. Export strips fields that a user can not specify.

Return type

v1.Pod


Produces

This API call produces the following media types according to the Accept request header; the media type will be conveyed by the Content-Type response header.

-
application/json
-
application/yaml
-
application/vnd.kubernetes.protobuf

Responses

200

OK v1.Pod

401

Unauthorized
</code></pre><p>There are two ways to access OpenAPI spec:</p><ul><li>From <code>kuber-apiserver</code>/swagger.json. This file will have all enabled GroupVersions routes and models and would be most up-to-date file with an specific <code>kube-apiserver</code>.</li><li>From Kubernetes GitHub repository with all core GroupVersions enabled. You can access it on <a href=https://github.com/kubernetes/kubernetes/blob/master/api/openapi-spec/swagger.json>master</a> or an specific release (for example <a href=https://github.com/kubernetes/kubernetes/blob/release-1.5/api/openapi-spec/swagger.json>1.5 release</a>).</li></ul><p>There are numerous <a href=http://swagger.io/tools/>tools</a> that works with this spec. For example, you can use the <a href=http://swagger.io/swagger-editor/>swagger editor</a> to open the spec file and render documentation, as well as generate clients; or you can directly use <a href=http://swagger.io/swagger-codegen/>swagger codegen</a> to generate documentation and clients. The clients this generates will mostly work out of the box--but you will need some support for authorization and some Kubernetes specific utilities. Use <a href=https://github.com/kubernetes-incubator/client-python>python client</a> as a template to create your own client.</p><p>If you want to get involved in development of OpenAPI support, client libraries, or report a bug, you can get in touch with developers at <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG-API-Machinery</a>.</p><p><em>--Mehdy Bohlool, Software Engineer, Google</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9e45a8ed610420252c4ab6b425d46fba>Cluster Federation in Kubernetes 1.5</h1><div class="td-byline mb-4"><time datetime=2016-12-22 class=text-muted>Thursday, December 22, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/12/five-days-of-kubernetes-1-5/>series of in-depth articles</a> on what's new in Kubernetes 1.5</em></p><p>In the latest <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5 release</a>, you’ll notice that support for Cluster Federation is maturing. That functionality was introduced in Kubernetes 1.3, and the 1.5 release includes a number of new features, including an easier setup experience and a step closer to supporting all Kubernetes API objects.</p><p>A new command line tool called ‘<strong><a href=/docs/admin/federation/kubefed/>kubefed</a></strong>’ was introduced to make getting started with Cluster Federation much simpler. Also, alpha level support was added for Federated DaemonSets, Deployments and ConfigMaps. In summary:</p><ul><li><strong>DaemonSets</strong> are Kubernetes deployment rules that guarantee that a given pod is always present at every node, as new nodes are added to the cluster (more <a href=/docs/admin/daemons/>info</a>).</li><li><strong>Deployments</strong> describe the desired state of Replica Sets (more <a href=/docs/user-guide/deployments/>info</a>).</li><li><strong>ConfigMaps</strong> are variables applied to Replica Sets (which greatly improves image reusability as their parameters can be externalized - more <a href=/docs/user-guide/configmap/>info</a>).
<strong>Federated DaemonSets</strong> , <strong>Federated Deployments</strong> , <strong>Federated ConfigMaps</strong> take the qualities of the base concepts to the next level. For instance, Federated DaemonSets guarantee that a pod is deployed on every node of the newly added cluster.</li></ul><p>But what actually is “federation”? Let’s explain it by what needs it satisfies. Imagine a service that operates globally. Naturally, all its users expect to get the same quality of service, whether they are located in Asia, Europe, or the US. What this means is that the service must respond equally fast to requests at each location. This sounds simple, but there’s lots of logic involved behind the scenes. This is what Kubernetes Cluster Federation aims to do.</p><p>How does it work? One of the Kubernetes clusters must become a master by running a <strong>Federation Control Plane</strong>. In practice, this is a controller that monitors the health of other clusters, and provides a single entry point for administration. The entry point behaves like a typical Kubernetes cluster. It allows creating <a href=/docs/user-guide/replicasets/>Replica Sets</a>, <a href=/docs/user-guide/deployments/>Deployments</a>, <a href=/docs/user-guide/services/>Services</a>, but the federated control plane passes the resources to underlying clusters. This means that if we request the federation control plane to create a Replica Set with 1,000 replicas, it will spread the request across all underlying clusters. If we have 5 clusters, then by default each will get its share of 200 replicas.</p><p>This on its own is a powerful mechanism. But there’s more. It’s also possible to create a Federated Ingress. Effectively, this is a global application-layer load balancer. Thanks to an understanding of the application layer, it allows load balancing to be “smarter” -- for instance, by taking into account the geographical location of clients and servers, and routing the traffic between them in an optimal way.</p><p>In summary, with Kubernetes Cluster Federation, we can facilitate administration of all the clusters (single access point), but also optimize global content delivery around the globe. In the following sections, we will show how it works.</p><p><strong>Creating a Federation Plane</strong></p><p>In this exercise, we will federate a few clusters. For convenience, all commands have been grouped into 6 scripts available <a href=https://github.com/ContainerSolutions/k8shserver/tree/master/scripts>here</a>:</p><ul><li>0-settings.sh</li><li>1-create.sh</li><li>2-getcredentials.sh</li><li>3-initfed.sh</li><li>4-joinfed.sh</li><li>5-destroy.sh
First we need to define several variables (0-settings.sh)</li></ul><pre><code>$ cat 0-settings.sh &amp;&amp; . 0-settings.sh

# this project create 3 clusters in 3 zones. FED\_HOST\_CLUSTER points to the one, which will be used to deploy federation control plane

export FED\_HOST\_CLUSTER=us-east1-b


# Google Cloud project name

export FED\_PROJECT=\&lt;YOUR PROJECT e.g. company-project\&gt;


# DNS suffix for this federation. Federated Service DNS names are published with this suffix. This must be a real domain name that you control and is programmable by one of the DNS providers (Google Cloud DNS or AWS Route53)

export FED\_DNS\_ZONE=\&lt;YOUR DNS SUFFIX e.g. example.com\&gt;
</code></pre><p>And get kubectl and kubefed binaries. (for installation instructions refer to guides <a href=/docs/user-guide/prereqs/>here</a> and <a href=/docs/admin/federation/kubefed/#getting-kubefed>here</a>).<br>Now the setup is ready to create a few Google Container Engine (GKE) clusters with gcloud container clusters create (1-create.sh). In this case one is in US, one in Europe and one in Asia.</p><pre><code>$ cat 1-create.sh &amp;&amp; . 1-create.sh

gcloud container clusters create gce-us-east1-b --project=${FED\_PROJECT} --zone=us-east1-b --scopes cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite


gcloud container clusters create gce-europe-west1-b --project=${FED\_PROJECT} --zone=europe-west1-b --scopes cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite


gcloud container clusters create gce-asia-east1-a --project=${FED\_PROJECT} --zone=asia-east1-a --scopes cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite
</code></pre><p>The next step is fetching kubectl configuration with gcloud -q container clusters get-credentials (2-getcredentials.sh). The configurations will be used to indicate the current context for kubectl commands.</p><pre><code>$ cat 2-getcredentials.sh &amp;&amp; . 2-getcredentials.sh

gcloud -q container clusters get-credentials gce-us-east1-b --zone=us-east1-b --project=${FED\_PROJECT}


gcloud -q container clusters get-credentials gce-europe-west1-b --zone=europe-west1-b --project=${FED\_PROJECT}


gcloud -q container clusters get-credentials gce-asia-east1-a --zone=asia-east1-a --project=${FED\_PROJECT}
</code></pre><p>Let’s verify the setup:</p><pre><code>$ kubectl config get-contexts

CURRENT   NAME CLUSTER  AUTHINFO  NAMESPACE

\*         

gke\_container-solutions\_europe-west1-b\_gce-europe-west1-b

gke\_container-solutions\_europe-west1-b\_gce-europe-west1-b   

gke\_container-solutions\_europe-west1-b\_gce-europe-west1-b      

gke\_container-solutions\_us-east1-b\_gce-us-east1-b

gke\_container-solutions\_us-east1-b\_gce-us-east1-b           

gke\_container-solutions\_us-east1-b\_gce-us-east1-b

gke\_container-solutions\_asia-east1-a\_gce-asia-east1-a

gke\_container-solutions\_asia-east1-a\_gce-asia-east1-a  

gke\_container-solutions\_asia-east1-a\_gce-asia-east1-a
</code></pre><p>We have 3 clusters. One, indicated by the FED_HOST_CLUSTER environment variable, will be used to run the federation plane. For this, we will use the kubefed init federation command (3-initfed.sh).</p><pre><code>$ cat 3-initfed.sh &amp;&amp; . 3-initfed.sh

kubefed init federation --host-cluster-context=gke\_${FED\_PROJECT}\_${FED\_HOST\_CLUSTER}\_gce-${FED\_HOST\_CLUSTER} --dns-zone-name=${FED\_DNS\_ZONE}
</code></pre><p>You will notice that after executing the above command, a new kubectl context has appeared:</p><pre><code>$ kubectl config get-contexts

CURRENT   NAME  CLUSTER  AUTHINFO NAMESPACE

...         

federation

federation
</code></pre><p>The federation context will become our administration entry point. Now it’s time to join clusters (4-joinfed.sh):</p><pre><code>$ cat 4-joinfed.sh &amp;&amp; . 4-joinfed.sh

kubefed --context=federation join cluster-europe-west1-b --cluster-context=gke\_${FED\_PROJECT}\_europe-west1-b\_gce-europe-west1-b --host-cluster-context=gke\_${FED\_PROJECT}\_${FED\_HOST\_CLUSTER}\_gce-${FED\_HOST\_CLUSTER}


kubefed --context=federation join cluster-asia-east1-a --cluster-context=gke\_${FED\_PROJECT}\_asia-east1-a\_gce-asia-east1-a --host-cluster-context=gke\_${FED\_PROJECT}\_${FED\_HOST\_CLUSTER}\_gce-${FED\_HOST\_CLUSTER}


kubefed --context=federation join cluster-us-east1-b --cluster-context=gke\_${FED\_PROJECT}\_us-east1-b\_gce-us-east1-b --host-cluster-context=gke\_${FED\_PROJECT}\_${FED\_HOST\_CLUSTER}\_gce-${FED\_HOST\_CLUSTER}
</code></pre><p>Note that cluster gce-us-east1-b is used here to run the federation control plane and also to work as a worker cluster. This circular dependency helps to use resources more efficiently and it can be verified by using the kubectl --context=federation get clusters command:</p><pre><code>$ kubectl --context=federation get clusters

NAME                        STATUS    AGE

cluster-asia-east1-a        Ready     7s

cluster-europe-west1-b      Ready     10s

cluster-us-east1-b          Ready     10s
</code></pre><p>We are good to go.</p><p><strong>Using Federation To Run An Application</strong></p><p>In our <a href=https://github.com/ContainerSolutions/k8shserver>repository</a> you will find instructions on how to build a docker image with a web service that displays the container’s hostname and the Google Cloud Platform (GCP) zone.</p><p>An example output might look like this:</p><pre><code>{&quot;hostname&quot;:&quot;k8shserver-6we2u&quot;,&quot;zone&quot;:&quot;europe-west1-b&quot;}
</code></pre><p>Now we will deploy the Replica Set (<a href=https://github.com/ContainerSolutions/k8shserver/blob/master/rs/k8shserver.yaml>k8shserver.yaml</a>):</p><pre><code>$ kubectl --context=federation create -f rs/k8shserver
</code></pre><p>And a Federated Service (<a href=https://github.com/ContainerSolutions/k8shserver/blob/master/services/k8shserver.yaml>k8shserver.yaml</a>):</p><pre><code>$ kubectl --context=federation create -f service/k8shserver
</code></pre><p>As you can see, the two commands refer to the “federation” context, i.e. to the federation control plane. After a few minutes, you will realize that underlying clusters run the Replica Set and the Service.</p><p><strong>Creating The Ingress</strong></p><p>After the Service is ready, we can create <a href=/docs/user-guide/ingress/>Ingress</a> - the global load balancer. The command is like this:</p><pre><code>kubectl --context=federation create -f ingress/k8shserver.yaml
</code></pre><p>The contents of the file point to the service we created in the previous step:</p><pre><code>apiVersion: extensions/v1beta1

kind: Ingress

metadata:

  name: k8shserver

spec:

  backend:

    serviceName: k8shserver

    servicePort: 80
</code></pre><p>After a few minutes, we should get a global IP address:</p><pre><code>$ kubectl --context=federation get ingress

NAME         HOSTS     ADDRESS          PORTS     AGE

k8shserver   \*         130.211.40.125   80        20m
</code></pre><p>Effectively, the response of:</p><pre><code>$ curl 130.211.40.125
</code></pre><p>depends on the location of client. Something like this would be expected in the US:</p><pre><code>{&quot;hostname&quot;:&quot;k8shserver-w56n4&quot;,&quot;zone&quot;:&quot;us-east1-b&quot;}
</code></pre><p>Whereas in Europe, we might have:</p><pre><code>{&quot;hostname&quot;:&quot;k8shserver-z31p1&quot;,&quot;zone&quot;:&quot;eu-west1-b&quot;}
</code></pre><p>Please refer to this <a href=https://github.com/kubernetes/kubernetes/issues/39087>issue</a> for additional details on how everything we've described works.</p><p><strong>Demo</strong></p><p><strong>Summary</strong></p><p>Cluster Federation is actively being worked on and is still not fully General Available. Some APIs are in beta and others are in alpha. Some features are missing, for instance cross-cloud load balancing is not supported (federated ingress currently only works on Google Cloud Platform as it depends on GCP <a href=https://cloud.google.com/compute/docs/load-balancing/http/>HTTP(S) Load Balancing</a>).</p><p>Nevertheless, as the functionality matures, it will become an enabler for all companies that aim at global markets, but currently cannot afford sophisticated administration techniques as used by the likes of Netflix or Amazon. That’s why we closely watch the technology, hoping that it soon fulfills its promise.</p><p>PS. When done, remember to destroy your clusters:</p><pre><code>$ . 5-destroy.sh
</code></pre><p><em>-__-Lukasz Guminski, Software Engineer at Container Solutions. Allan Naim, Product Manager, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f405bcd4c695718170de3c183e0a7d25>Windows Server Support Comes to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-12-21 class=text-muted>Wednesday, December 21, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/12/five-days-of-kubernetes-1-5/>series of in-depth articles</a> on what's new in Kubernetes 1.5</em></p><p>Extending on the theme of giving users choice, <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5 release</a> includes the support for Windows Servers. WIth more than <a href=http://www.gartner.com/document/3446217>80%</a> of enterprise apps running Java on Linux or .Net on Windows, Kubernetes is previewing capabilities that extends its reach to the mass majority of enterprise workloads. </p><p>The new Kubernetes Windows Server 2016 and Windows Container support includes public preview with the following features:</p><ul><li><p><strong>Containerized Multiplatform Applications</strong> - Applications developed in operating system neutral languages like Go and .NET Core were previously impossible to orchestrate between Linux and Windows. Now, with support for Windows Server 2016 in Kubernetes, such applications can be deployed on both Windows Server as well as Linux, giving the developer choice of the operating system runtime. This capability has been desired by customers for almost two decades. </p></li><li><p><strong>Support for Both Windows Server Containers and Hyper-V Containers</strong> - There are two types of containers in Windows Server 2016. Windows Containers is similar to Docker containers on Linux, and uses kernel sharing. The other, called Hyper-V Containers, is more lightweight than a virtual machine while at the same time offering greater isolation, its own copy of the kernel, and direct memory assignment. Kubernetes can orchestrate both these types of containers. </p></li><li><p><strong>Expanded Ecosystem of Applications</strong> - One of the key drivers of introducing Windows Server support in Kubernetes is to expand the ecosystem of applications supported by Kubernetes: IIS, .NET, Windows Services, ASP.NET, .NET Core, are some of the application types that can now be orchestrated by Kubernetes, running inside a container on Windows Server.</p></li><li><p><strong>Coverage for Heterogeneous Data Centers</strong> - Organizations already use Kubernetes to host tens of thousands of application instances across Global 2000 and Fortune 500. This will allow them to expand Kubernetes to the large footprint of Windows Server. </p></li></ul><p>The process to bring Windows Server to Kubernetes has been a truly multi-vendor effort and championed by the <a href=https://github.com/kubernetes/community/blob/master/sig-windows/README.md>Windows Special Interest Group (SIG)</a> - Apprenda, Google, Red Hat and Microsoft were all involved in bringing Kubernetes to Windows Server. On the community effort to bring Kubernetes to Windows Server, Taylor Brown, Principal Program Manager at Microsoft stated that “This new Kubernetes community work furthers Windows Server container support options for popular orchestrators, reinforcing Microsoft’s commitment to choice and flexibility for both Windows and Linux ecosystems.”</p><p><strong>Guidance for Current Usage</strong></p><p>|
Where to use Windows Server support?
|
Right now organizations should start testing Kubernetes on Windows Server and provide feedback. Most organizations take months to set up hardened production environments and general availability should be available in next few releases of Kubernetes.
|
|
What works?
|
Most of the Kubernetes constructs, such as Pods, Services, Labels, etc. work with Windows Containers.
|
|
What doesn’t work yet?
|</p><ul><li>Pod abstraction is not same due to networking namespaces. Net result is that Windows containers in a single POD cannot communicate over localhost. Linux containers can share networking stack by placing them in the same network namespace.</li><li>DNS capabilities are not fully implemented</li><li>UDP is not supported inside a container</li></ul><p>|
|
When will it be ready for all production workloads (general availability)?
|
The goal is to refine the networking and other areas that need work to get Kubernetes users a production version of Windows Server 2016 - including with Windows Nano Server and Windows Server Core installation options - support in the next couple releases.
|</p><p><strong>Technical Demo</strong></p><p><strong>Roadmap</strong></p><p>Support for Windows Server-based containers is in alpha release mode for Kubernetes 1.5, but the community is not stopping there. Customers want enterprise hardened container scheduling and management for their entire tech portfolio. That has to include full parity of features among Linux and Windows Server in production. The <a href=https://github.com/kubernetes/community/blob/master/sig-windows/README.md>Windows Server SIG</a> will deliver that parity within the next one or two releases of Kubernetes through a few key areas of investment:</p><ul><li><strong>Networking</strong> - the SIG will continue working side by side with Microsoft to enhance the networking backbone of Windows Server Containers, specifically around lighting up container mode networking and native network overlay support for container endpoints. </li><li><strong>OOBE</strong> - Improving the setup, deployment, and diagnostics for a Windows Server node, including the ability to deploy to any cloud (Azure, AWS, GCP)</li><li><strong>Runtime Operations</strong> - the SIG will play a key part in defining the monitoring interface of the Container Runtime Interface (CRI), leveraging it to provide deep insight and monitoring for Windows Server-based containers
<strong>Get Started</strong></li></ul><p>To get started with Kubernetes on Windows Server 2016, please visit the <a href=/docs/getting-started-guides/windows/>GitHub guide</a> for more details.<br>If you want to help with Windows Server support, then please connect with the <a href=https://github.com/kubernetes/community/blob/master/sig-windows/README.md>Windows Server SIG</a> or connect directly with Michael Michael, the SIG lead, on <a href=https://github.com/michmike>GitHub</a>. </p><p><em>--<a href=https://twitter.com/michmike77>Michael Michael</a>, Senior Director of Product Management, Apprenda </em></p><p>| <img src=https://lh6.googleusercontent.com/1Lqqd5m0gHECz_yHvTas4eOOkFnB64h9j65Flrb5OHmIoaAZLUr64y2kukx5m7_QbBxnk_plxfxsQymhnO9UrcGGixDx_ZG7w0tJIzV_pnljLJLk3u3o8P1wJxNJiKbf0L077eYO alt> |
| Kubernetes on Windows Server 2016 Architecture |</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0ddb1478a7369bfd58673119e4301139>StatefulSet: Run and Scale Stateful Applications Easily in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-12-20 class=text-muted>Tuesday, December 20, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/12/five-days-of-kubernetes-1-5/>series of in-depth articles</a> on what's new in Kubernetes 1.5</em></p><p>In the latest release, <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5</a>, we’ve moved the feature formerly known as PetSet into beta as <a href=/docs/concepts/abstractions/controllers/statefulsets/>StatefulSet</a>. There were no major changes to the API Object, other than the community selected name, but we added the semantics of “at most one pod per index” for deployment of the Pods in the set. Along with ordered deployment, ordered termination, unique network names, and persistent stable storage, we think we have the right primitives to support many containerized stateful workloads. We don’t claim that the feature is 100% complete (it is software after all), but we believe that it is useful in its current form, and that we can extend the API in a backwards-compatible way as we progress toward an eventual GA release.</p><p><strong>When is StatefulSet the Right Choice for my Storage Application?</strong></p><p><a href=/docs/user-guide/deployments/>Deployments</a> and <a href=/docs/user-guide/replicasets/>ReplicaSets</a> are a great way to run stateless replicas of an application on Kubernetes, but their semantics aren’t really right for deploying stateful applications. The purpose of StatefulSet is to provide a controller with the correct semantics for deploying a wide range of stateful workloads. However, moving your storage application onto Kubernetes isn’t always the correct choice. Before you go all in on converging your storage tier and your orchestration framework, you should ask yourself a few questions.</p><p><strong>Can your application run using remote storage or does it require local storage media?</strong></p><p>Currently, we recommend using StatefulSets with remote storage. Therefore, you must be ready to tolerate the performance implications of network attached storage. Even with storage optimized instances, you won’t likely realize the same performance as locally attached, solid state storage media. Does the performance of network attached storage, on your cloud, allow your storage application to meet its SLAs? If so, running your application in a StatefulSet provides compelling benefits from the perspective of automation. If the node on which your storage application is running fails, the Pod containing the application can be rescheduled onto another node, and, as it’s using network attached storage media, its data are still available after it’s rescheduled.</p><p><strong>Do you need to scale your storage application?</strong></p><p>What is the benefit you hope to gain by running your application in a StatefulSet? Do you have a single instance of your storage application for your entire organization? Is scaling your storage application a problem that you actually have? If you have a few instances of your storage application, and they are successfully meeting the demands of your organization, and those demands are not rapidly increasing, you’re already at a local optimum.</p><p>If, however, you have an ecosystem of microservices, or if you frequently stamp out new service footprints that include storage applications, then you might benefit from automation and consolidation. If you’re already using Kubernetes to manage the stateless tiers of your ecosystem, you should consider using the same infrastructure to manage your storage applications.</p><p><strong>How important is predictable performance?</strong></p><p>Kubernetes doesn’t yet support isolation for network or storage I/O across containers. Colocating your storage application with a noisy neighbor can reduce the QPS that your application can handle. You can mitigate this by scheduling the Pod containing your storage application as the only tenant on a node (thus providing it a dedicated machine) or by using Pod anti-affinity rules to segregate Pods that contend for network or disk, but this means that you have to actively identify and mitigate hot spots.</p><p>If squeezing the absolute maximum QPS out of your storage application isn’t your primary concern, if you’re willing and able to mitigate hotspots to ensure your storage applications meet their SLAs, and if the ease of turning up new "footprints" (services or collections of services), scaling them, and flexibly re-allocating resources is your primary concern, Kubernetes and StatefulSet might be the right solution to address it.</p><p><strong>Does your application require specialized hardware or instance types?</strong></p><p>If you run your storage application on high-end hardware or extra-large instance sizes, and your other workloads on commodity hardware or smaller, less expensive images, you may not want to deploy a heterogenous cluster. If you can standardize on a single instance size for all types of apps, then you may benefit from the flexible resource reallocation and consolidation, that you get from Kubernetes.</p><p><strong>A Practical Example - ZooKeeper</strong></p><p><a href=https://zookeeper.apache.org/doc/current/>ZooKeeper</a> is an interesting use case for StatefulSet for two reasons. First, it demonstrates that StatefulSet can be used to run a distributed, strongly consistent storage application on Kubernetes. Second, it's a prerequisite for running workloads like <a href=http://hadoop.apache.org/>Apache Hadoop</a> and <a href=https://kafka.apache.org/>Apache Kakfa</a> on Kubernetes. An <a href=/docs/tutorials/stateful-application/zookeeper/>in-depth tutorial</a> on deploying a ZooKeeper ensemble on Kubernetes is available in the Kubernetes documentation, and we’ll outline a few of the key features below.</p><p><strong>Creating a ZooKeeper Ensemble</strong><br>Creating an ensemble is as simple as using <a href=/docs/user-guide/kubectl/kubectl_create/>kubectl create</a> to generate the objects stored in the manifest.</p><pre><code>$ kubectl create -f [http://k8s.io/docs/tutorials/stateful-application/zookeeper.yaml](https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/stateful-application/zookeeper.yaml)

service &quot;zk-headless&quot; created

configmap &quot;zk-config&quot; created

poddisruptionbudget &quot;zk-budget&quot; created

statefulset &quot;zk&quot; created
</code></pre><p>When you create the manifest, the StatefulSet controller creates each Pod, with respect to its ordinal, and waits for each to be Running and Ready prior to creating its successor.</p><pre><code>$ kubectl get -w -l app=zk

NAME      READY     STATUS    RESTARTS   AGE

zk-0      0/1       Pending   0          0s

zk-0      0/1       Pending   0         0s

zk-0      0/1       Pending   0         7s

zk-0      0/1       ContainerCreating   0         7s

zk-0      0/1       Running   0         38s

zk-0      1/1       Running   0         58s

zk-1      0/1       Pending   0         1s

zk-1      0/1       Pending   0         1s

zk-1      0/1       ContainerCreating   0         1s

zk-1      0/1       Running   0         33s

zk-1      1/1       Running   0         51s

zk-2      0/1       Pending   0         0s

zk-2      0/1       Pending   0         0s

zk-2      0/1       ContainerCreating   0         0s

zk-2      0/1       Running   0         25s

zk-2      1/1       Running   0         40s
</code></pre><p>Examining the hostnames of each Pod in the StatefulSet, you can see that the Pods’ hostnames also contain the Pods’ ordinals.</p><pre><code>$ for i in 0 1 2; do kubectl exec zk-$i -- hostname; done

zk-0

zk-1

zk-2
</code></pre><p>ZooKeeper stores the unique identifier of each server in a file called “myid”. The identifiers used for ZooKeeper servers are just natural numbers. For the servers in the ensemble, the “myid” files are populated by adding one to the ordinal extracted from the Pods’ hostnames.</p><pre><code>$ for i in 0 1 2; do echo &quot;myid zk-$i&quot;;kubectl exec zk-$i -- cat /var/lib/zookeeper/data/myid; done

myid zk-0

1

myid zk-1

2

myid zk-2

3
</code></pre><p>Each Pod has a unique network address based on its hostname and the network domain controlled by the zk-headless Headless Service.</p><pre><code>$  for i in 0 1 2; do kubectl exec zk-$i -- hostname -f; done

zk-0.zk-headless.default.svc.cluster.local

zk-1.zk-headless.default.svc.cluster.local

zk-2.zk-headless.default.svc.cluster.local
</code></pre><p>The combination of a unique Pod ordinal and a unique network address allows you to populate the ZooKeeper servers’ configuration files with a consistent ensemble membership.</p><pre><code>$  kubectl exec zk-0 -- cat /opt/zookeeper/conf/zoo.cfg

clientPort=2181

dataDir=/var/lib/zookeeper/data

dataLogDir=/var/lib/zookeeper/log

tickTime=2000

initLimit=10

syncLimit=2000

maxClientCnxns=60

minSessionTimeout= 4000

maxSessionTimeout= 40000

autopurge.snapRetainCount=3

autopurge.purgeInteval=1

server.1=zk-0.zk-headless.default.svc.cluster.local:2888:3888

server.2=zk-1.zk-headless.default.svc.cluster.local:2888:3888

server.3=zk-2.zk-headless.default.svc.cluster.local:2888:3888
</code></pre><p>StatefulSet lets you deploy ZooKeeper in a consistent and reproducible way. You won’t create more than one server with the same id, the servers can find each other via a stable network addresses, and they can perform leader election and replicate writes because the ensemble has consistent membership.</p><p>The simplest way to verify that the ensemble works is to write a value to one server and to read it from another. You can use the “zkCli.sh” script that ships with the ZooKeeper distribution, to create a ZNode containing some data.</p><pre><code>$  kubectl exec zk-0 zkCli.sh create /hello world

...


WATCHER::


WatchedEvent state:SyncConnected type:None path:null

Created /hello
</code></pre><p>You can use the same script to read the data from another server in the ensemble.</p><pre><code>$  kubectl exec zk-1 zkCli.sh get /hello

...


WATCHER::


WatchedEvent state:SyncConnected type:None path:null

world

...
</code></pre><p>You can take the ensemble down by deleting the zk StatefulSet.</p><pre><code>$  kubectl delete statefulset zk

statefulset &quot;zk&quot; deleted
</code></pre><p>The cascading delete destroys each Pod in the StatefulSet, with respect to the reverse order of the Pods’ ordinals, and it waits for each to terminate completely before terminating its predecessor.</p><pre><code>$  kubectl get pods -w -l app=zk

NAME      READY     STATUS    RESTARTS   AGE

zk-0      1/1       Running   0          14m

zk-1      1/1       Running   0          13m

zk-2      1/1       Running   0          12m

NAME      READY     STATUS        RESTARTS   AGE

zk-2      1/1       Terminating   0          12m

zk-1      1/1       Terminating   0         13m

zk-0      1/1       Terminating   0         14m

zk-2      0/1       Terminating   0         13m

zk-2      0/1       Terminating   0         13m

zk-2      0/1       Terminating   0         13m

zk-1      0/1       Terminating   0         14m

zk-1      0/1       Terminating   0         14m

zk-1      0/1       Terminating   0         14m

zk-0      0/1       Terminating   0         15m

zk-0      0/1       Terminating   0         15m

zk-0      0/1       Terminating   0         15m
</code></pre><p>You can use <a href=/docs/user-guide/kubectl/kubectl_apply/>kubectl apply</a> to recreate the zk StatefulSet and redeploy the ensemble.</p><pre><code>$  kubectl apply -f [http://k8s.io/docs/tutorials/stateful-application/zookeeper.yaml](https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/stateful-application/zookeeper.yaml)

service &quot;zk-headless&quot; configured

configmap &quot;zk-config&quot; configured

statefulset &quot;zk&quot; created
</code></pre><p>If you use the “zkCli.sh” script to get the value entered prior to deleting the StatefulSet, you will find that the ensemble still serves the data.</p><pre><code>$  kubectl exec zk-2 zkCli.sh get /hello

...


WATCHER::


WatchedEvent state:SyncConnected type:None path:null

world

...
</code></pre><p>StatefulSet ensures that, even if all Pods in the StatefulSet are destroyed, when they are rescheduled, the ZooKeeper ensemble can elect a new leader and continue to serve requests.</p><p><strong>Tolerating Node Failures</strong></p><p>ZooKeeper replicates its state machine to different servers in the ensemble for the explicit purpose of tolerating node failure. By default, the Kubernetes Scheduler could deploy more than one Pod in the zk StatefulSet to the same node. If the zk-0 and zk-1 Pods were deployed on the same node, and that node failed, the ZooKeeper ensemble couldn’t form a quorum to commit writes, and the ZooKeeper service would experience an outage until one of the Pods could be rescheduled.</p><p>You should always provision headroom capacity for critical processes in your cluster, and if you do, in this instance, the Kubernetes Scheduler will reschedule the Pods on another node and the outage will be brief.</p><p>If the SLAs for your service preclude even brief outages due to a single node failure, you should use a <a href=/docs/user-guide/node-selection/>PodAntiAffinity</a> annotation. The manifest used to create the ensemble contains such an annotation, and it tells the Kubernetes Scheduler to not place more than one Pod from the zk StatefulSet on the same node.</p><p><strong>Tolerating Planned Maintenance</strong></p><p>The manifest used to create the ZooKeeper ensemble also creates a <a href=/docs/admin/disruptions/>PodDistruptionBudget</a>, zk-budget. The zk-budget informs Kubernetes about the upper limit of disruptions (unhealthy Pods) that the service can tolerate.</p><pre><code> {

              &quot;podAntiAffinity&quot;: {

                &quot;requiredDuringSchedulingRequiredDuringExecution&quot;: [{

                  &quot;labelSelector&quot;: {

                    &quot;matchExpressions&quot;: [{

                      &quot;key&quot;: &quot;app&quot;,

                      &quot;operator&quot;: &quot;In&quot;,

                      &quot;values&quot;: [&quot;zk-headless&quot;]

                    }]

                  },

                  &quot;topologyKey&quot;: &quot;kubernetes.io/hostname&quot;

                }]

              }

            }

}
</code></pre><pre><code>$ kubectl get poddisruptionbudget zk-budget

NAME        MIN-AVAILABLE   ALLOWED-DISRUPTIONS   AGE

zk-budget   2               1                     2h
</code></pre><p>zk-budget indicates that at least two members of the ensemble must be available at all times for the ensemble to be healthy. If you attempt to drain a node prior taking it offline, and if draining it would terminate a Pod that violates the budget, the drain operation will fail. If you use <a href=/docs/reference/generated/kubectl/kubectl-commands#drain>kubectl drain</a>, in conjunction with PodDisruptionBudgets, to cordon your nodes and to evict all Pods prior to maintenance or decommissioning, you can ensure that the procedure won’t be disruptive to your stateful applications.</p><p><strong>Looking Forward</strong></p><p>As the Kubernetes development looks towards GA, we are looking at a long list of suggestions from users. If you want to dive into our backlog, checkout the <a href=https://github.com/kubernetes/kubernetes/labels/area%2Fstateful-apps>GitHub issues with the stateful label</a>. However, as the resulting API would be hard to comprehend, we don't expect to implement all of these feature requests. Some feature requests, like support for rolling updates, better integration with node upgrades, and using fast local storage, would benefit most types of stateful applications, and we expect to prioritize these. The intention of StatefulSet is to be able to run a large number of applications well, and not to be able to run all applications perfectly. With this in mind, we avoided implementing StatefulSets in a way that relied on hidden mechanisms or inaccessible features. Anyone can write a controller that works similarly to StatefulSets. We call this "making it forkable."</p><p>Over the next year, we expect many popular storage applications to each have their own community-supported, dedicated controllers or "<a href=https://coreos.com/blog/introducing-operators.html>operators</a>". We've already heard of work on custom controllers for etcd, Redis, and ZooKeeper. We expect to write some more ourselves and to support the community in developing others.</p><p>The Operators for <a href=https://coreos.com/blog/introducing-the-etcd-operator.html>etcd</a> and <a href=https://coreos.com/blog/the-prometheus-operator.html>Prometheus</a> from CoreOS, demonstrate an approach to running stateful applications on Kubernetes that provides a level of automation and integration beyond that which is possible with StatefulSet alone. On the other hand, using a generic controller like StatefulSet or Deployment means that a wide range of applications can be managed by understanding a single config object. We think Kubernetes users will appreciate having the choice of these two approaches.</p><p><em>--Kenneth Owens & Eric Tune, Software Engineers, Google</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ba63ed484cd3b615d9113d8d667125ed>Five Days of Kubernetes 1.5</h1><div class="td-byline mb-4"><time datetime=2016-12-19 class=text-muted>Monday, December 19, 2016</time></div><p>With the help of our growing community of 1,000 contributors, we pushed some 5,000 commits to extend support for production workloads and deliver <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads/>Kubernetes 1.5</a>. While many improvements and new features have been added, we selected few to highlight in a series of in-depths posts listed below. </p><p>This progress is our commitment in continuing to make Kubernetes best way to manage your production workloads at scale.</p><table><thead><tr><th></th><th>Five Days of Kubernetes 1.5</th></tr></thead><tbody><tr><td>Day 1</td><td><a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes>Introducing Container Runtime Interface (CRI) in Kubernetes</a></td></tr><tr><td>Day 2</td><td><a href=https://kubernetes.io/blog/2016/12/statefulset-run-scale-stateful-applications-in-kubernetes>StatefulSet: Run and Scale Stateful Applications Easily in Kubernetes</a></td></tr><tr><td>Day 3</td><td><a href=https://kubernetes.io/blog/2016/12/windows-server-support-kubernetes>Windows Server Support Comes to Kubernetes</a></td></tr><tr><td>Day 4</td><td><a href=https://kubernetes.io/blog/2016/12/cluster-federation-in-kubernetes-1-5/>Cluster Federation in Kubernetes 1.5</a></td></tr><tr><td>Day 5</td><td><a href=https://kubernetes.io/blog/2016/12/kubernetes-supports-openapi>Kubernetes supports OpenAPI</a></td></tr></tbody></table><p>Connect</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-494578f429515204637242c396d1a982>Introducing Container Runtime Interface (CRI) in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-12-19 class=text-muted>Monday, December 19, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/12/five-days-of-kubernetes-1-5/>series of in-depth articles</a> on what's new in Kubernetes 1.5</em></p><p>At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. We call this the “Container Runtime”. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving. As part of the effort to make Kubernetes more extensible, we've been working on a new plugin API for container runtimes in Kubernetes, called "CRI".</p><p><strong>What is the CRI and why does Kubernetes need it?</strong></p><p>Each container runtime has it own strengths, and many users have asked for Kubernetes to support more runtimes. In the Kubernetes 1.5 release, we are proud to introduce the <a href=https://github.com/kubernetes/kubernetes/blob/242a97307b34076d5d8f5bbeb154fa4d97c9ef1d/docs/devel/container-runtime-interface.md>Container Runtime Interface</a> (CRI) -- a plugin interface which enables kubelet to use a wide variety of container runtimes, without the need to recompile. CRI consists of a <a href=https://developers.google.com/protocol-buffers/>protocol buffers</a> and <a href=http://www.grpc.io/>gRPC API</a>, and <a href=https://github.com/kubernetes/kubernetes/tree/release-1.5/pkg/kubelet/server/streaming>libraries</a>, with additional specifications and tools under active development. CRI is being released as Alpha in <a href=https://kubernetes.io/blog/2016/12/kubernetes-1-5-supporting-production-workloads>Kubernetes 1.5</a>.</p><p>Supporting interchangeable container runtimes is not a new concept in Kubernetes. In the 1.3 release, we announced the <a href=https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-Kubernetes>rktnetes</a> project to enable <a href=https://github.com/coreos/rkt>rkt container engine</a> as an alternative to the Docker container runtime. However, both Docker and rkt were integrated directly and deeply into the kubelet source code through an internal and volatile interface. Such an integration process requires a deep understanding of Kubelet internals and incurs significant maintenance overhead to the Kubernetes community. These factors form high barriers to entry for nascent container runtimes. By providing a clearly-defined abstraction layer, we eliminate the barriers and allow developers to focus on building their container runtimes. This is a small, yet important step towards truly enabling pluggable container runtimes and building a healthier ecosystem.</p><p><strong>Overview of CRI</strong><br>Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.</p><p><a href=https://cl.ly/3I2p0D1V0T26/Image%202016-12-19%20at%2017.13.16.png><img src=https://cl.ly/3I2p0D1V0T26/Image%202016-12-19%20at%2017.13.16.png alt></a></p><p>The protocol buffers <a href=https://github.com/kubernetes/kubernetes/blob/release-1.5/pkg/kubelet/api/v1alpha1/runtime/api.proto>API</a> includes two gRPC services, ImageService, and RuntimeService. The ImageService provides RPCs to pull an image from a repository, inspect, and remove an image. The RuntimeService contains RPCs to manage the lifecycle of the pods and containers, as well as calls to interact with containers (exec/attach/port-forward). A monolithic container runtime that manages both images and containers (e.g., Docker and rkt) can provide both services simultaneously with a single socket. The sockets can be set in Kubelet by --container-runtime-endpoint and --image-service-endpoint flags.<br><strong>Pod and container lifecycle management</strong></p><pre><code>service RuntimeService {

    // Sandbox operations.

    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}  
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}  
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}  
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}  
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}  

    // Container operations.  
    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}  
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}  
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}  
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}  
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}  
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}

    ...  
}
</code></pre><p>A Pod is composed of a group of application containers in an isolated environment with resource constraints. In CRI, this environment is called PodSandbox. We intentionally leave some room for the container runtimes to interpret the PodSandbox differently based on how they operate internally. For hypervisor-based runtimes, PodSandbox might represent a virtual machine. For others, such as Docker, it might be Linux namespaces. The PodSandbox must respect the pod resources specifications. In the v1alpha1 API, this is achieved by launching all the processes within the pod-level cgroup that kubelet creates and passes to the runtime.</p><p>Before starting a pod, kubelet calls RuntimeService.RunPodSandbox to create the environment. This includes setting up networking for a pod (e.g., allocating an IP). Once the PodSandbox is active, individual containers can be created/started/stopped/removed independently. To delete the pod, kubelet will stop and remove containers before stopping and removing the PodSandbox.</p><p>Kubelet is responsible for managing the lifecycles of the containers through the RPCs, exercising the container lifecycles hooks and liveness/readiness checks, while adhering to the restart policy of the pod.</p><p><strong>Why an imperative container-centric interface?</strong></p><p>Kubernetes has a declarative API with a <em>Pod</em> resource. One possible design we considered was for CRI to reuse the declarative <em>Pod</em> object in its abstraction, giving the container runtime freedom to implement and exercise its own control logic to achieve the desired state. This would have greatly simplified the API and allowed CRI to work with a wider spectrum of runtimes. We discussed this approach early in the design phase and decided against it for several reasons. First, there are many Pod-level features and specific mechanisms (e.g., the crash-loop backoff logic) in kubelet that would be a significant burden for all runtimes to reimplement. Second, and more importantly, the Pod specification was (and is) still evolving rapidly. Many of the new features (e.g., init containers) would not require any changes to the underlying container runtimes, as long as the kubelet manages containers directly. CRI adopts an imperative container-level interface so that runtimes can share these common features for better development velocity. This doesn't mean we're deviating from the "level triggered" philosophy - kubelet is responsible for ensuring that the actual state is driven towards the declared state.</p><p><strong>Exec/attach/port-forward requests</strong></p><pre><code>service RuntimeService {

    ...

    // ExecSync runs a command in a container synchronously.  
    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}  
    // Exec prepares a streaming endpoint to execute a command in the container.  
    rpc Exec(ExecRequest) returns (ExecResponse) {}  
    // Attach prepares a streaming endpoint to attach to a running container.  
    rpc Attach(AttachRequest) returns (AttachResponse) {}  
    // PortForward prepares a streaming endpoint to forward ports from a PodSandbox.  
    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}

    ...  
}
</code></pre><p>Kubernetes provides features (e.g. kubectl exec/attach/port-forward) for users to interact with a pod and the containers in it. Kubelet today supports these features either by invoking the container runtime’s native method calls or by using the tools available on the node (e.g., nsenter and socat). Using tools on the node is not a portable solution because most tools assume the pod is isolated using Linux namespaces. In CRI, we explicitly define these calls in the API to allow runtime-specific implementations.</p><p>Another potential issue with the kubelet implementation today is that kubelet handles the connection of all streaming requests, so it can become a bottleneck for the network traffic on the node. When designing CRI, we incorporated this feedback to allow runtimes to eliminate the middleman. The container runtime can start a separate streaming server upon request (and can potentially account the resource usage to the pod!), and return the location of the server to kubelet. Kubelet then returns this information to the Kubernetes API server, which opens a streaming connection directly to the runtime-provided server and connects it to the client.</p><p>There are many other aspects of CRI that are not covered in this blog post. Please see the list of <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md#design-docs-and-proposals>design docs and proposals</a> for all the details.</p><p><strong>Current status</strong></p><p>Although CRI is still in its early stages, there are already several projects under development to integrate container runtimes using CRI. Below are a few examples:</p><ul><li><a href=https://cri-o.io/>cri-o</a>: OCI conformant runtimes.</li><li><a href=https://github.com/kubernetes-incubator/rktlet>rktlet</a>: the rkt container runtime.</li><li><a href=https://github.com/kubernetes/frakti>frakti</a>: hypervisor-based container runtimes.</li><li><a href=https://github.com/kubernetes/kubernetes/tree/release-1.5/pkg/kubelet/dockershim>docker CRI shim</a>.</li></ul><p>If you are interested in trying these alternative runtimes, you can follow the individual repositories for the latest progress and instructions.</p><p>For developers interested in integrating a new container runtime, please see the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>developer guide</a> for the known limitations and issues of the API. We are actively incorporating feedback from early developers to improve the API. Developers should expect occasional API breaking changes (it is Alpha, after all).</p><p><strong>Try the new CRI-Docker integration</strong></p><p>Kubelet does not yet use CRI by default, but we are actively working on making this happen. The first step is to re-integrate Docker with kubelet using CRI. In the 1.5 release, we extended kubelet to support CRI, and also added a built-in CRI shim for Docker. This allows kubelet to start the gRPC server on Docker’s behalf. To try out the new kubelet-CRI-Docker integration, you simply have to start the Kubernetes API server with --feature-gates=StreamingProxyRedirects=true to enable the new streaming redirect feature, and then start the kubelet with --experimental-cri=true.</p><p>Besides a few <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md#docker-cri-integration-known-issues>missing features</a>, the new integration has consistently passed the main end-to-end tests. We plan to expand the test coverage soon and would like to encourage the community to report any issues to help with the transition.</p><p><strong>CRI with Minikube</strong></p><p>If you want to try out the new integration, but don’t have the time to spin up a new test cluster in the cloud yet, <a href=https://github.com/kubernetes/minikube>minikube</a> is a great tool to quickly spin up a local cluster. Before you start, follow the <a href=https://github.com/kubernetes/minikube>instructions</a> to download and install minikube.</p><ol><li>Check the available Kubernetes versions and pick the latest 1.5.x version available. We will use v1.5.0-beta.1 as an example.</li></ol><pre><code>$ minikube get-k8s-versions
</code></pre><ol start=2><li>Start a minikube cluster with the built-in docker CRI integration.</li></ol><pre><code>$ minikube start --kubernetes-version=v1.5.0-beta.1 --extra-config=kubelet.EnableCRI=true --network-plugin=kubenet --extra-config=kubelet.PodCIDR=10.180.1.0/24 --iso-url=http://storage.googleapis.com/minikube/iso/buildroot/minikube-v0.0.6.iso
</code></pre><p>--extra-config=kubelet.EnableCRI=true` turns on the CRI implementation in kubelet. --network-plugin=kubenet and --extra-config=kubelet.PodCIDR=10.180.1.0/24 sets the network plugin to kubenet and ensures a PodCIDR is assigned to the node. Alternatively, you can use the cni plugin which does not rely on the PodCIDR. --iso-url sets an iso image for minikube to launch the node with. The image used in the example</p><ol start=3><li>Check the minikube log to check that CRI is enabled.</li></ol><pre><code>$ minikube logs | grep EnableCRI

I1209 01:48:51.150789    3226 localkube.go:116] Setting EnableCRI to true on kubelet.
</code></pre><ol start=4><li>Create a pod and check its status. You should see a “SandboxReceived” event as a proof that Kubelet is using CRI!</li></ol><pre><code>$ kubectl run foo --image=gcr.io/google\_containers/pause-amd64:3.0

deployment &quot;foo&quot; created

$ kubectl describe pod foo

...

... From                Type   Reason          Message  
... -----------------   -----  --------------- -----------------------------

...{default-scheduler } Normal Scheduled       Successfully assigned foo-141968229-v1op9 to minikube  
...{kubelet minikube}   Normal SandboxReceived Pod sandbox received, it will be created.

...
</code></pre><p>_Note that kubectl attach/exec/port-forward does not work with CRI enabled in minikube yet, but this <a href=https://github.com/kubernetes/minikube/issues/896>will be addressed in the newer version of minikube</a>. _</p><p>Community</p><p>CRI is being actively developed and maintained by the Kubernetes <a href=https://github.com/kubernetes/community/blob/master/README.md#special-interest-groups-sig>SIG-Node</a> community. We’d love to hear feedback from you. To join the community:</p><ul><li>Post issues or feature requests on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Join the #sig-node channel on <a href=https://kubernetes.slack.com/>Slack</a></li><li>Subscribe to the <a href=mailto:kubernetes-sig-node@googlegroups.com>SIG-Node mailing list</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul><p><em>--Yu-Ju Hong, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a92c18fbb36d093063ff234fa1be4c8a>Kubernetes 1.5: Supporting Production Workloads</h1><div class="td-byline mb-4"><time datetime=2016-12-13 class=text-muted>Tuesday, December 13, 2016</time></div><p>Today we’re announcing the release of Kubernetes 1.5. This release follows close on the heels of KubeCon/CloundNativeCon, where users gathered to share how they’re running their applications on Kubernetes. Many of you expressed interest in running stateful applications in containers with the eventual goal of running all applications on Kubernetes. If you have been waiting to try running a distributed database on Kubernetes, or for ways to guarantee application disruption SLOs for stateful and stateless apps, this release has solutions for you. </p><p>StatefulSet and PodDisruptionBudget are moving to beta. Together these features provide an easier way to deploy and scale stateful applications, and make it possible to perform cluster operations like node upgrade without violating application disruption SLOs.</p><p>You will also find usability improvements throughout the release, starting with the kubectl command line interface you use so often. For those who have found it hard to set up a multi-cluster federation, a new command line tool called ‘kubefed’ is here to help. And a much requested multi-zone Highly Available (HA) master setup script has been added to kube-up. </p><p>Did you know the Kubernetes community is working to support Windows containers? If you have .NET developers, take a look at the work on Windows containers in this release. This work is in early stage alpha and we would love your feedback.</p><p>Lastly, for those interested in the internals of Kubernetes, 1.5 introduces Container Runtime Interface or CRI, which provides an internal API abstracting the container runtime from kubelet. This decoupling of the runtime gives users choice in selecting a runtime that best suits their needs. This release also introduces containerized node conformance tests that verify that the node software meets the minimum requirements to join a Kubernetes cluster.</p><p><strong>What’s New</strong></p><p><a href=/docs/concepts/abstractions/controllers/statefulsets/><strong>StatefulSet</strong></a> beta (formerly known as PetSet) allows workloads that require persistent identity or per-instance storage to be <a href=/docs/tutorials/stateful-application/basic-stateful-set/#creating-a-statefulset>created</a>, <a href=/docs/tutorials/stateful-application/basic-stateful-set/#scaling-a-statefulset>scaled</a>, <a href=/docs/tutorials/stateful-application/basic-stateful-set/#deleting-statefulsets>deleted</a> and <a href=/docs/tasks/manage-stateful-set/debugging-a-statefulset/>repaired</a> on Kubernetes. You can use StatefulSets to ease the deployment of any stateful service, and tutorial examples are available in the repository. In order to ensure that there are never two pods with the same identity, the Kubernetes node controller no longer force deletes pods on unresponsive nodes. Instead, it waits until the old pod is confirmed dead in one of several ways: automatically when the kubelet reports back and confirms the old pod is terminated; automatically when a cluster-admin deletes the node; or when a database admin confirms it is safe to proceed by force deleting the old pod. Users are now warned if they try to force delete pods via the CLI. For users who will be migrating from PetSets to StatefulSets, please follow the upgrade <a href=/docs/tasks/manage-stateful-set/upgrade-pet-set-to-stateful-set>guide</a>.</p><p><strong><a href=/docs/admin/disruptions/>PodDisruptionBudget</a></strong> beta is an API object that specifies the minimum number or minimum percentage of replicas of a collection of pods that must be up at any time. With PodDisruptionBudget, an application deployer can ensure that cluster operations that voluntarily evict pods will never take down so many simultaneously as to cause data loss, an outage, or an unacceptable service degradation. In Kubernetes 1.5 the “kubectl drain” command supports PodDisruptionBudget, allowing safe draining of nodes for maintenance activities, and it will soon also be used by node upgrade and cluster autoscaler (when removing nodes). This can be useful for a quorum based application to ensure the number of replicas running is never below the number needed for quorum, or for a web front end to ensure the number of replicas serving load never falls below a certain percentage.</p><p><strong><a href=/docs/admin/federation/kubefed.md>Kubefed</a></strong> alpha is a new command line tool to help you manage federated clusters, making it easy to deploy new federation control planes and add or remove clusters from existing federations. Also new in cluster federation is the addition of <a href=/docs/user-guide/federation/configmap.md>ConfigMaps</a> alpha and <a href=/docs/user-guide/federation/daemonsets.md>DaemonSets</a> alpha and <a href=/docs/user-guide/federation/deployment.md>deployments</a> alpha to the <a href=/docs/user-guide/federation/index.md>federation API</a> allowing you to create, update and delete these objects across multiple clusters from a single endpoint.</p><p><strong><a href=/docs/admin/ha-master-gce.md>HA Masters</a></strong> alpha provides the ability to create and delete clusters with highly available (replicated) masters on GCE using the kube-up/kube-down scripts. Allows setup of zone distributed HA masters, with at least one etcd replica per zone, at least one API server per zone, and master-elected components like scheduler and controller-manager distributed across zones.</p><p><strong><a href=/docs/getting-started-guides/windows/>Windows server containers</a></strong> alpha provides initial support for Windows Server 2016 nodes and scheduling Windows Server Containers. </p><p><strong><a href=https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/devel/container-runtime-interface.md>Container Runtime Interface</a></strong> (CRI) alpha introduces the v1 CRI API to allow pluggable container runtimes; an experimental docker-CRI integration is ready for testing and feedback.</p><p><a href=/docs/admin/node-conformance.md><strong>Node conformance test</strong></a> beta is a containerized test framework that provides a system verification and functionality test for nodes. The test validates whether the node meets the minimum requirements for Kubernetes; a node that passes the tests is qualified to join a Kubernetes. Node conformance test is available at: gcr.io/google_containers/node-test:0.2 for users to verify node setup.</p><p>These are just some of the highlights in our last release for the year. For a complete list please visit the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#v151>release notes</a>. </p><p><strong>Availability</strong><br>Kubernetes 1.5 is available for download <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.5.1>here</a> on GitHub and via <a href=http://get.k8s.io/>get.k8s.io</a>. To get started with Kubernetes, try one of the <a href=/docs/tutorials/kubernetes-basics/>new interactive tutorials</a>. Don’t forget to take 1.5 for a spin before the holidays! </p><p><strong>User Adoption</strong><br>It’s been a year-and-a-half since GA, and the rate of <a href=http://kubernetes.io/case-studies/>Kubernetes user adoption</a> continues to surpass estimates. Organizations running production workloads on Kubernetes include the world's largest companies, young startups, and everything in between. Since Kubernetes is open and runs anywhere, we’ve seen adoption on a diverse set of platforms; Pokémon Go (Google Cloud), Ticketmaster (AWS), SAP (OpenStack), Box (bare-metal), and hybrid environments that mix-and-match the above. Here are a few user highlights:</p><ul><li><strong><a href=https://kubernetes.io/blog/2016/10/kubernetes-and-openstack-at-yahoo-japan>Yahoo! JAPAN</a></strong> -- built an automated tool chain making it easy to go from code push to deployment, all while running OpenStack on Kubernetes. </li><li><strong><a href=http://www.techbetter.com/walmart-will-manage-200-distribution-centers-oneops-jenkins-nexus-kubernetes/>Walmart</a></strong> -- will use Kubernetes with OneOps to manage its incredible distribution centers, helping its team with speed of delivery, systems uptime and asset utilization.  </li><li><strong><a href="https://www.youtube.com/watch?v=YkOY7DgXKyw">Monzo</a></strong> -- a European startup building a mobile first bank, is using Kubernetes to power its core platform that can handle extreme performance and consistency requirements.</li></ul><p><strong>Kubernetes Ecosystem</strong><br>The Kubernetes ecosystem is growing rapidly, including Microsoft's support for Kubernetes in Azure Container Service, VMware's integration of Kubernetes in its Photon Platform, and Canonical’s commercial support for Kubernetes. This is in addition to the thirty plus <a href=https://kubernetes.io/blog/2016/10/kubernetes-service-technology-partners-program>Technology & Service Partners</a> that already provide commercial services for Kubernetes users. </p><p>The CNCF recently announced the <a href=https://kubernetes.io/blog/2016/11/kubernetes-certification-training-and-managed-service-provider-program>Kubernetes Managed Service Provider</a> (KMSP) program, a pre-qualified tier of service providers with experience helping enterprises successfully adopt Kubernetes. Furthering the knowledge and awareness of Kubernetes, The Linux Foundation, in partnership with CNCF, will develop and operate the Kubernetes training and certification program -- the first course designed is <a href=https://training.linuxfoundation.org/linux-courses/system-administration-training/kubernetes-fundamentals>Kubernetes Fundamentals</a>.</p><p><strong>Community Velocity</strong><br>In the past three months we’ve seen more than a hundred new contributors join the project with some 5,000 commits pushed, reaching new milestones by bringing the total for the core project to 1,000+ contributors and 40,000+ commits. This incredible momentum is only possible by having an open design, being open to new ideas, and empowering an open community to be welcoming to new and senior contributors alike. A big thanks goes out to the release team for 1.5 -- Saad Ali of Google, Davanum Srinivas of Mirantis, and Caleb Miles of CoreOS for their work bringing the 1.5 release to light.</p><p>Offline, the community can be found at one of the many Kubernetes related <a href=https://www.meetup.com/topics/kubernetes/>meetups</a> around the world. The strength and scale of the community was visible in the crowded halls of CloudNativeCon/KubeCon Seattle (the recorded user talks are <a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2PqgIGU1Qmi8nY7dqn9PCr4">here</a>). The next C<a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/>loudNativeCon + KubeCon is in Berlin</a> March 29-30, 2017, be sure to get your ticket and <a href=https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSc0lPQhSuDusPXLKJDTcWrH3DbOuoQlTD0lB4IGUz6NAmcf2g/viewform>submit your talk</a> before the CFP deadline of Dec 16th.</p><p>Ready to start contributing? Share your voice at our weekly <a href=https://kubernetes.io/community/>community meeting</a>. </p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li></ul><p>Thank you for your contributions and support!</p><p><em>-- Aparna Sinha, Senior Product Manager, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-7b694c77d4fe4176f6ec0f57581364dc>From Network Policies to Security Policies</h1><div class="td-byline mb-4"><time datetime=2016-12-08 class=text-muted>Thursday, December 08, 2016</time></div><p><em>Editor's note: Today’s post is by Bernard Van De Walle, Kubernetes Lead Engineer, at Aporeto, showing how they took a new approach to the Kubernetes network policy enforcement.</em></p><p><strong>Kubernetes Network Policies </strong></p><p>Kubernetes supports a <a href=/docs/user-guide/networkpolicies/>new API for network policies</a> that provides a sophisticated model for isolating applications and reducing their attack surface. This feature, which came out of the <a href=https://github.com/kubernetes/community/wiki/SIG-Network>SIG-Network group</a>, makes it very easy and elegant to define network policies by using the built-in labels and selectors Kubernetes constructs.</p><p>Kubernetes has left it up to third parties to implement these network policies and does not provide a default implementation.</p><p>We want to introduce a new way to think about “Security” and “Network Policies”. We want to show that security and reachability are two different problems, and that security policies defined using endpoints (pods labels for example) do not specifically need to be implemented using network primitives.</p><p>Most of us at <a href=https://www.aporeto.com/>Aporeto</a> come from a Network/SDN background, and we knew how to implement those policies by using traditional networking and firewalling: Translating the pods identity and policy definitions to network constraints, such as IP addresses, subnets, and so forth.</p><p>However, we also knew from past experiences that using an external control plane also introduces a whole new set of challenges: This distribution of ACLs requires very tight synchronization between Kubernetes workers; and every time a new pod is instantiated, ACLs need to be updated on all other pods that have some policy related to the new pod. Very tight synchronization is fundamentally a quadratic state problem and, while shared state mechanisms can work at a smaller scale, they often have convergence, security, and eventual consistency issues in large scale clusters. </p><p><strong>From Network Policies to Security Policies</strong></p><p>At Aporeto, we took a different approach to the network policy enforcement, by actually decoupling the network from the policy. We open sourced our solution as <a href=https://github.com/aporeto-inc/trireme>Trireme</a>, which translates the network policy to an authorization policy, and it implements a transparent authentication and authorization function for any communication between pods. Instead of using IP addresses to identify pods, it defines a cryptographically signed identity for each pod as the set of its associated labels. Instead of using ACLs or packet filters to enforce policy, it uses an authorization function where a container can only receive traffic from containers with an identity that matches the policy requirements. </p><p>The authentication and authorization function in Trireme is overlaid on the TCP negotiation sequence. Identity (i.e. set of labels) is captured as a JSON Web Token (JWT), signed by local keys, and exchanged during the Syn/SynAck negotiation. The receiving worker validates that the JWTs are signed by a trusted authority (authentication step) and validates against a cached copy of the policy that the connection can be accepted. Once the connection is accepted, the rest of traffic flows through the Linux kernel and all of the protections that it can potentially offer (including conntrack capabilities if needed). The current implementation uses a simple user space process that captures the initial negotiation packets and attaches the authorization information as payload. The JWTs include nonces that are validated during the Ack packet and can defend against man-in-the-middle or replay attacks.</p><p><img src=https://lh3.googleusercontent.com/PhkJ4eoRc50gm6oSTZbw138l3jzVKjjQrn2mNHjys9Cu7RG-q2X-f5PX07ZY6xjbIQT0ud8oMSX6yNwjDpmDq3a3lYWcc_gBYJBjvBLP8PIHZaTW54fJppDze9pYxOmZY-JNqQ1Y alt></p><p>The Trireme implementation talks directly to the Kubernetes master without an external controller and receives notifications on policy updates and pod instantiations so that it can maintain a local cache of the policy and update the authorization rules as needed. There is no requirement for any shared state between Trireme components that needs to be synchronized. Trireme can be deployed either as a standalone process in every worker or by using <a href=/docs/admin/daemons/>Daemon Sets</a>. In the latter case, Kubernetes takes ownership of the lifecycle of the Trireme pods. </p><p>Trireme's simplicity is derived from the separation of security policy from network transport. Policy enforcement is linked directly to the labels present on the connection, irrespective of the networking scheme used to make the pods communicate. This identity linkage enables tremendous flexibility to operators to use any networking scheme they like without tying security policy enforcement to network implementation details. Also, the implementation of security policy across the federated clusters becomes simple and viable.</p><p><strong>Kubernetes and Trireme deployment</strong></p><p>Kubernetes is unique in its ability to scale and provide an extensible security support for the deployment of containers and microservices. Trireme provides a simple, secure, and scalable mechanism for enforcing these policies. </p><p>You can deploy and try Trireme on top of Kubernetes by using a provided Daemon Set. You'll need to modify some of the YAML parameters based on your cluster architecture. All the steps are described in detail in the <a href=https://github.com/aporeto-inc/trireme-kubernetes/tree/master/deployment>deployment GitHub folder</a>. The same folder contains an example 3-tier policy that you can use to test the traffic pattern.</p><p>To learn more, download the code, and engage with the project, visit:</p><ul><li>Trireme on <a href=https://github.com/aporeto-inc/trireme-kubernetes>GitHub</a></li><li>Trireme for Kubernetes by Aporeto on <a href=https://github.com/aporeto-inc/trireme-kubernetes>GitHub</a></li></ul><p><em>--Bernard Van De Walle, Kubernetes lead engineer, <a href=https://www.aporeto.com/>Aporeto</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-293519915b8d1da34a529c467aa7befc>Kompose: a tool to go from Docker-compose to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-11-22 class=text-muted>Tuesday, November 22, 2016</time></div><p><em>Editor's note: Today’s post is by Sebastien Goasguen, Founder of Skippbox, showing a new tool to move from ‘docker-compose’ to Kubernetes.</em></p><p>At <a href=http://www.skippbox.com/>Skippbox</a>, we developed <strong>kompose</strong> a tool to automatically transform your Docker Compose application into Kubernetes manifests. Allowing you to start a Compose application on a Kubernetes cluster with a single kompose up command. We’re extremely happy to have donated kompose to the <a href=https://github.com/kubernetes-incubator>Kubernetes Incubator</a>. So here’s a quick introduction about it and some motivating factors that got us to develop it.</p><p>Docker is terrific for developers. It allows everyone to get started quickly with an application that has been packaged in a Docker image and is available on a Docker registry. To build a multi-container application, Docker has developed Docker-compose (aka Compose). Compose takes in a yaml based manifest of your multi-container application and starts all the required containers with a single command docker-compose up. However Compose only works locally or with a Docker Swarm cluster.</p><p>But what if you wanted to use something else than Swarm? Like Kubernetes of course.</p><p>The Compose format is not a standard for defining distributed applications. Hence you are left re-writing your application manifests in your container orchestrator of choice.</p><p>We see kompose as a terrific way to expose Kubernetes principles to Docker users as well as to easily migrate from Docker Swarm to Kubernetes to operate your applications in production.</p><p>Over the summer, Kompose has found a new gear with help from Tomas Kral and Suraj Deshmukh from Red Hat, and Janet Kuo from Google. Together with our own lead kompose developer Nguyen An-Tu they are making kompose even more exciting. We proposed Kompose to the Kubernetes Incubator within the SIG-apps and we received approval from the general Kubernetes community; you can now find kompose in the <a href=https://github.com/kubernetes-incubator/kompose>Kubernetes Incubator</a>.</p><p>Kompose now supports Docker-compose v2 format, persistent volume claims have been added recently, as well as multiple container per pods. It can also be used to target OpenShift deployments, by specifying a different provider than the default Kubernetes. Kompose is also now available in Fedora packages and we look forward to see it in CentOS distributions in the coming weeks.</p><p>kompose is a single Golang binary that you build or install from the <a href=https://github.com/kubernetes-incubator/kompose>release on GitHub</a>. Let’s skip the build instructions and dive straight into an example.</p><p>Let's take it for a spin!</p><p><strong>Guestbook application with Docker</strong></p><p>The Guestbook application has become the canonical example for Kubernetes. In Docker-compose format, the <strong>guestbook</strong> can be started with this minimal file:</p><pre><code>version: &quot;2&quot;



services:

  redis-master:

    image: gcr.io/google\_containers/redis:e2e

    ports:

      - &quot;6379&quot;

  redis-slave:

    image: gcr.io/google\_samples/gb-redisslave:v1

    ports:

      - &quot;6379&quot;

    environment:

      - GET\_HOSTS\_FROM=dns

  frontend:

    image: gcr.io/google-samples/gb-frontend:v4

    ports:

      - &quot;80:80&quot;

    environment:

      - GET\_HOSTS\_FROM=dns
</code></pre><p>It consists of three services. A redis-master node, a set of redis-slave that can be scaled and find the redis-master via its DNS name. And a PHP frontend that exposes itself on port 80. The resulting application allows you to leave short messages which are stored in the redis cluster.</p><p>To get it started with docker-compose on a vanilla Docker host do:</p><pre><code>$ docker-compose -f docker-guestbook.yml up -d

Creating network &quot;examples\_default&quot; with the default driver

Creating examples\_redis-slave\_1

Creating examples\_frontend\_1

Creating examples\_redis-master\_1
</code></pre><p>So far so good, this is plain Docker usage. Now let’s see how to get this on Kubernetes without having to re-write anything.</p><p><strong>Guestbook with 'kompose'</strong></p><p>Kompose currently has three main commands up, down and convert. Here for simplicity we will show a single usage to bring up the Guestbook application.</p><p>Similarly to docker-compose, we can use the kompose up command pointing to the Docker-compose file representing the Guestbook application. Like so:</p><pre><code>$ kompose -f ./examples/docker-guestbook.yml up

We are going to create Kubernetes deployment and service for your dockerized application.

If you need more kind of controllers, use 'kompose convert' and 'kubectl create -f' instead.



INFO[0000] Successfully created service: redis-master

INFO[0000] Successfully created service: redis-slave

INFO[0000] Successfully created service: frontend

INFO[0000] Successfully created deployment: redis-master

INFO[0000] Successfully created deployment: redis-slave

INFO[0000] Successfully created deployment: frontend



Application has been deployed to Kubernetes. You can run 'kubectl get deployment,svc' for details.
</code></pre><p>kompose automatically converted the Docker-compose file into Kubernetes objects. By default, it created one deployment and one service per <strong>compose</strong> services. In addition it automatically detected your current Kubernetes endpoint and created the resources onto it. A set of flags can be used to generate Replication Controllers, Replica Sets or Daemon Sets instead of Deployments.</p><p>And that's it! Nothing else to do, the conversion happened automatically.<br>Now, if you already now Kubernetes a bit, you’re familiar with the client kubectl and you can check what was created on your cluster.</p><pre><code>$ kubectl get pods,svc,deployments

NAME                             READY        STATUS        RESTARTS     AGE

frontend-3780173733-0ayyx        1/1          Running       0            1m

redis-master-3028862641-8miqn    1/1          Running       0            1m

redis-slave-3788432149-t3ejp     1/1          Running       0            1m

NAME                             CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE

frontend                         10.0.0.34    \&lt;none\&gt;        80/TCP       1m

redis-master                     10.0.0.219   \&lt;none\&gt;        6379/TCP     1m

redis-slave                      10.0.0.84    \&lt;none\&gt;        6379/TCP     1m

NAME                             DESIRED      CURRENT       UP-TO-DATE



AVAILABLE   AGE

frontend                         1            1             1            1           1m

redis-master                     1            1             1            1           1m

redis-slave                      1            1             1            1           1m
</code></pre><p>Indeed you see the three services, the three deployments and the resulting three pods. To access the application quickly, access the <em>frontend</em> service locally and enjoy the Guestbook application, but this time started from a Docker-compose file.</p><p><img src=https://lh6.googleusercontent.com/2vTmKcVs-4nl6eYCwJcqCDEaSQ1uUtEmZ2ND0HMO-h8c_5CfU1OwJOuqOc6Eb_nymqdyvLbQK114xRp5U_hmeRHTyn1W_C7gJ6vf3E37CLKrx172XQWVkyko55Q3TfotX76tbMOZ alt=kompose.png></p><p>Hopefully this gave you a quick tour of kompose and got you excited. They are more exciting features, like creating different type of resources, creating Helm charts and even using the experimental Docker bundle format as input. Check Lachlan Evenson’s blog on <a href=https://deis.com/blog/2016/push-docker-dab-kubernetes-cluster/>using a Docker bundle with Kubernetes</a>. For an overall demo, see our talk from <a href="https://www.youtube.com/watch?v=zqUfPPNVjI8&index=42&list=PLj6h78yzYM2PqgIGU1Qmi8nY7dqn9PCr4">KubeCon</a></p><p>Head over to the <a href=https://github.com/kubernetes-incubator/kompose>Kubernetes Incubator</a> and check out kompose, it will help you move easily from your Docker compose applications to Kubernetes clusters in production.</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2458771745f80457c8fc3f1e6452617b>Kubernetes Containers Logging and Monitoring with Sematext</h1><div class="td-byline mb-4"><time datetime=2016-11-18 class=text-muted>Friday, November 18, 2016</time></div><p><em>Editor's note: Today’s post is by Stefan Thies, Developer Evangelist, at Sematext, showing key Kubernetes metrics and log elements to help you troubleshoot and tune Docker and Kubernetes.</em></p><p>Managing microservices in containers is typically done with Cluster Managers and Orchestration tools. Each container platform has a slightly different set of options to deploy containers or schedule tasks on each cluster node. Because we do <a href=http://sematext.com/kubernetes>container monitoring and logging</a> at Sematext, part of our job is to share our knowledge of these tools, especially as it pertains to container observability and devops. Today we’ll show a tutorial for Container Monitoring and Log Collection on Kubernetes.</p><p><strong>Dynamic Deployments Require Dynamic Monitoring</strong></p><p>The high level of automation for the container and microservice lifecycle makes the monitoring of Kubernetes more challenging than in more traditional, more static deployments. Any static setup to monitor specific application containers would not work because Kubernetes makes its own decisions according to the defined deployment rules. It is not only the deployed microservices that need to be monitored. It is equally important to watch metrics and logs for Kubernetes core services themselves, such as Kubernetes Master running etcd, controller-manager, scheduler and apiserver and Kubernetes Workers (fka minions) running kubelet and proxy service. Having a centralized place to keep an eye on all these services, their metrics and logs helps one spot problems in the cluster infrastructure. Kubernetes core services could be installed on bare metal, in virtual machines or as containers using Docker. Deploying Kubernetes core services in containers could be helpful with deployment and monitoring operations - tools for container monitoring would cover both core services and application containers. So how does one monitor such a complex and dynamic environment?</p><p><strong>Agent for Kubernetes Metrics and Logs</strong></p><p>There are a number of <a href=https://sematext.com/blog/2016/07/19/open-source-docker-monitoring-logging/>open source docker monitoring and logging projects</a> one can cobble together to build a monitoring and log collection system (or systems). The advantage is that the code is all free. The downside is that this takes times - both initially when setting it up and later when maintaining. That’s why we built <a href=http://sematext.com/docker>Sematext Docker Agent</a> - a modern, Docker-aware metrics, events, and log collection agent. It runs as a tiny container on every Docker host and collects logs, metrics and events for all cluster nodes and all containers. It discovers all containers (one pod might contain multiple containers) including containers for Kubernetes core services, if core services are deployed in Docker containers. Let’s see how to deploy this agent.</p><p>**Deploying Agent to all Kubernetes Nodes **</p><p>Kubernetes provides <a href=http://kubernetes.io/v1.1/docs/admin/daemons.html>DaemonSets</a>, which ensure pods are added to nodes as nodes are added to the cluster. We can use this to easily deploy Sematext Agent to each cluster node!<br>Configure Sematext Docker Agent for Kubernetes<br>Let’s assume you’ve created an SPM app for your Kubernetes metrics and events, and a Logsene app for your Kubernetes logs, each of which comes with its own token. The Sematext Docker Agent <a href=https://github.com/sematext/sematext-agent-docker>README</a> lists all configurations (e.g. filter for specific pods/images/containers), but we’ll keep it simple here.</p><ul><li>Grab the latest sematext-agent-daemonset.yml (raw plain-text) template (also shown below)</li><li>Save it somewhere on disk</li><li>Replace the SPM_TOKEN and LOGSENE_TOKEN placeholders with your SPM and Logsene App tokens</li></ul><pre><code>apiVersion: extensions/v1beta1  
kind: DaemonSet  
metadata:  
  name: sematext-agent  
spec:  
  template:  
    metadata:  
      labels:  
        app: sematext-agent  
    spec:  
      selector: {}  
      dnsPolicy: &quot;ClusterFirst&quot;  
      restartPolicy: &quot;Always&quot;  
      containers:  
      - name: sematext-agent  
        image: sematext/sematext-agent-docker:latest  
        imagePullPolicy: &quot;Always&quot;  
        env:  
        - name: SPM\_TOKEN  
          value: &quot;REPLACE THIS WITH YOUR SPM TOKEN&quot;  
        - name: LOGSENE\_TOKEN  
          value: &quot;REPLACE THIS WITH YOUR LOGSENE TOKEN&quot;  
        - name: KUBERNETES  
          value: &quot;1&quot;  
        volumeMounts:  
          - mountPath: /var/run/docker.sock  
            name: docker-sock  
          - mountPath: /etc/localtime  
            name: localtime  
      volumes:  
        - name: docker-sock  
          hostPath:  
            path: /var/run/docker.sock  
        - name: localtime  
          hostPath:  
            path: /etc/localtime
</code></pre><p><strong>Run Agent as DaemonSet</strong></p><p>Activate Sematext Agent Docker with <em>kubectl</em>:</p><pre><code>\&gt; kubectl create -f sematext-agent-daemonset.yml

daemonset &quot;sematext-agent-daemonset&quot; created
</code></pre><p>Now let’s check if the agent got deployed to all nodes:</p><pre><code>\&gt; kubectl get pods

NAME                   READY     STATUS              RESTARTS   AGE

sematext-agent-nh4ez   0/1       ContainerCreating   0          6s

sematext-agent-s47vz   0/1       ImageNotReady       0          6s
</code></pre><p>The status “ImageNotReady” or “ContainerCreating” might be visible for a short time because Kubernetes must download the image for sematext/sematext-agent-docker first. The setting imagePullPolicy: "Always" specified in sematext-agent-daemonset.yml makes sure that Sematext Agent gets updated automatically using the image from Docker-Hub.</p><p>If we check again we’ll see Sematext Docker Agent got deployed to (all) cluster nodes:</p><pre><code>\&gt; kubectl get pods -l sematext-agent

NAME                   READY     STATUS    RESTARTS   AGE

sematext-agent-nh4ez   1/1       Running   0          8s

sematext-agent-s47vz   1/1       Running   0          8s
</code></pre><p>Less than a minute after the deployment you should see your Kubernetes metrics and logs! Below are screenshots of various out of the box reports and explanations of various metrics’ meanings.</p><p><strong>Interpretation of Kubernetes Metrics</strong></p><p>The metrics from all Kubernetes nodes are collected in a single SPM App, which aggregates metrics on several levels:</p><ul><li>Cluster - metrics aggregated over all nodes displayed in SPM overview</li><li>Host / node level - metrics aggregated per node</li><li>Docker Image level - metrics aggregated by image name, e.g. all nginx webserver containers</li><li>Docker Container level - metrics aggregated for a single container</li></ul><p>| <img src=https://lh3.googleusercontent.com/THk0zW4Q2YUxPF7pcdcg8WVbut4_BZPFsHuqtBet3AnijJ84w8TYGmNQ5F_CCmOz3W7_DWuacFOZWtJQDGR7I_jRJIf6LIxT8uxuLr4DSPbFC2BOUHgGncgXqIaBGo-L-zrQnDVa alt> |
| Host and Container Metrics from the Kubernetes Cluster |</p><p>Each detailed chart has filter options for Node, Docker Image, and Docker Container. As Kubernetes uses the pod name in the name of the Docker Containers a search by pod name in the Docker Container filter makes it easy to select all containers for a specific pod.</p><p>Let’s have a look at a few Kubernetes (and Docker) key metrics provided by SPM.</p><p>Host Metrics such as CPU, Memory and Disk space usage. Docker images and containers consume more disk space than regular processes installed on a host. For example, an application image might include a Linux operating system and might have a size of 150-700 MB depending on the size of the base image and installed tools in the container. Data containers consume disk space on the host as well. In our experience watching the disk space and using cleanup tools is essential for continuous operations of Docker hosts.</p><p><img src=https://lh5.googleusercontent.com/CJ7BYLNV0dx6CSWpmFSFgDteCjzQYcVOEz5W5gUOa6rK_H1Z6ozImfRJLIWH3X5YCOOSH-EfFuMo4Tdj0EaC7XTZ0bpmCmIsw7hWrB_1ctxkdI7JC5dhBA3umCmr1QG0SqovIDa6 alt></p><p>Container count - represents the number of running containers per host</p><p>| <img src=https://lh5.googleusercontent.com/FUG46hzUj5FJSJgNLu4t6HIIa_QHcLXCDTgqHFoT711bO8M5BRd2w8hmzAk1ZQ0_iz7JkeDudoHNt50v_CaPWcanMOjSymiscMQZqBSudTZ4rrVDFWqkqtRNjOj9zrscQsrJ04Px alt> |
| Container Counters per Kubernetes Node over time |</p><p>Container Memory and Memory Fail Counters. These metrics are important to watch and very important to tune applications. Memory limits should fit the footprint of the deployed pod (application) to avoid situations where Kubernetes uses default limits (e.g. defined for a namespace), which could lead to OOM kills of containers. Memory fail counters reflect the number of failed memory allocations in a container, and in case of OOM kills a Docker Event is triggered. This event is then displayed in SPM because <a href=https://github.com/sematext/sematext-agent-docker>Sematext Docker Agents</a> collects all Docker Events. The best practice is to tune memory setting in a few iterations:</p><ul><li>Monitor memory usage of the application container</li><li>Set memory limits according to the observations</li><li>Continue monitoring of memory, memory fail counters, and Out-Of-Memory events. If OOM events happen, the container memory limits may need to be increased, or debugging is required to find the reason for the high memory consumptions.</li></ul><p>| <img src=https://lh6.googleusercontent.com/Qq1_FhJRC72H0fvc71Oy_RqxbmBe8IZ4L4JTtADxBfLAjopRv2tJW5Fvc8DstD6iOj9JKfNt8U2gWAxzedx9tdnHuld-k1agDMAXDyWM-AuLOs7IDi-KNxEj_p-Kwef12SjeAiVc alt> |
| Container memory usage, limits and fail counters |</p><p>Container CPU usage and throttled CPU time. The CPU usage can be limited by CPU shares - unlike memory, CPU usage it is not a hard limit. Containers might use more CPU as long the resource is available, but in situations where other containers need the CPU limits apply and the CPU gets throttled to the limit.</p><p><img src=https://lh5.googleusercontent.com/iSMZcZROnz6jovMg9XVlHSYFSiOgpgbrcJ0dVK7aXRaXq0psyAHE_Y4mN3aD0k2yRjH-Lgr-X3prNtBexFNmaNdWNXFd0MNnDSwjo8hbgNXydgRWjaT1X-_xbD6f_U92z9VMf4C7 alt></p><p>There are more <a href=https://sematext.com/blog/2016/06/28/top-docker-metrics-to-watch/>docker metrics to watch</a>, like disk I/O throughput, network throughput and network errors for containers, but let’s continue by looking at Kubernetes Logs next.</p><p><strong>Understand Kubernetes Logs</strong></p><p>Kubernetes containers’ logs are not much different from Docker container logs. However, Kubernetes users need to view logs for the deployed pods. That’s why it is very useful to have Kubernetes-specific information available for log search, such as:</p><ul><li>Kubernetes name space</li><li>Kubernetes pod name</li><li>Kubernetes container name</li><li>Docker image name</li><li>Kubernetes UID</li></ul><p>Sematext Docker Agent extracts this information from the Docker container names and tags all logs with the information mentioned above. Having these data extracted in individual fields makes it is very easy to watch logs of deployed pods, build reports from logs, quickly narrow down to problematic pods while troubleshooting, and so on! If Kubernetes core components (such as kubelet, proxy, api server) are deployed via Docker the Sematext Docker Agent will collect Kubernetes core components logs as well.</p><p>| <img src=https://lh6.googleusercontent.com/yiOiPMwqkH0FIyxDXfWi_Qs03JCwTag4gH5ZK3ylEuv3zJpymrZCec6YyhOPJwUkVTzAkN4mmL-DRsUVhluhdwgnZwsT7Vu1TDMrhEYpw2tFKc0Fe28O2_aw3kvBf3VZAB-hb5Mf alt> |
| All logs from Kubernetes containers in Logsene |</p><p>There are many other useful features Logsene and Sematext Docker Agent give you out of the box, such as:</p><ul><li><p>Automatic format detection and parsing of logs</p><ul><li>Sematext Docker Agent includes patterns to recognize and parse many log formats</li></ul></li><li><p>Custom pattern definitions for specific images and application types</p></li><li><p><a href=https://sematext.com/blog/2016/04/11/automatic-geo-ip-enrichment-for-docker-logs-2/>Automatic Geo-IP enrichment for container logs</a></p></li><li><p>Filtering logs e.g. to exclude noisy services</p></li><li><p>Masking of sensitive data in specific log fields (phone numbers, payment information, authentication tokens)</p></li><li><p>Alerts and scheduled reports based on logs</p></li><li><p>Analytics for structured logs e.g. in Kibana or Grafana</p></li></ul><p>Most of those topics are described in our <a href=https://sematext.com/blog/2015/08/12/docker-log-management/>Docker Log Management</a> post and are relevant for Kubernetes log management as well. If you want to learn more about <a href=http://blog.sematext.com/2016/01/12/docker-swarm-collecting-metrics-events-logs/>Docker monitoring</a>, read more on our <a href=https://sematext.com/blog/tag/docker,kubernetes>blog</a>.</p><p><em>--Stefan Thies, Developer Evangelist, at Sematext</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f54b9570f4be9ede34f6357dcca19f53>Visualize Kubelet Performance with Node Dashboard</h1><div class="td-byline mb-4"><time datetime=2016-11-17 class=text-muted>Thursday, November 17, 2016</time></div><p>In Kubernetes 1.4, we introduced a new node performance analysis tool, called the <em>node performance dashboard</em>, to visualize and explore the behavior of the Kubelet in much richer details. This new feature will make it easy to understand and improve code performance for Kubelet developers, and lets cluster maintainer set configuration according to provided Service Level Objectives (SLOs).</p><p><strong>Background</strong></p><p>A Kubernetes cluster is made up of both master and worker nodes. The master node manages the cluster’s state, and the worker nodes do the actual work of running and managing pods. To do so, on each worker node, a binary, called <a href=/docs/admin/kubelet/>Kubelet</a>, watches for any changes in pod configuration, and takes corresponding actions to make sure that containers run successfully. High performance of the Kubelet, such as low latency to converge with new pod configuration and efficient housekeeping with low resource usage, is essential for the entire Kubernetes cluster. To measure this performance, Kubernetes uses <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/devel/e2e-tests.md#overview>end-to-end (e2e) tests</a> to continuously monitor benchmark changes of latest builds with new features.</p><p><strong>Kubernetes SLOs are defined by the following benchmarks</strong> :</p><p><strong>* API responsiveness</strong> : 99% of all API calls return in less than 1s.<br><strong>* Pod startup time</strong> : 99% of pods and their containers (with pre-pulled images) start within 5s.</p><p>Prior to 1.4 release, we’ve only measured and defined these at the cluster level, opening up the risk that other factors could influence the results. Beyond these, we also want to have more performance related SLOs such as the maximum number of pods for a specific machine type allowing maximum utilization of your cluster. In order to do the measurement correctly, we want to introduce a set of tests isolated to just a node’s performance. In addition, we aim to collect more fine-grained resource usage and operation tracing data of Kubelet from the new tests.</p><p><strong>Data Collection</strong></p><p>The node specific density and resource usage tests are now added into e2e-node test set since 1.4. The resource usage is measured by a standalone cAdvisor pod for flexible monitoring interval (comparing with Kubelet integrated cAdvisor). The performance data, such as latency and resource usage percentile, are recorded in persistent test result logs. The tests also record time series data such as creation time, running time of pods, as well as real-time resource usage. Tracing data of Kubelet operations are recorded in its log stored together with test results.</p><p><strong>Node Performance Dashboard</strong></p><p>Since Kubernetes 1.4, we are continuously building the newest Kubelet code and running node performance tests. The data is collected by our new performance dashboard available at <a href=http://node-perf-dash.k8s.io/>node-perf-dash.k8s.io</a>. Figure 1 gives a preview of the dashboard. You can start to explore it by selecting a test, either using the drop-down list of short test names (region (a)) or by choosing test options one by one (region (b)). The test details show up in region (c) containing the full test name from Ginkgo (the Go test framework used by Kubernetes). Then select a node type (image and machine) in region (d).</p><p>| <img src=https://lh5.googleusercontent.com/xREqs-NpWw2isELQ3YekYYMXRsY0fTs0t8lBR5xbZDB02mOAfQAnidXo8AF9hOICBUFI20kD6BVvTR0vDS1ErgQ8fVxP530TWUkyZTeV_KziI9uHvZOrHk5E304MeiLfdEPG2fzz alt>|
| Figure 1. Select a test to display in node performance dashboard. |</p><p>The "BUILDS" page exhibits the performance data across different builds (Figure 2). The plots include pod startup latency, pod creation throughput, and CPU/memory usage of Kubelet and runtime (currently Docker). In this way it’s easy to monitor the performance change over time as new features are checked in.</p><p>| <img src=https://lh4.googleusercontent.com/lMNEuppUPvdzuLNPPAUSiuZJ7mB575sLJYsn1NlTaiibLPl8Ocyg3t0hcdKjCZVd1U61plZnK6WHJUtWTvIBqcZkGiEStL6kGHVwTzHKcmIWIVQHbGZl4SkKgQM6ygBTsaQul1nw alt> |
| Figure 2. Performance data across different builds. |</p><p><strong>Compare Different Node Configurations</strong></p><p>It’s always interesting to compare the performance between different configurations, such as comparing startup latency of different machine types, different numbers of pods, or comparing resource usage of hosting different number of pods. The dashboard provides a convenient way to do this. Just click the "Compare it" button the right up corner of test selection menu (region (e) in Figure 1). The selected tests will be added to a comparison list in the "COMPARISON" page, as shown in Figure 3. Data across a series of builds are aggregated to a single value to facilitate comparison and are displayed in bar charts.</p><p>| <img src=https://lh4.googleusercontent.com/B0M-LCr8iVTVRFC5u8Ni08-sXCu7BJAHXXFRTrT_ecPYi4ZHr7ylhkmbUlqwewNvRPmxH63DadNe72AA7jthoGm3cWChZclX7ARPdSFxNQbKqkwSgVLK2y6as02Y2hlQ2kSIcfpn alt> |
| Figure 3. Compare different test configurations. |</p><p><strong>Time Series and Tracing: Diving Into Performance Data</strong></p><p>Pod startup latency is an important metric for Kubelet, especially when creating a large number of pods per node. Using the dashboard you can see the change of latency, for example, when creating 105 pods, as shown in Figure 4. When you see the highly variable lines, you might expect that the variance is due to different builds. However, as these test here were run against the same Kubernetes code, we can conclude the variance is due to performance fluctuation. The variance is close to 40s when we compare the 99% latency of build #162 and #173, which is very large. To drill into the source of the fluctuation, let’s check out the "TIME SERIES" page.</p><p>| <img src=https://lh5.googleusercontent.com/4WM9bX-Vzn-h2otSaVcES4FBPDeTFuIueo_uRXDctpKPO_lFAANjRj9QmezSn5x81QLcDAq8ui_Gvbik1edyjUwPKWQNKjbW7uSNwCFnGg7Bd1KqqU1U7B1gvwzK_X6Wo7DJjYH3 alt> |
| Figure 4. Pod startup latency when creating 105 pods. |</p><p>Looking specifically at build #162, we are able to see that the tracing data plotted in the pod creation latency chart (Figure 5). Each curve is an accumulated histogram of the number of pod operations which have already arrive at a certain tracing probe. The timestamp of tracing pod is either collected from the performance tests or by parsing the Kubelet log. Currently we collect the following tracing data:</p><ul><li>"create" (in test): the test creates pods through API client;</li><li>"running" (in test): the test watches that pods are running from API server;</li><li>"pod_config_change": pod config change detected by Kubelet SyncLoop;</li><li>"runtime_manager": runtime manager starts to create containers;</li><li>"infra_container_start": the infra container of a pod starts;</li><li>"container_start': the container of a pod starts;</li><li>"pod_running": a pod is running;</li><li>"pod_status_running": status manager updates status for a running pod;</li></ul><p>The time series chart illustrates that it is taking a long time for the status manager to update pod status (the data of "running" is not shown since it overlaps with "pod_status_running"). We figure out this latency is introduced due to the query per second (QPS) limits of Kubelet to the API server (default is 5). After being aware of this, we find in additional tests that by increasing QPS limits, curve "running" gradually converges with "pod_running', and results in much lower latency. Therefore the previous e2e test pod startup results reflect the combined latency of both Kubelet and time of uploading status, the performance of Kubelet is thus under-estimated.</p><p>| <img src=https://lh3.googleusercontent.com/_8y02WcgZ7ETvDTeZ893rZYNuIR2j32_jnl7O1Mj3cP9Y7I3C-gegDgSdYX1VtTpGDUo6JEouueSj8hGWPJSXj_5GcC9nE21tjIXgTIrwRXW-0jYpXdRh6oDSSdQ1XKPyXIf3yQu alt> |
| Figure 5. Time series page using data from build #162. |</p><p>Further, by comparing the time series data of build #162 (Figure 5) and build #173 (Figure 6), we find that the performance pod startup latency fluctuation actually happens during updating pod statuses. Build #162 has several straggler "pod_status_running" events with a long latency tails. It thus provides useful ideas for future optimization. </p><p>| <img src=https://lh5.googleusercontent.com/51IY9sNPEdtEe-HGz75Q4ggt73ngE0p9gsq6B0m6RDJ13MklYZ3s6xREFhWIxwJt0zFBiY6BvDHwLZ57G9UARfXy1wcAb1DwD48poUrXFHgcRVXUe3tfCoCSpZ477NGTA3A8Njrg alt> |
| Figure 6. Pod startup latency of build #173. |</p><p>In future we plan to use events in Kubernetes which has a fixed log format to collect tracing data more conveniently. Instead of extracting existing log entries, then you can insert your own tracing probes inside Kubelet and obtain the break-down latency of each segment. </p><p>You can check the latency between any two probes across different builds in the “TRACING” page, as shown in Figure 7. For example, by selecting "pod_config_change" as the start probe, and "pod_status_running' as the end probe, it gives the latency variance of Kubelet over continuous builds without status updating overhead. With this feature, developers are able to monitor the performance change of a specific part of code inside Kubelet.</p><p>| <img src=https://lh5.googleusercontent.com/nycM01gswI-Z_JxLqHiEjuJZRCg6fwiCiN7HjvKk_iNALN7KihiQB6zdfHDJpf7DLY16qVhIDr6b8qlzOJ9U77fIlBGs-F8eJ3El78pd0wKgNI73PkgEswMFCA5wBLGnYjZqF3PU alt> |
| Figure 7. Plotting latency between any two probes. |</p><p><strong>Future Work</strong></p><p>The <a href=http://node-perf-dash.k8s.io/>node performance dashboard</a> is a brand new feature. It is still alpha version under active development. We will keep optimizing the data collecting and visualization, providing more tests, metrics and tools to the developers and the cluster maintainers. </p><p>Please join our community and help us build the future of Kubernetes! If you’re particularly interested in nodes or performance testing, participate by chatting with us in our <a href=https://kubernetes.slack.com/messages/sig-scale/>Slack channel</a> or join our meeting which meets every Tuesday at 10 AM PT on this <a href=https://github.com/kubernetes/community/tree/master/sig-node>SIG-Node Hangout</a>.</p><p><em>--Zhou Fang, Software Engineering Intern, Google</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0b41d8cdfadf3d0d3eb7dcd9678b71b2>CNCF Partners With The Linux Foundation To Launch New Kubernetes Certification, Training and Managed Service Provider Program</h1><div class="td-byline mb-4"><time datetime=2016-11-08 class=text-muted>Tuesday, November 08, 2016</time></div><p>Today the CNCF is pleased to launch a new training, certification and Kubernetes Managed Service Provider (KMSP) program. </p><p>The goal of the program is to ensure enterprises get the support they’re looking for to get up to speed and roll out new applications more quickly and more efficiently. The Linux Foundation, in partnership with CNCF, will develop and operate the Kubernetes training and certification.</p><p>Interested in this course? Sign up <a href=https://training.linuxfoundation.org/linux-courses/system-administration-training/kubernetes-fundamentals>here</a> to pre-register. The course, expected to be available in early 2017, is open now at the discounted price of $99 (regularly $199) for a limited time, and the certification program is expected to be available in the second quarter of 2017. </p><p>The KMSP program is a pre-qualified tier of highly vetted service providers who have deep experience helping enterprises successfully adopt Kubernetes. The KMSP partners offer SLA-backed Kubernetes support, consulting, professional services and training for organizations embarking on their Kubernetes journey. In contrast to the Kubernetes Service Partners program outlined recently in <a href=https://kubernetes.io/blog/2016/10/kubernetes-service-technology-partners-program>this blog</a>, to become a Kubernetes Managed Service Provider the following additional requirements must be met: three or more certified engineers, an active contributor to Kubernetes, and a business model to support enterprise end users. </p><p>As part of the program, a new CNCF Certification Working Group is starting up now. The group will help define the program's open source curriculum, which will be available under the <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons By Attribution 4.0 International license</a> for anyone to use. Any Kubernetes expert can join the working group via this <a href=https://lists.cncf.io/mailman/listinfo/cncf-kubernetescertwg.>link</a>. Google has committed to assist, and many others, including Apprenda, Container Solutions, CoreOS, Deis and Samsung SDS, have expressed interest in participating in the Working Group.</p><p>To learn more about the new program and the first round of KMSP partners that we expect to grow weekly, check out today's announcement <a href=https://www.cncf.io/announcement/2016/11/08/cloud-native-computing-foundation-launches-certification-training-managed-service-provider-program-kubernetes>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-84fe92b048c2f52dc4fc501556e035ad>Bringing Kubernetes Support to Azure Container Service</h1><div class="td-byline mb-4"><time datetime=2016-11-07 class=text-muted>Monday, November 07, 2016</time></div><p><em>Editor's note: Today’s post is by Brendan Burns, Partner Architect, at Microsoft & Kubernetes co-founder talking about bringing Kubernetes to Azure Container Service.</em></p><p>With more than a thousand people coming to <a href=http://events.linuxfoundation.org/events/kubecon>KubeCon</a> in my hometown of Seattle, nearly three years after I helped start the Kubernetes project, it’s amazing and humbling to see what a small group of people and a radical idea have become after three years of hard work from a large and growing community. In July of 2014, scarcely a month after Kubernetes became publicly available, Microsoft announced its initial support for Azure. The release of <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>Kubernetes 1.4</a>, brought support for native Microsoft networking, <a href=https://github.com/kubernetes/kubernetes/pull/28821>load-balancer</a> and <a href=https://github.com/kubernetes/kubernetes/pull/29836>disk integration</a>. </p><p>Today, Microsoft <a href=https://azure.microsoft.com/en-us/blog/azure-container-service-the-cloud-s-most-open-option-for-containers/>announced</a> the next step in Kubernetes on Azure: the introduction of Kubernetes as a supported orchestrator in Azure Container Service (ACS). It’s been really exciting for me to join the ACS team and help build this new addition. The integration of Kubernetes into ACS means that with a few clicks in the Azure portal, or by running a single command in the new python-based Azure command line tool, you will be able to create a fully functional Kubernetes cluster that is integrated with the rest of your Azure resources.</p><p>Kubernetes is available in public preview in Azure Container Service today. Community participation has always been an important part of the Kubernetes experience. Over the next few months, I hope you’ll join us and provide your feedback on the experience as we bring it to general availability.</p><p>In the spirit of community, we are also excited to announce a new open source project: <a href=https://github.com/azure/acs-engine>ACS Engine</a>. The goal of ACS Engine is to provide an open, community driven location to develop and share best practices for orchestrating containers on Azure. All of our knowledge of running containers in Azure has been captured in that repository, and we look forward to improving and extending it as we move forward with the community. Going forward, the templates in ACS Engine will be the basis for clusters deployed via the ACS API, and thus community driven improvements, features and more will have a natural path into the Azure Container Service. We’re excited to invite you to join us in improving ACS. Prior to the creation of ACS Engine, customers with unique requirements not supported by the ACS API needed to maintain variations on our templates. While these differences start small, they grew larger over time as the mainline template was improved and users also iterated their templates. These differences and drift really impact the ability for users to collaborate, since their templates are all different. Without the ability to share and collaborate, it’s difficult to form a community since every user is siloed in their own variant.</p><p>To solve this problem, the core of ACS Engine is a template processor, built in Go, that enables you to dynamically combine different pieces of configuration together to form a final template that can be used to build up your cluster. Thus, each user can mix and match the pieces build the final container cluster that suits their needs. At the same time, each piece can be built and maintained collaboratively by the community. We’ve been beta testing this approach with some customers and the feedback we’ve gotten so far has been really positive.</p><p>Beyond services to help you run containers on Azure, I think it’s incredibly important to improve the experience of developing and deploying containerized applications to Kubernetes. To that end, I’ve been doing a bunch of work lately to build a Kubernetes extension for the really excellent, open source, <a href=https://code.visualstudio.com/>Visual Studio Code</a>. The Kubernetes extension enables you to quickly deploy JSON or YAML files you are editing onto a Kubernetes cluster. Additionally, it enables you to import existing Kubernetes objects into Code for easy editing. Finally, it enables synchronization between your running containers and the source code that you are developing for easy debugging of issues you are facing in production.</p><p>But really, a demo is worth a thousand words, so please have a look at this <a href="https://www.youtube.com/watch?v=nhY9XdzNbbY">video</a>:</p><p>Of course, like everything else in Kubernetes it’s released as open source, and I look forward to working on it further with the community. Thanks again, I look forward to seeing everyone at the OpenShift Gathering today, as well as at the Microsoft Azure booth during KubeCon tomorrow and Wednesday. Welcome to Seattle!</p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b8f188e91bc6dd64e07580f41f6625e>Modernizing the Skytap Cloud Micro-Service Architecture with Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-11-07 class=text-muted>Monday, November 07, 2016</time></div><p><em>Editor's note: Today’s guest post is by the Tools and Infrastructure Engineering team at Skytap, a public cloud provider focused on empowering DevOps workflows, sharing their experience on adopting Kubernetes. </em></p><p><a href=https://www.skytap.com/>Skytap</a> is a global public cloud that provides our customers the ability to save and clone complex virtualized environments in any given state. Our customers include enterprise organizations running applications in a hybrid cloud, educational organizations providing <a href=https://www.skytap.com/solutions/virtual-training/>virtual training labs</a>, users who need easy-to-maintain development and test labs, and a variety of organizations with diverse DevOps workflows.</p><p>Some time ago, we started growing our business at an accelerated pace — our user base and our engineering organization continue to grow simultaneously. These are exciting, rewarding challenges! However, it's difficult to scale applications and organizations smoothly, and we’re approaching the task carefully. When we first began looking at improvements to scale our toolset, it was very clear that traditional OS virtualization was not going to be an effective way to achieve our scaling goals. We found that the persistent nature of VMs encouraged engineers to build and maintain bespoke ‘pet’ VMs; this did not align well with our desire to build reusable runtime environments with a stable, predictable state. Fortuitously, growth in the Docker and Kubernetes communities has aligned with our growth, and the concurrent explosion in community engagement has (from our perspective) helped these tools mature.</p><p>In this article we’ll explore how Skytap uses Kubernetes as a key component in services that handle production workloads growing the Skytap Cloud.</p><p>As we add engineers, we want to maintain our agility and continue enabling ownership of components throughout the software development lifecycle. This requires a lot of modularization and consistency in key aspects of our process. Previously, we drove reuse with systems-level packaging through our VM and environment templates, but as we scale, containers have become increasingly important as a packaging mechanism due to their comparatively lightweight and precise control of the runtime environment. </p><p>In addition to this packaging flexibility, containers help us establish more efficient resource utilization, and they head off growing complexity arising from the natural inclination of teams to mix resources into large, highly-specialized VMs. For example, our operations team would install tools for monitoring health and resource utilization, a development team would deploy a service, and the security team might install traffic monitoring; combining all of that into a single VM greatly increases the test burden and often results in surprises—oops, you pulled in a new system-level Ruby gem!</p><p>Containerization of individual components in a service is pretty trivial with Docker. Getting started is easy, but as anyone who has built a distributed system with more than a handful of components knows, the real difficulties are deployment, scaling, availability, consistency, and communication between each unit in the cluster.</p><p><strong>Let’s containerize! </strong></p><p>We’d begun to trade a lot of our heavily-loved pet VMs for, <a href=https://ericsysmin.com/2016/03/07/pets-vs-cattle/>as the saying goes</a>, cattle.</p><pre><code>_____
/ Moo \
\---- /
       \   ^__^
        \  (oo)\_______
           (__)\       )\/\
               ||-----w |
               ||     ||
</code></pre><p>The challenges of distributed systems aren’t simplified by creating a large herd of free-range containers, though. When we started using containers, we recognized the need for a container management framework. We evaluated Docker Swarm, Mesosphere, and Kubernetes, but we found that the Mesosphere usage model didn’t match our needs — we need the ability to manage discrete VMs; this doesn’t match the Mesosphere ‘distributed operating system’ model — and Docker Swarm was still not mature enough. So, we selected Kubernetes.  </p><p>Launching Kubernetes and building a new distributed service is relatively easy (inasmuch as this can be said for such a service: you can’t beat <a href=https://en.wikipedia.org/wiki/CAP_theorem>CAP theorem</a>). However, we need to integrate container management with our existing platform and infrastructure. Some components of the platform are better served by VMs, and we need the ability to containerize services iteratively. </p><p>We broke this integration problem down into four categories: </p><ol><li>1.Service control and deployment</li><li>2.Inter-service communication</li><li>3.Infrastructure integration</li><li>4.Engineering support and education</li></ol><p><strong>Service Control and Deployment</strong></p><p>We use a custom extension of <a href=https://github.com/capistrano/capistrano>Capistrano</a> (we call it ‘Skycap’) to deploy services and manage those services at runtime. It is important for us to manage both containerized and classic services through a single, well-established framework. We also need to isolate Skycap from the inevitable breaking changes inherent in an actively-developed tool like Kubernetes. </p><p>To handle this, we use wrappers in to our service control framework that isolate kubectl behind Skycap and handle issues like ignoring spurious log messages.</p><p>Deployment adds a layer of complexity for us. Docker images are a great way to package software, but historically, we’ve deployed from source, not packages. Our engineering team expects that making changes to source is sufficient to get their work released; devs don’t expect to handle additional packaging steps. Rather than rebuild our entire deployment and orchestration framework for the sake of containerization, we use a continuous integration pipeline for our containerized services. We automatically build a new Docker image for every commit to a project, and then we tag it with the Mercurial (Hg) changeset number of that commit. On the Skycap side, a deployment from a specific Hg revision will then pull the Docker images that are tagged with that same revision number. </p><p>We reuse container images across multiple environments. This requires environment-specific configuration to be injected into each container instance. Until recently, we used similar source-based principles to inject these configuration values: each container would copy relevant configuration files from Hg by cURL-ing raw files from the repo at run time. Network availability and variability are a challenge best avoided, though, so we now load the configuration into Kubernetes’ <a href=https://kubernetes.io/blog/2016/04/configuration-management-with-containers><strong>ConfigMap</strong></a> feature. This not only simplifies our Docker images, but it also makes pod startup faster and more predictable (because containers don’t have to download files from Hg).   </p><p><strong>Inter-service communication</strong></p><p>Our services communicate using two primary methods. The first, message brokering, is typical for process-to-process communication within the Skytap platform. The second is through direct point-to-point TCP connections, which are typical for services that communicate with the outside world (such as web services). We’ll discuss the TCP method in the next section, as a component of infrastructure integration. </p><p>Managing direct connections between pods in a way that services can understand is complicated. Additionally, our containerized services need to communicate with classic VM-based services. To mitigate this complexity, we primarily use our existing message queueing system. This helped us avoid writing a TCP-based service discovery and load balancing system for handling traffic between pods and non-Kubernetes services. </p><p>This reduces our configuration load—services only need to know how to talk to the message queues, rather than to every other service they need to interact with. We have additional flexibility for things like managing the run-state of pods; messages buffer in the queue while nodes are restarting, and we avoid the overhead of re-configuring TCP endpoints each time a pod is added or removed from the cluster. Furthermore, the MQ model allows us to manage load balancing with a more accurate ‘pull’ based approach, in which recipients determine when they are ready to process a new message, instead of using heuristics like ‘least connections’ that simply count the number of open sockets to estimate load.  </p><p>Migrating MQ-enabled services to Kubernetes is relatively straightforward compared to migrating services that use the complex TCP-based direct or load balanced connections. Additionally, the isolation provided by the message broker means that the switchover from a classic service to a container-based service is essentially transparent to any other MQ-enabled service. </p><p><strong>Infrastructure Integration</strong></p><p>As an infrastructure provider, we face some unique challenges in configuring Kubernetes for use with our platform. <a href=https://aws.amazon.com/>AWS</a> & <a href=https://cloud.google.com/>GCP</a> provide out-of-box solutions that simplify Kubernetes provisioning but make assumptions about the underlying infrastructure that do not match our reality. Some organizations have purpose-built data centers. This option would have required us to abandon our existing load balancing infrastructure, our Puppet based provisioning system and the expertise we’d built up around these tools. We weren’t interested in abandoning the tools or our vested experience, so we needed a way to manage Kubernetes that could integrate with our world instead of rebuild it.</p><p>So, we use Puppet to provision and configure VMs that, in turn, run the Skytap Platform. We wrote custom deployment scripts to install Kubernetes on these, and we coordinate with our operations team to do capacity planning for Kube-master and Kube-node hosts. </p><p>In the previous section, we mentioned point-to-point TCP-based communication. For customer-facing services, the pods need a way to interface with Skytap’s layer 3 network infrastructure. Examples at Skytap include our web applications and API over HTTPS, Remote Desktop over Web Sockets, FTP, TCP/UDP port forwarding services, full public IPs, etc. We need careful management of network ingress and egress for this external traffic, and have historically used <a href=https://f5.com/>F5</a> load balancers. The MQ infrastructure for internal services is inadequate for handling this workload because the protocols used by various clients (like web browsers) are very specific and TCP is the lowest common denominator.</p><p>To get our load balancers communicating with our Kubernetes pods, we run the kube-proxy on each node. Load balancers route to the node, and kube-proxy handles the final handoff to the appropriate pod.</p><p>We mustn’t forget that Kubernetes needs to route traffic between pods (for both TCP-based and MQ-based messaging). We use the <a href=https://www.projectcalico.org/calico-networking-for-kubernetes/>Calico</a> plugin for Kubernetes networking, with a specialized service to reconfigure the F5 when Kubernetes launches or reaps pods. Calico handles route advertisement with <a href=https://en.wikipedia.org/wiki/Border_Gateway_Protocol>BGP</a>, which eases integration with the F5.</p><p>F5s also need to have their <a href=https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/ltm-concepts-11-2-0/ltm_pools.html>load balancing pool</a> reconfigured when pods enter or leave the cluster. The F5 appliance maintains a pool of load-balanced back-ends; ingress to a containerized service is directed through this pool to one of the nodes hosting a service pod. This is straightforward for static network configurations – but since we're using Kubernetes to manage pod replication and availability, our networking situation becomes dynamic. To handle changes, we have a 'load balancer' pod that monitors the Kubernetes svc object for changes; if a pod is removed or added, the ‘load balancer’ pod will detect this change through the svc object, and then update the F5 configuration through the appliance's web API. This way, Kubernetes transparently handles replication and failover/recovery, and the dynamic load balancer configuration lets this process remain invisible to the service or user who originated the request. Similarly, the combination of the Calico virtual network plus the F5 load balancer means that TCP connections should behave consistently for services that are running on both the traditional VM infrastructure, or that have been migrated to containers. </p><p><img src=https://lh4.googleusercontent.com/2wfBbW3zxYLPg8Xgl6GIAE9Xt9afjZfTAyfR0H6EzfdHAJyDjg7N1RCpZLoLG9N9wVAnsczXUBicJ4QUydCOJ1uZ6A1SP44ki-XAnpDYTiL5cLaXFoi2YtKjKYxC5hFoCoOs7nWM alt=kubernetes_f5_messaging.png></p><p>With dynamic reconfiguration of the network, the replication mechanics of Kubernetes make horizontal scaling and (most) failover/recovery very straightforward. We haven’t yet reached the reactive scaling milestone, but we've laid the groundwork with the Kubernetes and Calico infrastructure, making one avenue to implement it straightforward:</p><ul><li>Configure upper and lower bounds for service replication</li><li>Build a load analysis and scaling service (easy, right?)</li><li>If load patterns match the configured triggers in the scaling service (for example, request rate or volume above certain bounds), issue: kubectl scale --replicas=COUNT rc NAME</li></ul><p>This would allow us fine-grained control of autoscaling at the platform level, instead of from the applications themselves – but we’ll also evaluate <a href=/docs/user-guide/horizontal-pod-autoscaling/><strong>Horizontal Pod Autoscaling</strong></a> in Kubernetes; which may suit our need without a custom service. </p><p>Keep an eye on <a href=https://github.com/skytap>our GitHub account</a> and the <a href=https://www.skytap.com/blog/>Skytap blog</a>; as our solutions to problems like these mature, we hope to share what we’ve built with the open source community.</p><p><strong>Engineering Support</strong></p><p>A transition like our containerization project requires the engineers involved in maintaining and contributing to the platform change their workflow and learn new methods for creating and troubleshooting services. </p><p>Because a variety of learning styles require a multi-faceted approach, we handle this in three ways: with documentation, with direct outreach to engineers (that is, brownbag sessions or coaching teams), and by offering easy-to-access, ad-hoc support.  </p><p>We continue to curate a collection of documents that provide guidance on transitioning classic services to Kubernetes, creating new services, and operating containerized services. Documentation isn’t for everyone, and sometimes it’s missing or incomplete despite our best efforts, so we also run an internal #kube-help Slack channel, where anyone can stop in for assistance or arrange a more in-depth face-to-face discussion.</p><p>We have one more powerful support tool: we automatically construct and test prod-like environments that include this Kubernetes infrastructure, which allows engineers a lot of freedom to experiment and work with Kubernetes hands-on. We explore the details of automated environment delivery in more detail in <a href=https://www.skytap.com/blog/continuous-delivery-fully-functional-environments-skytap-part-1/>this post</a>.</p><p><strong>Final Thoughts</strong></p><p>We’ve had great success with Kubernetes and containerization in general, but we’ve certainly found that integrating with an existing full-stack environment has presented many challenges. While not exactly plug-and-play from an enterprise lifecycle standpoint, the flexibility and configurability of Kubernetes still remains a very powerful tool for building our modularized service ecosystem.</p><p>We love application modernization challenges. The Skytap platform is well suited for these sorts of migration efforts – we run Skytap in Skytap, of course, which helped us tremendously in our Kubernetes integration project. If you’re planning modernization efforts of your own, <a href=https://www.skytap.com/>connect with us</a>, we’re happy to help.</p><p><em>--Shawn Falkner-Horine and Joe Burchett, Tools and Infrastructure Engineering, Skytap</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f93ca998ac0832d18ab5e9f99c6068c8>Introducing Kubernetes Service Partners program and a redesigned Partners page</h1><div class="td-byline mb-4"><time datetime=2016-10-31 class=text-muted>Monday, October 31, 2016</time></div><p>Kubernetes has become a leading container orchestration system by being a powerful and flexible way to run distributed systems at scale. Through our very active open source community, equating to hundreds of person years of work, Kubernetes achieved four major releases in just one year to become a critical part of thousands of companies infrastructures. However, even with all that momentum, adopting cloud native computing is a significant transition for many organizations. It can be challenging to adopt a new methodology, and many teams are looking for advice and support through that journey.</p><p>Today, we’re excited to launch the Kubernetes <strong>Service Partners</strong> program. A Service Partner is a company that provides support and consulting for customers building applications on Kubernetes. This program is an addition to our existing Kubernetes <strong>Technology Partners</strong> who provide software and offer support services for their software. </p><p>The Service Partners provide hands-on best practice guidance for running your apps on Kubernetes, and are available to work with companies of all sizes to get started; the first batch of participants includes: Apprenda, Container Solutions, Deis, Livewyer, ReactiveOps and Samsung SDS. You’ll find their listings along with our existing Technology Partners on the newly redesigned <a href=http://kubernetes.io/partners/>Partners Page</a>, giving you a single view into the Kubernetes ecosystem. </p><p>The list of partners will grow weekly, and we look forward to collaborating with the community to build a vibrant Kubernetes ecosystem.</p><p><em>--Allan Naim, Product Manager, Google, on behalf of the Kubernetes team.</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-baa70553d58aac3ab52bfa4847e42a1c>Tail Kubernetes with Stern</h1><div class="td-byline mb-4"><time datetime=2016-10-31 class=text-muted>Monday, October 31, 2016</time></div><p><em>Editor’s note: today’s post is by Antti Kupila, Software Engineer, at Wercker, about building a tool to tail multiple pods and containers on Kubernetes.</em></p><p>We love Kubernetes here at <a href=http://wercker.com/>Wercker</a> and build all our infrastructure on top of it. When deploying anything you need to have good visibility to what's going on and logs are a first view into the inner workings of your application. Good old tail -f has been around for a long time and Kubernetes has this too, built right into <a href=/docs/user-guide/kubectl-overview/>kubectl</a>.</p><p>I should say that tail is by no means the tool to use for debugging issues but instead you should feed the logs into a more persistent place, such as <a href=https://www.elastic.co/products/elasticsearch>Elasticsearch</a>. However, there's still a place for tail where you need to quickly debug something or perhaps you don't have persistent logging set up yet (such as when developing an app in <a href=https://github.com/kubernetes/minikube>Minikube</a>).</p><p><strong>Multiple Pods</strong></p><p>Kubernetes has the concept of <a href=/docs/user-guide/replication-controller/>Replication Controllers</a> which ensure that n pods are running at the same time. This allows rolling updates and redundancy. Considering they're quite easy to set up there's really no reason not to do so.</p><p>However now there are multiple pods running and they all have a unique id. One issue here is that you'll need to know the exact pod id (kubectl get pods) but that changes every time a pod is created so you'll need to do this every time. Another consideration is the fact that Kubernetes load balances the traffic so you won't know at which pod the request ends up at. If you're tailing pod A but the traffic ends up at pod B you'll miss what happened.</p><p>Let's say we have a pod called service with 3 replicas. Here's what that would look like:</p><pre><code>$ kubectl get pods                         # get pods to find pod ids

$ kubectl log -f service-1786497219-2rbt1  # pod 1

$ kubectl log -f service-1786497219-8kfbp  # pod 2

$ kubectl log -f service-1786497219-lttxd  # pod 3
</code></pre><p><strong>Multiple containers</strong></p><p>We're heavy users <a href=http://www.grpc.io/>gRPC</a> for internal services and expose the gRPC endpoints over REST using <a href=https://github.com/grpc-ecosystem/grpc-gateway>gRPC Gateway</a>. Typically we have server and gateway living as two containers in the same pod (same binary that sets the mode by a cli flag). The gateway talks to the server in the same pod and both ports are exposed to Kubernetes. For internal services we can talk directly to the gRPC endpoint while our website communicates using standard REST to the gateway.</p><p>This poses a problem though; not only do we now have multiple pods but we also have multiple containers within the pod. When this is the case the built-in logging of kubectl requires you to specify which containers you want logs from.</p><p>If we have 3 replicas of a pod and 2 containers in the pod you'll need 6 kubectl log -f &lt;pod id> &lt;container id>. We work with big monitors but this quickly gets out of hand…</p><p>If our service pod has a server and gateway container we'd be looking at something like this:</p><pre><code>$ kubectl get pods                                 # get pods to find pod ids

$ kubectl describe pod service-1786497219-2rbt1    # get containers in pod

$ kubectl log -f service-1786497219-2rbt1 server   # pod 1

$ kubectl log -f service-1786497219-2rbt1 gateway  # pod 1

$ kubectl log -f service-1786497219-8kfbp server   # pod 2

$ kubectl log -f service-1786497219-8kfbp gateway  # pod 2

$ kubectl log -f service-1786497219-lttxd server   # pod 3

$ kubectl log -f service-1786497219-lttxd gateway  # pod 3
</code></pre><p><strong>Stern</strong></p><p>To get around this we built <a href=https://github.com/wercker/stern>Stern</a>. It's a super simple utility that allows you to specify both the pod id and the container id as regular expressions. Any match will be followed and the output is multiplexed together, prefixed with the pod and container id, and color-coded for human consumption (colors are stripped if piping to a file).</p><p>Here's how the service example would look:</p><pre><code>$ stern service
</code></pre><p>This will match any pod containing the word service and listen to all containers within it. If you only want to see traffic to the server container you could do stern --container server service and it'll stream the logs of all the server containers from the 3 pods.</p><p>The output would look something like this:</p><pre><code>$ stern service

+ service-1786497219-2rbt1 › server

+ service-1786497219-2rbt1 › gateway

+ service-1786497219-8kfbp › server

+ service-1786497219-8kfbp › gateway

+ service-1786497219-lttxd › server

+ service-1786497219-lttxd › gateway

+ service-1786497219-8kfbp server Log message from server

+ service-1786497219-2rbt1 gateway Log message from gateway

+ service-1786497219-8kfbp gateway Log message from gateway

+ service-1786497219-lttxd gateway Log message from gateway

+ service-1786497219-lttxd server Log message from server

+ service-1786497219-2rbt1 server Log message from server
</code></pre><p>In addition, if a pod is killed and recreated during a deployment Stern will stop listening to the old pod and automatically hook into the new one. There's no more need to figure out what the id of that newly created pod is.</p><p><strong>Configuration options</strong></p><p>Stern was deliberately designed to be minimal so there's not much to it. However, there are still a couple configuration options we can highlight here. They're very similar to the ones built into kubectl so if you're familiar with that you should feel right at home.</p><ul><li>timestamps adds the timestamp to each line</li><li>since shows log entries since a certain time (for instance --since 15min)</li><li>kube-config allows you to specify another Kubernetes config. Defaults to ~/.kube/config</li><li>namespace allows you to only limit the search to a certain namespaceRun stern --help for all options.</li></ul><p><strong>Examples</strong></p><p>Tail the gateway container running inside of the envvars pod on staging</p><pre><code> + stern --context staging --container gateway envvars
</code></pre><p>Show auth activity from 15min ago with timestamps</p><pre><code>+ stern -t --since 15m auth
</code></pre><p>Follow the development of some-new-feature in minikube</p><pre><code>+ stern --context minikube some-new-feature
</code></pre><p>View pods from another namespace</p><pre><code>+ stern --namespace kube-system kubernetes-dashboard
</code></pre><p><strong>Get Stern</strong></p><p>Stern is open source and <a href=https://github.com/wercker/stern>available on GitHub</a>, we'd love your contributions or ideas. If you don't want to build from source you can also download a precompiled binary from <a href=https://github.com/wercker/stern/releases>GitHub releases</a>.</p><p><a href=https://4.bp.blogspot.com/-oNscZEvpzVw/WBeWc4cW4zI/AAAAAAAAAyw/71okg07IPHM6dtBOubO_0kxdYxzwoUGOACLcB/s1600/stern-long.gif><img src=https://4.bp.blogspot.com/-oNscZEvpzVw/WBeWc4cW4zI/AAAAAAAAAyw/71okg07IPHM6dtBOubO_0kxdYxzwoUGOACLcB/s640/stern-long.gif alt></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1506704143da62d21a148e789d8291a9>How We Architected and Run Kubernetes on OpenStack at Scale at Yahoo! JAPAN</h1><div class="td-byline mb-4"><time datetime=2016-10-24 class=text-muted>Monday, October 24, 2016</time></div><p><em>Editor’s note: today’s post is by the Infrastructure Engineering team at Yahoo! JAPAN, talking about how they run OpenStack on Kubernetes. This post has been translated and edited for context with permission -- originally published on the <a href=http://techblog.yahoo.co.jp/infrastructure/os_n_k8s/>Yahoo! JAPAN engineering blog</a>. </em></p><p><strong>Intro</strong><br>This post outlines how Yahoo! JAPAN, with help from Google and Solinea, built an automation tool chain for “one-click” code deployment to Kubernetes running on OpenStack. </p><p>We’ll also cover the basic security, networking, storage, and performance needs to ensure production readiness. </p><p>Finally, we will discuss the ecosystem tools used to build the CI/CD pipeline, Kubernetes as a deployment platform on VMs/bare metal, and an overview of Kubernetes architecture to help you architect and deploy your own clusters. </p><p><strong>Preface</strong><br>Since our company started using OpenStack in 2012, our internal environment has changed quickly. Our initial goal of virtualizing hardware was achieved with OpenStack. However, due to the progress of cloud and container technology, we needed the capability to launch services on various platforms. This post will provide our example of taking applications running on OpenStack and porting them to Kubernetes.</p><p><strong>Coding Lifecycle</strong><br>The goal of this project is to create images for all required platforms from one application code, and deploy those images onto each platform. For example, when code is changed at the code registry, bare metal images, Docker containers and VM images are created by CI (continuous integration) tools, pushed into our image registry, then deployed to each infrastructure platform.</p><p><img src=https://lh4.googleusercontent.com/JyXP4-mfmZOznXQekSmHFoZ4iRTGLvSAWI37L4AAYzrstKnDVKIRcPhzqrxU1Cmm9bbqOpF_feXX7xDB-cPmbln6dmQ0VelksMSuZfhOlT0r8466yvUU456_OKyrn4wLphCniJuQ alt></p><p>We use following products in our CICD pipeline:</p><table><thead><tr><th>Function</th><th>Product</th></tr></thead><tbody><tr><td>Code registry</td><td>GitHub Enterprise</td></tr><tr><td>CI tools</td><td>Jenkins</td></tr><tr><td>Image registry</td><td>Artifactory</td></tr><tr><td>Bug tracking system</td><td>JIRA</td></tr><tr><td>deploying Bare metal platform</td><td>OpenStack Ironic</td></tr><tr><td>deploying VM platform</td><td>OpenStack</td></tr><tr><td>deploying container platform</td><td>Kubernetes</td></tr></tbody></table><p>Image Creation. Each image creation workflow is shown in the next diagram.</p><p><strong>VM Image Creation</strong> :</p><p><a href=https://4.bp.blogspot.com/-saBA4FKmJEM/WAppk0keRfI/AAAAAAAAAxM/7Y3uw-H3I0Ae_p6IqUu429pJqtwqTGxIgCLcB/s1600/Untitled%2Bdrawing.png><img src=https://4.bp.blogspot.com/-saBA4FKmJEM/WAppk0keRfI/AAAAAAAAAxM/7Y3uw-H3I0Ae_p6IqUu429pJqtwqTGxIgCLcB/s640/Untitled%2Bdrawing.png alt></a></p><ol><li>1.push code to GitHub</li><li>2.hook to Jenkins master</li><li>3.Launch job at Jenkins slave </li><li>4.checkout Packer repository</li><li>5.Run Service Job</li><li>6.Execute Packer by build script</li><li>7.Packer start VM for OpenStack Glance </li><li>8.Configure VM and install required applications</li><li>9.create snapshot and register to glance
10.10.Download the new created image from Glance
11.11.Upload the image to Artifactory</li></ol><p><strong>Bare Metal Image Creation:</strong></p><p><a href=https://1.bp.blogspot.com/-0aPKFfhF33k/WApqIabmf1I/AAAAAAAAAxQ/jR33xg1OoMolm9T2Jt3FFixZt6294zUsACLcB/s1600/Untitled%2Bdrawing%2B%25281%2529.png><img src=https://1.bp.blogspot.com/-0aPKFfhF33k/WApqIabmf1I/AAAAAAAAAxQ/jR33xg1OoMolm9T2Jt3FFixZt6294zUsACLcB/s640/Untitled%2Bdrawing%2B%25281%2529.png alt></a></p><ol><li>1.push code to GitHub</li><li>2.hook to Jenkins master</li><li>3.Launch job at Jenkins slave </li><li>4.checkout Packer repository</li><li>5.Run Service Job</li><li>6.Download base bare metal image by build script</li><li>7.build script execute diskimage-builder with Packer to create bare metal image</li><li>8.Upload new created image to Glance</li><li>9.Upload the image to Artifactory</li></ol><p><strong>Container Image Creation:</strong></p><p><a href=https://2.bp.blogspot.com/-5su8_2KmuYw/WApqvvw0k8I/AAAAAAAAAxU/36NZG0lTQ1whl-JcCuKCb-kjuISR-PSGwCLcB/s1600/Untitled%2Bdrawing%2B%25282%2529.png><img src=https://2.bp.blogspot.com/-5su8_2KmuYw/WApqvvw0k8I/AAAAAAAAAxU/36NZG0lTQ1whl-JcCuKCb-kjuISR-PSGwCLcB/s640/Untitled%2Bdrawing%2B%25282%2529.png alt></a></p><ol><li>1.push code to GitHub</li><li>2.hook to Jenkins master</li><li>3.Launch job at Jenkins slave </li><li>4.checkout Dockerfile repository</li><li>5.Run Service Job</li><li>6.Download base docker image from Artifactory</li><li>7.If no docker image found at Artifactory, download from Docker Hub</li><li>8.Execute docker build and create image </li><li>9.Upload the image to Artifactory</li></ol><p><strong>Platform Architecture.</strong></p><p>Let’s focus on the container workflow to walk through how we use Kubernetes as a deployment platform. This platform architecture is as below.</p><p><a href=https://2.bp.blogspot.com/-qiqHdUwASOU/WApsUZF7fRI/AAAAAAAAAxc/26b1XqOnybwWiqDoFUXW9QOxoG3ub7nDACLcB/s1600/Untitled%2Bdrawing%2B%25284%2529.png><img src=https://2.bp.blogspot.com/-qiqHdUwASOU/WApsUZF7fRI/AAAAAAAAAxc/26b1XqOnybwWiqDoFUXW9QOxoG3ub7nDACLcB/s400/Untitled%2Bdrawing%2B%25284%2529.png alt></a></p><table><thead><tr><th>Function</th><th>Product</th></tr></thead><tbody><tr><td>Infrastructure Services</td><td>OpenStack</td></tr><tr><td>Container Host</td><td>CentOS</td></tr><tr><td>Container Cluster Manager</td><td>Kubernetes</td></tr><tr><td>Container Networking</td><td>Project Calico</td></tr><tr><td>Container Engine</td><td>Docker</td></tr><tr><td>Container Registry</td><td>Artifactory</td></tr><tr><td>Service Registry</td><td>etcd</td></tr><tr><td>Source Code Management</td><td>GitHub Enterprise</td></tr><tr><td>CI tool</td><td>Jenkins</td></tr><tr><td>Infrastructure Provisioning</td><td>Terraform</td></tr><tr><td>Logging</td><td>Fluentd, Elasticsearch, Kibana</td></tr><tr><td>Metrics</td><td>Heapster, Influxdb, Grafana</td></tr><tr><td>Service Monitoring</td><td>Prometheus</td></tr></tbody></table><p>We use CentOS for Container Host (OpenStack instances) and install Docker, Kubernetes, Calico, etcd and so on. Of course, it is possible to run various container applications on Kubernetes. In fact, we run OpenStack as one of those applications. That's right, OpenStack on Kubernetes on OpenStack. We currently have more than 30 OpenStack clusters, that quickly become hard to manage and operate. As such, we wanted to create a simple, base OpenStack cluster to provide the basic functionality needed for Kubernetes and make our OpenStack environment easier to manage.</p><p><strong>Kubernetes Architecture</strong></p><p>Let me explain Kubernetes architecture in some more detail. The architecture diagram is below.</p><p><a href=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/kubernetes.png><img src=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/kubernetes.png alt></a></p><p>|Product |Description |
|OpenStack Keystone|Kubernetes Authentication and Authorization |
|OpenStack Cinder |External volume used from Pod (grouping of multiple containers) |
|kube-apiserver |Configure and validate objects like Pod or Services (definition of access to services in container) through REST API|
|kube-scheduler |Allocate Pods to each node |
|kube-controller-manager |Execute Status management, manage replication controller |
|kubelet |Run on each node as agent and manage Pod |
|calico |Enable inter-Pod connection using BGP |
|kube-proxy |Configure iptable NAT tables to configure IP and load balance (ClusterIP) |
|etcd |Distribute KVS to store Kubernetes and Calico information |
|etcd-proxy |Run on each node and transfer client request to etcd clusters|</p><p><strong>Tenant Isolation</strong> To enable multi-tenant usage like OpenStack, we utilize OpenStack Keystone for authentication and authorization.</p><p><strong>Authentication</strong> With a Kubernetes plugin, OpenStack Keystone can be used for Authentication. By Adding authURL of Keystone at startup Kubernetes API server, we can use OpenStack OS_USERNAME and OS_PASSWORD for Authentication. <strong>Authorization</strong> We currently use the ABAC (Attribute-Based Access Control) mode of Kubernetes Authorization. We worked with a consulting company, Solinea, who helped create a utility to convert OpenStack Keystone user and tenant information to Kubernetes JSON policy file that maps Kubernetes ABAC user and namespace information to OpenStack tenants. We then specify that policy file when launching Kubernetes API Server. This utility also creates namespaces from tenant information. These configurations enable Kubernetes to authenticate with OpenStack Keystone and operate in authorized namespaces. <strong>Volumes and Data Persistence</strong> Kubernetes provides “Persistent Volumes” subsystem which works as persistent storage for Pods. “Persistent Volumes” is capable to support cloud-provider storage, it is possible to utilize OpenStack cinder-volume by using OpenStack as cloud provider. <strong>Networking</strong> Flannel and various networking exists as networking model for Kubernetes, we used Project Calico for this project. Yahoo! JAPAN recommends to build data center with pure L3 networking like redistribute ARP validation or IP CLOS networking, Project Calico matches this direction. When we apply overlay model like Flannel, we cannot access to Pod IP from outside of Kubernetes clusters. But Project Calico makes it possible. We also use Project Calico for Load Balancing we describe later.</p><p><a href=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/network.png><img src=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/network.png alt></a></p><p>In Project Calico, broadcast production IP by BGP working on BIRD containers (OSS routing software) launched on each nodes of Kubernetes. By default, it broadcast in cluster only. By setting peering routers outside of clusters, it makes it possible to access a Pod from outside of the clusters. <strong>External Service Load Balancing</strong></p><p>There are multiple choices of external service load balancers (access to services from outside of clusters) for Kubernetes such as NodePort, LoadBalancer and Ingress. We could not find solution which exactly matches our requirements. However, we found a solution that almost matches our requirements by broadcasting Cluster IP used for Internal Service Load Balancing (access to services from inside of clusters) with Project Calico BGP which enable External Load Balancing at Layer 4 from outside of clusters.</p><p><img src=https://lh3.googleusercontent.com/CsOiOnIOI_EVNmlsasU47LTyY53GZp1br2Ww83dFf8i5IMvO0o84rOhZEIAt9zTskYvsDKO0rjAwa9WZtkeAw8IE6NmzUEktDTAn8TDvQneD5yg_TQWBmg-qwvWyuU9CYLKzUUkA alt></p><p><strong>Service Discovery</strong></p><p>Service Discovery is possible at Kubernetes by using SkyDNS addon. This is provided as cluster internal service, it is accessible in cluster like ClusterIP. By broadcasting ClusterIP by BGP, name resolution works from outside of clusters. By combination of Image creation workflow and Kubernetes, we built the following tool chain which makes it easy from code push to deployment.</p><p><a href=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/workflow_k8s_all.png><img src=https://s.yimg.jp/images/tecblog/2016-1H/os_n_k8s/workflow_k8s_all.png alt></a></p><p><strong>Summary</strong></p><p>In summary, by combining Image creation workflows and <a href=http://www.kubernetes.io/>Kubernetes</a>, Yahoo! JAPAN, with help from <a href=https://cloud.google.com/>Google</a> and <a href=http://www.solinea.com/>Solinea</a>, successfully built an automated tool chain which makes it easy to go from code push to deployment, while taking multi-tenancy, authn/authz, storage, networking, service discovery and other necessary factors for production deployment. We hope you found the discussion of ecosystem tools used to build the CI/CD pipeline, Kubernetes as a deployment platform on VMs/bare-metal, and the overview of Kubernetes architecture to help you architect and deploy your own clusters. Thank you to all of the people who helped with this project. <em>--Norifumi Matsuya, Hirotaka Ichikawa, Masaharu Miyamoto and Yuta Kinoshita.</em> <em>This post has been translated and edited for context with permission -- originally published on the <a href=http://techblog.yahoo.co.jp/infrastructure/os_n_k8s/>Yahoo! JAPAN engineer blog</a> where this was one in a series of posts focused on Kubernetes.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-2d950d4e9814e2f8994cbf690cc47031>Building Globally Distributed Services using Kubernetes Cluster Federation</h1><div class="td-byline mb-4"><time datetime=2016-10-14 class=text-muted>Friday, October 14, 2016</time></div><p><em>Editor's note: Today’s post is by Allan Naim, Product Manager, and Quinton Hoole, Staff Engineer at Google, showing how to deploy a multi-homed service behind a global load balancer and have requests sent to the closest cluster.</em></p><p>In Kubernetes 1.3, we announced Kubernetes Cluster Federation and introduced the concept of Cross Cluster Service Discovery, enabling developers to deploy a service that was sharded across a federation of clusters spanning different zones, regions or cloud providers. This enables developers to achieve higher availability for their applications, without sacrificing quality of service, as detailed in our <a href=https://kubernetes.io/blog/2016/07/cross-cluster-services>previous</a> blog post.</p><p>In the latest release, <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>Kubernetes 1.4</a>, we've extended Cluster Federation to support Replica Sets, Secrets, Namespaces and Ingress objects. This means that you no longer need to deploy and manage these objects individually in each of your federated clusters. Just create them once in the federation, and have its built-in controllers automatically handle that for you.</p><p><a href=/docs/user-guide/federation/replicasets/><strong>Federated Replica Sets</strong></a> leverage the same configuration as non-federated Kubernetes Replica Sets and automatically distribute Pods across one or more federated clusters. By default, replicas are evenly distributed across all clusters, but for cases where that is not the desired behavior, we've introduced Replica Set preferences, which allow replicas to be distributed across only some clusters, or in non-equal proportions (<a href=https://github.com/kubernetes/kubernetes/blob/master/federation/apis/federation/types.go#L114>define annotations</a>).</p><p>Starting with Google Cloud Platform (GCP), we’ve introduced <a href=/docs/user-guide/federation/federated-ingress/><strong>Federated Ingress</strong></a> as a Kubernetes 1.4 alpha feature which enables external clients point to a single IP address and have requests sent to the closest cluster with usable capacity in any region, zone of the Federation.</p><p><a href=/docs/user-guide/federation/secrets/><strong>Federated Secrets</strong></a> automatically create and manage secrets across all clusters in a Federation, automatically ensuring that these are kept globally consistent and up-to-date, even if some clusters are offline when the original updates are applied.</p><p><a href=/docs/user-guide/federation/namespaces/><strong>Federated Namespaces</strong></a> are similar to the traditional <a href=/docs/user-guide/namespaces/>Kubernetes Namespaces</a> providing the same functionality. Creating them in the Federation control plane ensures that they are synchronized across all the clusters in Federation.</p><p><a href=/docs/user-guide/federation/events/><strong>Federated Events</strong></a> are similar to the traditional Kubernetes Events providing the same functionality. Federation Events are stored only in Federation control plane and are not passed on to the underlying kubernetes clusters.</p><p>Let’s walk through how all this stuff works. We’re going to provision 3 clusters per region, spanning 3 continents (Europe, North America and Asia).</p><p><a href=https://2.bp.blogspot.com/-Gj83DdcKqTI/WAE8pwAEZYI/AAAAAAAAAwI/9dbyBFipvDIGkPQWRB1dRxNwkrvzlcYMwCLcB/s1600/k8s%2Bfed%2Bmap.png><img src=https://2.bp.blogspot.com/-Gj83DdcKqTI/WAE8pwAEZYI/AAAAAAAAAwI/9dbyBFipvDIGkPQWRB1dRxNwkrvzlcYMwCLcB/s400/k8s%2Bfed%2Bmap.png alt></a></p><p>The next step is to federate these clusters. Kelsey Hightower developed a <a href=https://github.com/kelseyhightower/kubernetes-cluster-federation>tutorial</a> for setting up a Kubernetes Cluster Federation. Follow the tutorial to configure a Cluster Federation with clusters in 3 zones in each of the 3 GCP regions, us-central1, europe-west1 and asia-east1. For the purpose of this blog post, we’ll provision the Federation Control Plane in the us-central1-b zone. Note that more highly available, multi-cluster deployments are also available, but not used here in the interests of simplicity.</p><p>The rest of the blog post assumes that you have a running Kubernetes Cluster Federation provisioned.</p><p>Let’s verify that we have 9 clusters in 3 regions running.</p><pre><code>$ kubectl --context=federation-cluster get clusters


NAME              STATUS    AGE  
gce-asia-east1-a     Ready     17m  
gce-asia-east1-b     Ready     15m  
gce-asia-east1-c     Ready     10m  
gce-europe-west1-b   Ready     7m  
gce-europe-west1-c   Ready     7m  
gce-europe-west1-d   Ready     4m  
gce-us-central1-a    Ready     1m  
gce-us-central1-b    Ready     53s  
gce-us-central1-c    Ready     39s
</code></pre><table><thead><tr><th>You can download the source used in this blog post <a href=https://github.com/allannaim/federated-ingress-sample>here</a>. The source consists of the following files:</th><th></th></tr></thead><tbody><tr><td>configmaps/zonefetch.yaml</td><td>retrieves the zone from the instance metadata server and concatenates into volume mount path</td></tr><tr><td>replicasets/nginx-rs.yaml</td><td>deploys a Pod consisting of an nginx and busybox container</td></tr><tr><td>ingress/ingress.yaml</td><td>creates a load balancer with a global VIP that distributes requests to the closest nginx backend</td></tr><tr><td>services/nginx.yaml</td><td>exposes the nginx backend as an external service</td></tr></tbody></table><p>In our example, we’ll be deploying the service and ingress object using the federated control plane. The <a href=/docs/user-guide/configmap/>ConfigMap</a> object isn’t currently supported by Federation, so we’ll be deploying it manually in each of the underlying Federation clusters. Our cluster deployment will look as follows:</p><p>We’re going to deploy a Service that is sharded across our 9 clusters. The backend deployment will consist of a Pod with 2 containers:</p><ul><li>busybox container that fetches the zone and outputs an HTML with the zone embedded in it into a Pod volume mount path</li><li>nginx container that reads from that Pod volume mount path and serves an HTML containing the zone it’s running in</li></ul><p>Let’s start by creating a federated service object in the federation-cluster context.</p><pre><code>$ kubectl --context=federation-cluster create -f services/nginx.yaml
</code></pre><p>It will take a few minutes for the service to propagate across the 9 clusters.</p><pre><code>$ kubectl --context=federation-cluster describe services nginx


Name:                   nginx  
Namespace:              default  
Labels:                 app=nginx  
Selector:               app=nginx  
Type:                   LoadBalancer  
IP:  
LoadBalancer Ingress:   108.59.xx.xxx, 104.199.xxx.xxx, ...  
Port:                   http    80/TCP

NodePort:               http    30061/TCP  
Endpoints:              &lt;none&gt;  
Session Affinity:       None
</code></pre><p>Let’s now create a Federated Ingress. Federated Ingresses are created in much that same way as traditional <a href=/docs/user-guide/ingress/>Kubernetes Ingresses</a>: by making an API call which specifies the desired properties of your logical ingress point. In the case of Federated Ingress, this API call is directed to the Federation API endpoint, rather than a Kubernetes cluster API endpoint. The API for Federated Ingress is 100% compatible with the API for traditional Kubernetes Services.</p><pre><code>$ cat ingress/ingress.yaml   

apiVersion: extensions/v1beta1  
kind: Ingress  
metadata:  
  name: nginx  
spec:  
  backend:  
    serviceName: nginx  
    servicePort: 80
</code></pre><pre><code>$ kubectl --context=federation-cluster create -f ingress/ingress.yaml   
ingress &quot;nginx&quot; created
</code></pre><p>Once created, the Federated Ingress controller automatically:</p><ol><li>1.creates matching Kubernetes Ingress objects in every cluster underlying your Cluster Federation</li><li>2.ensures that all of these in-cluster ingress objects share the same logical global L7 (i.e. HTTP(S)) load balancer and IP address</li><li>3.monitors the health and capacity of the service “shards” (i.e. your Pods) behind this ingress in each cluster</li><li>4.ensures that all client connections are routed to an appropriate healthy backend service endpoint at all times, even in the event of Pod, cluster, availability zone or regional outages
We can verify the ingress objects are matching in the underlying clusters. Notice the ingress IP addresses for all 9 clusters is the same.</li></ol><pre><code>$ for c in $(kubectl config view -o jsonpath='{.contexts[*].name}'); do kubectl --context=$c get ingress; done  

NAME      HOSTS     ADDRESS   PORTS     AGE  
nginx     \*                   80        1h  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        40m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        1h  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        26m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        1h  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        25m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        38m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        3m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        57m  
NAME      HOSTS     ADDRESS          PORTS     AGE  
nginx     \*         130.211.40.xxx   80        56m
</code></pre><p>Note that in the case of Google Cloud Platform, the logical L7 load balancer is not a single physical device (which would present both a single point of failure, and a single global network routing choke point), but rather a <a href=https://cloud.google.com/load-balancing/>truly global, highly available load balancing managed service</a>, globally reachable via a single, static IP address.</p><p>Clients inside your federated Kubernetes clusters (i.e. Pods) will be automatically routed to the cluster-local shard of the Federated Service backing the Ingress in their cluster if it exists and is healthy, or the closest healthy shard in a different cluster if it does not. Note that this involves a network trip to the HTTP(S) load balancer, which resides outside your local Kubernetes cluster but inside the same GCP region.</p><p>The next step is to schedule the service backends. Let’s first create the ConfigMap in each cluster in the Federation.</p><p>We do this by submitting the ConfigMap to each cluster in the Federation.</p><pre><code>$ for c in $(kubectl config view -o jsonpath='{.contexts[\*].name}'); do kubectl --context=$c create -f configmaps/zonefetch.yaml; done
</code></pre><p>Let’s have a quick peek at our Replica Set:</p><pre><code>$ cat replicasets/nginx-rs.yaml


apiVersion: extensions/v1beta1  
kind: ReplicaSet  
metadata:  
  name: nginx  
  labels:  
    app: nginx  
    type: demo  
spec:  
  replicas: 9  
  template:  
    metadata:  
      labels:  
        app: nginx  
    spec:  
      containers:  
      - image: nginx  
        name: frontend  
        ports:  
          - containerPort: 80  
        volumeMounts:  
        - name: html-dir  
          mountPath: /usr/share/nginx/html  
      - image: busybox  
        name: zone-fetcher  
        command:  
          - &quot;/bin/sh&quot;  
          - &quot;-c&quot;  
          - &quot;/zonefetch/zonefetch.sh&quot;  
        volumeMounts:  
        - name: zone-fetch  
          mountPath: /zonefetch  
        - name: html-dir  
          mountPath: /usr/share/nginx/html  
      volumes:  
        - name: zone-fetch  
          configMap:  
            defaultMode: 0777  
            name: zone-fetch  
        - name: html-dir  
          emptyDir:  
            medium: &quot;&quot;
</code></pre><p>The Replica Set consists of 9 replicas, spread evenly across 9 clusters within the Cluster Federation. Annotations can also be used to control which clusters Pods are scheduled to. This is accomplished by adding annotations to the Replica Set spec, as follows:</p><pre><code>apiVersion: extensions/v1beta1  
kind: ReplicaSet  
metadata:  
  name: nginx-us  
  annotations:  
    federation.kubernetes.io/replica-set-preferences: ```  
        {  
            &quot;rebalance&quot;: true,  
            &quot;clusters&quot;: {  
                &quot;gce-us-central1-a&quot;: {  
                    &quot;minReplicas&quot;: 2,  
                    &quot;maxReplicas&quot;: 4,  
                    &quot;weight&quot;: 1  
                },  
                &quot;gce-us-central10b&quot;: {  
                    &quot;minReplicas&quot;: 2,  
                    &quot;maxReplicas&quot;: 4,  
                    &quot;weight&quot;: 1  
                }  
            }  
        }
</code></pre><p>For the purpose of our demo, we’ll keep things simple and spread our Pods evenly across the Cluster Federation.</p><p>Let’s create the federated Replica Set:</p><pre><code>$ kubectl --context=federation-cluster create -f replicasets/nginx-rs.yaml
</code></pre><p>Verify the Replica Sets and Pods were created in each cluster:</p><pre><code>$ for c in $(kubectl config view -o jsonpath='{.contexts[\*].name}'); do kubectl --context=$c get rs; done  

NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         42s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         14m  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         45s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         46s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         47s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         48s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         49s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         49s  
NAME      DESIRED   CURRENT   READY     AGE  
nginx     1         1         1         49s


$ for c in $(kubectl config view -o jsonpath='{.contexts[\*].name}'); do kubectl --context=$c get po; done  

NAME          READY     STATUS    RESTARTS   AGE  
nginx-ph8zx   2/2       Running   0          25s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-sbi5b   2/2       Running   0          27s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-pf2dr   2/2       Running   0          28s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-imymt   2/2       Running   0          30s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-9cd5m   2/2       Running   0          31s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-vxlx4   2/2       Running   0          33s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-itagl   2/2       Running   0          33s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-u7uyn   2/2       Running   0          33s  
NAME          READY     STATUS    RESTARTS   AGE  
nginx-i0jh6   2/2       Running   0          34s
</code></pre><p>Below is an illustration of how the nginx service and associated ingress deployed. To summarize, we have a global VIP (130.211.23.176) exposed using a Global L7 load balancer that forwards requests to the closest cluster with available capacity.</p><p><a href=https://1.bp.blogspot.com/-vDz5dEG_-yI/WAE81YPVlYI/AAAAAAAAAwM/jvt46qwIViQbsbftCqFenUocGfssuLbjwCLcB/s1600/Copy%2Bof%2BFederation%2BBlog%2BDrawing%2B%25281%2529.png><img src=https://1.bp.blogspot.com/-vDz5dEG_-yI/WAE81YPVlYI/AAAAAAAAAwM/jvt46qwIViQbsbftCqFenUocGfssuLbjwCLcB/s640/Copy%2Bof%2BFederation%2BBlog%2BDrawing%2B%25281%2529.png alt></a></p><p>To test this out, we’re going to spin up 2 Google Cloud Engine (GCE) instances, one in us-west1-b and the other in asia-east1-a. All client requests are automatically routed, via the shortest network path, to a healthy Pod in the closest cluster to the origin of the request. So for example, HTTP(S) requests from Asia will be routed directly to the closest cluster in Asia that has available capacity. If there are no such clusters in Asia, the request will be routed to the next closest cluster (in this case the U.S.). This works irrespective of whether the requests originate from a GCE instance or anywhere else on the internet. We only use a GCE instance for simplicity in the demo.</p><p><img src=https://lh5.googleusercontent.com/GcJh6wsYfi8Kjb94g7tS1sOtWqDGohfcthfuEu1mqAkRe7zktmthFjkUke3jY_KqZi-T4wgENBkdZyrCYNV5PjaykF-a1f0io2RRaptBndrxY0qiFM1Q7O089B46JVeZwP__ImXg alt></p><p>We can SSH directly into the VMs using the Cloud Console or by issuing a gcloud SSH command.</p><pre><code>$ gcloud compute ssh test-instance-asia --zone asia-east1-a

-----

user@test-instance-asia:~$ curl 130.211.40.186  
&lt;!DOCTYPE html&gt;  
&lt;html&gt;  
&lt;head&gt;  
&lt;title&gt;Welcome to the global site!&lt;/title&gt;  
&lt;/head&gt;  
&lt;body&gt;  
&lt;h1&gt;Welcome to the global site! You are being served from asia-east1-b&lt;/h1&gt;  
&lt;p&gt;Congratulations!&lt;/p&gt;


user@test-instance-asia:~$ exit

----


$ gcloud compute ssh test-instance-us --zone us-west1-b

----

user@test-instance-us:~$ curl 130.211.40.186  
&lt;!DOCTYPE html&gt;  
&lt;html&gt;  
&lt;head&gt;  
&lt;title&gt;Welcome to the global site!&lt;/title&gt;  
&lt;/head&gt;  
&lt;body&gt;  
&lt;h1&gt;Welcome to the global site! You are being served from us-central1-b&lt;/h1&gt;  
&lt;p&gt;Congratulations!&lt;/p&gt;


----
</code></pre><p>Federations of Kubernetes Clusters can include clusters running in different cloud providers (e.g. GCP, AWS), and on-premises (e.g. on OpenStack). However, in Kubernetes 1.4, Federated Ingress is only supported across Google Cloud Platform clusters. In future versions we intend to support hybrid cloud Ingress-based deployments.</p><p>To summarize, we walked through leveraging the Kubernetes 1.4 Federated Ingress alpha feature to deploy a multi-homed service behind a global load balancer. External clients point to a single IP address and are sent to the closest cluster with usable capacity in any region, zone of the Federation, providing higher levels of availability without sacrificing latency or ease of operation.</p><p>We'd love to hear feedback on Kubernetes Cross Cluster Services. To join the community:</p><ul><li>Post issues or feature requests on <a href=https://github.com/kubernetes/kubernetes/tree/master/federation>GitHub</a></li><li>Join us in the #federation channel on <a href=https://kubernetes.slack.com/messages/sig-federation>Slack</a></li><li>Participate in the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-federation>Cluster Federation SIG</a></li><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Follow Kubernetes on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-16fb1c5dead23666e30b1418a94197dd>Helm Charts: making it simple to package and deploy common applications on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-10-10 class=text-muted>Monday, October 10, 2016</time></div><p>There are thousands of people and companies packaging their applications for deployment on Kubernetes. This usually involves crafting a few different Kubernetes resource definitions that configure the application runtime, as well as defining the mechanism that users and other apps leverage to communicate with the application. There are some very common applications that users regularly look for guidance on deploying, such as databases, CI tools, and content management systems. These types of applications are usually not ones that are developed and iterated on by end users, but rather their configuration is customized to fit a specific use case. Once that application is deployed users can link it to their existing systems or leverage their functionality to solve their pain points.</p><p>For best practices on how these applications should be configured, users could look at the many resources available such as: the <a href=https://github.com/kubernetes/kubernetes/tree/master/examples>examples folder</a> in the Kubernetes repository, the Kubernetes <a href=https://github.com/kubernetes/contrib>contrib repository</a>, the <a href=https://github.com/helm/charts>Helm Charts repository</a>, and the <a href=https://github.com/bitnami/charts>Bitnami Charts repository</a>. While these different locations provided guidance, it was not always formalized or consistent such that users could leverage similar installation procedures across different applications.</p><p>So what do you do when there are too many places for things to be found?</p><p><a href=https://lh5.googleusercontent.com/l6CowJsfGRoH2wgWHlxtId4Foil2Fcs7AZ0NbOT7jGrXliESRSc6jNH8bdMmfpU-_gDRqy9UDSYCj7WaSKF1ZLK1a7t2qNo5JaIOglozee2SDIPteuOZ6aHzNMyBBJXukBv0zF9x><img src=https://lh5.googleusercontent.com/l6CowJsfGRoH2wgWHlxtId4Foil2Fcs7AZ0NbOT7jGrXliESRSc6jNH8bdMmfpU-_gDRqy9UDSYCj7WaSKF1ZLK1a7t2qNo5JaIOglozee2SDIPteuOZ6aHzNMyBBJXukBv0zF9x alt></a></p><p><a href=https://xkcd.com/927/>xkcd Standards</a></p><p>In this case, we’re not creating Yet Another Place for Applications, rather promoting an existing one as the canonical location. As part of the Special Interest Group Apps (<a href=https://github.com/kubernetes/community/tree/master/sig-apps>SIG Apps</a>) work for the <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>Kubernetes 1.4 release</a>, we began to provide a home for these Kubernetes deployable applications that provides continuous releases of well documented and user friendly packages. These packages are being created as Helm <a href=https://github.com/kubernetes/helm/blob/master/docs/charts.md><strong>Charts</strong></a> and can be installed using the Helm tool. <strong><a href=https://github.com/kubernetes/helm>Helm</a></strong> allows users to easily templatize their Kubernetes manifests and provide a set of configuration parameters that allows users to customize their deployment.</p><p><strong>Helm is the package manager</strong> (analogous to yum and apt) and <strong>Charts are packages</strong> (analogous to debs and rpms). The home for these Charts is the <a href=https://github.com/kubernetes/charts>Kubernetes Charts repository</a> which provides continuous integration for pull requests, as well as automated releases of Charts in the master branch.</p><p>There are two main folders where charts reside. The <a href=https://github.com/kubernetes/charts/tree/master/stable>stable folder</a> hosts those applications which meet minimum requirements such as proper documentation and inclusion of only Beta or higher Kubernetes resources. The <a href=https://github.com/kubernetes/charts/tree/master/incubator>incubator folder</a> provides a place for charts to be submitted and iterated on until they’re ready for promotion to stable at which time they will automatically be pushed out to the default repository. For more information on the repository structure and requirements for being in stable, have a look at <a href=https://github.com/kubernetes/charts#repository-structure>this section in the README</a>.</p><p>The following applications are now available:</p><table><thead><tr><th>Stable repository</th><th>Incubating repository</th></tr></thead><tbody><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/drupal>Drupal</a></td><td><a href=https://github.com/kubernetes/charts/tree/master/incubator/consul>Consul</a></td></tr><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/jenkins>Jenkins</a></td><td><a href=https://github.com/kubernetes/charts/tree/master/incubator/elasticsearch>Elasticsearch</a></td></tr><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/mariadb>MariaDB</a></td><td><a href=https://github.com/kubernetes/charts/tree/master/incubator/etcd>etcd</a></td></tr><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/mysql>MySQL</a></td><td><a href=https://github.com/helm/charts/tree/master/stable/grafana>Grafana</a></td></tr><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/redmine>Redmine</a></td><td><a href=https://github.com/helm/charts/tree/master/stable/mongodb>MongoDB</a></td></tr><tr><td><a href=https://github.com/kubernetes/charts/tree/master/stable/wordpress>Wordpress</a></td><td><a href=https://github.com/kubernetes/charts/tree/master/incubator/patroni>Patroni</a></td></tr><tr><td></td><td><a href=https://github.com/helm/charts/tree/master/stable/prometheus>Prometheus</a></td></tr><tr><td></td><td><a href=https://github.com/helm/charts/tree/master/stable/spark>Spark</a></td></tr><tr><td></td><td><a href=https://github.com/kubernetes/charts/tree/master/incubator/zookeeper>ZooKeeper</a></td></tr></tbody></table><p><strong>Example workflow for a Chart developer</strong></p><ol><li><a href=https://github.com/kubernetes/helm/blob/master/docs/charts.md>Create a chart</a></li><li>Developer provides parameters via the <a href=https://github.com/kubernetes/helm/blob/master/docs/charts.md#values-files>values.yaml</a> file allowing users to customize their deployment. This can be seen as the API between chart devs and chart users.</li><li>A <a href=https://github.com/kubernetes/charts/tree/master/stable/mariadb>README</a> is written to help describe the application and its parameterized values.</li><li>Once the application installs properly and the values customize the deployment appropriately, the developer adds a <a href=https://github.com/helm/helm/blob/dev-v2/docs/charts.md>NOTES.txt</a> file that is shown as soon as the user installs. This file generally points out the next steps for the user to connect to or use the application.</li><li>If the application requires persistent storage, the developer adds a mechanism to store the data such that pod restarts do not lose data. Most charts requiring this today are using <a href=https://kubernetes.io/blog/2016/10/dynamic-provisioning-and-storage-in-kubernetes>dynamic volume provisioning</a> to abstract away underlying storage details from the user which allows a single configuration to work against Kubernetes installations.</li><li>Submit a <a href=https://github.com/kubernetes/charts/pulls>Pull Request to the Kubernetes Charts repo</a>. Once tested and reviewed, the PR will be merged.</li><li>Once merged to the master branch, the chart will be packaged and released to Helm’s default repository and available for users to install.</li></ol><p><strong>Example workflow for a Chart user</strong></p><ol><li>1.<a href=https://helm.sh/docs/intro/quickstart/#install-helm>Install Helm</a></li><li>2.<a href=https://helm.sh/docs/intro/quickstart/#install-an-example-chart>Initialize Helm</a></li><li>3.<a href=https://helm.sh/docs/intro/using_helm/#helm-search-finding-charts>Search for a chart</a></li></ol><pre><code>$ helm search  
NAME VERSION DESCRIPTION stable/drupal 0.3.1 One of the most versatile open source content m...stable/jenkins 0.1.0 A Jenkins Helm chart for Kubernetes. stable/mariadb 0.4.0 Chart for MariaDB stable/mysql 0.1.0 Chart for MySQL stable/redmine 0.3.1 A flexible project management web application. stable/wordpress 0.3.0 Web publishing platform for building blogs and ...
</code></pre><ol start=4><li>4.<a href=https://helm.sh/docs/intro/using_helm/#helm-install-installing-a-package>Install the chart</a></li></ol><pre><code>$ helm install stable/jenkins
</code></pre><ol start=5><li>5.After the install</li></ol><pre><code>Notes:



1. Get your 'admin' user password by running:

  printf $(printf '\%o' `kubectl get secret --namespace default brawny-frog-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password[*]}&quot;`);echo



2. Get the Jenkins URL to visit by running these commands in the same shell:

\*\*\*\* NOTE: It may take a few minutes for the LoadBalancer IP to be available.                      \*\*\*\*

\*\*\*\*       You can watch the status of by running 'kubectl get svc -w brawny-frog-jenkins' \*\*\*\*

  export SERVICE\_IP=$(kubectl get svc --namespace default brawny-frog-jenkins -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

  echo http://$SERVICE\_IP:8080/login
</code></pre><ol start=3><li>Login with the password from step 1 and the username: admin</li></ol><p>For more information on running Jenkins on Kubernetes, visit <a href=https://cloud.google.com/solutions/jenkins-on-container-engine>here</a>.</p><p><strong>Conclusion</strong></p><p>Now that you’ve seen workflows for both developers and users, we hope that you’ll join us in consolidating the breadth of application deployment knowledge into a more centralized place. Together we can raise the quality bar for both developers and users of Kubernetes applications. We’re always looking for feedback on how we can better our process. Additionally, we’re looking for contributions of new charts or updates to existing ones. Join us in the following places to get engaged:</p><ul><li>SIG Apps - <a href=https://kubernetes.slack.com/messages/sig-apps/>Slack Channel</a></li><li>SIG Apps - <a href=https://github.com/kubernetes/community/tree/master/sig-apps#meeting>Weekly Meeting</a></li><li><a href=https://github.com/kubernetes/charts/issues>Submit a Kubernetes Charts Issue</a>
A big thank you to the folks at Bitnami, Deis, Google and the <a href=https://github.com/kubernetes/charts/graphs/contributors>other contributors</a> who have helped get the Charts repository to where it is today. We still have a lot of work to do but it's been wonderful working together as a community to move this effort forward.</li></ul><p><em>--Vic Iglesias, Cloud Solutions Architect, Google</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-92cc300cd30f566a0af0bcdc0a7dacf3>Dynamic Provisioning and Storage Classes in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-10-07 class=text-muted>Friday, October 07, 2016</time></div><p>Storage is a critical part of running containers, and Kubernetes offers some powerful primitives for managing it. Dynamic volume provisioning, a feature unique to Kubernetes, allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users. This feature was introduced as alpha in Kubernetes 1.2, and has been improved and promoted to beta in the <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>latest release, 1.4</a>. This release makes dynamic provisioning far more flexible and useful.</p><p><strong>What’s New?</strong></p><p>The alpha version of dynamic provisioning only allowed a single, hard-coded provisioner to be used in a cluster at once. This meant that when Kubernetes determined storage needed to be dynamically provisioned, it always used the same volume plugin to do provisioning, even if multiple storage systems were available on the cluster. The provisioner to use was inferred based on the cloud environment - EBS for AWS, Persistent Disk for Google Cloud, Cinder for OpenStack, and vSphere Volumes on vSphere. Furthermore, the parameters used to provision new storage volumes were fixed: only the storage size was configurable. This meant that all dynamically provisioned volumes would be identical, except for their storage size, even if the storage system exposed other parameters (such as disk type) for configuration during provisioning.</p><p>Although the alpha version of the feature was limited in utility, it allowed us to “get some miles” on the idea, and helped determine the direction we wanted to take.</p><p>The beta version of dynamic provisioning, new in Kubernetes 1.4, introduces a <a href=/docs/user-guide/persistent-volumes/#storageclasses>new API object</a>, StorageClass. Multiple StorageClass objects can be defined each specifying a volume plugin (aka provisioner) to use to provision a volume and the set of parameters to pass to that provisioner when provisioning. This design allows cluster administrators to define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don’t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options.</p><p><strong>How Do I use It?</strong></p><p>Below is an example of how a cluster administrator would expose two tiers of storage, and how a user would select and use one. For more details, see the <a href=/docs/user-guide/persistent-volumes/#storageclasses>reference</a> and <a href=https://github.com/kubernetes/kubernetes/tree/release-1.4/examples/experimental/persistent-volume-provisioning>example</a> docs.</p><p><strong>Admin Configuration</strong></p><p>The cluster admin defines and deploys two StorageClass objects to the Kubernetes cluster:</p><pre><code>kind: StorageClass

apiVersion: storage.k8s.io/v1beta1

metadata:

  name: slow

provisioner: kubernetes.io/gce-pd

parameters:

  type: pd-standard
</code></pre><p>This creates a storage class called “slow” which will provision standard disk-like Persistent Disks.</p><pre><code>kind: StorageClass

apiVersion: storage.k8s.io/v1beta1

metadata:

  name: fast

provisioner: kubernetes.io/gce-pd

parameters:

  type: pd-ssd
</code></pre><p>This creates a storage class called “fast” which will provision SSD-like Persistent Disks.</p><p><strong>User Request</strong></p><p>Users request dynamically provisioned storage by including a storage class in their PersistentVolumeClaim. For the beta version of this feature, this is done via the volume.beta.kubernetes.io/storage-class annotation. The value of this annotation must match the name of a StorageClass configured by the administrator.</p><p>To select the “fast” storage class, for example, a user would create the following PersistentVolumeClaim:</p><pre><code>{

  &quot;kind&quot;: &quot;PersistentVolumeClaim&quot;,

  &quot;apiVersion&quot;: &quot;v1&quot;,

  &quot;metadata&quot;: {

    &quot;name&quot;: &quot;claim1&quot;,

    &quot;annotations&quot;: {

        &quot;volume.beta.kubernetes.io/storage-class&quot;: &quot;fast&quot;

    }

  },

  &quot;spec&quot;: {

    &quot;accessModes&quot;: [

      &quot;ReadWriteOnce&quot;

    ],

    &quot;resources&quot;: {

      &quot;requests&quot;: {

        &quot;storage&quot;: &quot;30Gi&quot;

      }

    }

  }

}
</code></pre><p>This claim will result in an SSD-like Persistent Disk being automatically provisioned. When the claim is deleted, the volume will be destroyed.</p><p><strong>Defaulting Behavior</strong></p><p>Dynamic Provisioning can be enabled for a cluster such that all claims are dynamically provisioned without a storage class annotation. This behavior is enabled by the cluster administrator by marking one StorageClass object as “default”. A StorageClass can be marked as default by adding the storageclass.beta.kubernetes.io/is-default-class annotation to it.</p><p>When a default StorageClass exists and a user creates a PersistentVolumeClaim without a storage-class annotation, the new <a href=https://github.com/kubernetes/kubernetes/pull/30900>DefaultStorageClass</a> admission controller (also introduced in v1.4), automatically adds the class annotation pointing to the default storage class.</p><p><strong>Can I Still Use the Alpha Version?</strong></p><p>Kubernetes 1.4 maintains backwards compatibility with the alpha version of the dynamic provisioning feature to allow for a smoother transition to the beta version. The alpha behavior is triggered by the existance of the alpha dynamic provisioning annotation (volume. <strong>alpha</strong>.kubernetes.io/storage-class). Keep in mind that if the beta annotation (volume. <strong>beta</strong>.kubernetes.io/storage-class) is present, it takes precedence, and triggers the beta behavior.</p><p>Support for the <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/devel/api_changes.md#alpha-beta-and-stable-versions>alpha version</a> is deprecated and will be removed in a future release.</p><p><strong>What’s Next?</strong></p><p>Dynamic Provisioning and Storage Classes will continue to evolve and be refined in future releases. Below are some areas under consideration for further development.</p><p><strong>Standard Cloud Provisioners</strong></p><p>For deployment of Kubernetes to cloud providers, we are <a href=https://github.com/kubernetes/kubernetes/pull/31617/files>considering</a> automatically creating a provisioner for the cloud’s native storage system. This means that a standard deployment on AWS would result in a StorageClass that provisions EBS volumes, a standard deployment on Google Cloud would result in a StorageClass that provisions GCE PDs. It is also being debated whether these provisioners should be marked as default, which would make dynamic provisioning the default behavior (no annotation required).</p><p><strong>Out-of-Tree Provisioners</strong></p><p>There has been ongoing discussion about whether Kubernetes storage plugins should live “in-tree” or “out-of-tree”. While the details for how to implement out-of-tree plugins is still in the air, there is <a href=https://github.com/kubernetes/kubernetes/pull/30285>a proposa</a>l introducing a standardized way to implement out-of-tree dynamic provisioners.</p><p><strong>How Do I Get Involved?</strong></p><p>If you’re interested in getting involved with the design and development of Kubernetes Storage, join the <a href=https://github.com/kubernetes/community/tree/master/sig-storage>Kubernetes Storage Special-Interest-Group</a> (SIG). We’re rapidly growing and always welcome new contributors.</p><p><em>-- Saad Ali, Software Engineer, Google</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21b445dae15d024753bd2d6582499298>How we improved Kubernetes Dashboard UI in 1.4 for your production needs​</h1><div class="td-byline mb-4"><time datetime=2016-10-03 class=text-muted>Monday, October 03, 2016</time></div><p>With the release of <a href=https://kubernetes.io/blog/2016/09/kubernetes-1-4-making-it-easy-to-run-on-kuberentes-anywhere/>Kubernetes 1.4</a> last week, Dashboard – the official web UI for Kubernetes – has a number of exciting updates and improvements of its own. The past three months have been busy ones for the Dashboard team, and we’re excited to share the resulting features of that effort here. If you’re not familiar with Dashboard, the <a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>GitHub repo</a> is a great place to get started.</p><p>A quick recap before unwrapping our shiny new features: Dashboard was initially released March 2016. One of the focuses for Dashboard throughout its lifetime has been the onboarding experience; it’s a less intimidating way for Kubernetes newcomers to get started, and by showing multiple resources at once, it provides contextualization lacking in <a href=/docs/user-guide/kubectl-overview/>kubectl</a> (the CLI). After that initial release though, the product team realized that fine-tuning for a beginner audience was getting ahead of ourselves: there were still fundamental product requirements that Dashboard needed to satisfy in order to have a productive UX to onboard new users too. That became our mission for this release: closing the gap between Dashboard and kubectl by showing more resources, leveraging a web UI’s strengths in monitoring and troubleshooting, and architecting this all in a user friendly way.</p><p><strong>Monitoring Graphs</strong><br>Real time visualization is a strength that UI’s have over CLI’s, and with 1.4 we’re happy to capitalize on that capability with the introduction of real-time CPU and memory usage graphs for all workloads running on your cluster. Even with the numerous third-party solutions for monitoring, Dashboard should include at least some basic out-of-the box functionality in this area. Next up on the roadmap for graphs is extending the timespan the graph represents, adding drill-down capabilities to reveal more details, and improving the UX of correlating data between different graphs.</p><p><a href=https://lh5.googleusercontent.com/q2xNqiQkdcaAY9UdAlxXJkhofpb-AwMKoxE8Jdd3qRB0v8qffi4_s8GUaszmYGclNemAWCrEmbTqegKPfRoUgYHy9aRAYILXqRX1BCdLBQCUGHd-Euv0PuT5VI9viT3iSXBRHshv><img src=https://lh5.googleusercontent.com/q2xNqiQkdcaAY9UdAlxXJkhofpb-AwMKoxE8Jdd3qRB0v8qffi4_s8GUaszmYGclNemAWCrEmbTqegKPfRoUgYHy9aRAYILXqRX1BCdLBQCUGHd-Euv0PuT5VI9viT3iSXBRHshv alt></a></p><p><strong>Logs</strong><br>Based on user research with Kubernetes’ predecessor <a href=http://research.google.com/pubs/pub43438.html>Borg</a> and continued community feedback, we know logs are tremendously important to users. For this reason we’re constantly looking for ways to improve these features in Dashboard. This release includes a fix for an issue wherein large numbers of logs would crash the system, as well as the introduction of the ability to view logs by date.</p><p><strong>Showing More Resources</strong><br>The previous release brought all workloads to Dashboard: Pods, Pet Sets, Daemon Sets, Replication Controllers, Replica Set, Services, & Deployments. With 1.4, we expand upon that set of objects by including Services, Ingresses, Persistent Volume Claims, Secrets, & Config Maps. We’ve also introduced an “Admin” section with the Namespace-independent global objects of Namespaces, Nodes, and Persistent Volumes. With the addition of roles, these will be shown only to cluster operators, and developers’ side nav will begin with the Namespace dropdown.</p><p>Like glue binding together a loose stack of papers into a book, we needed some way to impose order on these resources for their value to be realized, so one of the features we’re most excited to announce in 1.4 is navigation.</p><p><strong>Navigation</strong><br>In 1.1, all resources were simply stacked on top of each other in a single page. The introduction of a side nav provides quick access to any aspect of your cluster you’d like to check out. Arriving at this solution meant a lot of time put toward thinking about the hierarchy of Kubernetes objects – a difficult task since by design things fit together more like a living organism than a nested set of linear relationships. The solution we’ve arrived at balances the organizational need for grouping and desire to retain a bird’s-eye view of as much relevant information as possible. The design of the side nav is simple and flexible, in order to accommodate more resources in the future. Its top level objects (e.g. “Workloads”, “Services and Discovery”) roll up their child objects and will eventually include aggregated data for said objects.</p><p><a href=https://lh4.googleusercontent.com/wam1i4Y3GGLwNFxynWYK17me9UDCaw3yo0dDqqTt7Y79bJ5YK7uHd3yreRnftPOtRkOvo-CjlWNPEx2raBdCN5JTxG2fU3fwqeIPsDaeuqhnWl0IrSYQ32uC7cVt2q51LQNhialX><img src=https://lh4.googleusercontent.com/wam1i4Y3GGLwNFxynWYK17me9UDCaw3yo0dDqqTt7Y79bJ5YK7uHd3yreRnftPOtRkOvo-CjlWNPEx2raBdCN5JTxG2fU3fwqeIPsDaeuqhnWl0IrSYQ32uC7cVt2q51LQNhialX alt></a></p><p><strong>Closer Alignment with Material Design</strong><br>Dashboard follows Google’s <a href=https://material.google.com/>Material design</a> system, and the implementation of those principles is refined in the new UI: the global create options have been reduced from two choices to one initial “Create” button, the official Kubernetes logo is displayed as an SVG rather than simply as text, and cards were introduced to help better group different types of content (e.g. a table of Replication Controllers and a table of Pods on your “Workloads” page). Material’s guidelines around desktop-focused enterprise-level software are currently limited (and instead focus on a mobile-first context), so we’ve had to improvise with some aspects of the UI and have worked closely with the UX team at Google Cloud Platform to do this – drawing on their expertise in implementing Material in a more information-dense setting.</p><p><strong>Sample Use Case</strong><br>To showcase Dashboard 1.4’s new suite of features and how they’ll make users’ lives better in the real world, let’s imagine the following scenario:</p><p>I am a cluster operator and a customer pings me warning that their app, Kubernetes Dashboard, is suffering performance issues. My first step in addressing the issue is to switch to the correct Namespace, kube-system, to examine what could be going on.</p><p><img src=https://lh5.googleusercontent.com/R95VuEQ8GkjTTeJXX-4EE-f-oD4UXYCPGZ5et4YYLuUiB0K3hXSndyFPYHmrKeySBc2t3tMy4B9mT-dr8rIr0WRQLq4Bhe6ygA4GqNLSYvvZcsmdGxeozw3jr8fSDCinG0NSsAjp alt></p><p>Once in the relevant Namespace, I check out my Deployments to see if anything seems awry. Sure enough, I notice a spike in CPU usage.</p><p><img src=https://lh5.googleusercontent.com/rViAg6xFe219i7qxeBRU62-1SFBLI6VIg3pbU5HBmvIKsb3KJFr5RldP0vziVXao3u-hWM3EMvzTNnSFRQWCTViaQiVbAv_PTjd87s7GOZelroeL4gjcfFU3JljrOKKnWL3Wzy5c alt></p><p>I realize we need to perform a rolling update to a newer version of that app that can handle the increased requests it’s evidently getting, so I update this Deployment’s image, which in turn creates a new <a href=/docs/user-guide/replicasets/>Replica Set</a>.</p><p><img src=https://lh4.googleusercontent.com/RdA8N8LPDwnAb-RDX4MHNmHvxc8YRlID79-5WmGJQb7NYuz8oZseVorzATQZWOTTQ_-yp8roniNKuBqmQewzYzyvBRdHcQf_VENm2Qqde0v6LW9-L1FLmqsUx8h9Z5RYfpD_alXx alt></p><p>Now that Replica Set’s been created, I can open the logs for one of its pods to confirm that it’s been successfully connected to the API server.</p><p><img src=https://lh3.googleusercontent.com/zg_lrCL0kH7ai6ZUGz4YKwIfQpwLXnF-mvK9UUL3TZ4ryNLSCSW7Anha5VjoEdwlkSp8-Fhgz16srzPTpoHzguwrGllPp10m2O_rFAfm2W1tq_5ow4FzfAwYVM4Sm1-HuMtcDY34 alt></p><p>Easy as that, we’ve debugged our issue. Dashboard provided us a centralized location to scan for the origin of the problem, and once we had that identified we were able to drill down and address the root of the problem.</p><p><strong>Why the Skipped Versions?</strong><br>If you’ve been following along with Dashboard since 1.0,  you may have been confused by the jump in our versioning; we went 1.0, 1.1...1.4. We did this to synchronize with the main Kubernetes distro, and hopefully going forward this will make that relationship easier to understand.</p><p><strong>There’s a Lot More Where That Came From</strong><br>Dashboard is gaining momentum, and these early stages are a very exciting and rewarding time to be involved. If you’d like to learn more about contributing, check out <a href=https://github.com/kubernetes/community/blob/master/sig-ui/README.md>SIG UI</a>. Chat with us Kubernetes Slack: <a href=https://kubernetes.slack.com/messages/sig-ui/>#sig-ui channel</a>.</p><p><em>--Dan Romlein, UX designer, Apprenda</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-443d8a6c83042d3ba3f3b04ea096de55>How we made Kubernetes insanely easy to install</h1><div class="td-byline mb-4"><time datetime=2016-09-28 class=text-muted>Wednesday, September 28, 2016</time></div><p><em>Editor's note: Today’s post is by <a href=https://twitter.com/lmarsden>Luke Marsden</a>, Head of Developer Experience, at Weaveworks, showing the Special Interest Group Cluster-Lifecycle’s recent work on kubeadm, a tool to make installing Kubernetes much simpler.</em></p><p>Over at <a href=https://github.com/kubernetes/community/blob/master/sig-cluster-lifecycle/README.md>SIG-cluster-lifecycle</a>, we've been hard at work the last few months on kubeadm, a tool that makes Kubernetes dramatically easier to install. We've heard from users that installing Kubernetes is harder than it should be, and we want folks to be focused on writing great distributed apps not wrangling with infrastructure!</p><p>There are three stages in setting up a Kubernetes cluster, and we decided to focus on the second two (to begin with):</p><ol><li><strong>Provisioning</strong> : getting some machines</li><li><strong>Bootstrapping</strong> : installing Kubernetes on them and configuring certificates</li><li><strong>Add-ons</strong> : installing necessary cluster add-ons like DNS and monitoring services, a pod network, etc
We realized early on that there's enormous variety in the way that users want to <strong>provision</strong> their machines.</li></ol><p>They use lots of different cloud providers, private clouds, bare metal, or even Raspberry Pi's, and almost always have their own preferred tools for automating provisioning machines: Terraform or CloudFormation, Chef, Puppet or Ansible, or even PXE booting bare metal. So we made an important decision: <strong>kubeadm would not provision machines</strong>. Instead, the only assumption it makes is that the user has some <a href=/docs/getting-started-guides/kubeadm/#prerequisites>computers running Linux</a>.</p><p>Another important constraint was we didn't want to just build another tool that "configures Kubernetes from the outside, by poking all the bits into place". There are many external projects out there for doing this, but we wanted to aim higher. We chose to actually improve the Kubernetes core itself to make it easier to install. Luckily, a lot of the groundwork for making this happen had already been started.</p><p>We realized that if we made Kubernetes insanely easy to install manually, it should be obvious to users how to automate that process using any tooling.</p><p>So, enter <a href=/docs/getting-started-guides/kubeadm/>kubeadm</a>. It has no infrastructure dependencies, and satisfies the requirements above. It's easy to use and should be easy to automate. It's still in <strong>alpha</strong> , but it works like this:</p><ul><li>You install Docker and the official Kubernetes packages for you distribution.</li><li>Select a master host, run kubeadm init.</li><li>This sets up the control plane and outputs a kubeadm join [...] command which includes a secure token.</li><li>On each host selected to be a worker node, run the kubeadm join [...] command from above.</li><li>Install a pod network. <a href=https://github.com/weaveworks/weave-kube>Weave Net</a> is a great place to start here. Install it using just kubectl apply -f <a href=https://git.io/weave-kube>https://git.io/weave-kube</a>
Presto! You have a working Kubernetes cluster! <a href=/docs/getting-started-guides/kubeadm/>Try kubeadm today</a>. </li></ul><p>For a video walkthrough, check this out:</p><p>Follow the <a href=/docs/getting-started-guides/kubeadm/>kubeadm getting started guide</a> to try it yourself, and please give us <a href=https://github.com/kubernetes/kubernetes/issues/new>feedback on GitHub</a>, mentioning <strong>@kubernetes/sig-cluster-lifecycle</strong>!</p><p>Finally, I want to give a huge shout-out to so many people in the SIG-cluster-lifecycle, without whom this wouldn't have been possible. I'll mention just a few here:</p><ul><li><a href=https://twitter.com/jbeda>Joe Beda</a> kept us focused on keeping things simple for the user.</li><li><a href=https://twitter.com/errordeveloper>Mike Danese</a> at Google has been an incredible technical lead and always knows what's happening. Mike also tirelessly kept up on the many code reviews necessary.</li><li><a href=https://twitter.com/errordeveloper>Ilya Dmitrichenko</a>, my colleague at Weaveworks, wrote most of the kubeadm code and also kindly helped other folks contribute.</li><li><a href=https://twitter.com/kubernetesonarm>Lucas Käldström</a> from Finland has got to be the youngest contributor in the group and was merging last-minute pull requests on the Sunday night before his school math exam.</li><li><a href=https://twitter.com/brandonphilips>Brandon Philips</a> and his team at CoreOS led the development of TLS bootstrapping, an essential component which we couldn't have done without.</li><li><a href=https://twitter.com/dgood>Devan Goodwin</a> from Red Hat built the JWS discovery service that Joe imagined and sorted out our RPMs.</li><li><a href=https://twitter.com/el_ppires>Paulo Pires</a> from Portugal jumped in to help out with external etcd support and picked up lots of other bits of work.</li><li>And many other contributors!</li></ul><p>This truly has been an excellent cross-company and cross-timezone achievement, with a lovely bunch of people. There's lots more work to do in SIG-cluster-lifecycle, so if you’re interested in these challenges join our SIG. Looking forward to collaborating with you all!</p><p><em>--<a href=https://twitter.com/lmarsden>Luke Marsden</a>, Head of Developer Experience at <a href=https://twitter.com/weaveworks>Weaveworks</a></em></p><ul><li>Try <a href=/docs/getting-started-guides/kubeadm/>kubeadm</a> to install Kubernetes today</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd677efd34376db88d210c2ea44e4f7e>How Qbox Saved 50% per Month on AWS Bills Using Kubernetes and Supergiant</h1><div class="td-byline mb-4"><time datetime=2016-09-27 class=text-muted>Tuesday, September 27, 2016</time></div><p><em>Editor’s Note: Today’s post is by the team at Qbox, a hosted Elasticsearch provider sharing their experience with Kubernetes and how it helped save them fifty-percent off their cloud bill. </em></p><p>A little over a year ago, we at Qbox faced an existential problem. Just about all of the major IaaS providers either launched or acquired services that competed directly with our <a href=https://qbox.io/>Hosted Elasticsearch</a> service, and many of them started offering it for free. The race to zero was afoot unless we could re-engineer our infrastructure to be more performant, more stable, and less expensive than the VM approach we had had before, and the one that is in use by our IaaS brethren. With the help of Kubernetes, Docker, and Supergiant (our own hand-rolled layer for managing distributed and stateful data), we were able to deliver 50% savings, a mid-five figure sum. At the same time, support tickets plummeted. We were so pleased with the results that we decided to <a href=https://github.com/supergiant/supergiant>open source Supergiant</a> as its own standalone product. This post will demonstrate how we accomplished it.</p><p>Back in 2013, when not many were even familiar with Elasticsearch, we launched our as-a-service offering with a dedicated, direct VM model. We hand-selected certain instance types optimized for Elasticsearch, and users configured single-tenant, multi-node clusters running on isolated virtual machines in any region. We added a markup on the per-compute-hour price for the DevOps support and monitoring, and all was right with the world for a while as Elasticsearch became the global phenomenon that it is today.</p><p><strong>Background</strong><br>As we grew to thousands of clusters, and many more thousands of nodes, it wasn’t just our AWS bill getting out of hand. We had 4 engineers replacing dead nodes and answering support tickets all hours of the day, every day. What made matters worse was the volume of resources allocated compared to the usage. We had thousands of servers with a collective CPU utilization under 5%. We were spending too much on processors that were doing absolutely nothing. </p><p>How we got there was no great mystery. VM’s are a finite resource, and with a very compute-intensive, burstable application like Elasticsearch, we would be juggling the users that would either undersize their clusters to save money or those that would over-provision and overspend. When the aforementioned competitive pressures forced our hand, we had to re-evaluate everything.</p><p><strong>Adopting Docker and Kubernetes</strong><br>Our team avoided Docker for a while, probably on the vague assumption that the network and disk performance we had with VMs wouldn't be possible with containers. That assumption turned out to be entirely wrong.</p><p>To run performance tests, we had to find a system that could manage networked containers and volumes. That's when we discovered Kubernetes. It was alien to us at first, but by the time we had familiarized ourselves and built a performance testing tool, we were sold. It was not just as good as before, it was better.</p><p>The performance improvement we observed was due to the number of containers we could “pack” on a single machine. Ironically, we began the Docker experiment wanting to avoid “noisy neighbor,” which we assumed was inevitable when several containers shared the same VM. However, that isolation also acted as a bottleneck, both in performance and cost. To use a real-world example, If a machine has 2 cores and you need 3 cores, you have a problem. It’s rare to come across a public-cloud VM with 3 cores, so the typical solution is to buy 4 cores and not utilize them fully.</p><p>This is where Kubernetes really starts to shine. It has the concept of <a href=/docs/user-guide/compute-resources/>requests and limits</a>, which provides granular control over resource sharing. Multiple containers can share an underlying host VM without the fear of “noisy neighbors”. They can request exclusive control over an amount of RAM, for example, and they can define a limit in anticipation of overflow. It’s practical, performant, and cost-effective multi-tenancy. We were able to deliver the best of both the single-tenant and multi-tenant worlds.</p><p><strong>Kubernetes + Supergiant</strong><br>We built <a href=https://supergiant.io/>Supergiant</a> originally for our own Elasticsearch customers. Supergiant solves Kubernetes complications by allowing pre-packaged and re-deployable application topologies. In more specific terms, Supergiant lets you use Components, which are somewhat similar to a microservice. Components represent an almost-uniform set of Instances of software (e.g., Elasticsearch, MongoDB, your web application, etc.). They roll up all the various Kubernetes and cloud operations needed to deploy a complex topology into a compact entity that is easy to manage.</p><p>For Qbox, we went from needing 1:1 nodes to approximately 1:11 nodes. Sure, the nodes were larger, but the utilization made a substantial difference. As in the picture below, we could cram a whole bunch of little instances onto one big instance and not lose any performance. Smaller users would get the added benefit of higher network throughput by virtue of being on bigger resources, and they would also get greater CPU and RAM bursting.</p><p><img src=https://lh6.googleusercontent.com/vV7vzgS8fl-wJSTVbtE7aveWwBf2kXH348ItU0uvakWa-TeO9sJQxr9IuccNa1L9NOLqIBWEDg1ASgChjoBdeDgkiazJ0z9x4r439YukgVz3yOXcouCZXaPjMQOXjmWTm8tBBOqh alt=sg-example.png></p><p><strong>Adding Up the Cost Savings</strong><br>The <a href=https://supergiant.io/blog/supergiant-packing-algorithm-unique-save-money>packing algorithm</a> in Supergiant, with its increased utilization, resulted in an immediate 25% drop in our infrastructure footprint. Remember, this came with better performance and fewer support tickets. We could dial up the packing algorithm and probably save even more money. Meanwhile, because our nodes were larger and far more predictable, we could much more fully leverage the economic goodness that is AWS Reserved Instances. We went with 1-year partial RI’s, which cut the remaining costs by 40%, give or take. Our customers still had the flexibility to spin up, down, and out their Elasticsearch nodes, without forcing us to constantly juggle, combine, split, and recombine our reservations. At the end of the day, we saved 50%. That is $600k per year that can go towards engineering salaries instead of enriching our IaaS provider. </p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e90162db2220a568bae8d0960d896400>Kubernetes 1.4: Making it easy to run on Kubernetes anywhere</h1><div class="td-byline mb-4"><time datetime=2016-09-26 class=text-muted>Monday, September 26, 2016</time></div><p>Today we’re happy to announce the release of Kubernetes 1.4.</p><p>Since the release to general availability just over 15 months ago, Kubernetes has continued to grow and achieve broad adoption across the industry. From brand new startups to large-scale businesses, users have described how big a difference Kubernetes has made in building, deploying and managing distributed applications. However, one of our top user requests has been making Kubernetes itself easier to install and use. We’ve taken that feedback to heart, and 1.4 has several major improvements.</p><p>These setup and usability enhancements are the result of concerted, coordinated work across the community - more than 20 contributors from SIG-Cluster-Lifecycle came together to greatly simplify the Kubernetes user experience, covering improvements to installation, startup, certificate generation, discovery, networking, and application deployment.</p><p>Additional product highlights in this release include simplified cluster deployment on any cloud, easy installation of stateful apps, and greatly expanded Cluster Federation capabilities, enabling a straightforward deployment across multiple clusters, and multiple clouds.</p><p><strong>What’s new:</strong></p><p><strong>Cluster creation with two commands -</strong> To get started with Kubernetes a user must provision nodes, install Kubernetes and bootstrap the cluster. A common request from users is to have an easy, portable way to do this on any cloud (public, private, or bare metal).</p><ul><li>Kubernetes 1.4 introduces ‘<a href=/docs/getting-started-guides/kubeadm/>kubeadm</a>’ which reduces bootstrapping to two commands, with no complex scripts involved. Once kubernetes is installed, kubeadm init starts the master while kubeadm join joins the nodes to the cluster.</li><li>Installation is also streamlined by packaging Kubernetes with its dependencies, for most major Linux distributions including Red Hat and Ubuntu Xenial. This means users can now install Kubernetes using familiar tools such as apt-get and yum.</li><li>Add-on deployments, such as for an overlay network, can be reduced to one command by using a <a href=/docs/admin/daemons/>DaemonSet</a>.</li><li>Enabling this simplicity is a new certificates API and its use for kubelet <a href=/docs/admin/master-node-communication/#kubelet-tls-bootstrap>TLS bootstrap</a>, as well as a new discovery API.</li></ul><p><strong>Expanded stateful application support -</strong> While cloud-native applications are built to run in containers, many existing applications need additional features to make it easy to adopt containers. Most commonly, these include stateful applications such as batch processing, databases and key-value stores. In Kubernetes 1.4, we have introduced a number of features simplifying the deployment of such applications, including: </p><ul><li><a href=/docs/user-guide/scheduled-jobs/>ScheduledJob</a> is introduced as Alpha so users can run batch jobs at regular intervals.</li><li>Init-containers are Beta, addressing the need to run one or more containers before starting the main application, for example to sequence dependencies when starting a database or multi-tier app.</li><li><a href=/docs/user-guide/persistent-volumes/>Dynamic PVC Provisioning</a> moved to Beta. This feature now enables cluster administrators to expose multiple storage provisioners and allows users to select them using a new Storage Class API object.  </li><li>Curated and pre-tested <a href=https://github.com/kubernetes/charts>Helm charts</a> for common stateful applications such as MariaDB, MySQL and Jenkins will be available for one-command launches using version 2 of the Helm Package Manager.</li></ul><p><strong>Cluster federation API additions -</strong> One of the most requested capabilities from our global customers has been the ability to build applications with clusters that span regions and clouds. </p><ul><li><a href=/docs/user-guide/federation/replicasets/>Federated Replica Sets</a> Beta - replicas can now span some or all clusters enabling cross region or cross cloud replication. The total federated replica count and relative cluster weights / replica counts are continually reconciled by a federated replica-set controller to ensure you have the pods you need in each region / cloud.</li><li>Federated Services are now Beta, and <a href=/docs/user-guide/federation/secrets/>secrets</a>, <a href=/docs/user-guide/federation/events>events</a> and <a href=/docs/user-guide/federation/namespaces>namespaces</a> have also been added to the federation API.</li><li><a href=/docs/user-guide/federation/federated-ingress/>Federated Ingress</a> Alpha - starting with Google Cloud Platform (GCP), users can create a single L7 globally load balanced VIP that spans services deployed across a federation of clusters within GCP. With Federated Ingress in GCP, external clients point to a single IP address and are sent to the closest cluster with usable capacity in any region or zone of the federation in GCP.</li></ul><p><strong>Container security support -</strong> Administrators of multi-tenant clusters require the ability to provide varying sets of permissions among tenants, infrastructure components, and end users of the system.</p><ul><li><a href=/docs/user-guide/pod-security-policy/>Pod Security Policy</a> is a new object that enables cluster administrators to control the creation and validation of security contexts for pods/containers. Admins can associate service accounts, groups, and users with a set of constraints to define a security context.</li><li><a href=/docs/admin/apparmor/>AppArmor</a> support is added, enabling admins to run a more secure deployment, and provide better auditing and monitoring of their systems. Users can configure a container to run in an AppArmor profile by setting a single field.</li></ul><p><strong>Infrastructure enhancements - </strong> We continue adding to the scheduler, storage and client capabilities in Kubernetes based on user and ecosystem needs.</p><ul><li>Scheduler - introducing <a href=/docs/user-guide/node-selection/>inter-pod affinity and anti-affinity</a> Alpha for users who want to customize how Kubernetes co-locates or spreads their pods. Also <a href=/docs/admin/rescheduler/#guaranteed-scheduling-of-critical-add-on-pods>priority scheduling capability for cluster add-ons</a> such as DNS, Heapster, and the Kube Dashboard.</li><li>Disruption SLOs - Pod Disruption Budget is introduced to limit impact of pods deleted by cluster management operations (such as node upgrade) at any one time.</li><li>Storage - New <a href=/docs/user-guide/volumes/>volume plugins</a> for Quobyte and Azure Data Disk have been added.</li><li>Clients - Swagger 2.0 support is added, enabling non-Go clients.</li></ul><p><strong>Kubernetes Dashboard UI -</strong> lastly, a great looking Kubernetes <a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard UI</a> with 90% CLI parity for at-a-glance management.</p><p>For a complete list of updates see the <a href=https://github.com/kubernetes/kubernetes/pull/33410>release notes</a> on GitHub. Apart from features the most impressive aspect of Kubernetes development is the community of contributors. This is particularly true of the 1.4 release, the full breadth of which will unfold in upcoming weeks.</p><p><strong>Availability</strong><br>Kubernetes 1.4 is available for download at <a href=http://get.k8s.io/>get.k8s.io</a> and via the open source repository hosted on <a href=http://github.com/kubernetes/kubernetes>GitHub</a>. To get started with Kubernetes try the <a href=/docs/hellonode/>Hello World app</a>.</p><p>To get involved with the project, join the <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>weekly community meeting</a> or start contributing to the project here (marked help). </p><p><strong>Users and Case Studies</strong><br>Over the past fifteen months since the Kubernetes 1.0 GA release, the <a href=http://kubernetes.io/case-studies/>adoption and enthusiasm</a> for this project has surpassed everyone's imagination. Kubernetes runs in production at hundreds of organization and thousands more are in development. Here are a few unique highlights of companies running Kubernetes: </p><ul><li><strong><a href=https://www.box.com/>Box</a> --</strong> accelerated their time to delivery from six months to launch a service to less than a week. <a href=https://blog.box.com/blog/kubernetes-box-microservices-maximum-velocity/>Read more</a> on how Box runs mission critical production services on Kubernetes.</li><li><strong><a href=https://www.pearson.com/>Pearson</a> --</strong> minimized complexity and increased their engineer productivity. <a href=http://kubernetes.io/case-studies/pearson>Read how</a> Pearson is using Kubernetes to reinvent the world’s largest educational company. </li><li><strong><a href=https://openai.com/>OpenAI</a> --</strong> a non-profit artificial intelligence research company, built <a href=https://openai.com/blog/infrastructure-for-deep-learning/>infrastructure for deep learning</a> with Kubernetes to maximize productivity for researchers allowing them to focus on the science.</li></ul><p>We’re very grateful to our community of over 900 contributors who contributed more than 5,000 commits to make this release possible. To get a closer look on how the community is using Kubernetes, join us at the user conference <a href=http://events.linuxfoundation.org/events/kubecon>KubeCon</a> to hear directly from users and contributors.</p><p><strong>Connect</strong></p><ul><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a> </li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul><p>Thank you for your support! </p><p><em>-- Aparna Sinha, Product Manager, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-2ef1d895f447dec7f1aabe5f71823479>High performance network policies in Kubernetes clusters</h1><div class="td-byline mb-4"><time datetime=2016-09-21 class=text-muted>Wednesday, September 21, 2016</time></div><p><em>Editor's note: today’s post is by Juergen Brendel, Pritesh Kothari and Chris Marino co-founders of Pani Networks, the sponsor of the Romana project, the network policy software used for these benchmark tests.</em></p><p><strong>Network Policies</strong></p><p>Since the release of Kubernetes 1.3 back in July, users have been able to define and enforce network policies in their clusters. These policies are firewall rules that specify permissible types of traffic to, from and between pods. If requested, Kubernetes blocks all traffic that is not explicitly allowed. Policies are applied to groups of pods identified by common labels. Labels can then be used to mimic traditional segmented networks often used to isolate layers in a multi-tier application: You might identify your front-end and back-end pods by a specific “segment” label, for example. Policies control traffic between those segments and even traffic to or from external sources.</p><p><strong>Segmenting traffic</strong></p><p>What does this mean for the application developer? At last, Kubernetes has gained the necessary capabilities to provide "<a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defence in depth</a>". Traffic can be segmented and different parts of your application can be secured independently. For example, you can very easily protect each of your services via specific network policies: All the pods identified by a <a href=/docs/user-guide/replication-controller/>Replication Controller</a> behind a service are already identified by a specific label. Therefore, you can use this same label to apply a policy to those pods.</p><p>Defense in depth has long been recommended as best <a href=https://kubernetes.io/blog/2016/08/security-best-practices-kubernetes-deployment>practice</a>. This kind of isolation between different parts or layers of an application is easily achieved on AWS and OpenStack by applying security groups to VMs.</p><p>However, prior to network policies, this kind of isolation for containers was not possible. VXLAN overlays can provide simple network isolation, but application developers need more fine grained control over the traffic accessing pods. As you can see in this simple example, Kubernetes network policies can manage traffic based on source and origin, protocol and port.</p><pre><code>apiVersion: extensions/v1beta1  
kind: NetworkPolicy  
metadata:  
 name: pol1  
spec:  
 podSelector:  
   matchLabels:  
     role: backend  
 ingress:  
 - from:  
   - podSelector:  
      matchLabels:  
       role: frontend  
   ports:  
   - protocol: tcp  
     port: 80
</code></pre><p><strong>Not all network backends support policies</strong></p><p>Network policies are an exciting feature, which the Kubernetes community has worked on for a long time. However, it requires a networking backend that is capable of applying the policies. By themselves, simple routed networks or the commonly used <a href=https://github.com/coreos/flannel>flannel</a> network driver, for example, cannot apply network policy.</p><p>There are only a few policy-capable networking backends available for Kubernetes today: <a href=http://romana.io/>Romana</a>, <a href=http://projectcalico.org/>Calico</a>, and <a href=https://github.com/tigera/canal>Canal</a>; with <a href=http://www.weave.works/>Weave</a> indicating support in the near future. Red Hat’s OpenShift includes network policy features as well.</p><p>We chose Romana as the back-end for these tests because it configures pods to use natively routable IP addresses in a full L3 configuration. Network policies, therefore, can be applied directly by the host in the Linux kernel using iptables rules. This results is a high performance, easy to manage network.</p><p><strong>Testing performance impact of network policies</strong></p><p>After network policies have been applied, network packets need to be checked against those policies to verify that this type of traffic is permissible. But what is the performance penalty for applying a network policy to every packet? Can we use all the great policy features without impacting application performance? We decided to find out by running some tests.</p><p>Before we dive deeper into these tests, it is worth mentioning that ‘performance’ is a tricky thing to measure, network performance especially so.</p><p><em>Throughput</em> (i.e. data transfer speed measured in Gpbs) and <em>latency</em> (time to complete a request) are common measures of network performance. The performance impact of running an overlay network on throughput and latency has been examined previously <a href=https://smana.kubespray.io/index.php/posts/kubernetes-net-bench>here</a> and <a href=http://machinezone.github.io/research/networking-solutions-for-kubernetes/>here</a>. What we learned from these tests is that Kubernetes networks are generally pretty fast, and servers have no trouble saturating a 1G link, with or without an overlay. It's only when you have 10G networks that you need to start thinking about the overhead of encapsulation.</p><p>This is because during a typical network performance benchmark, there’s no application logic for the host CPU to perform, leaving it available for whatever network processing is required. <strong><em>For this reason we ran our tests in an operating range that did not saturate the link, or the CPU. This has the effect of isolating the impact of processing network policy rules on the host</em></strong>. For these tests we decided to measure latency as measured by the average time required to complete an HTTP request across a range of response sizes.</p><p><strong>Test setup</strong></p><ul><li>Hardware: Two servers with Intel Core i5-5250U CPUs (2 core, 2 threads per core) running at 1.60GHz, 16GB RAM and 512GB SSD. NIC: Intel Ethernet Connection I218-V (rev 03)</li><li>Ubuntu 14.04.5</li><li>Kubernetes 1.3 for data collection (verified samples on <a href=http://v1.4.0-beta.5/>v1.4.0-beta.5</a>)</li><li><a href=https://github.com/romana/romana>Romana v0.9.3.1</a></li><li>Client and server load test <a href=https://github.com/paninetworks/testing-tools>software</a></li></ul><p>For the tests we had a client pod send 2,000 HTTP requests to a server pod. HTTP requests were sent by the client pod at a rate that ensured that neither the server nor network ever saturated. We also made sure each request started a new TCP session by disabling persistent connections (i.e. HTTP <a href=https://en.wikipedia.org/wiki/HTTP_persistent_connection>keep-alive</a>). We ran each test with different response sizes and measured the average request duration time (how long does it take to complete a request of that size). Finally, we repeated each set of measurements with different policy configurations.</p><p>Romana detects Kubernetes network policies when they’re created, translates them to Romana’s own policy format, and then applies them on all hosts. Currently, Kubernetes network policies only apply to ingress traffic. This means that outgoing traffic is not affected.</p><p>First, we conducted the test without any policies to establish a baseline. We then ran the test again, increasing numbers of policies for the test's network segment. The policies were of the common “allow traffic for a given protocol and port” format. To ensure packets had to traverse all the policies, we created a number of policies that did not match the packet, and finally a policy that would result in acceptance of the packet.</p><p>The table below shows the results, measured in milliseconds for different request sizes and numbers of policies:</p><p>Response Size</p><p>|Policies |.5k |1k |10k |100k |1M |
|---|---|---|---|---|
|0 |0.732 |0.738 |1.077 |2.532 |10.487 |
|10 |0.744 |0.742 |1.084 |2.570 |10.556 |
|50 |0.745 |0.755 |1.086 |2.580 |10.566 |
|100 |0.762 |0.770 |1.104 |2.640 |10.597 |
|200 |0.783 |0.783 |1.147 |2.652 |10.677 |</p><p>What we see here is that, as the number of policies increases, processing network policies introduces a very small delay, never more than 0.2ms, even after applying 200 policies. For all practical purposes, no meaningful delay is introduced when network policy is applied. Also worth noting is that doubling the response size from 0.5k to 1.0k had virtually no effect. This is because for very small responses, the fixed overhead of creating a new connection dominates the overall response time (i.e. the same number of packets are transferred).</p><p><img src=https://lh3.googleusercontent.com/2M6D3zIPSiBE1LUZ3I5oVlZtfVVGP-aK6P3Qsb_siG0Jy16zeE1pNIZGLxeRh4SLCNUKY53A0Qbcm-dwwqz6ResSLjdb1oosXywOK5oK_uU6inVWQTPtztj9cv_6JK-EESVeeoq9 alt></p><p>Note: .5k and 1k lines overlap at ~.8ms in the chart above</p><p>Even as a percentage of baseline performance, the impact is still very small. The table below shows that for the smallest response sizes, the worst case delay remains at 7%, or less, up to 200 policies. For the larger response sizes the delay drops to about 1%.</p><p>Response Size</p><table><thead><tr><th>Policies</th><th>.5k</th><th>1k</th><th>10k</th><th>100k</th><th>1M</th></tr></thead><tbody><tr><td>0</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>0.0%</td></tr><tr><td>10</td><td>-1.6%</td><td>-0.5%</td><td>-0.6%</td><td>-1.5%</td><td>-0.7%</td></tr><tr><td>50</td><td>-1.8%</td><td>-2.3%</td><td>-0.8%</td><td>-1.9%</td><td>-0.8%</td></tr><tr><td>100</td><td>-4.1%</td><td>-4.3%</td><td>-2.5%</td><td>-4.3%</td><td>-1.0%</td></tr><tr><td>200</td><td>-7.0%</td><td>-6.1%</td><td>-6.5%</td><td>-4.7%</td><td>-1.8%</td></tr></tbody></table><p><img src=https://lh6.googleusercontent.com/Bwpuko0UBaTQrL0h9_wDtnmsa0ijk6KD82BDVtHCCMuM4zATPppHKLv9lDoWBYvTbO89nPqIIA5jLYMfdxv7O6jIwRqHg_chVvBOz0-yZ_j2YhXop5Tg2a-a86swu_tBQhEPVGH3 alt></p><p>What is also interesting in these results is that as the number of policies increases, we notice that larger requests experience a smaller relative (i.e. percentage) performance degradation.</p><p>This is because when Romana installs iptables rules, it ensures that packets belonging to established connection are evaluated first. The full list of policies only needs to be traversed for the first packets of a connection. After that, the connection is considered ‘established’ and the connection’s state is stored in a fast lookup table. For larger requests, therefore, most packets of the connection are processed with a quick lookup in the ‘established’ table, rather than a full traversal of all rules. This iptables optimization results in performance that is largely independent of the number of network policies.</p><p>Such ‘flow tables’ are common optimizations in network equipment and it seems that iptables uses the same technique quite effectively.</p><p>Its also worth noting that in practise, a reasonably complex application may configure a few dozen rules per segment. It is also true that common network optimization techniques like Websockets and persistent connections will improve the performance of network policies even further (especially for small request sizes), since connections are held open longer and therefore can benefit from the established connection optimization.</p><p>These tests were performed using Romana as the backend policy provider and other network policy implementations may yield different results. However, what these tests show is that for almost every application deployment scenario, network policies can be applied using Romana as a network back end without any negative impact on performance.</p><p>If you wish to try it for yourself, we invite you to check out <a href=http://romana.io/>Romana</a>. In our <a href=https://github.com/romana/romana>GitHub repo</a> you can find an easy to use installer, which works with AWS, Vagrant VMs or any other servers. You can use it to quickly get you started with a Romana powered Kubernetes or OpenStack cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-781919e7905e0e2d92c56ebdc7f69aa8>Creating a PostgreSQL Cluster using Helm</h1><div class="td-byline mb-4"><time datetime=2016-09-09 class=text-muted>Friday, September 09, 2016</time></div><p><em>Editor’s note: Today’s guest post is by Jeff McCormick, a developer at Crunchy Data, showing how to deploy a PostgreSQL cluster using Helm, a Kubernetes package manager.</em></p><p><a href=http://www.crunchydata.com/>Crunchy Data</a> supplies a set of open source PostgreSQL and PostgreSQL related containers. The Crunchy PostgreSQL Container Suite includes containers that deploy, monitor, and administer the open source PostgreSQL database, for more details view this GitHub <a href=https://github.com/crunchydata/crunchy-containers>repository</a>.</p><p>In this post we’ll show you how to deploy a PostgreSQL cluster using <a href=https://github.com/kubernetes/helm>Helm</a>, a Kubernetes package manager. For reference, the Crunchy Helm Chart examples used within this post are located <a href=https://github.com/CrunchyData/crunchy-containers/tree/master/examples/kubehelm/crunchy-postgres>here</a>, and the pre-built containers can be found on DockerHub at <a href=https://hub.docker.com/u/crunchydata/dashboard/>this location</a>.</p><p>This example will create the following in your Kubernetes cluster:</p><ul><li>postgres master service</li><li>postgres replica service</li><li>postgres 9.5 master database (pod)</li><li>postgres 9.5 replica database (replication controller)</li></ul><p><img src=https://lh5.googleusercontent.com/Ff3vRGv3RHsrbAvJUFpVTehohw-OI2AeFmeVSVrdJuU0mjx3lKTa07YlaB_a7rW65rfAdupyeSqOT2DyxnSJ6_y4sXY5DhW14qM-vkxRo32969VZEpUNrZ3hIFdwJ9T04Ev6w2to alt=HelmBlogDiagram.jpg></p><p>This example creates a simple Postgres streaming replication deployment with a master (read-write), and a single asynchronous replica (read-only). You can scale up the number of replicas dynamically.</p><p><strong>Contents</strong></p><p>The example is made up of various Chart files as follows:</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:center>values.yaml</td><td style=text-align:center>This file contains values which you can reference within the database templates allowing you to specify in one place values like database passwords</td></tr><tr><td style=text-align:center>templates/master-pod.yaml</td><td style=text-align:center>The postgres master database pod definition. This file causes a single postgres master pod to be created.</td></tr><tr><td style=text-align:center>templates/master-service.yaml</td><td style=text-align:center>The postgres master database has a service created to act as a proxy. This file causes a single service to be created to proxy calls to the master database.</td></tr><tr><td style=text-align:center>templates/replica-rc.yaml</td><td style=text-align:center>The postgres replica database is defined by this file. This file causes a replication controller to be created which allows the postgres replica containers to be scaled up on-demand.</td></tr><tr><td style=text-align:center>templates/replica-service.yaml</td><td style=text-align:center>This file causes the service proxy for the replica database container(s) to be created.</td></tr></tbody></table><p><strong>Installation</strong></p><p><a href=https://github.com/kubernetes/helm#install>Install Helm</a> according to their GitHub documentation and then install the examples as follows:</p><pre><code>helm init

cd crunchy-containers/examples/kubehelm

helm install ./crunchy-postgres
</code></pre><p><strong>Testing</strong></p><p>After installing the Helm chart, you will see the following services:</p><pre><code>kubectl get services  
NAME              CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE  
crunchy-master    10.0.0.171   \&lt;none\&gt;        5432/TCP   1h  
crunchy-replica   10.0.0.31    \&lt;none\&gt;        5432/TCP   1h  
kubernetes        10.0.0.1     \&lt;none\&gt;        443/TCP    1h
</code></pre><p>It takes about a minute for the replica to begin replicating with the master. To test out replication, see if replication is underway with this command, enter password for the password when prompted:</p><pre><code>psql -h crunchy-master -U postgres postgres -c 'table pg\_stat\_replication'
</code></pre><p>If you see a line returned from that query it means the master is replicating to the slave. Try creating some data on the master:</p><pre><code>psql -h crunchy-master -U postgres postgres -c 'create table foo (id int)'

psql -h crunchy-master -U postgres postgres -c 'insert into foo values (1)'
</code></pre><p>Then verify that the data is replicated to the slave:</p><pre><code>psql -h crunchy-replica -U postgres postgres -c 'table foo'
</code></pre><p>You can scale up the number of read-only replicas by running the following kubernetes command:</p><pre><code>kubectl scale rc crunchy-replica --replicas=2
</code></pre><p>It takes 60 seconds for the replica to start and begin replicating from the master.</p><p>The Kubernetes Helm and Charts projects provide a streamlined way to package up complex applications and deploy them on a Kubernetes cluster. Deploying PostgreSQL clusters can sometimes prove challenging, but the task is greatly simplified using Helm and Charts.</p><p><em>--Jeff McCormick, Developer, Crunchy Data</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0397a899d8e711d2c9cd190f224eb8e0>Deploying to Multiple Kubernetes Clusters with kit</h1><div class="td-byline mb-4"><time datetime=2016-09-06 class=text-muted>Tuesday, September 06, 2016</time></div><p><em>Editor’s note: today’s guest post is by Chesley Brown, Full-Stack Engineer, at InVision, talking about how they build and open sourced kit to help them to continuously deploy updates to multiple clusters.</em></p><p>Our Docker journey at InVision may sound familiar. We started with Docker in our development environments, trying to get consistency there first. We wrangled our legacy monolith application into Docker images and streamlined our Dockerfiles to minimize size and amp the efficiency. Things were looking good. Did we learn a lot along the way? For sure. But at the end of it all, we had our entire engineering team working with Docker locally for their development environments. Mission accomplished! Well, not quite. Development was one thing, but moving to production was a whole other ballgame.</p><p><strong>Along Came Kubernetes</strong></p><p>Kubernetes came into our lives during our evaluation of orchestrators and schedulers last December. AWS ECS was still fresh and Docker had just released 1.9 (networking overlay release). We spent the month evaluating our choices, narrowing it down to native Docker tooling (Machine, Swarm, Compose), ECS and Kubernetes. Well, needless to say, Kubernetes was our clear winner and we started the new year moving headlong to leverage Kubernetes to get us to production. But it wasn't long when we ran into a tiny complication...</p><p><strong>Automated Deployments With A Catch</strong></p><p>Here at <a href=https://www.invisionapp.com/>InVision</a>, we have a unique challenge. We just don’t have a single production environment running Kubernetes, but several, all needing automated updates via our CI/CD process. And although the code running on these environments was similar, the configurations were not. Things needed to work smoothly, automatically, as we couldn't afford to add friction to the deploy process or encumber our engineering teams.</p><p>Having several near duplicate clusters could easily turn into a Kubernetes manifest nightmare. Anti-patterns galore, as we copy and paste 95% of the manifests to get a new cluster. Scalable? No. Headache? Yes. Keeping those manifests up-to-date and accurate would be a herculean (and error-prone) task. We needed something easier, something that allows reuse, keeping the maintenance low, and that we could incorporate into our CI/CD system.</p><p>So after looking for a project or tooling that could fit our needs, we came up empty. At InVision, we love to create tools to help us solve problems, and figuring we may not be the only team in this situation we decided to roll up our sleeves and created something of our own. The result is our open-source tool, kit! (short for Kubernetes + git)</p><p><strong>Hello kit!</strong></p><p><a href=https://raw.githubusercontent.com/InVisionApp/kit/master/media/kit-logo-horz-sm.png><img src=https://raw.githubusercontent.com/InVisionApp/kit/master/media/kit-logo-horz-sm.png alt></a></p><p><a href=https://github.com/InVisionApp/kit>kit</a> is a suite of components that, when plugged into your CI/CD system and source control, allows you to continuously deploy updates (or entirely new services!) to as many clusters as needed, all leveraging webhooks and without having to host an external service.</p><p>Using kit’s templating format, you can define your service files once and have them reused across multiple clusters. It works by building on top of your usual Kubernetes manifest files allowing them to be defined once and then reused across clusters by only defining the unique configuration needed for that specific cluster. This allows you to easily build the orchestration for your application and deploy it to as many clusters as needed. It also allows the ability to group variations of your application so you could have clusters that run the “development” version of your application while others run the “production” version and so on.</p><p>Developers simply commit code to their branches as normal and kit deploys to all clusters running that service. Kit then manages updating the image and tag that is used for a given service directly to the repository containing all your kit manifest templates. This means any and all changes to your clusters, from environment variables, or configurations to image updates are all tracked under source control history providing you with an audit trail for every cluster you have.</p><p>We made all of this Open Source so you can <a href=https://github.com/InVisionApp/kit>check out the kit repo</a>!</p><p><strong>Is kit Right For Us?</strong></p><p>If you are running Kubernetes across several clusters (or namespaces) all needing to continuously deploy, you bet! Because using kit doesn’t require hosting any external server, your team can leverage the webhooks you probably already have with github and your CI/CD system to get started. From there you create a repo to host your Kubernetes manifest files which tells what services are deployed to which clusters. Complexity of these files is greatly simplified thanks to kit’s templating engine.The kit-image-deployer component is incorporated into the CI/CD process and whenever a developer commits code to master and the build passes, it’s automatically deployed to all configured clusters.</p><p><strong>So What Are The Components?</strong><br><a href=https://4.bp.blogspot.com/-BdD0AgQKFWY/V87u5p7uw2I/AAAAAAAAArM/Z6_279MSn2AVDmO192GtPPTuVBbLgsHCQCLcB/s1600/kit.png><img src=https://4.bp.blogspot.com/-BdD0AgQKFWY/V87u5p7uw2I/AAAAAAAAArM/Z6_279MSn2AVDmO192GtPPTuVBbLgsHCQCLcB/s640/kit.png alt></a><br>kit is comprised of several components each building on the next. The general flow is a developer commits code to their repository, an image is built and then kit-image-deployer commits the new image and tag to your manifests repository. From there the kit-deploymentizer runs, parsing all your manifest templates to generate the raw Kubernetes manifest files. Finally the kit-deployer runs and takes all the built manifest files and deploys them to all the appropriate clusters. Here is a summary of the components and the flow:</p><p><strong><a href=https://github.com/InVisionApp/kit-image-deployer>kit-image-deployer</a></strong><br>A service that can be used to update given yaml files within a git repository with a new Docker image path. This can be used in collaboration with kit-deploymentizer and kit-deployer to automatically update the images used for a service across multiple clusters.</p><p><a href=https://github.com/InVisionApp/kit-deploymentizer><strong>kit-deploymentizer</strong></a><br>This service intelligently builds deployment files as to allow reusability of environment variables and other forms of configuration. It also supports aggregating these deployments for multiple clusters. In the end, it generates a list of clusters and a list of deployment files for each of these clusters. Best used in collaboration with kit-deployer and kit-image-deployer to achieve a continuous deployment workflow.</p><p><a href=https://github.com/InVisionApp/kit-deployer><strong>kit-deployer</strong></a><br>Use this service to deploy files to multiple Kubernetes clusters. Just organize your manifest files into directories that match the names of your clusters (the name defined in your kubeconfig files). Then you provide a directory of kubeconfig files and the kit-deployer will asynchronously send all manifests up to their corresponding clusters.</p><p><strong>So What's Next?</strong></p><p>In the near future, we want to make deployments even smarter so as to handle updating things like mongo replicasets. We also want to add in smart monitoring to further improve on the self-healing nature of Kubernetes. We’re also working on adding additional integrations (such as Slack) and notification methods. And most importantly we’re working towards shifting more control to the individual developers of each service by allowing the kit manifest templates to exist in each individual service repository instead of a single master manifest repository. This will allow them to manage their service completely from development straight to production across all clusters.</p><p>We hope you take a closer look at <a href=https://github.com/InVisionApp/kit>kit</a> and tell us what you think! Check out our <a href=http://engineering.invisionapp.com/>InVision Engineering</a> blog for more posts about the cool things we are up to at InVision. If you want to work on kit or other interesting things like this, click through to <a href=https://www.invisionapp.com/company#jobs>our jobs page</a>. We'd love to hear from you!</p><p><em>--Chesley Brown, Full-Stack Engineer, at <a href=https://www.invisionapp.com/>InVision</a>.</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1f2dbd97148947aa9b519760ad190b8b>Cloud Native Application Interfaces</h1><div class="td-byline mb-4"><time datetime=2016-09-01 class=text-muted>Thursday, September 01, 2016</time></div><p><strong>Standard Interfaces (or, the Thirteenth Factor)</strong></p><p><em>--by Brian Grant and Craig Mcluckie, Google</em></p><p>When you say we need ‘software standards’ in erudite company, you get some interesting looks. Most concede that software standards have been central to the success of the boldest and most successful projects out there (like the Internet). Most are also skeptical about how they apply to the innovative world we live in today. Our projects are executed in week increments, not years. Getting bogged down behind mega-software-corporation-driven standards practices would be the death knell in this fluid, highly competitive world.</p><p>This isn’t about ‘those’ standards. The ones that emerge after years of deep consideration and negotiation that are eventually published by a body with a four-letter acronym for a name. This is about a different approach: finding what is working in the real world, and acting as a community to embrace it.</p><p>Let’s go back to first principles. To describe Cloud Native in one word, we'd choose "automatable".</p><p>Most existing applications are not. </p><p>Applications have many interfaces with their environment, whether with management infrastructure, shared services, or other applications. For us to remove the operator from patching, scaling, migrating an app from one environment to another, changing out dependencies, and handling of failure conditions, a set of well structured common interfaces is essential. It goes without saying that these interfaces must be designed for machines, not just humans. Machine-friendly interfaces allow automation systems to understand the systems under management, and create the loose coupling needed for applications to live in automated environments. </p><p>As containerized infrastructure gets built there are a set of critical interfaces available to applications that go far beyond what is available to a single node today. The adoption of ‘serverless patterns’ (meaning ephemeral, event driven function execution) will further compound the need to make sense of running code in an environment that is completely decoupled from the node. The services needed will start with application configuration and extend to monitoring, logging, autoscaling and beyond. The set of capabilities will only grow as applications continue to adapt to be fuller citizens in a "cloud native" world.</p><p>Exploring one example a little further, a number of service-discovery solutions have been developed but are often tied to a particular storage implementation, a particular programming language, a non-standard protocol, and/or are opinionated in some other way (e.g., dictating application naming structure). This makes them unsuitable for general-purpose use. While DNS has limitations (that will eventually need to be addressed), it's at least a standard protocol with room for innovation in its implementation. This is demonstrated by CoreDNS and other cloud-native DNS implementations. </p><p>When we look inside the systems at Google, we have been able to achieve very high levels of automation without formal interface definitions thanks to a largely homogeneous software and hardware environment. Adjacent systems can safely make assumptions about interfaces, and by providing a set of universally used libraries we can skirt the issue. A good example of this is our log format doesn’t need to be formally specified because the libraries that generate logs are maintained by the teams that maintain the logs processing systems. This means that we have been able to get by to date without something like fluentd (which is solving the problem in the community of interfacing with logging systems).</p><p>Even though Google has managed to get by this way, it hurts us. One way is when we acquire a company. Porting their technology to run in our automation systems requires a spectacular amount of work. Doing that work while continuing to innovate is particularly tough. Even more significant though, there’s a lot of innovation happening in the open source world that isn’t easy for us to tap into. When new technology emerges, we would like to be able to experiment with it, adopt it piecemeal, and perhaps contribute back to it. When you run a vertically integrated, bespoke stack, that is a hard thing to do.</p><p>The lack of standard interfaces leaves customers with three choices: </p><ul><li>Live with high operations cost (the status quo), and accept that your developers in many cases will spend the majority of their time dealing with the care and feeding of applications.</li><li>Sign-up to be like Google (build your own everything, down to the concrete in the floor). </li><li>Rely on a single, or a small collection of vendors to provide a complete solution and accept some degree of lock-in. Few in companies of any size (from enterprise to startup) find this appealing.
It is our belief that an open community is more powerful and that customers benefit when there is competition at every layer of the stack. It should be possible to pull together a stack with best-of-breed capabilities at every level -- logging, monitoring, orchestration, container runtime environment, block and file-system storage, SDN technology, etc. </li></ul><p>Standardizing interfaces (at least by convention) between the management system and applications is critical. One might consider the use of common conventions for interfaces as a thirteenth factor (expanding on the <a href=https://12factor.net/>12-factor methodology</a>) in creating modern systems that work well in the cloud and at scale.</p><p>Kubernetes and Cloud Native Computing Foundation (<a href=https://cncf.io/>CNCF</a>) represent a great opportunity to support the emergence of standard interfaces, and to support the emergence of a fully automated software world. We’d love to see this community embrace the ideal of promoting standard interfaces from working technology. The obvious first step is to identify the immediate set of critical interfaces, and establish working groups in CNCF to start assess what exists in this area as candidates, and to sponsor work to start developing standard interfaces that work across container formats, orchestrators, developer tools and the myriad other systems that are needed to deliver on the Cloud Native vision.</p><p><em>--Brian Grant and Craig Mcluckie, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d6be24e48058aca09b2667629db622d7>Security Best Practices for Kubernetes Deployment</h1><div class="td-byline mb-4"><time datetime=2016-08-31 class=text-muted>Wednesday, August 31, 2016</time></div><p><em>Note: some of the recommendations in this post are no longer current. Current cluster hardening options are described in this <a href=/docs/tasks/administer-cluster/securing-a-cluster/>documentation</a>.</em></p><p><em>Editor’s note: today’s post is by Amir Jerbi and Michael Cherny of Aqua Security, describing security best practices for Kubernetes deployments, based on data they’ve collected from various use-cases seen in both on-premises and cloud deployments.</em></p><p>Kubernetes provides many controls that can greatly improve your application security. Configuring them requires intimate knowledge with Kubernetes and the deployment’s security requirements. The best practices we highlight here are aligned to the container lifecycle: build, ship and run, and are specifically tailored to Kubernetes deployments. We adopted these best practices in <a href=http://blog.aquasec.com/running-a-security-service-in-google-cloud-real-world-example>our own SaaS deployment</a> that runs Kubernetes on Google Cloud Platform.</p><p>The following are our recommendations for deploying a secured Kubernetes application:</p><p>**Ensure That Images Are Free of Vulnerabilities **<br>Having running containers with vulnerabilities opens your environment to the risk of being easily compromised. Many of the attacks can be mitigated simply by making sure that there are no software components that have known vulnerabilities.</p><ul><li><p><strong>Implement Continuous Security Vulnerability Scanning</strong> -- Containers might include outdated packages with known vulnerabilities (CVEs). This cannot be a ‘one off’ process, as new vulnerabilities are published every day. An ongoing process, where images are continuously assessed, is crucial to insure a required security posture.</p></li><li><p><strong>Regularly Apply Security Updates to Your Environment</strong> -- Once vulnerabilities are found in running containers, you should always update the source image and redeploy the containers. Try to avoid direct updates (e.g. ‘apt-update’) to the running containers, as this can break the image-container relationship. Upgrading containers is extremely easy with the Kubernetes rolling updates feature - this allows gradually updating a running application by upgrading its images to the latest version.</p></li></ul><p><strong>Ensure That Only Authorized Images are Used in Your Environment</strong><br>Without a process that ensures that only images adhering to the organization’s policy are allowed to run, the organization is open to risk of running vulnerable or even malicious containers. Downloading and running images from unknown sources is dangerous. It is equivalent to running software from an unknown vendor on a production server. Don’t do that.</p><p>Use private registries to store your approved images - make sure you only push approved images to these registries. This alone already narrows the playing field, reducing the number of potential images that enter your pipeline to a fraction of the hundreds of thousands of publicly available images. Build a CI pipeline that integrates security assessment (like vulnerability scanning), making it part of the build process.</p><p>The CI pipeline should ensure that only vetted code (approved for production) is used for building the images. Once an image is built, it should be scanned for security vulnerabilities, and only if no issues are found then the image would be pushed to a private registry, from which deployment to production is done. A failure in the security assessment should create a failure in the pipeline, preventing images with bad security quality from being pushed to the image registry.</p><p>There is work in progress being done in Kubernetes for image authorization plugins (expected in Kubernetes 1.4), which will allow preventing the shipping of unauthorized images. For more info see this <a href=https://github.com/kubernetes/kubernetes/pull/27129>pull request</a>.</p><p><strong>Limit Direct Access to Kubernetes Nodes</strong><br>You should limit SSH access to Kubernetes nodes, reducing the risk for unauthorized access to host resource. Instead you should ask users to use "kubectl exec", which will provide direct access to the container environment without the ability to access the host.</p><p>You can use Kubernetes <a href=/docs/reference/access-authn-authz/authorization/>Authorization Plugins</a> to further control user access to resources. This allows defining fine-grained-access control rules for specific namespace, containers and operations.</p><p><strong>Create Administrative Boundaries between Resources</strong><br>Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.</p><p>For example: the following policy will allow ‘alice’ to read pods from namespace ‘fronto’.</p><pre><code>{

  &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;,

  &quot;kind&quot;: &quot;Policy&quot;,

  &quot;spec&quot;: {

    &quot;user&quot;: &quot;alice&quot;,

    &quot;namespace&quot;: &quot;fronto&quot;,

    &quot;resource&quot;: &quot;pods&quot;,

    &quot;readonly&quot;: true

  }

}
</code></pre><p><strong>Define Resource Quota</strong><br>An option of running resource-unbound containers puts your system in risk of DoS or “noisy neighbor” scenarios. To prevent and minimize those risks you should define resource quotas. By default, all resources in Kubernetes cluster are created with unbounded CPU and memory requests/limits. You can create resource quota policies, attached to Kubernetes namespace, in order to limit the CPU and memory a pod is allowed to consume.</p><p>The following is an example for namespace resource quota definition that will limit number of pods in the namespace to 4, limiting their CPU requests between 1 and 2 and memory requests between 1GB to 2GB.</p><p>compute-resources.yaml:</p><pre><code>apiVersion: v1  
kind: ResourceQuota  
metadata:  
  name: compute-resources  
spec:  
  hard:  
    pods: &quot;4&quot;  
    requests.cpu: &quot;1&quot;  
    requests.memory: 1Gi  
    limits.cpu: &quot;2&quot;  
    limits.memory: 2Gi
</code></pre><p>Assign a resource quota to namespace:</p><pre><code>kubectl create -f ./compute-resources.yaml --namespace=myspace
</code></pre><p><strong>Implement Network Segmentation</strong></p><p>Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to.</p><p>One of the challenges in Kubernetes deployments is creating network segmentation between pods, services and containers. This is a challenge due to the “dynamic” nature of container network identities (IPs), along with the fact that containers can communicate both inside the same node or between nodes.</p><p>Users of Google Cloud Platform can benefit from automatic firewall rules, preventing cross-cluster communication. A similar implementation can be deployed on-premises using network firewalls or SDN solutions. There is work being done in this area by the Kubernetes <a href=https://github.com/kubernetes/community/wiki/SIG-Network>Network SIG</a>, which will greatly improve the pod-to-pod communication policies. A new network policy API should address the need to create firewall rules around pods, limiting the network access that a containerized can have.</p><p>The following is an example of a network policy that controls the network for “backend” pods, only allowing inbound network access from “frontend” pods:</p><pre><code>POST /apis/net.alpha.kubernetes.io/v1alpha1/namespaces/tenant-a/networkpolicys  
{  
  &quot;kind&quot;: &quot;NetworkPolicy&quot;,

  &quot;metadata&quot;: {

    &quot;name&quot;: &quot;pol1&quot;

  },

  &quot;spec&quot;: {

    &quot;allowIncoming&quot;: {

      &quot;from&quot;: [{

        &quot;pods&quot;: { &quot;segment&quot;: &quot;frontend&quot; }

      }],

      &quot;toPorts&quot;: [{

        &quot;port&quot;: 80,

        &quot;protocol&quot;: &quot;TCP&quot;

      }]

    },

    &quot;podSelector&quot;: {

      &quot;segment&quot;: &quot;backend&quot;

    }

  }

}
</code></pre><p>Read more about Network policies <a href=https://kubernetes.io/blog/2016/04/Kubernetes-Network-Policy-APIs>here</a>.</p><p><strong>Apply Security Context to Your Pods and Containers</strong></p><p>When designing your containers and pods, make sure that you configure the security context for your pods, containers and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. Some of the important parameters are:</p><table><thead><tr><th style=text-align:center>Security Context Setting</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center>SecurityContext->runAsNonRoot</td><td style=text-align:center>Indicates that containers should run as non-root user</td></tr><tr><td style=text-align:center>SecurityContext->Capabilities</td><td style=text-align:center>Controls the Linux capabilities assigned to the container.</td></tr><tr><td style=text-align:center>SecurityContext->readOnlyRootFilesystem</td><td style=text-align:center>Controls whether a container will be able to write into the root filesystem.</td></tr><tr><td style=text-align:center>PodSecurityContext->runAsNonRoot</td><td style=text-align:center>Prevents running a container with 'root' user as part of the pod</td></tr></tbody></table><p>The following is an example for pod definition with security context parameters:</p><pre><code>apiVersion: v1  
kind: Pod  
metadata:  
  name: hello-world  
spec:  
  containers:  
  # specification of the pod’s containers  
  # ...  
  securityContext:  
    readOnlyRootFilesystem: true  
    runAsNonRoot: true
</code></pre><p>Reference <a href=/docs/api-reference/v1/definitions/#_v1_podsecuritycontext>here</a>.</p><p>In case you are running containers with elevated privileges (--privileged) you should consider using the “DenyEscalatingExec” admission control. This control denies exec and attach commands to pods that run with escalated privileges that allow host access. This includes pods that run as privileged, have access to the host IPC namespace, and have access to the host PID namespace. For more details on admission controls, see the Kubernetes <a href=/docs/reference/access-authn-authz/admission-controllers/>documentation</a>.</p><p><strong>Log Everything</strong></p><p>Kubernetes supplies cluster-based logging, allowing to log container activity into a central log hub. When a cluster is created, the standard output and standard error output of each container can be ingested using a Fluentd agent running on each node into either Google Stackdriver Logging or into Elasticsearch and viewed with Kibana.</p><p><strong>Summary</strong></p><p>Kubernetes supplies many options to create a secured deployment. There is no one-size-fit-all solution that can be used everywhere, so a certain degree of familiarity with these options is required, as well as an understanding of how they can enhance your application’s security.</p><p>We recommend implementing the best practices that were highlighted in this blog, and use Kubernetes flexible configuration capabilities to incorporate security processes into the continuous integration pipeline, automating the entire process with security seamlessly “baked in”.</p><p><em>--Michael Cherny, Head of Security Research, and Amir Jerbi, CTO and co-founder <a href=https://www.aquasec.com/>Aqua Security</a></em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0bdd9be1eea1fe9ccb870ef33266eb15>Scaling Stateful Applications using Kubernetes Pet Sets and FlexVolumes with Datera Elastic Data Fabric</h1><div class="td-byline mb-4"><time datetime=2016-08-29 class=text-muted>Monday, August 29, 2016</time></div><p><em>Editor’s note: today’s guest post is by Shailesh Mittal, Software Architect and Ashok Rajagopalan, Sr Director Product at Datera Inc, talking about Stateful Application provisioning with Kubernetes on Datera Elastic Data Fabric.</em></p><p><strong>Introduction</strong></p><p>Persistent volumes in Kubernetes are foundational as customers move beyond stateless workloads to run stateful applications. While Kubernetes has supported stateful applications such as MySQL, Kafka, Cassandra, and Couchbase for a while, the introduction of Pet Sets has significantly improved this support. In particular, the procedure to sequence the provisioning and startup, the ability to scale and associate durably by <a href=/docs/user-guide/petset/>Pet Sets</a> has provided the ability to automate to scale the “Pets” (applications that require consistent handling and durable placement).</p><p>Datera, elastic block storage for cloud deployments, has <a href=http://datera.io/blog-library/8/19/datera-simplifies-stateful-containers-on-kubernetes-13>seamlessly integrated with Kubernetes</a> through the <a href=/docs/user-guide/volumes/#flexvolume>FlexVolume</a> framework. Based on the first principles of containers, Datera allows application resource provisioning to be decoupled from the underlying physical infrastructure. This brings clean contracts (aka, no dependency or direct knowledge of the underlying physical infrastructure), declarative formats, and eventually portability to stateful applications.</p><p>While Kubernetes allows for great flexibility to define the underlying application infrastructure through yaml configurations, Datera allows for that configuration to be passed to the storage infrastructure to provide persistence. Through the notion of Datera AppTemplates, in a Kubernetes environment, stateful applications can be automated to scale.</p><p><strong>Deploying Persistent Storage</strong></p><p>Persistent storage is defined using the Kubernetes <a href=/docs/user-guide/persistent-volumes/#persistent-volumes>PersistentVolume</a> subsystem. PersistentVolumes are volume plugins and define volumes that live independently of the lifecycle of the pod that is using it. They are implemented as NFS, iSCSI, or by cloud provider specific storage system. Datera has developed a volume plugin for PersistentVolumes that can provision iSCSI block storage on the Datera Data Fabric for Kubernetes pods.</p><p>The Datera volume plugin gets invoked by kubelets on minion nodes and relays the calls to the Datera Data Fabric over its REST API. Below is a sample deployment of a PersistentVolume with the Datera plugin:</p><pre><code> apiVersion: v1

 kind: PersistentVolume

 metadata:

   name: pv-datera-0

 spec:

   capacity:

     storage: 100Gi

   accessModes:

     - ReadWriteOnce

   persistentVolumeReclaimPolicy: Retain

   flexVolume:

     driver: &quot;datera/iscsi&quot;

     fsType: &quot;xfs&quot;

     options:

       volumeID: &quot;kube-pv-datera-0&quot;

       size: “100&quot;

       replica: &quot;3&quot;

       backstoreServer: &quot;[tlx170.tlx.daterainc.com](http://tlx170.tlx.daterainc.com/):7717”
</code></pre><p>This manifest defines a PersistentVolume of 100 GB to be provisioned in the Datera Data Fabric, should a pod request the persistent storage.</p><pre><code>[root@tlx241 /]# kubectl get pv

NAME          CAPACITY   ACCESSMODES   STATUS      CLAIM     REASON    AGE

pv-datera-0   100Gi        RWO         Available                       8s

pv-datera-1   100Gi        RWO         Available                       2s

pv-datera-2   100Gi        RWO         Available                       7s

pv-datera-3   100Gi        RWO         Available                       4s
</code></pre><p><strong>Configuration</strong></p><p>The Datera PersistenceVolume plugin is installed on all minion nodes. When a pod lands on a minion node with a valid claim bound to the persistent storage provisioned earlier, the Datera plugin forwards the request to create the volume on the Datera Data Fabric. All the options that are specified in the PersistentVolume manifest are sent to the plugin upon the provisioning request.</p><p>Once a volume is provisioned in the Datera Data Fabric, volumes are presented as an iSCSI block device to the minion node, and kubelet mounts this device for the containers (in the pod) to access it.</p><p><img src=https://lh4.googleusercontent.com/ILlUm1HrWhGa8uTt97dQ786Gn20FHFZkavfucz05NHv6moZWiGDG7GlELM6o4CSzANWvZckoAVug5o4jMg17a-PbrfD1FRbDPeUCIc8fKVmVBNUsUPshWanXYkBa3gIJy5BnhLmZ alt></p><p><strong>Using Persistent Storage</strong></p><p>Kubernetes PersistentVolumes are used along with a pod using PersistentVolume Claims. Once a claim is defined, it is bound to a PersistentVolume matching the claim’s specification. A typical claim for the PersistentVolume defined above would look like below:</p><pre><code>kind: PersistentVolumeClaim

apiVersion: v1

metadata:

 name: pv-claim-test-petset-0

spec:

 accessModes:

   - ReadWriteOnce

 resources:

   requests:

     storage: 100Gi
</code></pre><p>When this claim is defined and it is bound to a PersistentVolume, resources can be used with the pod specification:</p><pre><code>[root@tlx241 /]# kubectl get pv

NAME          CAPACITY   ACCESSMODES   STATUS      CLAIM                            REASON    AGE

pv-datera-0   100Gi      RWO           Bound       default/pv-claim-test-petset-0             6m

pv-datera-1   100Gi      RWO           Bound       default/pv-claim-test-petset-1             6m

pv-datera-2   100Gi      RWO           Available                                              7s

pv-datera-3   100Gi      RWO           Available                                              4s


[root@tlx241 /]# kubectl get pvc

NAME                     STATUS    VOLUME        CAPACITY   ACCESSMODES   AGE

pv-claim-test-petset-0   Bound     pv-datera-0   0                        3m

pv-claim-test-petset-1   Bound     pv-datera-1   0                        3m
</code></pre><p>A pod can use a PersistentVolume Claim like below:</p><pre><code>apiVersion: v1

kind: Pod

metadata:

 name: kube-pv-demo

spec:

 containers:

 - name: data-pv-demo

   image: nginx

   volumeMounts:

   - name: test-kube-pv1

     mountPath: /data

   ports:

   - containerPort: 80

 volumes:

 - name: test-kube-pv1

   persistentVolumeClaim:

     claimName: pv-claim-test-petset-0
</code></pre><p>The result is a pod using a PersistentVolume Claim as a volume. It in-turn sends the request to the Datera volume plugin to provision storage in the Datera Data Fabric.</p><pre><code>[root@tlx241 /]# kubectl describe pods kube-pv-demo

Name:       kube-pv-demo

Namespace:  default

Node:       tlx243/172.19.1.243

Start Time: Sun, 14 Aug 2016 19:17:31 -0700

Labels:     \&lt;none\&gt;

Status:     Running

IP:         10.40.0.3

Controllers: \&lt;none\&gt;

Containers:

 data-pv-demo:

   Container ID: [docker://ae2a50c25e03143d0dd721cafdcc6543fac85a301531110e938a8e0433f74447](about:blank)

   Image:   nginx

   Image ID: [docker://sha256:0d409d33b27e47423b049f7f863faa08655a8c901749c2b25b93ca67d01a470d](about:blank)

   Port:    80/TCP

   State:   Running

     Started:  Sun, 14 Aug 2016 19:17:34 -0700

   Ready:   True

   Restart Count:  0

   Environment Variables:  \&lt;none\&gt;

Conditions:

 Type           Status

 Initialized    True

 Ready          True

 PodScheduled   True

Volumes:

 test-kube-pv1:

   Type:  PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)

   ClaimName:   pv-claim-test-petset-0

   ReadOnly:    false

 default-token-q3eva:

   Type:        Secret (a volume populated by a Secret)

   SecretName:  default-token-q3eva

   QoS Tier:  BestEffort

Events:

 FirstSeen LastSeen Count From SubobjectPath Type Reason Message

 --------- -------- ----- ---- ------------- -------- ------ -------

 43s 43s 1 {default-scheduler } Normal Scheduled Successfully assigned kube-pv-demo to tlx243

 42s 42s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulling pulling image &quot;nginx&quot;

 40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulled Successfully pulled image &quot;nginx&quot;

 40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Created Created container with docker id ae2a50c25e03

 40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Started Started container with docker id ae2a50c25e03
</code></pre><p>The persistent volume is presented as iSCSI device at minion node (tlx243 in this case):</p><pre><code>[root@tlx243 ~]# lsscsi

[0:2:0:0]    disk    SMC      SMC2208          3.24  /dev/sda

[11:0:0:0]   disk    DATERA   IBLOCK           4.0   /dev/sdb


[root@tlx243 datera~iscsi]# mount  ``` grep sdb

/dev/sdb on /var/lib/kubelet/pods/6b99bd2a-628e-11e6-8463-0cc47ab41442/volumes/datera~iscsi/pv-datera-0 type xfs (rw,relatime,attr2,inode64,noquota)
</code></pre><p>Containers running in the pod see this device mounted at /data as specified in the manifest:</p><pre><code>[root@tlx241 /]# kubectl exec kube-pv-demo -c data-pv-demo -it bash

root@kube-pv-demo:/# mount  ``` grep data

/dev/sdb on /data type xfs (rw,relatime,attr2,inode64,noquota)
</code></pre><p><strong>Using Pet Sets</strong></p><p>Typically, pods are treated as stateless units, so if one of them is unhealthy or gets superseded, Kubernetes just disposes it. In contrast, a PetSet is a group of stateful pods that has a stronger notion of identity. The goal of a PetSet is to decouple this dependency by assigning identities to individual instances of an application that are not anchored to the underlying physical infrastructure.</p><p>A PetSet requires {0..n-1} Pets. Each Pet has a deterministic name, PetSetName-Ordinal, and a unique identity. Each Pet has at most one pod, and each PetSet has at most one Pet with a given identity. A PetSet ensures that a specified number of “pets” with unique identities are running at any given time. The identity of a Pet is comprised of:</p><ul><li>a stable hostname, available in DNS</li><li>an ordinal index</li><li>stable storage: linked to the ordinal & hostname</li></ul><p>A typical PetSet definition using a PersistentVolume Claim looks like below:</p><pre><code># A headless service to create DNS records

apiVersion: v1

kind: Service

metadata:

 name: test-service

 labels:

   app: nginx

spec:

 ports:

 - port: 80

   name: web

 clusterIP: None

 selector:

   app: nginx

---

apiVersion: apps/v1alpha1

kind: PetSet

metadata:

 name: test-petset

spec:

 serviceName: &quot;test-service&quot;

 replicas: 2

 template:

   metadata:

     labels:

       app: nginx

     annotations:

       [pod.alpha.kubernetes.io/initialized:](http://pod.alpha.kubernetes.io/initialized:) &quot;true&quot;

   spec:

     terminationGracePeriodSeconds: 0

     containers:

     - name: nginx

       image: [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)

       ports:

       - containerPort: 80

         name: web

       volumeMounts:

       - name: pv-claim

         mountPath: /data

 volumeClaimTemplates:

 - metadata:

     name: pv-claim

     annotations:

       [volume.alpha.kubernetes.io/storage-class:](http://volume.alpha.kubernetes.io/storage-class:) anything

   spec:

     accessModes: [&quot;ReadWriteOnce&quot;]

     resources:

       requests:

         storage: 100Gi
</code></pre><p>We have the following PersistentVolume Claims available:</p><pre><code>[root@tlx241 /]# kubectl get pvc

NAME                     STATUS    VOLUME        CAPACITY   ACCESSMODES   AGE

pv-claim-test-petset-0   Bound     pv-datera-0   0                        41m

pv-claim-test-petset-1   Bound     pv-datera-1   0                        41m

pv-claim-test-petset-2   Bound     pv-datera-2   0                        5s

pv-claim-test-petset-3   Bound     pv-datera-3   0                        2s
</code></pre><p>When this PetSet is provisioned, two pods get instantiated:</p><pre><code>[root@tlx241 /]# kubectl get pods

NAMESPACE     NAME                        READY     STATUS    RESTARTS   AGE

default       test-petset-0               1/1       Running   0          7s

default       test-petset-1               1/1       Running   0          3s
</code></pre><p>Here is how the PetSet test-petset instantiated earlier looks like:</p><pre><code>[root@tlx241 /]# kubectl describe petset test-petset

Name: test-petset

Namespace: default

Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)

Selector: app=nginx

Labels: app=nginx

Replicas: 2 current / 2 desired

Annotations: \&lt;none\&gt;

CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700

Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed

No volumes.

No events.
</code></pre><p>Once a PetSet is instantiated, such as test-petset below, upon increasing the number of replicas (i.e. the number of pods started with that PetSet), more pods get instantiated and more PersistentVolume Claims get bound to new pods:</p><pre><code>[root@tlx241 /]# kubectl patch petset test-petset -p'{&quot;spec&quot;:{&quot;replicas&quot;:&quot;3&quot;}}'

&quot;test-petset” patched


[root@tlx241 /]# kubectl describe petset test-petset

Name: test-petset

Namespace: default

Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)

Selector: app=nginx

Labels: app=nginx

Replicas: 3 current / 3 desired

Annotations: \&lt;none\&gt;

CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700

Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed

No volumes.

No events.


[root@tlx241 /]# kubectl get pods

NAME                        READY     STATUS    RESTARTS   AGE

test-petset-0               1/1       Running   0          29m

test-petset-1               1/1       Running   0          28m

test-petset-2               1/1       Running   0          9s
</code></pre><p>Now the PetSet is running 3 pods after patch application.</p><p>When the above PetSet definition is patched to have one more replica, it introduces one more pod in the system. This in turn results in one more volume getting provisioned on the Datera Data Fabric. So volumes get dynamically provisioned and attached to a pod upon the PetSet scaling up.</p><p>To support the notion of durability and consistency, if a pod moves from one minion to another, volumes do get attached (mounted) to the new minion node and detached (unmounted) from the old minion to maintain persistent access to the data.</p><p><strong>Conclusion</strong></p><p>This demonstrates Kubernetes with Pet Sets orchestrating stateful and stateless workloads. While the Kubernetes community is working on expanding the FlexVolume framework’s capabilities, we are excited that this solution makes it possible for Kubernetes to be run more widely in the datacenters.</p><p>Join and contribute: Kubernetes <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-storage>Storage SIG</a>.</p><ul><li><a href=http://get.k8s.io/>Download Kubernetes</a></li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on the <a href=http://slack.k8s.io/>k8s Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ae83c9a79ab68c86b7fe479bf9b90a6e>Kubernetes Namespaces: use cases and insights</h1><div class="td-byline mb-4"><time datetime=2016-08-16 class=text-muted>Tuesday, August 16, 2016</time></div><p><em>“Who's on first, What's on second, I Don't Know's on third” </em></p><p><em><a href="https://www.youtube.com/watch?v=kTcRRaXV-fg">Who's on First?</a> by Abbott and Costello</em></p><p><strong>Introduction</strong></p><p>Kubernetes is a system with several concepts. Many of these concepts get manifested as “objects” in the RESTful API (often called “resources” or “kinds”). One of these concepts is <a href=/docs/user-guide/namespaces/>Namespaces</a>. In Kubernetes, Namespaces are the way to partition a single Kubernetes cluster into multiple virtual clusters. In this post we’ll highlight examples of how our customers are using Namespaces. </p><p>But first, a metaphor: Namespaces are like human family names. A family name, e.g. Wong, identifies a family unit. Within the Wong family, one of its members, e.g. Sam Wong, is readily identified as just “Sam” by the family. Outside of the family, and to avoid “Which Sam?” problems, Sam would usually be referred to as “Sam Wong”, perhaps even “Sam Wong from San Francisco”.  </p><p>Namespaces are a logical partitioning capability that enable one Kubernetes cluster to be used by multiple users, teams of users, or a single user with multiple applications without concern for undesired interaction. Each user, team of users, or application may exist within its Namespace, isolated from every other user of the cluster and operating as if it were the sole user of the cluster. (Furthermore, <a href=/docs/admin/resourcequota/>Resource Quotas</a> provide the ability to allocate a subset of a Kubernetes cluster’s resources to a Namespace.)</p><p>For all but the most trivial uses of Kubernetes, you will benefit by using Namespaces. In this post, we’ll cover the most common ways that we’ve seen Kubernetes users on Google Cloud Platform use Namespaces, but our list is not exhaustive and we’d be interested to learn other examples from you.</p><p><strong>Use-cases covered</strong></p><ul><li>Roles and Responsibilities in an enterprise for namespaces</li><li>Partitioning landscapes: dev vs. test vs. prod</li><li>Customer partitioning for non-multi-tenant scenarios</li><li>When not to use namespaces</li></ul><p><strong>Use-case #1: Roles and Responsibilities in an Enterprise</strong></p><p>A typical enterprise contains multiple business/technology entities that operate independently of each other with some form of overarching layer of controls managed by the enterprise itself. Operating a Kubernetes clusters in such an environment can be done effectively when roles and responsibilities pertaining to Kubernetes are defined. </p><p>Below are a few recommended roles and their responsibilities that can make managing Kubernetes clusters in a large scale organization easier.</p><ul><li>Designer/Architect role: This role will define the overall namespace strategy, taking into account product/location/team/cost-center and determining how best to map these to Kubernetes Namespaces. Investing in such a role prevents namespace proliferation and “snowflake” Namespaces.</li><li>Admin role: This role has admin access to all Kubernetes clusters. Admins can create/delete clusters and add/remove nodes to scale the clusters. This role will be responsible for patching, securing and maintaining the clusters. As well as implementing Quotas between the different entities in the organization. The Kubernetes Admin is responsible for implementing the namespaces strategy defined by the Designer/Architect. </li></ul><p>These two roles and the actual developers using the clusters will also receive support and feedback from the enterprise security and network teams on issues such as security isolation requirements and how namespaces fit this model, or assistance with networking subnets and load-balancers setup.</p><p>Anti-patterns</p><ol><li>Isolated Kubernetes usage “Islands” without centralized control: Without the initial investment in establishing a centralized control structure around Kubernetes management there is a risk of ending with a “mushroom farm” topology i.e. no defined size/shape/structure of clusters within the org. The result is a difficult to manage, higher risk and elevated cost due to underutilization of resources.</li><li>Old-world IT controls choking usage and innovation: A common tendency is to try and transpose existing on-premises controls/procedures onto new dynamic frameworks .This results in weighing down the agile nature of these frameworks and nullifying the benefits of rapid dynamic deployments.</li><li>Omni-cluster: Delaying the effort of creating the structure/mechanism for namespace management can result in one large omni-cluster that is hard to peel back into smaller usage groups. </li></ol><p><strong>Use-case #2: Using Namespaces to partition development landscapes</strong></p><p>Software development teams customarily partition their development pipelines into discrete units. These units take various forms and use various labels but will tend to result in a discrete dev environment, a testing|QA environment, possibly a staging environment and finally a production environment. The resulting layouts are ideally suited to Kubernetes Namespaces. Each environment or stage in the pipeline becomes a unique namespace.</p><p>The above works well as each namespace can be templated and mirrored to the next subsequent environment in the dev cycle, e.g. dev->qa->prod. The fact that each namespace is logically discrete allows the development teams to work within an isolated “development” namespace. DevOps (The closest role at Google is called <a href=https://landing.google.com/sre/interview/ben-treynor.html>Site Reliability Engineering</a> “SRE”)  will be responsible for migrating code through the pipelines and ensuring that appropriate teams are assigned to each environment. Ultimately, DevOps is solely responsible for the final, production environment where the solution is delivered to the end-users.</p><p>A major benefit of applying namespaces to the development cycle is that the naming of software components (e.g. micro-services/endpoints) can be maintained without collision across the different environments. This is due to the isolation of the Kubernetes namespaces, e.g. serviceX in dev would be referred to as such across all the other namespaces; but, if necessary, could be uniquely referenced using its full qualified name serviceX.development.mycluster.com in the development namespace of mycluster.com.</p><p>Anti-patterns</p><ol><li>Abusing the namespace benefit resulting in unnecessary environments in the development pipeline. So; if you don’t do staging deployments, don’t create a “staging” namespace.</li><li>Overcrowding namespaces e.g. having all your development projects in one huge “development” namespace. Since namespaces attempt to partition, use these to partition by your projects as well. Since Namespaces are flat, you may wish something similar to: projectA-dev, projectA-prod as projectA’s namespaces.</li></ol><p><strong>Use-case #3: Partitioning of your Customers</strong></p><p>If you are, for example, a consulting company that wishes to manage separate applications for each of your customers, the partitioning provided by Namespaces aligns well. You could create a separate Namespace for each customer, customer project or customer business unit to keep these distinct while not needing to worry about reusing the same names for resources across projects.</p><p>An important consideration here is that Kubernetes does not currently provide a mechanism to enforce access controls across namespaces and so we recommend that you do not expose applications developed using this approach externally.</p><p>Anti-patterns</p><ol><li>Multi-tenant applications don’t need the additional complexity of Kubernetes namespaces since the application is already enforcing this partitioning.</li><li>Inconsistent mapping of customers to namespaces. For example, you win business at a global corporate, you may initially consider one namespace for the enterprise not taking into account that this customer may prefer further partitioning e.g. BigCorp Accounting and BigCorp Engineering. In this case, the customer’s departments may each warrant a namespace.</li></ol><p><strong>When Not to use Namespaces</strong></p><p>In some circumstances Kubernetes Namespaces will not provide the isolation that you need. This may be due to geographical, billing or security factors. For all the benefits of the logical partitioning of namespaces, there is currently no ability to enforce the partitioning. Any user or resource in a Kubernetes cluster may access any other resource in the cluster regardless of namespace. So, if you need to protect or isolate resources, the ultimate namespace is a separate Kubernetes cluster against which you may apply your regular security|ACL controls.</p><p>Another time when you may consider not using namespaces is when you wish to reflect a geographically distributed deployment. If you wish to deploy close to US, EU and Asia customers, a Kubernetes cluster deployed locally in each region is recommended.</p><p>When fine-grained billing is required perhaps to chargeback by cost-center or by customer, the recommendation is to leave the billing to your infrastructure provider. For example, in Google Cloud Platform (GCP), you could use a separate GCP <a href=https://cloud.google.com/compute/docs/projects>Project</a> or <a href=https://support.google.com/cloud/answer/6288653>Billing Account</a> and deploy a Kubernetes cluster to a specific-customer’s project(s).</p><p>In situations where confidentiality or compliance require complete opaqueness between customers, a Kubernetes cluster per customer/workload will provide the desired level of isolation. Once again, you should delegate the partitioning of resources to your provider.</p><p>Work is underway to provide (a) ACLs on Kubernetes Namespaces to be able to enforce security; (b) to provide Kubernetes <a href=https://kubernetes.io/blog/2016/07/cross-cluster-services>Cluster Federation</a>. Both mechanisms will address the reasons for the separate Kubernetes clusters in these anti-patterns. </p><p>An easy to grasp <strong>anti-pattern</strong> for Kubernetes namespaces is versioning. You should not use Namespaces as a way to disambiguate versions of your Kubernetes resources. Support for versioning is present in the containers and container registries as well as in Kubernetes Deployment resource. Multiple versions should coexist by utilizing the Kubernetes container model which also provides for auto migration between versions with deployments. Furthermore versions scope namespaces will cause massive proliferation of namespaces within a cluster making it hard to manage.</p><p><strong>Caveat Gubernator</strong></p><p>You may wish to, but you cannot create a hierarchy of namespaces. Namespaces cannot be nested within one another. You can’t, for example, create my-team.my-org as a namespace but could perhaps have team-org.</p><p>Namespaces are easy to create and use but it’s also easy to deploy code inadvertently into the wrong namespace. Good DevOps hygiene suggests documenting and automating processes where possible and this will help. The other way to avoid using the wrong namespace is to set a <a href=/docs/user-guide/kubectl/kubectl_config_set-context/>kubectl context</a>. </p><p>As mentioned previously, Kubernetes does not (currently) provide a mechanism to enforce security across Namespaces. You should only use Namespaces within trusted domains (e.g. internal use) and not use Namespaces when you need to be able to provide guarantees that a user of the Kubernetes cluster or ones its resources be unable to access any of the other Namespaces resources. This enhanced security functionality is being discussed in the Kubernetes Special Interest Group for Authentication and Authorization, get involved at <a href=https://github.com/kubernetes/kubernetes/wiki/SIG-Auth>SIG-Auth</a>. </p><p><em>--Mike Altarace & Daz Wilkin, Strategic Customer Engineers, Google Cloud Platform</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-28828ac2d323ef978d2063693d4b7de1>SIG Apps: build apps for and operate them in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-08-16 class=text-muted>Tuesday, August 16, 2016</time></div><p><em>Editor’s note: This post is by the Kubernetes SIG-Apps team sharing how they focus on the developer and devops experience of running applications in Kubernetes.</em></p><p>Kubernetes is an incredible manager for containerized applications. Because of this, <a href=https://kubernetes.io/blog/2016/02/sharethis-kubernetes-in-production>numerous</a> <a href=https://blog.box.com/blog/kubernetes-box-microservices-maximum-velocity/>companies</a> <a href=http://techblog.yahoo.co.jp/infrastructure/os_n_k8s/>have</a> <a href=http://www.nextplatform.com/2015/11/12/inside-ebays-shift-to-kubernetes-and-containers-atop-openstack/>started</a> to run their applications in Kubernetes.</p><p>Kubernetes Special Interest Groups (<a href=https://github.com/kubernetes/community/blob/master/README.md#special-interest-groups-sig>SIGs</a>) have been around to support the community of developers and operators since around the 1.0 release. People organized around networking, storage, scaling and other operational areas.</p><p>As Kubernetes took off, so did the need for tools, best practices, and discussions around building and operating cloud native applications. To fill that need the Kubernetes <a href=https://github.com/kubernetes/community/tree/master/sig-apps>SIG Apps</a> came into existence.</p><p>SIG Apps is a place where companies and individuals can:</p><ul><li>see and share demos of the tools being built to enable app operators</li><li>learn about and discuss needs of app operators</li><li>organize around efforts to improve the experience</li></ul><p>Since the inception of SIG Apps we’ve had demos of projects like <a href=https://github.com/opencredo/kubefuse>KubeFuse</a>, <a href=https://github.com/kubespray/kpm>KPM</a>, and <a href=https://stacksmith.bitnami.com/>StackSmith</a>. We’ve also executed on a survey of those operating apps in Kubernetes.</p><p>From the survey results we’ve learned a number of things including:</p><ul><li>That 81% of respondents want some form of autoscaling</li><li>To store secret information 47% of respondents use built-in secrets. At reset these are not currently encrypted. (If you want to help add encryption there is an <a href=https://github.com/kubernetes/kubernetes/issues/10439>issue</a> for that.) </li><li>The most responded questions had to do with 3rd party tools and debugging</li><li>For 3rd party tools to manage applications there were no clear winners. There are a wide variety of practices</li><li>An overall complaint about a lack of useful documentation. (Help contribute to the docs <a href=https://github.com/kubernetes/kubernetes.github.io>here</a>.)</li><li>There’s a lot of data. Many of the responses were optional so we were surprised that 935 of all questions across all candidates were filled in. If you want to look at the data yourself it’s <a href="https://docs.google.com/spreadsheets/d/15SUL7QTpR4Flrp5eJ5TR8A5ZAFwbchfX2QL4MEoJFQ8/edit?usp=sharing">available online</a>.</li></ul><p>When it comes to application operation there’s still a lot to be figured out and shared. If you've got opinions about running apps, tooling to make the experience better, or just want to lurk and learn about what's going please come join us.</p><ul><li>Chat with us on SIG-Apps <a href=https://kubernetes.slack.com/messages/sig-apps>Slack channel</a></li><li>Email as at SIG-Apps <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-apps>mailing list</a></li><li>Join our open meetings: weekly at 9AM PT on Wednesdays, <a href=https://github.com/kubernetes/community/blob/master/sig-apps/README.md#meeting>full details here</a>.</li></ul><p><em>--Matt Farina, Principal Engineer, Hewlett Packard Enterprise</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e61f98be9ec40572e2f7acaff60a9177>Create a Couchbase cluster using Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-08-15 class=text-muted>Monday, August 15, 2016</time></div><p><em>Editor’s note: today’s guest post is by Arun Gupta, Vice President Developer Relations at Couchbase, showing how to setup a Couchbase cluster with Kubernetes.</em></p><p><a href=http://www.couchbase.com/nosql-databases/couchbase-server>Couchbase Server</a> is an open source, distributed NoSQL document-oriented database. It exposes a fast key-value store with managed cache for submillisecond data operations, purpose-built indexers for fast queries and a query engine for executing SQL queries. For mobile and Internet of Things (IoT) environments, <a href=http://developer.couchbase.com/mobile>Couchbase Lite</a> runs native on-device and manages sync to Couchbase Server.</p><p>Couchbase Server 4.5 was <a href=http://blog.couchbase.com/2016/june/announcing-couchbase-server-4.5>recently announced</a>, bringing <a href=http://developer.couchbase.com/documentation/server/4.5/introduction/whats-new.html>many new features</a>, including <a href=http://www.couchbase.com/press-releases/couchbase-announces-support-for-docker-containers>production certified support for Docker</a>. Couchbase is supported on a wide variety of orchestration frameworks for Docker containers, such as Kubernetes, Docker Swarm and Mesos, for full details visit <a href=http://couchbase.com/containers>this page</a>.</p><p>This blog post will explain how to create a Couchbase cluster using Kubernetes. This setup is tested using Kubernetes 1.3.3, Amazon Web Services, and Couchbase 4.5 Enterprise Edition.</p><p>Like all good things, this post is standing on the shoulder of giants. The design pattern used in this blog was defined in a <a href=https://twitter.com/arungupta/status/703378246432231424>Friday afternoon hack</a> with <a href=https://twitter.com/saturnism>@saturnism</a>. A working version of the configuration files was <a href=https://twitter.com/arungupta/status/759059647680552962>contributed</a> by <a href=http://twitter.com/r_schmiddy>@r_schmiddy</a>.</p><p><strong>Couchbase Cluster</strong></p><p>A cluster of Couchbase Servers is typically deployed on commodity servers. Couchbase Server has a peer-to-peer topology where all the nodes are equal and communicate to each other on demand. There is no concept of master nodes, slave nodes, config nodes, name nodes, head nodes, etc, and all the software loaded on each node is identical. It allows the nodes to be added or removed without considering their “type”. This model works particularly well with cloud infrastructure in general. For Kubernetes, this means that we can use the exact same container image for all Couchbase nodes.</p><p>A typical Couchbase cluster creation process looks like:</p><ul><li>Start Couchbase: Start n Couchbase servers</li><li>Create cluster: Pick any server, and add all other servers to it to create the cluster</li><li>Rebalance cluster: Rebalance the cluster so that data is distributed across the cluster</li></ul><p>In order to automate using Kubernetes, the cluster creation is split into a “master” and “worker” Replication Controller (RC).</p><p>The master RC has only one replica and is also published as a Service. This provides a single reference point to start the cluster creation. By default services are visible only from inside the cluster. This service is also exposed as a load balancer. This allows the <a href=http://developer.couchbase.com/documentation/server/current/admin/ui-intro.html>Couchbase Web Console</a> to be accessible from outside the cluster.</p><p>The worker RC use the exact same image as master RC. This keeps the cluster homogenous which allows to scale the cluster easily.</p><p><img src=https://lh6.googleusercontent.com/yS4MqPJG6hQeFa8jcLL9CKy0dD6waghxzFAccS5OIDQJwGNsRmBN531RsByypTBILdJ0yFT3HmbaXOCKgiUr836zx50uOnxa5SIeWb1VaOqo_adepGnJe4L-LATAQtlrQgte7Je1 alt></p><p>Configuration files used in this blog are available <a href=http://github.com/arun-gupta/couchbase-kubernetes/tree/master/cluster>here</a>. Let’s create the Kubernetes resources to create the Couchbase cluster.</p><p><strong>Create Couchbase “master” Replication Controller</strong></p><p>Couchbase master RC can be created using the following configuration file:</p><pre><code>apiVersion: v1  
kind: ReplicationController  
metadata:  
  name: couchbase-master-rc  
spec:  
  replicas: 1  
  selector:  
    app: couchbase-master-pod  
  template:  
    metadata:  
      labels:  
        app: couchbase-master-pod  
    spec:  
      containers:  
      - name: couchbase-master  
        image: arungupta/couchbase:k8s  
        env:  
          - name: TYPE  
            value: MASTER  
        ports:  
        - containerPort: 8091  
----  
apiVersion: v1  
kind: Service  
metadata:   
  name: couchbase-master-service  
  labels:   
    app: couchbase-master-service  
spec:   
  ports:  
    - port: 8091  
  selector:   
    app: couchbase-master-pod  
  type: LoadBalancer
</code></pre><p>This configuration file creates a couchbase-master-rc Replication Controller. This RC has one replica of the pod created using the arungupta/couchbase:k8s image. This image is created using the Dockerfile <a href=http://github.com/arun-gupta/couchbase-kubernetes/blob/master/cluster/Dockerfile>here</a>. This Dockerfile uses a <a href=https://github.com/arun-gupta/couchbase-kubernetes/blob/master/cluster/configure-node.sh>configuration script</a> to configure the base Couchbase Docker image. First, it uses <a href=http://developer.couchbase.com/documentation/server/current/rest-api/rest-endpoints-all.html>Couchbase REST API</a> to setup memory quota, setup index, data and query services, security credentials, and loads a sample data bucket. Then, it invokes the appropriate <a href=http://developer.couchbase.com/documentation/server/current/cli/cbcli-intro.html>Couchbase CLI</a> commands to add the Couchbase node to the cluster or add the node and rebalance the cluster. This is based upon three environment variables:</p><ul><li>TYPE: Defines whether the joining pod is worker or master</li><li>AUTO_REBALANCE: Defines whether the cluster needs to be rebalanced</li><li>COUCHBASE_MASTER: Name of the master service</li></ul><p>For this first configuration file, the TYPE environment variable is set to MASTER and so no additional configuration is done on the Couchbase image.</p><p>Let’s create and verify the artifacts.</p><p>Create Couchbase master RC:</p><pre><code>kubectl create -f cluster-master.yml   
replicationcontroller &quot;couchbase-master-rc&quot; created  
service &quot;couchbase-master-service&quot; created
</code></pre><p>List all the services:</p><pre><code>kubectl get svc  
NAME                       CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE  
couchbase-master-service   10.0.57.201                 8091/TCP   30s  
kubernetes                 10.0.0.1      \&lt;none\&gt;        443/TCP    5h
</code></pre><p>Output shows that couchbase-master-service is created.</p><p>Get all the pods:</p><pre><code>kubectl get po  
NAME                        READY     STATUS    RESTARTS   AGE  
couchbase-master-rc-97mu5   1/1       Running   0          1m
</code></pre><p>A pod is created using the Docker image specified in the configuration file.</p><p>Check the RC:</p><pre><code>kubectl get rc  
NAME                  DESIRED   CURRENT   AGE  
couchbase-master-rc   1         1         1m
</code></pre><p>It shows that the desired and current number of pods in the RC are matching.</p><p>Describe the service:</p><pre><code>kubectl describe svc couchbase-master-service  
Name: couchbase-master-service  
Namespace: default  
Labels: app=couchbase-master-service  
Selector: app=couchbase-master-pod  
Type: LoadBalancer  
IP: 10.0.57.201  
LoadBalancer Ingress: a94f1f286590c11e68e100283628cd6c-1110696566.us-west-2.elb.amazonaws.com  
Port: \&lt;unset\&gt; 8091/TCP  
NodePort: \&lt;unset\&gt; 30019/TCP  
Endpoints: 10.244.2.3:8091  
Session Affinity: None  
Events:

  FirstSeen LastSeen Count From SubobjectPath Type Reason Message

  --------- -------- ----- ---- ------------- -------- ------ -------

  2m 2m 1 {service-controller } Normal CreatingLoadBalancer Creating load balancer

  2m 2m 1 {service-controller } Normal CreatedLoadBalancer Created load balancer
</code></pre><p>Among other details, the address shown next to LoadBalancer Ingress is relevant for us. This address is used to access the Couchbase Web Console.</p><p>Wait for ~3 mins for the load balancer to be ready to receive requests. Couchbase Web Console is accessible at &lt;ip>:8091 and looks like:</p><p><img src=https://lh5.googleusercontent.com/LFD6JM9zdP7iriH501VipG06GYEs98aEnWJABvxkZZBZeNJpmXXd2FZFQ9C8nn8gOySQoyrzsgR0c021EGmGWobkgFEgSuYt4lfp6lLooYaX4WhisFPHF_7qqUK4TQKhS9w0G0vb alt></p><p>The image used in the configuration file is configured with the Administrator username and password password. Enter the credentials to see the console:</p><p><img src=https://lh3.googleusercontent.com/iyU_n0wfoxl3ItXmRVW6xPHMHWQV72okNvLmXxn2Cc3NR4NCtgGzkNXOB_ZuiU_dulID-wQKKsQMVgFoAuSlxItYdJrBRECWAa25Xqp9M3-f8u0KEUXx9jYQPskkv-jJagvB9Sje alt></p><p>Click on Server Nodes to see how many Couchbase nodes are part of the cluster. As expected, it shows only one node:</p><p><img src=https://lh5.googleusercontent.com/TPeK0R9HFs73TxGhK2y37ui1R0dtpTCTHM4ItrBxjWIHP8lhilPGpZbnOl_d9PaLu4DzUPHvbShX1IXUDgglpRskx6wAiHEM1520kyYYlgKiEZWaJBXd0Coy-tL7Zty2tdHHBSUb alt></p><p>Click on Data Buckets to see a sample bucket that was created as part of the image:</p><p><img src=https://lh5.googleusercontent.com/kj-v_sgXzeFTY_Dm6IZyTbZ6QgRKn_zIxqsCmpVqlaykOMvVejgiRvvyAs1qyqDWMJDya58XBtWBQJrd04XHp7VfQ_SdzssmfwzRvodwynIXqqJLT_NPsBbJ7soJSeswynFFUvVk alt></p><p>This shows the travel-sample bucket is created and has 31,591 JSON documents.</p><p><strong>Create Couchbase “worker” Replication Controller</strong><br>Now, let’s create a worker replication controller. It can be created using the configuration file:</p><pre><code>apiVersion: v1  
kind: ReplicationController  
metadata:  
  name: couchbase-worker-rc  
spec:  
  replicas: 1  
  selector:  
    app: couchbase-worker-pod  
  template:  
    metadata:  
      labels:  
        app: couchbase-worker-pod  
    spec:  
      containers:  
      - name: couchbase-worker  
        image: arungupta/couchbase:k8s  
        env:  
          - name: TYPE  
            value: &quot;WORKER&quot;  
          - name: COUCHBASE\_MASTER  
            value: &quot;couchbase-master-service&quot;  
          - name: AUTO\_REBALANCE  
            value: &quot;false&quot;  
        ports:  
        - containerPort: 8091
</code></pre><p>This RC also creates a single replica of Couchbase using the same arungupta/couchbase:k8s image. The key differences here are:</p><ul><li>TYPE environment variable is set to WORKER. This adds a worker Couchbase node to be added to the cluster.</li><li>COUCHBASE_MASTER environment variable is passed the value of couchbase-master-service. This uses the service discovery mechanism built into Kubernetes for pods in the worker and the master to communicate.</li><li>AUTO_REBALANCE environment variable is set to false. This ensures that the node is only added to the cluster but the cluster itself is not rebalanced. Rebalancing is required to re-distribute data across multiple nodes of the cluster. This is the recommended way as multiple nodes can be added first, and then cluster can be manually rebalanced using the Web Console.
Let’s create a worker:</li></ul><pre><code>kubectl create -f cluster-worker.yml   
replicationcontroller &quot;couchbase-worker-rc&quot; created
</code></pre><p>Check the RC:</p><pre><code>kubectl get rc  
NAME                  DESIRED   CURRENT   AGE  
couchbase-master-rc   1         1         6m  
couchbase-worker-rc   1         1         22s
</code></pre><p>A new couchbase-worker-rc is created where the desired and the current number of instances are matching.</p><p>Get all pods:</p><pre><code>kubectl get po  
NAME                        READY     STATUS    RESTARTS   AGE  
couchbase-master-rc-97mu5   1/1       Running   0          6m  
couchbase-worker-rc-4ik02   1/1       Running   0          46s
</code></pre><p>An additional pod is now created. Each pod’s name is prefixed with the corresponding RC’s name. For example, a worker pod is prefixed with couchbase-worker-rc.</p><p>Couchbase Web Console gets updated to show that a new Couchbase node is added. This is evident by red circle with the number 1 on the Pending Rebalance tab.</p><p><img src=https://lh4.googleusercontent.com/cyrpBtTCFq3rv5YcMvgVdhQN_hQ5WXVxhJSvFdIMQY3GIFJgYVGUpCBrGAp6MBqxHSzfxyEUddF2ijoMncJxKvAgGlKkDMuLOyu0kQKaWi78OW0dmXUtiNQcs0rEEvR_akFtb4UB alt></p><p>Clicking on the tab shows the IP address of the node that needs to be rebalanced:</p><p><img src=https://lh4.googleusercontent.com/a6-OinZVl2Ij_fnIswNI1TWbbRgkG-KIlfPSaIrGSm36i0GS0Uk0CjcqEW64jGydDE4EpuYC4kaOkSbHi6-3kbwdngRyP-qZMOkM3jpf4dZrhJJ0qHgxUp9GcItUOnR5MzuGQ7kF alt></p><p><strong>Scale Couchbase cluster</strong></p><p>Now, let’s scale the Couchbase cluster by scaling the replicas for worker RC:</p><pre><code>kubectl scale rc couchbase-worker-rc --replicas=3  
replicationcontroller &quot;couchbase-worker-rc&quot; scaled
</code></pre><p>Updated state of RC shows that 3 worker pods have been created:</p><pre><code>kubectl get rc  
NAME                  DESIRED   CURRENT   AGE  
couchbase-master-rc   1         1         8m  
couchbase-worker-rc   3         3         2m
</code></pre><p>This can be verified again by getting the list of pods:</p><pre><code>kubectl get po  
NAME                        READY     STATUS    RESTARTS   AGE  
couchbase-master-rc-97mu5   1/1       Running   0          8m  
couchbase-worker-rc-4ik02   1/1       Running   0          2m  
couchbase-worker-rc-jfykx   1/1       Running   0          53s  
couchbase-worker-rc-v8vdw   1/1       Running   0          53s
</code></pre><p>Pending Rebalance tab of Couchbase Web Console shows that 3 servers have now been added to the cluster and needs to be rebalanced.</p><p><img src=https://lh4.googleusercontent.com/WhruCJSzQs3HpqxcklPGe7vqHQ8Ie1XUZk88vZxrngLcgRowT3Kj_-abNPOQ3t_Jzt-pi6Gn2PGXsncAnh6TGdQr71T1pls7FfgszjTJojf72Wk-Aq-obgf1WslrEvjK0gTLmS0W alt></p><p>Rebalance Couchbase Cluster</p><p>Finally, click on Rebalance button to rebalance the cluster. A message window showing the current state of rebalance is displayed:</p><p><img src=https://lh6.googleusercontent.com/xy-hy554W3dopsGDRYPhMdmaLLh3mknBtZNOHs6U5jqX5SX46u8-pExyt5kRaLGLzaypgX8nXDt2RQd29_AXCgHbzkr3vVYe9OteDX0tUjGnsTGSwrazvk6ziBkj52OM1yNN7dea alt></p><p>Once all the nodes are rebalanced, Couchbase cluster is ready to serve your requests:</p><p><img src=https://lh5.googleusercontent.com/WoxQdHde7aws5lc3g1OaG3xDwZtZA1OhLpeHJOtTJnuW4-OrT9VhyVENoWyYYfRaE-h2HDxp3mS2XIj5_P2Uyg-p7ApL8BjgtEfNQscvQd0i7HnSPBy7Ow9PiIsP1IyIlM89-f_V alt></p><p>In addition to creating a cluster, Couchbase Server supports a range of <a href=http://developer.couchbase.com/documentation/server/current/ha-dr/ha-dr-intro.html>high availability and disaster recovery</a> (HA/DR) strategies. Most HA/DR strategies rely on a multi-pronged approach of maximizing availability, increasing redundancy within and across data centers, and performing regular backups.</p><p>Now that your Couchbase cluster is ready, you can run your first <a href=http://developer.couchbase.com/documentation/server/current/travel-app/index.html>sample application</a>.</p><p>For further information check out the Couchbase <a href=http://developer.couchbase.com/server>Developer Portal</a> and <a href=https://forums.couchbase.com/>Forums</a>, or see questions on <a href=http://stackoverflow.com/questions/tagged/couchbase>Stack Overflow</a>.</p><p><em>--Arun Gupta, Vice President Developer Relations at Couchbase</em></p><ul><li><a href=http://get.k8s.io/>Download</a> Kubernetes</li><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a></li><li>Post questions (or answer questions) on <a href=http://stackoverflow.com/questions/tagged/kubernetes>Stack Overflow</a></li><li>Connect with the community on <a href=http://slack.k8s.io/>Slack</a></li><li>Follow <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> on Twitter for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2c86abe782cde8635d5620f1e6bc5e7e>Challenges of a Remotely Managed, On-Premises, Bare-Metal Kubernetes Cluster</h1><div class="td-byline mb-4"><time datetime=2016-08-02 class=text-muted>Tuesday, August 02, 2016</time></div><p><em>Today's post is written by Bich Le, chief architect at Platform9, describing how their engineering team overcame challenges in remotely managing bare-metal Kubernetes clusters. </em></p><p><strong>Introduction</strong></p><p>The recently announced <a href=https://platform9.com/press/platform9-makes-easy-deploy-docker-containers-production-scale/>Platform9 Managed Kubernetes</a> (PMK) is an on-premises enterprise Kubernetes solution with an unusual twist: while clusters run on a user’s internal hardware, their provisioning, monitoring, troubleshooting and overall life cycle is managed remotely from the Platform9 SaaS application. While users love the intuitive experience and ease of use of this deployment model, this approach poses interesting technical challenges. In this article, we will first describe the motivation and deployment architecture of PMK, and then present an overview of the technical challenges we faced and how our engineering team addressed them.</p><p><strong>Multi-OS bootstrap model</strong></p><p>Like its predecessor, <a href=https://platform9.com/products/kvm/>Managed OpenStack</a>, PMK aims to make it as easy as possible for an enterprise customer to deploy and operate a “private cloud”, which, in the current context, means one or more Kubernetes clusters. To accommodate customers who standardize on a specific Linux distro, our installation process uses a “bare OS” or “bring your own OS” model, which means that an administrator deploys PMK to existing Linux nodes by installing a simple RPM or Deb package on their favorite OS (Ubuntu-14, CentOS-7, or RHEL-7). The package, which the administrator downloads from their Platform9 SaaS portal, starts an agent which is preconfigured with all the information and credentials needed to securely connect to and register itself with the customer’s Platform9 SaaS controller running on the WAN.</p><p><strong>Node management</strong></p><p>The first challenge was configuring Kubernetes nodes in the absence of a bare-metal cloud API and SSH access into nodes. We solved it using the <em>node pool</em> concept and configuration management techniques. Every node running the agent automatically shows up in the SaaS portal, which allows the user to <em>authorize</em> the node for use with Kubernetes. A newly authorized node automatically enters a <em>node pool</em>, indicating that it is available but not used in any clusters. Independently, the administrator can create one or more Kubernetes clusters, which start out empty. At any later time, he or she can request one or more nodes to be attached to any cluster. PMK fulfills the request by transferring the specified number of nodes from the pool to the cluster. When a node is authorized, its agent becomes a configuration management agent, polling for instructions from a CM server running in the SaaS application and capable of downloading and configuring software.</p><p>Cluster creation and node attach/detach operations are exposed to administrators via a REST API, a CLI utility named <em>qb</em>, and the SaaS-based Web UI. The following screenshot shows the Web UI displaying one 3-node cluster named clus100, one empty cluster clus101, and the three nodes.</p><p><img src=https://lh3.googleusercontent.com/Tn67P9fhhPqCNF6xYl6mfVehG8AtLcLOM0NMW3YukBkWB5cSpYofkLQo1vrqsZiDBON05GC4ZQwWgEV9YBdoNA6Hzy_loS0cvT3BzkxmLesk6UsX_xugsrGppJD-Mc8fjHIF2QrU alt=clusters_and_containervisors_view.png></p><p><strong>Cluster initialization</strong></p><p>The first time one or more nodes are attached to a cluster, PMK configures the nodes to form a complete Kubernetes cluster. Currently, PMK automatically decides the number and placement of Master and Worker nodes. In the future, PMK will give administrators an “advanced mode” option allowing them to override and customize those decisions. Through the CM server, PMK then sends to each node a configuration and a set of scripts to initialize each node according to the configuration. This includes installing or upgrading Docker to the required version; starting 2 docker daemons (bootstrap and main), creating the etcd K/V store, establishing the flannel network layer, starting kubelet, and running the Kubernetes appropriate for the node’s role (master vs. worker). The following diagram shows the component layout of a fully formed cluster.</p><p><img src=https://lh6.googleusercontent.com/ZQZoFL6tDpkiberG_X1CREitwNIDCHnRajnOlJqByU-4HzRQi1RRoDlGj7pGRaqD2a7Yg4xBwQx7oHp_mR8ie96O5w_KMT84av-JMsPMHXeoBpVYn3iJKeGZkWG4q0J06OZMuLIe alt=architecture.png></p><p><strong>Containerized kubelet?</strong></p><p>Another hurdle we encountered resulted from our original decision to run kubelet as recommended by the <a href=/docs/getting-started-guides/docker-multinode/>Multi-node Docker Deployment Guide</a>. We discovered that this approach introduces complexities that led to many difficult-to-troubleshoot bugs that were sensitive to the combined versions of Kubernetes, Docker, and the node OS. Example: kubelet’s need to mount directories containing secrets into containers to support the <a href=/docs/user-guide/service-accounts/>Service Accounts</a> mechanism. It turns out that <a href=https://github.com/kubernetes/kubernetes/issues/6848>doing this from inside of a container is tricky</a>, and requires a <a href=https://github.com/kubernetes/kubernetes/blob/release-1.0/pkg/util/mount/nsenter_mount.go#L37>complex sequence of steps</a> that turned out to be fragile. After fixing a continuing stream of issues, we finally decided to run kubelet as a native program on the host OS, resulting in significantly better stability.</p><p><strong>Overcoming networking hurdles</strong></p><p>The Beta release of PMK currently uses <a href=https://github.com/coreos/flannel>flannel with UDP back-end</a> for the network layer. In a Kubernetes cluster, many infrastructure services need to communicate across nodes using a variety of ports (443, 4001, etc..) and protocols (TCP and UDP). Often, customer nodes intentionally or unintentionally block some or all of the traffic, or run existing services that conflict with the required ports, resulting in non-obvious failures. To address this, we try to detect configuration problems early and inform the administrator immediately. PMK runs a “preflight” check on all nodes participating in a cluster before installing the Kubernetes software. This means running small test programs on each node to verify that (1) the required ports are available for binding and listening; and (2) nodes can connect to each other using all required ports and protocols. These checks run in parallel and take less than a couple of seconds before cluster initialization.</p><p><strong>Monitoring</strong></p><p>One of the values of a SaaS-managed private cloud is constant monitoring and early detection of problems by the SaaS team. Issues that can be addressed without intervention by the customer are handled automatically, while others trigger proactive communication with the customer via UI alerts, email, or real-time channels. Kubernetes monitoring is a huge topic worthy of its own blog post, so we’ll just briefly touch upon it. We broadly classify the problem into layers: (1) hardware & OS, (2) Kubernetes core (e.g. API server, controllers and kubelets), (3) add-ons (e.g. <a href=https://github.com/skynetservices/skydns>SkyDNS</a> & <a href=https://github.com/kubernetes/contrib/tree/master/service-loadbalancer>ServiceLoadbalancer</a>) and (4) applications. We are currently focused on layers 1-3. A major source of issues we’ve seen is add-on failures. If either DNS or the ServiceLoadbalancer reverse http proxy (soon to be upgraded to an <a href=https://github.com/kubernetes/contrib/tree/master/ingress/controllers>Ingress Controller</a>) fails, application services will start failing. One way we detect such failures is by monitoring the components using the Kubernetes API itself, which is proxied into the SaaS controller, allowing the PMK support team to monitor any cluster resource. To detect service failure, one metric we pay attention to is <em>pod restarts</em>. A high restart count indicates that a service is continually failing.</p><p><strong>Future topics</strong></p><p>We faced complex challenges in other areas that deserve their own posts: (1) <em>Authentication and authorization with <a href=http://docs.openstack.org/developer/keystone/>Keystone</a></em>, the identity manager used by Platform9 products; (2) <em>Software upgrades</em>, i.e. how to make them brief and non-disruptive to applications; and (3) <em>Integration with customer’s external load-balancers</em> (in the absence of good automation APIs).</p><p><strong>Conclusion</strong></p><p><a href=https://platform9.com/products/docker/>Platform9 Managed Kubernetes</a> uses a SaaS-managed model to try to hide the complexity of deploying, operating and maintaining bare-metal Kubernetes clusters in customers’ data centers. These requirements led to the development of a unique cluster deployment and management architecture, which in turn led to unique technical challenges.This article described an overview of some of those challenges and how we solved them. For more information on the motivation behind PMK, feel free to view Madhura Maskasky's <a href=https://platform9.com/blog/containers-as-a-service-kubernetes-docker/>blog post</a>.</p><p><em>--Bich Le, Chief Architect, Platform9</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4491736ff7cd2b1bb3cd806b10bfa89a>Why OpenStack's embrace of Kubernetes is great for both communities</h1><div class="td-byline mb-4"><time datetime=2016-07-26 class=text-muted>Tuesday, July 26, 2016</time></div><p>Today, <a href=https://www.mirantis.com/>Mirantis</a>, the leading contributor to <a href="http://stackalytics.com/?release=mitaka">OpenStack</a>, <a href=https://techcrunch.com/2016/07/25/openstack-will-soon-be-able-to-run-on-top-of-kubernetes/>announced</a> that it will re-write its private cloud platform to use Kubernetes as its underlying orchestration engine. We think this is a great step forward for both the OpenStack and Kubernetes communities. With Kubernetes under the hood, OpenStack users will benefit from the tremendous efficiency, manageability and resiliency that Kubernetes brings to the table, while positioning their applications to use more cloud-native patterns. The Kubernetes community, meanwhile, can feel confident in their choice of orchestration framework, while gaining the ability to manage both container- and VM-based applications from a single platform.</p><p><strong>The Path to Cloud Native</strong></p><p>Google spent over ten years developing, applying and refining the principles of cloud native computing. A cloud-native application is:</p><ul><li>Container-packaged. Applications are composed of hermetically sealed, reusable units across diverse environments;</li><li>Dynamically scheduled, for increased infrastructure efficiency and decreased operational overhead; and </li><li>Microservices-based. Loosely coupled components significantly increase the overall agility, resilience and maintainability of applications.</li></ul><p>These principles have enabled us to build the largest, most efficient, most powerful cloud infrastructure in the world, which anyone can access via <a href=http://cloud.google.com/>Google Cloud Platform</a>. They are the same principles responsible for the recent surge in popularity of Linux containers. Two years ago, we open-sourced Kubernetes to spur adoption of containers and scalable, microservices-based applications, and the recently released <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes version 1.3</a> introduces a number of features to bridge enterprise and cloud native workloads. We expect that adoption of cloud-native principles will drive the same benefits within the OpenStack community, as well as smoothing the path between OpenStack and the public cloud providers that embrace them.</p><p><strong>Making OpenStack better</strong></p><p>We hear from enterprise customers that they want to move towards cloud-native infrastructure and application patterns. Thus, it is hardly surprising that OpenStack would also move in this direction [1], with large OpenStack users such as <a href=http://fortune.com/2016/04/23/ebay-parlays-new-age-tools/>eBay</a> and <a href=http://thenewstack.io/tns-analysts-show-95-consider-containerizing-openstack/>GoDaddy</a> adopting Kubernetes as key components of their stack. Kubernetes and cloud-native patterns will improve OpenStack lifecycle management by enabling rolling updates, versioning, and canary deployments of new components and features. In addition, OpenStack users will benefit from self-healing infrastructure, making OpenStack easier to manage and more resilient to the failure of core services and individual compute nodes. Finally, OpenStack users will realize the developer and resource efficiencies that come with a container-based infrastructure.</p><p><strong>OpenStack is a great tool for Kubernetes users</strong></p><p>Conversely, incorporating Kubernetes into OpenStack will give Kubernetes users access to a robust framework for deploying and managing applications built on virtual machines. As users move to the cloud-native model, they will be faced with the challenge of managing hybrid application architectures that contain some mix of virtual machines and Linux containers. The combination of Kubernetes and OpenStack means that they can do so on the same platform using a common set of tools.</p><p>We are excited by the ever increasing momentum of the cloud-native movement as embodied by Kubernetes and related projects, and look forward to working with Mirantis, its partner Intel, and others within the OpenStack community to brings the benefits of cloud-native to their applications and infrastructure.</p><p><em>--Martin Buhr, Product Manager, Strategic Initiatives, Google</em></p><p>[1] Check out the announcement of Kubernetes-OpenStack Special Interest Group <a href=https://kubernetes.io/blog/2016/04/introducing-kubernetes-openstack-sig>here</a>, and a great talk about OpenStack on Kubernetes by CoreOS CEO Alex Polvi at the most recent OpenStack summit <a href="https://www.youtube.com/watch?v=e-j9FOO-i84">here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-802922a147c9feff6990b4a886b0f000>A Very Happy Birthday Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-07-21 class=text-muted>Thursday, July 21, 2016</time></div><p>Last year at OSCON, I got to reconnect with a bunch of friends and see what they have been working on. That turned out to be the <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP0Ljwa9J98xUd6UlM604Y-l">Kubernetes 1.0 launch event</a>. Even that day, it was clear the project was supported by a broad community -- a group that showed an ambitious vision for distributed computing. </p><p>Today, on the first anniversary of the Kubernetes 1.0 launch, it’s amazing to see what a community of dedicated individuals can do. Kubernauts have collectively put in <a href=https://www.openhub.net/p/kubernetes>237 person years of coding effort</a> since launch to bring forward our most recent <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>release 1.3</a>. However the community is much more than simply coding effort. It is made up of people -- individuals that have given their expertise and energy to make this project flourish. With more than 830 diverse contributors, from independents to the largest companies in the world, it’s their work that makes Kubernetes stand out. Here are stories from a couple early contributors reflecting back on the project:</p><ul><li><a href=https://www.box.com/blog/kubernetes-box-microservices-maximum-velocity/>Sam Ghods</a>, services architect and co-founder at Box</li><li><a href=https://kubernetes.io/blog/2016/07/oh-the-places-you-will-go>Justin Santa Barbara</a>, independent Kubernetes contributor</li><li><a href=https://kubernetes.io/blog/2016/07/the-bet-on-kubernetes>Clayton Coleman</a>, contributor and architect on Kubernetes on OpenShift at Red Hat</li></ul><p>The community is also more than online GitHub and Slack conversation; year one saw the launch of KubeCon, the Kubernetes user conference, which started as a grassroot effort that brought together 1,000 individuals between two events in San Francisco and London. The advocacy continues with users globally. There are more than <a href=http://www.meetup.com/topics/kubernetes/>130 Meetup groups</a> that mention Kubernetes, many of which are helping celebrate Kubernetes’ birthday. To join the celebration, participate at one of the 20 <a href="https://twitter.com/search?q=k8sbday&src=typd"><strong>#k8sbday</strong></a> parties worldwide: <a href=http://www.meetup.com/Microservices-and-Containers-Austin/>Austin</a>, <a href=http://www.meetup.com/Bangalore-Kubernetes-Meetup/>Bangalore</a>, <a href=http://www.meetup.com/Kubernetes-Meetup-Beijing/events/232537953/>Beijing</a>, <a href=http://www.meetup.com/Boston-OpenShift-Meetup/>Boston</a>, <a href=http://www.meetup.com/Cape-Town-DevOps>Cape Town</a>, <a href=http://www.meetup.com/ccog-meetup/events/231626855/>Charlotte</a>, <a href=http://www.meetup.com/de-DE/Kubernetes-Meetup-Cologne/>Cologne</a>, <a href=http://www.meetup.com/Kubernetes-Geneva/>Geneva</a>, <a href=http://www.meetup.com/inovex-karlsruhe/events/232561446/>Karlsruhe</a>, <a href=http://www.meetup.com/Docker-Kisumu/events/232595339/>Kisumu</a>, <a href=http://www.meetup.com/Kubernetes-Montreal/events/232726956/>Montreal</a>, <a href=http://www.meetup.com/Cloud-Native-PDX>Portland</a>, <a href=http://www.meetup.com/Raleigh-Openshift-Meetup/>Raleigh</a>, <a href=http://www.meetup.com/Triangle-Kubernetes-Meetup/>Research Triangle</a>, <a href=https://www.eventbrite.com/e/kubernetes-birthday-bash-tickets-26250411688>San Francisco</a>, <a href=http://www.meetup.com/Seattle-Kubernetes-Meetup/>Seattle</a>, <a href=http://www.meetup.com/GCPUGSG/events/232659329/>Singapore</a>, <a href=http://www.meetup.com/Bay-Area-Kubernetes-Meetup/events/232623207/>SF Bay Area</a>, or <a href=http://www.meetup.com/DC-Kubernetes-Meetup/>Washington DC</a>.</p><p>The Kubernetes community continues to work to make our project more welcoming and open to our <em>kollaborators</em>. This spring, Kubernetes and KubeCon moved to the Cloud Native Compute Foundation (<a href=https://cncf.io/>CNCF</a>), a Linux Foundation Project, to accelerate the collaborative vision outlined only a year ago at OSCON …. lifting a glass to another great year.</p><p><a href=https://1.bp.blogspot.com/-Wn9QJb6wQ7w/V5Cm1Y2iKhI/AAAAAAAAAnc/SZ3yFFcxjmoqAmz9chp8o2KJJUoKI0KQwCLcB/s1600/k8s%2BCommit%2BInfographic.png><img src=https://1.bp.blogspot.com/-Wn9QJb6wQ7w/V5Cm1Y2iKhI/AAAAAAAAAnc/SZ3yFFcxjmoqAmz9chp8o2KJJUoKI0KQwCLcB/s640/k8s%2BCommit%2BInfographic.png alt></a></p><p><em>-- Sarah Novotny, Kubernetes Community Wonk</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e74a934a1afe2c3d5f1bb068f967c67f>Happy Birthday Kubernetes. Oh, the places you’ll go!</h1><div class="td-byline mb-4"><time datetime=2016-07-21 class=text-muted>Thursday, July 21, 2016</time></div><p><em>Editor’s note, Today’s guest post is from an independent Kubernetes contributor, Justin Santa Barbara, sharing his reflection on growth of the project from inception to its future.</em></p><p><strong>Dear K8s,</strong></p><p><em>It’s hard to believe you’re only one - you’ve grown up so fast. On the occasion of your first birthday, I thought I would write a little note about why I was so excited when you were born, why I feel fortunate to be part of the group that is raising you, and why I’m eager to watch you continue to grow up!</em></p><p><em>--Justin</em></p><p>You started with an excellent foundation - good declarative functionality, built around a solid API with a well defined schema and the machinery so that we could evolve going forwards. And sure enough, over your first year you grew so fast: autoscaling, HTTP load-balancing support (Ingress), support for persistent workloads including clustered databases (PetSets). You’ve made friends with more clouds (welcome Azure & OpenStack to the family), and even started to span zones and clusters (Federation). And these are just some of the most visible changes - there’s so much happening inside that brain of yours!</p><p>I think it’s wonderful you’ve remained so open in all that you do - you seem to write down everything on GitHub - for better or worse. I think we’ve all learned a lot about that on the way, like the perils of having engineers make scaling statements that are then weighed against claims made without quite the same framework of precision and rigor. But I’m proud that you chose not to lower your standards, but rose to the challenge and just ran faster instead - it might not be the most realistic approach, but it is the only way to move mountains!</p><p>And yet, somehow, you’ve managed to avoid a lot of the common dead-ends that other open source software has fallen into, particularly as those projects got bigger and the developers end up working on it more than they use it directly. How did you do that? There’s a probably-apocryphal story of an employee at IBM that makes a huge mistake, and is summoned to meet with the big boss, expecting to be fired, only to be told “We just spent several million dollars training you. Why would we want to fire you?”. Despite all the investment google is pouring into you (along with Redhat and others), I sometimes wonder if the mistakes we are avoiding could be worth even more. There is a very open development process, yet there’s also an “oracle” that will sometimes course-correct by telling us what happens two years down the road if we make a particular design decision. This is a parent you should probably listen to!</p><p>And so although you’re only a year old, you really have an <a href="http://queue.acm.org/detail.cfm?id=2898444">old soul</a>. I’m just one of the <a href=https://kubernetes.io/blog/2016/07/happy-k8sbday-1>many people raising you</a>, but it’s a wonderful learning experience for me to be able to work with the people that have built these incredible systems and have all this domain knowledge. Yet because we started from scratch (rather than taking the existing Borg code) we’re at the same level and can still have genuine discussions about how to raise you. Well, at least as close to the same level as we could ever be, but it’s to their credit that they are all far too nice ever to mention it!</p><p>If I would pick just two of the wise decisions those brilliant people made:</p><ul><li>Labels & selectors give us declarative “pointers”, so we can say “why” we want things, rather than listing the things directly. It’s the secret to how you can scale to <a href=https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set>great heights</a>; not by naming each step, but saying “a thousand more steps just like that first one”.</li><li>Controllers are state-synchronizers: we specify the goals, and your controllers will indefatigably work to bring the system to that state. They work through that strongly-typed API foundation, and are used throughout the code, so Kubernetes is more of a set of a hundred small programs than one big one. It’s not enough to scale to thousands of nodes technically; the project also has to scale to thousands of developers and features; and controllers help us get there.</li></ul><p>And so on we will go! We’ll be replacing those controllers and building on more, and the API-foundation lets us build anything we can express in that way - with most things just a label or annotation away! But your thoughts will not be defined by language: with third party resources you can express anything you choose. Now we can build Kubernetes without building in Kubernetes, creating things that feel as much a part of Kubernetes as anything else. Many of the recent additions, like ingress, DNS integration, autoscaling and network policies were done or could be done in this way. Eventually it will be hard to imagine you before these things, but tomorrow’s standard functionality can start today, with no obstacles or gatekeeper, maybe even for an audience of one.</p><p>So I’m looking forward to seeing more and more growth happen further and further from the core of Kubernetes. We had to work our way through those phases; starting with things that needed to happen in the kernel of Kubernetes - like replacing replication controllers with deployments. Now we’re starting to build things that don’t require core changes. But we’re still still talking about infrastructure separately from applications. It’s what comes next that gets really interesting: when we start building applications that rely on the Kubernetes APIs. We’ve always had the Cassandra example that uses the Kubernetes API to self-assemble, but we haven’t really even started to explore this more widely yet. In the same way that the S3 APIs changed how we build things that remember, I think the k8s APIs are going to change how we build things that think.</p><p>So I’m looking forward to your second birthday: I can try to predict what you’ll look like then, but I know you’ll surpass even the most audacious things I can imagine. Oh, the places you’ll go!</p><p><em>-- Justin Santa Barbara, Independent Kubernetes Contributor</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1822219f56b54758063f2e9a87e9b2b1>The Bet on Kubernetes, a Red Hat Perspective</h1><div class="td-byline mb-4"><time datetime=2016-07-21 class=text-muted>Thursday, July 21, 2016</time></div><p><em>Editor’s note: Today’s guest post is from a Kubernetes contributor Clayton Coleman, Architect on OpenShift at Red Hat, sharing their adoption of the project from its beginnings.</em></p><p>Two years ago, Red Hat made a big bet on Kubernetes. We bet on a simple idea: that an open source community is the best place to build the future of application orchestration, and that only an open source community could successfully integrate the diverse range of capabilities necessary to succeed. As a Red Hatter, that idea is not far-fetched - we’ve seen it successfully applied in many communities, but we’ve also seen it fail, especially when a broad reach is not supported by solid foundations. On the one year anniversary of Kubernetes 1.0, two years after the first open-source commit to the Kubernetes project, it’s worth asking the question:</p><p><strong>Was Kubernetes the right bet?</strong></p><p>The success of software is measured by the successes of its users - whether that software enables for them new opportunities or efficiencies. In that regard, Kubernetes has succeeded beyond our wildest dreams. We know of hundreds of real production deployments of Kubernetes, in the enterprise through Red Hat’s multi-tenant enabled <a href=https://github.com/openshift/origin>OpenShift</a> distribution, on <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE), in heavily customized versions run by some of the world's largest software companies, and through the education, entertainment, startup, and do-it-yourself communities. Those deployers report improved time to delivery, standardized application lifecycles, improved resource utilization, and more resilient and robust applications. And that’s just from customers or contributors to the community - I would not be surprised if there were now thousands of installations of Kubernetes managing tens of thousands of real applications out in the wild.</p><p>I believe that reach to be a validation of the vision underlying Kubernetes: to build a platform for all applications by providing tools for each of the core patterns in distributed computing. Those patterns:</p><ul><li>simple replicated web software</li><li>distributed load balancing and service discovery</li><li>immutable images run in containers</li><li>co-location of related software into pods</li><li>simplified consumption of network attached storage</li><li>flexible and powerful resource scheduling</li><li>running batch and scheduled jobs alongside service workloads</li><li>managing and maintaining clustered software like databases and message queues</li></ul><p>Allow developers and operators to move to the next scale of abstraction, just like they have enabled Google and others in the tech ecosystem to scale to datacenter computers and beyond. From Kubernetes 1.0 to 1.3 we have continually improved the power and flexibility of the platform while ALSO improving performance, scalability, reliability, and usability. The explosion of integrations and tools that run on top of Kubernetes further validates core architectural decisions to be <a href=https://research.google.com/pubs/pub43438.html>composable</a>, to expose <a href=/docs/api/>open and flexible APIs</a>, and to <a href=/docs/whatisk8s/#kubernetes-is-not>deliberately limit the core platform</a> and encourage extension.</p><p>Today Kubernetes has one of the largest and most vibrant communities in the open source ecosystem, with almost a thousand contributors, one of the highest human-generated commit rates of any single-repository project on GitHub, over a thousand projects based around Kubernetes, and correspondingly active Stack Overflow and Slack channels. Red Hat is proud to be part of this ecosystem as the largest contributor to Kubernetes after Google, and every day more companies and individuals join us. The idea of Kubernetes found fertile ground, and you, the community, provided the excitement and commitment that made it grow.</p><p>So, did we bet correctly? For all the reasons above, and hundreds more: <strong>Yes</strong>.</p><p><strong>What’s next?</strong></p><p>Happy as we are with the success of Kubernetes, this is no time to rest! While there are many more features and improvements we want to build into Kubernetes, I think there is a general consensus that we want to focus on the only long term goal that matters - a healthy, successful, and thriving technical community around Kubernetes. As John F. Kennedy probably said: </p><p>> <em>Ask not what your community can do for you, but what you can do for your community</em></p><p>In a recent post to the kubernetes-dev list, Brian Grant <a href=https://groups.google.com/d/topic/kubernetes-dev/MoyWB66vAKY/discussion>laid out a great set of near term goals</a> - goals that help grow the community, refine how we execute, and enable future expansion. In each of the <a href=https://github.com/kubernetes/community/blob/master/README.md#special-interest-groups-sig>Kubernetes Special Interest Groups</a> we are trying to build sustainable teams that can execute across companies and communities, and we are actively working to ensure each of these SIGs is able to contribute, coordinate, and deliver across a diverse range of interests under one vision for the project.</p><p>Of special interest to us is the story of extension - how the core of Kubernetes can become the beating heart of the datacenter operating system, and enable even more patterns for application management to build on top of Kubernetes, not just into it. Work done in the 1.2 and 1.3 releases around third party APIs, API discovery, flexible scheduler policy, external authorization and authentication (beyond those built into Kubernetes) is just the start. When someone has a need, we want them to easily find a solution, and we also want it to be easy for others to consume and contribute to that solution. Likewise, the best way to prove ideas is to prototype them against real needs and to iterate against real problems, which should be easy and natural.</p><p>By Kubernetes’ second birthday, I hope to reflect back on a long year of refinement, user success, and community participation. It has been a privilege and an honor to contribute to Kubernetes, and it still feels like we are just getting started. Thank you, and I hope you come along for the ride!</p><p><em>-- Clayton Coleman, Contributor and Architect on Kubernetes and OpenShift at Red Hat. Follow him on Twitter and GitHub: @smarterclayton</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d0c04eabe8ccc353a49856a0440f8725>Bringing End-to-End Kubernetes Testing to Azure (Part 2)</h1><div class="td-byline mb-4"><time datetime=2016-07-18 class=text-muted>Monday, July 18, 2016</time></div><p><em>Editor’s Note: Today’s guest post is Part II from a <a href=https://kubernetes.io/blog/2016/06/bringing-end-to-end-testing-to-azure>series</a> by Travis Newhouse, Chief Architect at AppFormix, writing about their contributions to Kubernetes.</em></p><p>Historically, Kubernetes testing has been hosted by Google, running e2e tests on <a href=https://cloud.google.com/compute/>Google Compute Engine</a> (GCE) and <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE). In fact, the gating checks for the submit-queue are a subset of tests executed on these test platforms. Federated testing aims to expand test coverage by enabling organizations to host test jobs for a variety of platforms and contribute test results to benefit the Kubernetes project. Members of the Kubernetes test team at Google and SIG-Testing have created a <a href=http://storage.googleapis.com/kubernetes-test-history/static/index.html>Kubernetes test history dashboard</a> that publishes the results from all federated test jobs (including those hosted by Google).</p><p>In this blog post, we describe extending the e2e test jobs for Azure, and show how to contribute a federated test to the Kubernetes project.</p><p><strong>END-TO-END INTEGRATION TESTS FOR AZURE</strong></p><p>After successfully implementing <a href=https://kubernetes.io/blog/2016/06/bringing-end-to-end-testing-to-azure>“development distro” scripts to automate deployment of Kubernetes on Azure</a>, our next goal was to run e2e integration tests and share the results with the Kubernetes community.</p><p>We automated our workflow for executing e2e tests of Kubernetes on Azure by defining a nightly job in our private Jenkins server. Figure 2 shows the workflow that uses kube-up.sh to deploy Kubernetes on Ubuntu virtual machines running in Azure, then executes the e2e tests. On completion of the tests, the job uploads the test results and logs to a Google Cloud Storage directory, in a format that can be processed by the <a href=https://github.com/kubernetes/test-infra/tree/master/jenkins/test-history>scripts that produce the test history dashboard</a>. Our Jenkins job uses the hack/jenkins/e2e-runner.sh and hack/jenkins/upload-to-gcs.sh scripts to produce the results in the correct format.</p><p>| <img src=https://lh6.googleusercontent.com/TZiUu4sQ7G0XDvJgv9a1a4UEdxntOZDT9I3S42c8BOAyigxaysKmhJMen8vLaJ3UYaYKPIG9h-cyBOvTSI6kBgqnUQabe4xxZXhrUyVKGEaCDUnmNlBo__HNjzoYc_U7zM77_Dxe alt="Kubernetes on Azure - Flow Chart - New Page.png"> |
| Figure 2 - Nightly test job workflow |</p><p><strong>HOW TO CONTRIBUTE AN E2E TEST</strong></p><p>Throughout our work to create the Azure e2e test job, we have collaborated with members of <a href=https://github.com/kubernetes/community/tree/master/sig-testing>SIG-Testing</a> to find a way to publish the results to the Kubernetes community. The results of this collaboration are documentation and a streamlined process to contribute results from a federated test job. The steps to contribute e2e test results can be summarized in 4 steps.</p><ol><li>Create a <a href=https://cloud.google.com/storage/>Google Cloud Storage</a> bucket in which to publish the results.</li><li>Define an automated job to run the e2e tests. By setting a few environment variables, hack/jenkins/e2e-runner.sh deploys Kubernetes binaries and executes the tests.</li><li>Upload the results using hack/jenkins/upload-to-gcs.sh.</li><li>Incorporate the results into the test history dashboard by submitting a pull-request with modifications to a few files in <a href=https://github.com/kubernetes/test-infra>kubernetes/test-infra</a>.</li></ol><p>The federated tests documentation describes these steps in more detail. The scripts to run e2e tests and upload results simplifies the work to contribute a new federated test job. The specific steps to set up an automated test job and an appropriate environment in which to deploy Kubernetes are left to the reader’s preferences. For organizations using Jenkins, the jenkins-job-builder configurations for GCE and GKE tests may provide helpful examples.</p><p><strong>RETROSPECTIVE</strong></p><p>The e2e tests on Azure have been running for several weeks now. During this period, we have found two issues in Kubernetes. Weixu Zhuang immediately published fixes that have been merged into the Kubernetes master branch.</p><p>The first issue happened when we wanted to bring up the Kubernetes cluster using SaltStack on Azure using Ubuntu VMs. A commit (07d7cfd3) modified the OpenVPN certificate generation script to use a variable that was only initialized by scripts in the cluster/ubuntu. Strict checking on existence of parameters by the certificate generation script caused other platforms that use the script to fail (e.g. our changes to support Azure). We submitted a <a href=https://github.com/kubernetes/kubernetes/pull/21357>pull-request that fixed the issue</a> by initializing the variable with a default value to make the certificate generation scripts more robust across all platform types.</p><p>The second <a href=https://github.com/kubernetes/kubernetes/pull/22321>pull-request cleaned up an unused import</a> in the Daemonset unit test file. The import statement broke the unit tests with golang 1.4. Our nightly Jenkins job helped us find this error and we promptly pushed a fix for it.</p><p><strong>CONCLUSION AND FUTURE WORK</strong></p><p>The addition of a nightly e2e test job for Kubernetes on Azure has helped to define the process to contribute a federated test to the Kubernetes project. During the course of the work, we also saw the immediate benefit of expanding test coverage to more platforms when our Azure test job identified compatibility issues.</p><p>We want to thank Aaron Crickenberger, Erick Fejta, Joe Finney, and Ryan Hutchinson for their help to incorporate the results of our Azure e2e tests into the Kubernetes test history. If you’d like to get involved with testing to create a stable, high quality releases of Kubernetes, join us in the <a href=https://github.com/kubernetes/community/tree/master/sig-testing>Kubernetes Testing SIG (sig-testing)</a>.</p><p><em>--Travis Newhouse, Chief Architect at AppFormix</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1496574418064c58fdb070c88d97a679>Dashboard - Full Featured Web Interface for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-07-15 class=text-muted>Friday, July 15, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3</em></p><p><a href=http://github.com/kubernetes/dashboard>Kubernetes Dashboard</a> is a project that aims to bring a general purpose monitoring and operational web interface to the Kubernetes world. Three months ago we <a href=https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes>released</a> the first production ready version, and since then the dashboard has made massive improvements. In a single UI, you’re able to perform majority of possible interactions with your Kubernetes clusters without ever leaving your browser. This blog post breaks down new features introduced in the latest release and outlines the roadmap for the future. </p><p><strong>Full-Featured Dashboard</strong></p><p>Thanks to a large number of contributions from the community and project members, we were able to deliver many new features for <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3 release</a>. We have been carefully listening to all the great feedback we have received from our users (see the <a href=http://static.lwy.io/img/kubernetes_dashboard_infographic.png>summary infographics</a>) and addressed the highest priority requests and pain points.</p><p>The Dashboard UI now handles all workload resources. This means that no matter what workload type you run, it is visible in the web interface and you can do operational changes on it. For example, you can modify your stateful MySQL installation with <a href=/docs/user-guide/petset/>Pet Sets</a>, do a rolling update of your web server with Deployments or install cluster monitoring with DaemonSets. </p><p><a href=https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz><img src=https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz alt></a></p><p>In addition to viewing resources, you can create, edit, update, and delete them. This feature enables many use cases. For example, you can kill a failed Pod, do a rolling update on a Deployment, or just organize your resources. You can also export and import YAML configuration files of your cloud apps and store them in a version control system.</p><p><img src=https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O alt></p><p>The release includes a beta view of cluster nodes for administration and operational use cases. The UI lists all nodes in the cluster to allow for overview analysis and quick screening for problematic nodes. The details view shows all information about the node and links to pods running on it.</p><p><img src=https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_ alt></p><p>There are also many smaller scope new features that the we shipped with the release, namely: support for namespaced resources, internationalization, performance improvements, and many bug fixes (find out more in the <a href=https://github.com/kubernetes/dashboard/releases/tag/v1.1.0>release notes</a>). All these improvements result in a better and simpler user experience of the product.</p><p><strong>Future Work</strong></p><p>The team has ambitious plans for the future spanning across multiple use cases. We are also open to all feature requests, which you can post on our <a href=https://github.com/kubernetes/dashboard/issues>issue tracker</a>.</p><p>Here is a list of our focus areas for the following months:</p><ul><li><a href=https://github.com/kubernetes/dashboard/issues/961>Handle more Kubernetes resources</a> - To show all resources that a cluster user may potentially interact with. Once done, Dashboard can act as a complete replacement for CLI. </li><li><a href=https://github.com/kubernetes/dashboard/issues/962>Monitoring and troubleshooting</a> - To add resource usage statistics/graphs to the objects shown in Dashboard. This focus area will allow for actionable debugging and troubleshooting of cloud applications.</li><li><a href=https://github.com/kubernetes/dashboard/issues/964>Security, auth and logging in</a> - Make Dashboard accessible from networks external to a Cluster and work with custom authentication systems.</li></ul><p><strong>Connect With Us</strong></p><p>We would love to talk with you and hear your feedback!</p><ul><li>Email us at the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-ui>SIG-UI mailing list</a></li><li>Chat with us on the Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-ui/>#SIG-UI channel</a></li><li>Join our meetings: 4PM CEST. See the <a href="https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&ctz=Europe/Warsaw">SIG-UI calendar</a> for details.</li></ul><p><em>-- Piotr Bryk, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-60f9ed7fa5018fcbffae1e1455adcd36>Steering an Automation Platform at Wercker with Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-07-15 class=text-muted>Friday, July 15, 2016</time></div><p><em>Editor’s note: today’s guest post is by Andy Smith, the CTO of Wercker, sharing how Kubernetes helps them save time and speed up development.  </em></p><p>At <a href=http://wercker.com/>Wercker</a> we run millions of containers that execute our users’ CI/CD jobs. The vast majority of them are ephemeral and only last as long as builds, tests and deploys take to run, the rest are ephemeral, too -- aren't we all --, but tend to last a bit longer and run our infrastructure. As we are running many containers across many nodes, we were in need of a highly scalable scheduler that would make our lives easier, and as such, decided to implement Kubernetes.</p><p>Wercker is a container-centric automation platform that helps developers build, test and deploy their applications. We support any number of pipelines, ranging from building code, testing API-contracts between microservices, to pushing containers to registries, and deploying to schedulers. All of these pipeline jobs run inside Docker containers and each artifact can be a Docker container.</p><p>And of course we use Wercker to build Wercker, and deploy itself onto Kubernetes!</p><p><strong>Overview</strong></p><p>Because we are a platform for running multi-service cloud-native code we've made many design decisions around isolation. On the base level we use <a href=http://coreos.com/>CoreOS</a> and <a href=https://coreos.com/os/docs/latest/cloud-config.html>cloud-init</a> to bootstrap a cluster of heterogeneous nodes which I have named Patricians, Peasants, as well as controller nodes that don't have a cool name and are just called Controllers. Maybe we should switch to Constables.</p><p><img src=https://lh5.googleusercontent.com/i_Gtd1J9dekCxy7jJYZDZX0XmAmGD4f8qhrYG60FdVqnM87l-si44BGHjFdEFACZcx2E-rgRZNxuvniYDninlHAl9ZHyF2-jJjKUl-QQH8Au29hwVTbnDc0tP1Rv_Yd8mvt1tfoX alt=k8s-architecture.jpg></p><p>Patrician nodes are where the bulk of our infrastructure runs. These nodes have appropriate network interfaces to communicate with our backend services as well as be routable by various load balancers. This is where our logging is aggregated and sent off to logging services, our many microservices for reporting and processing the results of job runs, and our many microservices for handling API calls.</p><p>On the other end of the spectrum are the Peasant nodes where the public jobs are run. Public jobs consist of worker pods reading from a job queue and dynamically generating new runner pods to handle execution of the job. The job itself is an incarnation of our open source <a href=http://github.com/wercker/wercker>CLI tool</a>, the same one you can run on your laptop with Docker installed. These nodes have very limited access to the rest of the infrastructure and the containers the jobs themselves run in are even further isolated.</p><p>Controllers are controllers, I bet ours look exactly the same as yours.</p><p><strong>Dynamic Pods</strong></p><p>Our heaviest use of the Kubernetes API is definitely our system of creating dynamic pods to serve as the runtime environment for our actual job execution. After pulling job descriptions from the queue we define a new pod containing all the relevant environment for checking out code, managing a cache, executing a job and uploading artifacts. We launch the pod, monitor its progress, and destroy it when the job is done.</p><p><strong>Ingresses</strong></p><p>In order to provide a backend for HTTP API calls and allow self-registration of handlers we make use of the Ingress system in Kubernetes. It wasn't the clearest thing to set up, but reading through enough of the <a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-and-simplifying-advanced-networking-with-ingress/>nginx example</a> eventually got us to a good spot where it is easy to connect services to the frontend.</p><p><strong>Upcoming Features in 1.3</strong></p><p>While we generally treat all of our pods and containers as ephemeral and expect rapid restarts on failures, we are looking forward to Pet Sets and Init Containers as ways to optimize some of our processes. We are also pleased with official support for <a href=https://github.com/kubernetes/minikube>Minikube</a> coming along as it improves our local testing and development. </p><p><strong>Conclusion</strong></p><p>Kubernetes saves us the non-trivial task of managing many, many containers across many nodes. It provides a robust API and tooling for introspecting these containers, and it includes much built in support for logging, metrics, monitoring and debugging. Service discovery and networking alone saves us so much time and speeds development immensely.</p><p>Cheers to you Kubernetes, keep up the good work :)</p><p><em>-- Andy Smith, CTO, Wercker</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-7bef62438e14fb0960709d89c1672c30>Citrix + Kubernetes = A Home Run</h1><div class="td-byline mb-4"><time datetime=2016-07-14 class=text-muted>Thursday, July 14, 2016</time></div><p><em>Editor’s note: today’s guest post is by Mikko Disini, a Director of Product Management at Citrix Systems, sharing their collaboration experience on a Kubernetes integration. </em></p><p>Technical collaboration is like sports. If you work together as a team, you can go down the homestretch and pull through for a win. That’s our experience with the Google Cloud Platform team.</p><p>Recently, we approached Google Cloud Platform (GCP) to collaborate on behalf of Citrix customers and the broader enterprise market looking to migrate workloads. This migration required including the <a href=https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/>NetScaler Docker load balancer</a>, CPX, into Kubernetes nodes and resolving any issues with getting traffic into the CPX proxies.  </p><p><strong>Why NetScaler and Kubernetes?</strong></p><ol><li>Citrix customers want the same Layer 4 to Layer 7 capabilities from NetScaler that they have on-prem as they move to the cloud as they begin deploying their container and microservices architecture with Kubernetes </li><li>Kubernetes provides a proven infrastructure for running containers and VMs with automated workload delivery</li><li>NetScaler CPX provides Layer 4 to Layer 7 services and highly efficient telemetry data to a logging and analytics platform, <a href=https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/>NetScaler Management and Analytics System</a></li></ol><p>I wish all our experiences working together with a technical partner were as good as working with GCP. We had a list of issues to enable our use cases and were able to collaborate swiftly on a solution. To resolve these, GCP team offered in depth technical assistance, working with Citrix such that NetScaler CPX can spin up and take over as a client-side proxy running on each host. </p><p>Next, NetScaler CPX needed to be inserted in the data path of GCP ingress load balancer so that NetScaler CPX can spread traffic to front end web servers. The NetScaler team made modifications so that NetScaler CPX listens to API server events and configures itself to create a VIP, IP table rules and server rules to take ingress traffic and load balance across front end applications. Google Cloud Platform team provided feedback and assistance to verify modifications made to overcome the technical hurdles. Done!</p><p>NetScaler CPX use case is supported in <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3</a>. Citrix customers and the broader enterprise market will have the opportunity to leverage NetScaler with Kubernetes, thereby lowering the friction to move workloads to the cloud. </p><p>You can learn more about NetScaler CPX <a href=https://www.citrix.com/networking/microservices.html>here</a>.</p><p><em> -- Mikko Disini, Director of Product Management - NetScaler, Citrix Systems</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a60daf951d17ab6e4d59dd70eade6968>Cross Cluster Services - Achieving Higher Availability for your Kubernetes Applications</h1><div class="td-byline mb-4"><time datetime=2016-07-14 class=text-muted>Thursday, July 14, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3</em></p><p>As Kubernetes users scale their production deployments we’ve heard a clear desire to deploy services across zone, region, cluster and cloud boundaries. Services that span clusters provide geographic distribution, enable hybrid and multi-cloud scenarios and improve the level of high availability beyond single cluster multi-zone deployments. Customers who want their services to span one or more (possibly remote) clusters, need them to be reachable in a consistent manner from both within and outside their clusters.</p><p>In Kubernetes 1.3, our goal was to minimize the friction points and reduce the management/operational overhead associated with deploying a service with geographic distribution to multiple clusters. This post explains how to do this.</p><p>Note: Though the examples used here leverage Google Container Engine (<a href=https://cloud.google.com/container-engine/>GKE</a>) to provision Kubernetes clusters, they work anywhere you want to deploy Kubernetes.</p><p>Let’s get started. The first step is to create is to create Kubernetes clusters into 4 Google Cloud Platform (GCP) regions using GKE.</p><ul><li>asia-east1-b</li><li>europe-west1-b</li><li>us-east1-b</li><li>us-central1-b</li></ul><p>Let’s run the following commands to build the clusters:</p><pre><code>gcloud container clusters create gce-asia-east1 \

  --scopes cloud-platform \

  --zone asia-east1-b

gcloud container clusters create gce-europe-west1 \

  --scopes cloud-platform \

  --zone=europe-west1-b

gcloud container clusters create gce-us-east1 \

  --scopes cloud-platform \

  --zone=us-east1-b

gcloud container clusters create gce-us-central1 \

  --scopes cloud-platform \

  --zone=us-central1-b
</code></pre><p>Let’s verify the clusters are created:</p><pre><code>gcloud container clusters list

NAME              ZONE            MASTER\_VERSION  MASTER\_IP       NUM\_NODES  STATUS  
gce-asia-east1    asia-east1-b    1.2.4           104.XXX.XXX.XXX 3          RUNNING  
gce-europe-west1  europe-west1-b  1.2.4           130.XXX.XX.XX   3          RUNNING  
gce-us-central1   us-central1-b   1.2.4           104.XXX.XXX.XX  3          RUNNING  
gce-us-east1      us-east1-b      1.2.4           104.XXX.XX.XXX  3          RUNNING
</code></pre><p><a href=https://lh6.googleusercontent.com/LEMtlOvr6i_iK1DwVmS-ltSKU5PmjrrN287sxwvyiGH-QLjOhF25RUjVTVt4IUo-0oGXvj8bxfRFCxTZa_5Qfv_LjxglshTxcnpm73E6Uy7MgVPTiI2GevdwAogHenZIb2S6A6lr><img src=https://lh6.googleusercontent.com/LEMtlOvr6i_iK1DwVmS-ltSKU5PmjrrN287sxwvyiGH-QLjOhF25RUjVTVt4IUo-0oGXvj8bxfRFCxTZa_5Qfv_LjxglshTxcnpm73E6Uy7MgVPTiI2GevdwAogHenZIb2S6A6lr alt></a></p><p>The next step is to bootstrap the clusters and deploy the federation control plane on one of the clusters that has been provisioned. If you’d like to follow along, refer to Kelsey Hightower’s <a href=https://github.com/kelseyhightower/kubernetes-cluster-federation>tutorial</a> which walks through the steps involved.</p><p><strong>Federated Services</strong></p><p><a href=https://github.com/kubernetes/kubernetes/blob/release-1.3/docs/design/federated-services.md>Federated Services</a> are directed to the Federation API endpoint and specify the desired properties of your service.</p><p>Once created, the Federated Service automatically:</p><ul><li>creates matching Kubernetes Services in every cluster underlying your cluster federation,</li><li>monitors the health of those service "shards" (and the clusters in which they reside), and</li><li>manages a set of DNS records in a public DNS provider (like Google Cloud DNS, or AWS Route 53), thus ensuring that clients of your federated service can seamlessly locate an appropriate healthy service endpoint at all times, even in the event of cluster, availability zone or regional outages.</li></ul><p>Clients inside your federated Kubernetes clusters (i.e. Pods) will automatically find the local shard of the federated service in their cluster if it exists and is healthy, or the closest healthy shard in a different cluster if it does not.</p><p>Federations of Kubernetes Clusters can include clusters running in different cloud providers (e.g. GCP, AWS), and on-premise (e.g. on OpenStack). All you need to do is create your clusters in the appropriate cloud providers and/or locations, and register each cluster's API endpoint and credentials with your Federation API Server.</p><p>In our example, we have clusters created in 4 regions along with a federated control plane API deployed in one of our clusters, that we’ll be using to provision our service. See diagram below for visual representation.</p><p><img src=https://lh6.googleusercontent.com/4_s4eMx0Dihz3RHENvFN16WnbaIyLadoQhYp3AYgSijDz5tTwmpuYXw4wufBKUTp1nM1vGyiFpIy6LRu3wJoj4_RXvXj6XqqlBzBB2FCttvLZw-RLaqIVXDjPwHtsGE_Q_920Zqy alt></p><p><strong>Creating a Federated Service</strong></p><p>Let’s list out all the clusters in our federation:</p><pre><code>kubectl --context=federation-cluster get clusters

NAME               STATUS    VERSION   AGE  
gce-asia-east1     Ready               1m  
gce-europe-west1   Ready               57s  
gce-us-central1    Ready               47s  
gce-us-east1       Ready               34s
</code></pre><p>Let’s create a federated service object:</p><pre><code>kubectl --context=federation-cluster create -f services/nginx.yaml
</code></pre><p>The '--context=federation-cluster' flag tells kubectl to submit the request to the Federation API endpoint, with the appropriate credentials. The federated service will automatically create and maintain matching Kubernetes services in all of the clusters underlying your federation.</p><p>You can verify this by checking in each of the underlying clusters, for example:</p><pre><code>kubectl --context=gce-asia-east1a get svc nginx  
NAME      CLUSTER-IP     EXTERNAL-IP      PORT(S)   AGE  
nginx     10.63.250.98   104.199.136.89   80/TCP    9m
</code></pre><p>The above assumes that you have a context named 'gce-asia-east1a' configured in your client for your cluster in that zone. The name and namespace of the underlying services will automatically match those of the federated service that you created above.</p><p>The status of your Federated Service will automatically reflect the real-time status of the underlying Kubernetes services, for example:</p><pre><code>kubectl --context=federation-cluster describe services nginx  

Name:                   nginx  
Namespace:              default  
Labels:                 run=nginx  
Selector:               run=nginx  
Type:                   LoadBalancer  
IP:           
LoadBalancer Ingress:   104.XXX.XX.XXX, 104.XXX.XX.XXX, 104.XXX.XX.XXX, 104.XXX.XXX.XX  
Port:                   http    80/TCP  
Endpoints:              \&lt;none\&gt;  
Session Affinity:       None  
No events.
</code></pre><p>The 'LoadBalancer Ingress' addresses of your federated service corresponds with the 'LoadBalancer Ingress' addresses of all of the underlying Kubernetes services. For inter-cluster and inter-cloud-provider networking between service shards to work correctly, your services need to have an externally visible IP address. Service Type: Loadbalancer is typically used here.</p><p>Note also what we have not yet provisioned any backend Pods to receive the network traffic directed to these addresses (i.e. 'Service Endpoints'), so the federated service does not yet consider these to be healthy service shards, and has accordingly not yet added their addresses to the DNS records for this federated service.</p><p><strong>Adding Backend Pods</strong></p><p>To render the underlying service shards healthy, we need to add backend Pods behind them. This is currently done directly against the API endpoints of the underlying clusters (although in future the Federation server will be able to do all this for you with a single command, to save you the trouble). For example, to create backend Pods in our underlying clusters:</p><pre><code>for CLUSTER in asia-east1-a europe-west1-a us-east1-a us-central1-a  
do  
kubectl --context=$CLUSTER run nginx --image=nginx:1.11.1-alpine --port=80  
done
</code></pre><p><strong>Verifying Public DNS Records</strong></p><p>Once the Pods have successfully started and begun listening for connections, Kubernetes in each cluster (via automatic health checks) will report them as healthy endpoints of the service in that cluster. The cluster federation will in turn consider each of these service 'shards' to be healthy, and place them in serving by automatically configuring corresponding public DNS records. You can use your preferred interface to your configured DNS provider to verify this. For example, if your Federation is configured to use Google Cloud DNS, and a managed DNS domain 'example.com':</p><pre><code>$ gcloud dns managed-zones describe example-dot-com   

creationTime: '2016-06-26T18:18:39.229Z'  
description: Example domain for Kubernetes Cluster Federation  
dnsName: example.com.  
id: '3229332181334243121'  
kind: dns#managedZone  
name: example-dot-com  
nameServers:  
- ns-cloud-a1.googledomains.com.  
- ns-cloud-a2.googledomains.com.  
- ns-cloud-a3.googledomains.com.  
- ns-cloud-a4.googledomains.com.  

$ gcloud dns record-sets list --zone example-dot-com  

NAME                                                                                                 TYPE      TTL     DATA  
example.com.                                                                                       NS     21600  ns-cloud-e1.googledomains.com., ns-cloud-e2.googledomains.com.  
example.com.                                                                                      SOA     21600 ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 1209600 300  
nginx.mynamespace.myfederation.svc.example.com.                            A     180     104.XXX.XXX.XXX, 130.XXX.XX.XXX, 104.XXX.XX.XXX, 104.XXX.XXX.XX  
nginx.mynamespace.myfederation.svc.us-central1-a.example.com.     A     180     104.XXX.XXX.XXX  
nginx.mynamespace.myfederation.svc.us-central1.example.com.  
nginx.mynamespace.myfederation.svc.us-central1.example.com.         A    180     104.XXX.XXX.XXX, 104.XXX.XXX.XXX, 104.XXX.XXX.XXX  
nginx.mynamespace.myfederation.svc.asia-east1-a.example.com.       A    180     130.XXX.XX.XXX  
nginx.mynamespace.myfederation.svc.asia-east1.example.com.  
nginx.mynamespace.myfederation.svc.asia-east1.example.com.           A    180     130.XXX.XX.XXX, 130.XXX.XX.XXX  
nginx.mynamespace.myfederation.svc.europe-west1.example.com.  CNAME    180   nginx.mynamespace.myfederation.svc.example.com.  
... etc.
</code></pre><p>Note: If your Federation is configured to use AWS Route53, you can use one of the equivalent AWS tools, for example:</p><pre><code>$aws route53 list-hosted-zones

and

$aws route53 list-resource-record-sets --hosted-zone-id Z3ECL0L9QLOVBX
</code></pre><p>Whatever DNS provider you use, any DNS query tool (for example 'dig' or 'nslookup') will of course also allow you to see the records created by the Federation for you.</p><p><strong>Discovering a Federated Service from pods Inside your Federated Clusters</strong></p><p>By default, Kubernetes clusters come preconfigured with a cluster-local DNS server ('KubeDNS'), as well as an intelligently constructed DNS search path which together ensure that DNS queries like "myservice", "myservice.mynamespace", "bobsservice.othernamespace" etc issued by your software running inside Pods are automatically expanded and resolved correctly to the appropriate service IP of services running in the local cluster.</p><p>With the introduction of Federated Services and Cross-Cluster Service Discovery, this concept is extended to cover Kubernetes services running in any other cluster across your Cluster Federation, globally. To take advantage of this extended range, you use a slightly different DNS name (e.g. myservice.mynamespace.myfederation) to resolve federated services. Using a different DNS name also avoids having your existing applications accidentally traversing cross-zone or cross-region networks and you incurring perhaps unwanted network charges or latency, without you explicitly opting in to this behavior.</p><p>So, using our NGINX example service above, and the federated service DNS name form just described, let's consider an example: A Pod in a cluster in the us-central1-a availability zone needs to contact our NGINX service. Rather than use the service's traditional cluster-local DNS name ("nginx.mynamespace", which is automatically expanded to"nginx.mynamespace.svc.cluster.local") it can now use the service's Federated DNS name, which is"nginx.mynamespace.myfederation". This will be automatically expanded and resolved to the closest healthy shard of my NGINX service, wherever in the world that may be. If a healthy shard exists in the local cluster, that service's cluster-local (typically 10.x.y.z) IP address will be returned (by the cluster-local KubeDNS). This is exactly equivalent to non-federated service resolution.</p><p>If the service does not exist in the local cluster (or it exists but has no healthy backend pods), the DNS query is automatically expanded to "nginx.mynamespace.myfederation.svc.us-central1-a.example.com". Behind the scenes, this is finding the external IP of one of the shards closest to my availability zone. This expansion is performed automatically by KubeDNS, which returns the associated CNAME record. This results in a traversal of the hierarchy of DNS records in the above example, and ends up at one of the external IP's of the Federated Service in the local us-central1 region.</p><p>It is also possible to target service shards in availability zones and regions other than the ones local to a Pod by specifying the appropriate DNS names explicitly, and not relying on automatic DNS expansion. For example, "nginx.mynamespace.myfederation.svc.europe-west1.example.com" will resolve to all of the currently healthy service shards in Europe, even if the Pod issuing the lookup is located in the U.S., and irrespective of whether or not there are healthy shards of the service in the U.S. This is useful for remote monitoring and other similar applications.</p><p><strong>Discovering a Federated Service from Other Clients Outside your Federated Clusters</strong></p><p>For external clients, automatic DNS expansion described is no longer possible. External clients need to specify one of the fully qualified DNS names of the federated service, be that a zonal, regional or global name. For convenience reasons, it is often a good idea to manually configure additional static CNAME records in your service, for example:</p><pre><code>eu.nginx.acme.com        CNAME nginx.mynamespace.myfederation.svc.europe-west1.example.com.  
us.nginx.acme.com        CNAME nginx.mynamespace.myfederation.svc.us-central1.example.com.  
nginx.acme.com             CNAME nginx.mynamespace.myfederation.svc.example.com.
</code></pre><p>That way your clients can always use the short form on the left, and always be automatically routed to the closest healthy shard on their home continent. All of the required failover is handled for you automatically by Kubernetes Cluster Federation.</p><p><strong>Handling Failures of Backend Pods and Whole Clusters</strong></p><p>Standard Kubernetes service cluster-IP's already ensure that non-responsive individual Pod endpoints are automatically taken out of service with low latency. The Kubernetes cluster federation system automatically monitors the health of clusters and the endpoints behind all of the shards of your federated service, taking shards in and out of service as required. Due to the latency inherent in DNS caching (the cache timeout, or TTL for federated service DNS records is configured to 3 minutes, by default, but can be adjusted), it may take up to that long for all clients to completely fail over to an alternative cluster in in the case of catastrophic failure. However, given the number of discrete IP addresses which can be returned for each regional service endpoint (see e.g. us-central1 above, which has three alternatives) many clients will fail over automatically to one of the alternative IP's in less time than that given appropriate configuration.</p><p><strong>Community</strong></p><p>We'd love to hear feedback on Kubernetes Cross Cluster Services. To join the community:</p><ul><li>Post issues or feature requests on <a href=https://github.com/kubernetes/kubernetes/tree/release-1.8/federation>GitHub</a></li><li>Join us in the #federation channel on <a href=https://kubernetes.slack.com/messages/sig-federation>Slack</a></li><li>Participate in the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-federation>Cluster Federation SIG</a></li></ul><p>Please give Cross Cluster Services a try, and let us know how it goes!</p><p><em>-- Quinton Hoole, Engineering Lead, Google and Allan Naim, Product Manager, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9e91022359423a7a5fdefbcd200af963>Stateful Applications in Containers!? Kubernetes 1.3 Says “Yes!”</h1><div class="td-byline mb-4"><time datetime=2016-07-13 class=text-muted>Wednesday, July 13, 2016</time></div><p><em>Editor's note: today’s guest post is from Mark Balch, VP of Products at Diamanti, who’ll share more about the contributions they’ve made to Kubernetes.</em></p><p>Congratulations to the Kubernetes community on another <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>value-packed release</a>. A focus on stateful applications and federated clusters are two reasons why I’m so excited about 1.3. Kubernetes support for stateful apps such as Cassandra, Kafka, and MongoDB is critical. Important services rely on databases, key value stores, message queues, and more. Additionally, relying on one data center or container cluster simply won’t work as apps grow to serve millions of users around the world. Cluster federation allows users to deploy apps across multiple clusters and data centers for scale and resiliency.</p><p>You may have <a href=https://www.diamanti.com/blog/the-next-great-application-platform/>heard me say before</a> that containers are the next great application platform. Diamanti is accelerating container adoption for stateful apps in production - where performance and ease of deployment really matter. </p><p><strong>Apps Need More Than Cattle</strong></p><p>Beyond stateless containers like web servers (so-called “cattle” because they are interchangeable), users are increasingly deploying stateful workloads with containers to benefit from “build once, run anywhere” and to improve bare metal efficiency/utilization. These “pets” (so-called because each requires special handling) bring new requirements including longer life cycle, configuration dependencies, stateful failover, and performance sensitivity. Container orchestration must address these needs to successfully deploy and scale apps.</p><p>Enter <a href=/docs/user-guide/petset/>Pet Set</a>, a new object in Kubernetes 1.3 for improved stateful application support. Pet Set sequences through the startup phase of each database replica (for example), ensuring orderly master/slave configuration. Pet Set also simplifies service discovery by leveraging ubiquitous DNS SRV records, a well-recognized and long-understood mechanism.</p><p>Diamanti’s <a href=https://github.com/kubernetes/kubernetes/pull/13840>FlexVolume contribution</a> to Kubernetes enables stateful workloads by providing persistent volumes with low-latency storage and guaranteed performance, including enforced quality-of-service from container to media.</p><p><strong>A Federalist</strong></p><p>Users who are planning for application availability must contend with issues of failover and scale across geography. Cross-cluster federated services allows containerized apps to easily deploy across multiple clusters. Federated services tackles challenges such as managing multiple container clusters and coordinating service deployment and discovery across federated clusters.</p><p>Like a strictly centralized model, federation provides a common app deployment interface. With each cluster retaining autonomy, however, federation adds flexibility to manage clusters locally during network outages and other events. Cross-cluster federated services also applies consistent service naming and adoption across container clusters, simplifying DNS resolution.</p><p>It’s easy to imagine powerful multi-cluster use cases with cross-cluster federated services in future releases. An example is scheduling containers based on governance, security, and performance requirements. Diamanti’s scheduler extension was developed with this concept in mind. Our <a href=https://github.com/kubernetes/kubernetes/pull/13580>first implementation</a> makes the Kubernetes scheduler aware of network and storage resources local to each cluster node. Similar concepts can be applied in the future to broader placement controls with cross-cluster federated services. </p><p><strong>Get Involved</strong></p><p>With interest growing in stateful apps, work has already started to further enhance Kubernetes storage. The Storage Special Interest Group is discussing proposals to support local storage resources. Diamanti is looking forward to extend FlexVolume to include richer APIs that enable local storage and storage services including data protection, replication, and reduction. We’re also working on proposals for improved app placement, migration, and failover across container clusters through Kubernetes cross-cluster federated services.</p><p>Join the conversation and contribute! Here are some places to get started:</p><ul><li>Product Management <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-pm>group</a></li><li>Kubernetes <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-storage>Storage SIG</a> </li><li>Kubernetes <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-federation>Cluster Federation SIG</a></li></ul><p><em>-- Mark Balch, VP Products, <a href=https://diamanti.com/>Diamanti</a>. Twitter <a href=https://twitter.com/markbalch>@markbalch</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b79d59018d2be0d5c94232073f6200f5>Thousand Instances of Cassandra using Kubernetes Pet Set</h1><div class="td-byline mb-4"><time datetime=2016-07-13 class=text-muted>Wednesday, July 13, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3</em></p><h2 id=running-the-greek-pet-monster-races>Running The Greek Pet Monster Races</h2><p>For the <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3 launch</a>, we wanted to put the new Pet Set through its paces. By testing a thousand instances of <a href=https://cassandra.apache.org/>Cassandra</a>, we could make sure that Kubernetes 1.3 was production ready. Read on for how we adapted Cassandra to Kubernetes, and had our largest deployment ever.</p><p>It’s fairly straightforward to use containers with basic stateful applications today. Using a persistent volume, you can mount a disk in a pod, and ensure that your data lasts beyond the life of your pod. However, with deployments of distributed stateful applications, things can become more tricky. With Kubernetes 1.3, the new <a href=/docs/user-guide/petset/>Pet Set</a> component makes everything much easier. To test this new feature out at scale, we decided to host the Greek Pet Monster Races! We raced Centaurs and other Ancient Greek Monsters over hundreds of thousands of races across multiple availability zones.</p><p><a href=https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Cassandra1.jpeg/283px-Cassandra1.jpeg><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Cassandra1.jpeg/283px-Cassandra1.jpeg alt=File:Cassandra1.jpeg></a><br>As many of you know Kubernetes is from the Ancient Greek: κυβερνήτης. This means helmsman, pilot, steersman, or ship master. So in order to keep track of race results, we needed a data store, and we choose Cassandra. Κασσάνδρα, Cassandra who was the daughter of King of Priam and Queen Hecuba of Troy. With multiple references to the ancient Greek language, we thought it would be appropriate to race ancient Greek monsters.</p><p>From there the story kinda goes sideways because Cassandra was actually the Pets as well. Read on and we will explain.</p><p>One of the new exciting features in Kubernetes 1.3 is Pet Set. In order to organize the deployment of containers inside of Kubernetes, different deployment mechanisms are available. Examples of these components include Resource Controllers and Daemon Set. Pet Sets is a new feature that delivers the capability to deploy containers, as Pets, inside of Kubernetes. Pet Sets provide a guarantee of identity for various aspects of the pet / pod deployment: DNS name, consistent storage, and ordered pod indexing. Previously, using components like Deployments and Replication Controllers, would only deploy an application with a weak uncoupled identity. A weak identity is great for managing applications such as microservices, where service discovery is important, the application is stateless, and the naming of individual pods does not matter. Many software applications do require strong identity, including many different types of distributed stateful systems. Cassandra is a great example of a distributed application that requires consistent network identity, and stable storage.</p><p>Pet Sets provides the following capabilities:</p><ul><li>A stable hostname, available to others in DNS. Number is based off of the Pet Set name and starts at zero. For example cassandra-0.</li><li>An ordinal index of Pets. 0, 1, 2, 3, etc.</li><li>Stable storage linked to the ordinal and hostname of the Pet.</li><li>Peer discovery is available via DNS. With Cassandra the names of the peers are known before the Pets are created.</li><li>Startup and Teardown ordering. Which numbered Pet is going to be created next is known, and which Pet will be destroyed upon reducing the Pet Set size. This feature is useful for such admin tasks as draining data from a Pet, when reducing the size of a cluster.</li></ul><p>If your application has one or more of these requirements, then it may be a candidate for Pet Set.<br>A relevant analogy is that a Pet Set is composed of Pet dogs. If you have a white, brown or black dog and the brown dog runs away, you can replace it with another brown dog no one would notice. If over time you can keep replacing your dogs with only white dogs then someone would notice. Pet Set allows your application to maintain the unique identity or hair color of your Pets.</p><p>Example workloads for Pet Set:</p><ul><li>Clustered software like Cassandra, Zookeeper, etcd, or Elastic require stable membership.</li><li>Databases like MySQL or PostgreSQL that require a single instance attached to a persistent volume at any time.</li></ul><p>Only use Pet Set if your application requires some or all of these properties. Managing pods as stateless replicas is vastly easier.</p><p>So back to our races!</p><p>As we have mentioned, Cassandra was a perfect candidate to deploy via a Pet Set. A Pet Set is much like a <a href=/docs/user-guide/replication-controller/>Replica Controller</a> with a few new bells and whistles. Here's an example YAML manifest:</p><h1 id=headless-service-to-provide-dns-lookup>Headless service to provide DNS lookup</h1><pre><code>apiVersion: v1

kind: Service

metadata:

  labels:

    app: cassandra

  name: cassandra

spec:

  clusterIP: None

  ports:

    - port: 9042

  selector:

    app: cassandra-data

----

# new API name

apiVersion: &quot;apps/v1alpha1&quot;

kind: PetSet

metadata:

  name: cassandra

spec:

  serviceName: cassandra

  # replicas are the same as used by Replication Controllers

  # except pets are deployed in order 0, 1, 2, 3, etc

  replicas: 5

  template:

    metadata:

      annotations:

        pod.alpha.kubernetes.io/initialized: &quot;true&quot;

      labels:

        app: cassandra-data

    spec:

      # just as other component in Kubernetes one

      # or more containers are deployed

      containers:

      - name: cassandra

        image: &quot;cassandra-debian:v1.1&quot;

        imagePullPolicy: Always

        ports:

        - containerPort: 7000

          name: intra-node

        - containerPort: 7199

          name: jmx

        - containerPort: 9042

          name: cql

        resources:

          limits:

            cpu: &quot;4&quot;

            memory: 11Gi

          requests:

           cpu: &quot;4&quot;

           memory: 11Gi

        securityContext:

          privileged: true

        env:

          - name: MAX\_HEAP\_SIZE

            value: 8192M

          - name: HEAP\_NEWSIZE

            value: 2048M

          # this is relying on guaranteed network identity of Pet Sets, we

          # will know the name of the Pets / Pod before they are created

          - name: CASSANDRA\_SEEDS

            value: &quot;cassandra-0.cassandra.default.svc.cluster.local,cassandra-1.cassandra.default.svc.cluster.local&quot;

          - name: CASSANDRA\_CLUSTER\_NAME

            value: &quot;OneKDemo&quot;

          - name: CASSANDRA\_DC

            value: &quot;DC1-Data&quot;

          - name: CASSANDRA\_RACK

            value: &quot;OneKDemo-Rack1-Data&quot;

          - name: CASSANDRA\_AUTO\_BOOTSTRAP

            value: &quot;false&quot;

          # this variable is used by the read-probe looking

          # for the IP Address in a `nodetool status` command

          - name: POD\_IP

            valueFrom:

              fieldRef:

                fieldPath: status.podIP

        readinessProbe:

          exec:

            command:

            - /bin/bash

            - -c

            - /ready-probe.sh

          initialDelaySeconds: 15

          timeoutSeconds: 5

        # These volume mounts are persistent. They are like inline claims,

        # but not exactly because the names need to match exactly one of

        # the pet volumes.

        volumeMounts:

        - name: cassandra-data

          mountPath: /cassandra\_data

  # These are converted to volume claims by the controller

  # and mounted at the paths mentioned above.  Storage can be automatically

  # created for the Pets depending on the cloud environment.

  volumeClaimTemplates:

  - metadata:

      name: cassandra-data

      annotations:

        volume.alpha.kubernetes.io/storage-class: anything

    spec:

      accessModes: [&quot;ReadWriteOnce&quot;]

      resources:

        requests:
          storage: 380Gi
</code></pre><p>You may notice that these containers are on the rather large size, and it is not unusual to run Cassandra in production with 8 CPU and 16GB of ram. There are two key new features that you will notice above; dynamic volume provisioning, and of course Pet Set. The above manifest will create 5 Cassandra Pets / Pods starting with the number 0: cassandra-data-0, cassandra-data-1, etc.</p><p>In order to generate data for the races, we used another Kubernetes feature called Jobs. Simple python code was written to generate the random speed of the monster for every second of the race. Then that data, position information, winners, other data points, and metrics were stored in Cassandra. To visualize the data, we used JHipster to generate a AngularJS UI with Java services, and then used D3 for graphing.</p><p>An example of one of the Jobs:</p><pre><code>apiVersion: batch/v1

kind: Job

metadata:

  name: pet-race-giants

  labels:

    name: pet-races

spec:

  parallelism: 2

  completions: 4

  template:

    metadata:

      name: pet-race-giants

      labels:

        name: pet-races

    spec:

      containers:

      - name: pet-race-giants

        image: py3numpy-job:v1.0

        command: [&quot;pet-race-job&quot;, --length=100&quot;, &quot;--pet=Giants&quot;, &quot;--scale=3&quot;]

        resources:

          limits:

            cpu: &quot;2&quot;

          requests:

            cpu: &quot;2&quot;

      restartPolicy: Never
</code></pre><p><a href=https://upload.wikimedia.org/wikipedia/commons/0/0e/Polyphemus.gif><img src=https://upload.wikimedia.org/wikipedia/commons/0/0e/Polyphemus.gif alt=File:Polyphemus.gif></a>Since we are talking about Monsters, we had to go big. We deployed 1,009 minion nodes to <a href=https://cloud.google.com/compute/>Google Compute Engine</a> (GCE), spread across 4 zones, running a custom version of the Kubernetes 1.3 beta. We ran this demo on beta code since the demo was being set up before the 1.3 release date. For the minion nodes, GCE virtual machine n1-standard-8 machine size was chosen, which is vm with 8 virtual CPUs and 30GB of memory. It would allow for a single instance of Cassandra to run on one node, which is recommended for disk I/O.</p><p>Then the pets were deployed! One thousand of them, in two different Cassandra Data Centers. Cassandra distributed architecture is specifically tailored for multiple-data center deployment. Often multiple Cassandra data centers are deployed inside the same physical or virtual data center, in order to separate workloads. Data is replicated across all data centers, but workloads can be different between data centers and thus application tuning can be different. Data centers named 'DC1-Analytics' and ‘DC1-Data’ where deployed with 500 pets each. The race data was created by the python Batch Jobs connected to DC1-Data, and the JHipster UI was connected DC1-Analytics.</p><p>Here are the final numbers:</p><ul><li>8,072 Cores. The master used 24, minion nodes used the rest</li><li>1,009 IP addresses</li><li>1,009 routes setup by Kubernetes on Google Cloud Platform</li><li>100,510 GB persistent disk used by the Minions and the Master</li><li>380,020 GB SSD disk persistent disk. 20 GB for the master and 340 GB per Cassandra Pet.</li><li>1,000 deployed instances of Cassandra
Yes we deployed 1,000 pets, but one really did not want to join the party! Technically with the Cassandra setup, we could have lost 333 nodes without service or data loss.</li></ul><h3 id=limitations-with-pet-sets-in-1-3-release>Limitations with Pet Sets in 1.3 Release</h3><ul><li>Pet Set is an alpha resource not available in any Kubernetes release prior to 1.3.</li><li>The storage for a given pet must either be provisioned by a dynamic storage provisioner based on the requested storage class, or pre-provisioned by an admin.</li><li>Deleting the Pet Set will not delete any pets or Pet storage. You will need to delete your Pets and possibly its storage by hand.</li><li>All Pet Sets currently require a "governing service", or a Service responsible for the network identity of the pets. The user is responsible for this Service.</li><li>Updating an existing Pet Set is currently a manual process. You either need to deploy a new Pet Set with the new image version or orphan Pets one by one and update their image, which will join them back to the cluster.</li></ul><h4 id=resources-and-references>Resources and References</h4><ul><li>The source code for the demo is available on <a href=https://github.com/k8s-for-greeks/gpmr>GitHub</a>: (Pet Set examples will be merged into the Kubernetes Cassandra Examples).</li><li>More information about <a href=/docs/user-guide/jobs/>Jobs</a></li><li><a href=https://github.com/kubernetes/kubernetes.github.io/blob/release-1.3/docs/user-guide/petset.md>Documentation for Pet Set</a></li><li>Image credits: Cassandra <a href=https://commons.wikimedia.org/wiki/File:Cassandra1.jpeg>image</a> and Cyclops <a href=https://commons.wikimedia.org/wiki/File:Polyphemus.gif>image</a></li></ul><p><em>-- Chris Love, Senior DevOps Open Source Consultant for <a href=https://www.datapipe.com/>Datapipe</a>. <a href=https://twitter.com/chrislovecnm/>Twitter @chrislovecnm</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a9cee2fd740428ef2eeefa7c16810a78>Autoscaling in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-07-12 class=text-muted>Tuesday, July 12, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3</em></p><p>Customers using Kubernetes respond to end user requests quickly and ship software faster than ever before. But what happens when you build a service that is even more popular than you planned for, and run out of compute? In <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3</a>, we are proud to announce that we have a solution: autoscaling. On <a href=https://cloud.google.com/compute/>Google Compute Engine</a> (GCE) and <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE) (and coming soon on <a href=https://aws.amazon.com/>AWS</a>), Kubernetes will automatically scale up your cluster as soon as you need it, and scale it back down to save you money when you don’t.</p><h3 id=benefits-of-autoscaling>Benefits of Autoscaling</h3><p>To understand better where autoscaling would provide the most value, let’s start with an example. Imagine you have a 24/7 production service with a load that is variable in time, where it is very busy during the day in the US, and relatively low at night. Ideally, we would want the number of nodes in the cluster and the number of pods in deployment to dynamically adjust to the load to meet end user demand. The new Cluster Autoscaling feature together with Horizontal Pod Autoscaler can handle this for you automatically.</p><h3 id=setting-up-autoscaling-on-gce>Setting Up Autoscaling on GCE</h3><p>The following instructions apply to GCE. For GKE please check the autoscaling section in cluster operations manual available <a href=https://cloud.google.com/container-engine/docs/clusters/operations#create_a_cluster_with_autoscaling>here</a>.</p><p>Before we begin, we need to have an active GCE project with Google Cloud Monitoring, Google Cloud Logging and Stackdriver enabled. For more information on project creation, please read our <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/gce.md#prerequisites>Getting Started Guide</a>. We also need to download a recent version of Kubernetes project (version <a href=http://v1.3.0/>v1.3.0</a> or later).</p><p>First, we set up a cluster with Cluster Autoscaler turned on. The number of nodes in the cluster will start at 2, and autoscale up to a maximum of 5. To implement this, we’ll export the following environment variables:</p><pre><code>export NUM\_NODES=2

export KUBE\_AUTOSCALER\_MIN\_NODES=2

export KUBE\_AUTOSCALER\_MAX\_NODES=5

export KUBE\_ENABLE\_CLUSTER\_AUTOSCALER=true
</code></pre><p>and start the cluster by running:</p><pre><code>./cluster/kube-up.sh
</code></pre><p>The kube-up.sh script creates a cluster together with Cluster Autoscaler add-on. The autoscaler will try to add new nodes to the cluster if there are pending pods which could schedule on a new node.</p><p>Let’s see our cluster, it should have two nodes:</p><pre><code>$ kubectl get nodes

NAME                           STATUS                     AGE

kubernetes-master              Ready,SchedulingDisabled   2m

kubernetes-minion-group-de5q   Ready                      2m

kubernetes-minion-group-yhdx   Ready                      1m
</code></pre><h4 id=run-expose-php-apache-server>Run & Expose PHP-Apache Server</h4><p>To demonstrate autoscaling we will use a custom docker image based on php-apache server. The image can be found <a href=https://github.com/kubernetes/kubernetes/blob/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/user-guide/horizontal-pod-autoscaling/image>here</a>. It defines <a href=https://github.com/kubernetes/kubernetes/blob/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/user-guide/horizontal-pod-autoscaling/image/index.php>index.php</a> page which performs some CPU intensive computations.</p><p>First, we’ll start a deployment running the image and expose it as a service:</p><pre><code>$ kubectl run php-apache \   

  --image=gcr.io/google\_containers/hpa-example \

  --requests=cpu=500m,memory=500M --expose --port=80  

service &quot;php-apache&quot; createddeployment &quot;php-apache&quot; created
</code></pre><p>Now, we will wait some time and verify that both the deployment and the service were correctly created and are running:</p><pre><code>$ kubectl get deployment

NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE

php-apache   1         1         1            1           49s



$ kubectl get pods  
NAME                          READY     STATUS    RESTARTS   AGE

php-apache-2046965998-z65jn   1/1       Running   0          30s
</code></pre><p>We may now check that php-apache server works correctly by calling wget with the service's address:</p><pre><code>$ kubectl run -i --tty service-test --image=busybox /bin/sh  
Hit enter for command prompt  
$ wget -q -O- http://php-apache.default.svc.cluster.local

OK!
</code></pre><h4 id=starting-horizontal-pod-autoscaler>Starting Horizontal Pod Autoscaler</h4><p>Now that the deployment is running, we will create a Horizontal Pod Autoscaler for it. To create it, we will use kubectl autoscale command, which looks like this:</p><pre><code>$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
</code></pre><p>This defines a Horizontal Ppod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions. Roughly speaking, the horizontal autoscaler will increase and decrease the number of replicas (via the deployment) so as to maintain an average CPU utilization across all Pods of 50% (since each pod requests 500 milli-cores by <a href=https://github.com/kubernetes/kubernetes/blob/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/user-guide/horizontal-pod-autoscaling/README.md#kubectl-run>kubectl run</a>, this means average CPU usage of 250 milli-cores). See <a href=https://github.com/kubernetes/kubernetes/blob/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/design/horizontal-pod-autoscaler.md#autoscaling-algorithm>here</a> for more details on the algorithm.</p><p>We may check the current status of autoscaler by running:</p><pre><code>$ kubectl get hpa

NAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE

php-apache   Deployment/php-apache/scale   50%       0%        1         20        14s
</code></pre><p>Please note that the current CPU consumption is 0% as we are not sending any requests to the server (the CURRENT column shows the average across all the pods controlled by the corresponding replication controller).</p><h4 id=raising-the-load>Raising the Load</h4><p>Now, we will see how our autoscalers (Cluster Autoscaler and Horizontal Pod Autoscaler) react on the increased load of the server. We will start two infinite loops of queries to our server (please run them in different terminals):</p><pre><code>$ kubectl run -i --tty load-generator --image=busybox /bin/sh  
Hit enter for command prompt  
$ while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done
</code></pre><p>We need to wait a moment (about one minute) for stats to propagate. Afterwards, we will examine status of Horizontal Pod Autoscaler:</p><pre><code>$ kubectl get hpa

NAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE

php-apache   Deployment/php-apache/scale   50%       310%      1         20        2m



$ kubectl get deployment php-apache

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE

php-apache        7         7         7            3           4m
</code></pre><p>Horizontal Pod Autoscaler has increased the number of pods in our deployment to 7. Let’s now check, if all the pods are running:</p><pre><code>jsz@jsz-desk2:~/k8s-src$ kubectl get pods

php-apache-2046965998-3ewo6        0/1       Pending   0          1m

php-apache-2046965998-8m03k        1/1       Running   0          1m

php-apache-2046965998-ddpgp        1/1       Running   0          5m

php-apache-2046965998-lrik6        1/1       Running   0          1m

php-apache-2046965998-nj465        0/1       Pending   0          1m

php-apache-2046965998-tmwg1        1/1       Running   0          1m

php-apache-2046965998-xkbw1        0/1       Pending   0          1m
</code></pre><p>As we can see, some pods are pending. Let’s describe one of pending pods to get the reason of the pending state:</p><pre><code>$ kubectl describe pod php-apache-2046965998-3ewo6

Name: php-apache-2046965998-3ewo6

Namespace: default

...

Events:

  FirstSeen From SubobjectPath Type Reason Message



  1m {default-scheduler } Warning FailedScheduling pod (php-apache-2046965998-3ewo6) failed to fit in any node

fit failure on node (kubernetes-minion-group-yhdx): Insufficient CPU

fit failure on node (kubernetes-minion-group-de5q): Insufficient CPU



  1m {cluster-autoscaler } Normal TriggeredScaleUp pod triggered scale-up, mig: kubernetes-minion-group, sizes (current/new): 2/3
</code></pre><p>The pod is pending as there was no CPU in the system for it. We see there’s a TriggeredScaleUp event connected with the pod. It means that the pod triggered reaction of Cluster Autoscaler and a new node will be added to the cluster. Now we’ll wait for the reaction (about 3 minutes) and list all nodes:</p><pre><code>$ kubectl get nodes

NAME                           STATUS                     AGE

kubernetes-master              Ready,SchedulingDisabled   9m

kubernetes-minion-group-6z5i   Ready                      43s

kubernetes-minion-group-de5q   Ready                      9m

kubernetes-minion-group-yhdx   Ready                      9m
</code></pre><p>As we see a new node kubernetes-minion-group-6z5i was added by Cluster Autoscaler. Let’s verify that all pods are now running:</p><pre><code>$ kubectl get pods

NAME                               READY     STATUS    RESTARTS   AGE

php-apache-2046965998-3ewo6        1/1       Running   0          3m

php-apache-2046965998-8m03k        1/1       Running   0          3m

php-apache-2046965998-ddpgp        1/1       Running   0          7m

php-apache-2046965998-lrik6        1/1       Running   0          3m

php-apache-2046965998-nj465        1/1       Running   0          3m

php-apache-2046965998-tmwg1        1/1       Running   0          3m

php-apache-2046965998-xkbw1        1/1       Running   0          3m
</code></pre><p>After the node addition all php-apache pods are running!</p><h4 id=stop-load>Stop Load</h4><p>We will finish our example by stopping the user load. We’ll terminate both infinite while loops sending requests to the server and verify the result state:</p><pre><code>$ kubectl get hpa

NAME         REFERENCE                     TARGET    CURRENT   MINPODS   MAXPODS   AGE

php-apache   Deployment/php-apache/scale   50%       0%        1         10        16m



$ kubectl get deployment php-apache

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE

php-apache        1         1         1            1           14m
</code></pre><p>As we see, in the presented case CPU utilization dropped to 0, and the number of replicas dropped to 1.</p><p>After deleting pods most of the cluster resources are unused. Scaling the cluster down may take more time than scaling up because Cluster Autoscaler makes sure that the node is really not needed so that short periods of inactivity (due to pod upgrade etc) won’t trigger node deletion (see <a href=https://github.com/kubernetes/kubernetes.github.io/blob/release-1.3/docs/admin/cluster-management.md#cluster-autoscaling>cluster autoscaler doc</a>). After approximately 10-12 minutes you can verify that the number of nodes in the cluster dropped:</p><pre><code>$ kubectl get nodes

NAME                           STATUS                     AGE

kubernetes-master              Ready,SchedulingDisabled   37m

kubernetes-minion-group-de5q   Ready                      36m

kubernetes-minion-group-yhdx   Ready                      36m
</code></pre><p>The number of nodes in our cluster is now two again as node kubernetes-minion-group-6z5i was removed by Cluster Autoscaler.</p><h3 id=other-use-cases>Other use cases</h3><p>As we have shown, it is very easy to dynamically adjust the number of pods to the load using a combination of Horizontal Pod Autoscaler and Cluster Autoscaler.</p><p>However Cluster Autoscaler alone can also be quite helpful whenever there are irregularities in the cluster load. For example, clusters related to development or continuous integration tests can be less needed on weekends or at night. Batch processing clusters may have periods when all jobs are over and the new will only start in couple hours. Having machines that do nothing is a waste of money.</p><p>In all of these cases Cluster Autoscaler can reduce the number of unused nodes and give quite significant savings because you will only pay for these nodes that you actually need to run your pods. It also makes sure that you always have enough compute power to run your tasks.</p><p><em>-- Jerzy Szczepkowski and Marcin Wielgus, Software Engineers, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a5dbe52c882fae6ff5183978c49068da>Kubernetes in Rancher: the further evolution</h1><div class="td-byline mb-4"><time datetime=2016-07-12 class=text-muted>Tuesday, July 12, 2016</time></div><p><em>Editor’s note: today's guest post is from Alena Prokharchyk, Principal Software Engineer at Rancher Labs, who’ll share how they are incorporating new Kubernetes features into their platform.</em></p><p>Kubernetes was the first external orchestration platform supported by <a href=http://rancher.com/kubernetes>Rancher</a>, and since its release, it has become one of the most widely used among our users, and continues to grow rapidly in adoption. As Kubernetes has evolved, so has Rancher in terms of adapting new Kubernetes features. We’ve started with supporting Kubernetes version 1.1, then switched to 1.2 as soon as it was released, and now we’re working on supporting the exciting new features in 1.3. I’d like to walk you through the features that we’ve been adding support for during each of these stages.</p><h3 id=rancher-and-kubernetes-1-2>Rancher and Kubernetes 1.2</h3><p>Kubernetes 1.2 introduced enhanced Ingress object to simplify allowing inbound connections to reach the cluster services: here’s an excellent <a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-and-simplifying-advanced-networking-with-ingress/>blog post about ingress</a> policies. Ingress resource allows users to define host name routing rules and TLS config for the Load Balancer in a user friendly way. Then it should be backed up by an Ingress controller that would configure a corresponding cloud provider’s Load Balancer with the Ingress rules. Since Rancher already included a software defined Load Balancer based on HAproxy, we already supported all of the configuration requirements of the Ingress resource, and didn’t have to do any changes on the Rancher side to adopt Ingress. What we had to do was write an Ingress controller that would listen to Kubernetes ingress specific events, configure the Rancher Load Balancer accordingly, and propagate the Load Balancer public entry point back to Kubernetes:</p><p><img src=https://lh3.googleusercontent.com/C8wg_8Vih0evMIAEvCaX3IAbARddxhk5S_Mzv9jdpt87njQR9cbEEGZnFiWrKx7TPm-uPO1V4TP4LDOKvLg7gJ-19-esVMNhbkSf6fXSrbE3nS3Sr45rdP1c-VBuzShgpn9jDCiQ alt=Screen-Shot-2016-05-13-at-11.15.56-AM.png></p><p>Now, the ingress controller gets deployed as a part of our Rancher Kubernetes system stack, and is managed by Rancher. Rancher monitors Ingress controller health, and recreates it in case of any failures. In addition to standard ingress features, Rancher also lets you to horizontally scale the Load Balancer supporting the ingress service by specifying scale via Ingress annotations. For example:</p><pre><code>apiVersion: extensions/v1beta1

kind: Ingress

metadata:

 name: scalelb

 annotations:

 scale: &quot;2&quot;

spec:

  rules:

  - host: foo.bar.com

    http:

      paths:

      - path: /foo

        backend:

          serviceName: nginx-service

          servicePort: 80
</code></pre><p>As a result of the above, 2 instances of Rancher Load Balancer will get started on separate hosts, and Ingress will get updated with 2 public ip addresses:</p><pre><code>kubectl get ingress

NAME      RULE          BACKEND   ADDRESS

scalelb      -                    104.154.107.202, 104.154.107.203  // hosts ip addresses where Rancher LB instances are deployed

          foo.bar.com

          /foo           nginx-service:80
</code></pre><p>More details on Rancher Ingress Controller implementation for Kubernetes can be found here:</p><ul><li><a href=http://rancher.com/rancher-controller-for-the-kubernetes-ingress-feature/>Blog post</a></li><li><a href=http://docs.rancher.com/rancher/latest/en/kubernetes/ingress/>Rancher documentation on Ingress</a></li><li><a href=https://github.com/rancher/ingress-controller>Rancher ingress controller repo</a></li></ul><h3 id=rancher-and-kubernetes-1-3>Rancher and Kubernetes 1.3</h3><p>We’ve very excited about Kubernetes 1.3 release, and all the new features that are included with it. There are two that we are especially interested in: Stateful Apps and Cluster Federation.</p><h4 id=kubernetes-stateful-apps>Kubernetes Stateful Apps</h4><p>Stateful Apps is a new resource to Kubernetes to represent a set of pods in stateful application. This is an alternative to the using Replication Controllers, which are best leveraged for running stateless apps. This feature is specifically useful for apps that rely on quorum with leader election (such as MongoDB, Zookeeper, etcd) and decentralized quorum (Cassandra). Stateful Apps create and maintains a set of pods, each of which have a stable network identity. In order to provide the network identity, it must be possible to have a resolvable DNS name for the pod that is tied to the pod identity as per <a href=https://github.com/smarterclayton/kubernetes/blob/961f1f94c35d4979ac83bbad482090cb6c22781c/docs/proposals/petset.md>Kubernetes design doc</a>:</p><pre><code># service mongo pointing to pods created by PetSet mdb, with identities mdb-1, mdb-2, mdb-3


dig mongodb.namespace.svc.cluster.local +short A

172.130.16.50


dig mdb-1.mongodb.namespace.svc.cluster.local +short A

# IP of pod created for mdb-1


dig mdb-2.mongodb.namespace.svc.cluster.local +short A

# IP of pod created for mdb-2


dig mdb-3.mongodb.namespace.svc.cluster.local +short A

# IP of pod created for mdb-3
</code></pre><p>The above is implemented via an annotation on pods, which is surfaced to endpoints, and finally surfaced as DNS on the service that exposes those pods. Currently Rancher simplifies DNS configuration by leveraging Rancher DNS as a drop-in replacement for SkyDNS. Rancher DNS is fast, stable, and scalable - every host in cluster gets DNS server running. Kubernetes services get programmed to Rancher DNS, and being resolved to either service’s cluster IP from 10,43.x.x address space, or to set of Pod ip addresses for headless service. To make PetSet work with Kubernetes via Rancher, we’ll have to add support for Pod Identities to Rancher DNS configuration. We’re working on this now and should have it supported in one of the upcoming Rancher releases.</p><h4 id=cluster-federation>Cluster Federation</h4><p>Cluster Federation is a control plane of cluster federation in Kubernetes. It offers improved application availability by spreading applications across multiple clusters (the image below is a courtesy of Kubernetes):</p><p><img src=https://lh6.googleusercontent.com/jJjQ6wbYYG1y7rS7SXFNj1dsLrTEBbiOB9TfrkJAqayHVzBZwLguxMB6HLObCgpVGLKF7xdPd3wfdvQzB2a7Cq6cuqqXRRl3L5OfVPwKB34BxdpRUc1g7EgOdEkILH9E4sAfzHyb alt="Screen Shot 2016-07-07 at 1.46.55 PM.png"></p><p>Each Kubernetes cluster exposes an API endpoint and gets registered to Cluster Federation as a part of Federation object. Then using Cluster Federation API, you can create federated services. Those objects are comprised of multiple equivalent underlying Kubernetes resources. Assuming that the 3 clusters on the picture above belong to the same Federation object, each Service created via Cluster Federation, will get equivalent service created in each of the clusters. Besides that, a Cluster Federation service will get publicly resolvable DNS name resolvable to Kubernetes service’s public ip addresses (DNS record gets programmed to a one of the public DNS providers below):</p><p><img src=https://lh6.googleusercontent.com/gmL0eoE2Z_m-KQbidAxrHA_gL8EDoflYuu_DKSxRiSm2RqTde-nYwGD65YBWzZWkCnbEG6NJ_NHCo0oHTP-PxNqWXt7k5Vp76JBOTNawsmlTeehOrPVY6nTZnEMl2ZH0V73_7f9E alt="Screen Shot 2016-07-07 at 1.24.18 PM.png"></p><p>To support Cluster Federation via Kubernetes in Rancher, certain changes need to be done. Today each Kubernetes cluster is represented as a Rancher environment. In each Kubernetes environment, we create a full Kubernetes system stack comprised of several services: Kubernetes API server, Scheduler, Ingress controller, persistent etcd, Controller manager, Kubelet and Proxy (2 last ones run on every host). To setup Cluster Federation, we will create one extra environment where Cluster Federation stack is going to run:</p><p><img src=https://lh6.googleusercontent.com/_76MDeSl_ac2AqN2lvEKgmvrFuV9Mtt9qHngsKKBAy-rcpdMcyo_UyNYdK2z5POoZwBGptVXUoX-11UDHD4axY8Lco15KydIwVd_PlLC0xJ2GZ_-4JN7bkP4pj8SY7mQ4JUXGIL6 alt="Screen Shot 2016-07-07 at 1.23.14 PM.png"></p><p>Then every underlying Kubernetes cluster represented by Rancher environment, should be registered to a specific Cluster Federation. Potentially each cluster can be auto-discovered by Rancher Cluster Federation environment via label representing federation name on Kubernetes cluster. We’re still working through finalizing our design, but we’re very excited by this feature, and see a lot of use cases it can solve. Cluster Federation doc references:</p><ul><li>Kubernetes <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/design/federation-phase-1.md>cluster federation design doc</a></li><li>Kubernetes <a href=https://kubernetes.io/blog/2016/03/building-highly-available-applications-using-kubernetes-new-multi-zone-clusters-aka-ubernetes-lite/>blog post on multi zone clusters</a></li><li>Kubernetes <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/design/federated-services.md>federated services design doc</a></li></ul><h3 id=plans-for-kubernetes-1-4>Plans for Kubernetes 1.4</h3><p>When we launched Kubernetes support in Rancher we decided to maintain our own distribution of Kubernetes in order to support Rancher’s native networking. We were aware that by having our own distribution, we’d need to update it every time there were changes made to Kubernetes, but we felt it was necessary to support the use cases we were working on for users. As part of our work for 1.4 we looked at our networking approach again, and re-analyzed the initial need for our own fork of Kubernetes. Other than the networking integration, all of the work we’ve done with Kubernetes has been developed as a Kubernetes plugin:</p><ul><li>Rancher as a CloudProvider (to support Load Balancers).</li><li>Rancher as a CredentialProvider (to support Rancher private registries).</li><li>Rancher Ingress controller to back up Kubernetes ingress resource.</li></ul><p>So we’ve decided to eliminate the need of Rancher Kubernetes distribution, and try to upstream all our changes to the Kubernetes repo. To do that, we will be reworking our networking integration, and support Rancher networking as a <a href=/docs/admin/network-plugins/#cni>CNI plugin for Kubernetes</a>. More details on that will be shared as soon as the feature design is finalized, but expect it to come in the next 2-3 months. We will also continue investing in Rancher’s core capabilities integrated with Kubernetes, including, but not limited to:</p><ul><li>Access rights management via Rancher environment that represents Kubernetes cluster</li><li>Credential management and easy web-based access to standard kubectl cli</li><li>Load Balancing support</li><li>Rancher internal DNS support</li><li>Catalog support for Kubernetes templates</li><li>Enhanced UI to represent even more Kubernetes objects like: Deployment, Ingress, Daemonset.</li></ul><p>All of that is to make Kubernetes experience even more powerful and user intuitive. We’re so excited by all of the progress in the Kubernetes community, and thrilled to be participating. Kubernetes 1.3 is an incredibly significant release, and you’ll be able to upgrade to it very soon within Rancher.</p><p><em>-- Alena Prokharchyk, Principal Software Engineer, Rancher Labs. <a href=https://twitter.com/Lemonjet>Twitter @lemonjet</a> & <a href=https://github.com/alena1108>GitHub alena1108</a></em></p><p><img src=https://lh4.googleusercontent.com/isAt46fnmGerA0uPoTUlUS7y5MtmOYfMvKoTC52CK0ckUfFKVO_coY78jgLoQuxe4J3GVf3N2_IWCuKwxpRT6q_h4ek4yepfyWBmN_WSqyB2v7rRaZrpG4hPpuH0hIbIcmTDgUul alt=Rancher-and-Kubernetes.png></p></div><div class=td-content style=page-break-before:always><h1 id=pg-5229e3092ff989c2c2c3d40703f3ebc8>Five Days of Kubernetes 1.3</h1><div class="td-byline mb-4"><time datetime=2016-07-11 class=text-muted>Monday, July 11, 2016</time></div><p>Last week we <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>released Kubernetes 1.3</a>, two years from the day when the first Kubernetes commit was pushed to GitHub. Now 30,000+ commits later from over 800 contributors, this 1.3 releases is jam packed with updates driven by feedback from users.</p><p>While many new improvements and features have been added in the latest release, we’ll be highlighting several that stand-out. Follow along and read these in-depth posts on what’s new and how we continue to make Kubernetes the best way to manage containers at scale. </p><p>|
Day 1
|</p><p>* <a href=https://kubernetes.io/blog/2016/07/minikube-easily-run-kubernetes-locally>Minikube: easily run Kubernetes locally</a>
* <a href=https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-Kubernetes>rktnetes: brings rkt container engine to Kubernetes</a>
|
|
Day 2
|
* <a href=https://kubernetes.io/blog/2016/07/autoscaling-in-kubernetes>Autoscaling in Kubernetes</a><br>* <em>Partner post: <a href=https://kubernetes.io/blog/2016/07/kubernetes-in-rancher-further-evolution>Kubernetes in Rancher, the further evolution</a></em>
|
|
Day 3
|
* <a href=https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set>Deploying thousand instances of Cassandra using Pet Set</a><br>* <em>Partner post: <a href=https://kubernetes.io/blog/2016/07/stateful-applications-in-containers-kubernetes>Stateful Applications in Containers, by Diamanti</a></em>
|
|
Day 4
|
* <a href=https://kubernetes.io/blog/2016/07/cross-cluster-services>Cross Cluster Services</a><br><em>* Partner post: <a href=https://kubernetes.io/blog/2016/07/Citrix-NetScaler-and-Kubernetes>Citrix and NetScaler CPX</a></em>
|
|
Day 5
|
* <a href=https://kubernetes.io/blog/2016/07/dashboard-web-interface-for-kubernetes>Dashboard - Full Featured Web Interface for Kubernetes</a><br>* <em>Partner post: <a href=https://kubernetes.io/blog/2016/07/automation-platform-at-wercker-with-kubernetes>Steering an Automation Platform at Wercker with Kubernetes</a></em>
|
|
Bonus
|
* <a href=https://kubernetes.io/blog/2016/07/update-on-kubernetes-for-windows-server-containers/>Updates to Performance and Scalability</a>
|</p><p><strong>Connect</strong></p><p>We’d love to hear from you and see you participate in this growing community:</p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=https://stackoverflow.com/questions/tagged/kubernetes>Stackoverflow</a> </li><li>Connect with the community on <a href=http://slack.kubernetes.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-69c04566fe91de95a935091eb52cd6fc>Minikube: easily run Kubernetes locally</h1><div class="td-byline mb-4"><time datetime=2016-07-11 class=text-muted>Monday, July 11, 2016</time></div><p><em>Editor's note: This is the first post in a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3 </em></p><p>While Kubernetes is one of the best tools for managing containerized applications available today, and has been production-ready for over a year, Kubernetes has been missing a great local development platform.</p><p>For the past several months, several of us from the Kubernetes community have been working to fix this in the <a href=http://github.com/kubernetes/minikube>Minikube</a> repository on GitHub. Our goal is to build an easy-to-use, high-fidelity Kubernetes distribution that can be run locally on Mac, Linux and Windows workstations and laptops with a single command.</p><p>Thanks to lots of help from members of the community, we're proud to announce the official release of Minikube. This release comes with support for <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3</a>, new commands to make interacting with your local cluster easier and experimental drivers for xhyve (on macOS) and KVM (on Linux).</p><p><strong>Using Minikube</strong></p><p>Minikube ships as a standalone Go binary, so installing it is as simple as downloading Minikube and putting it on your path:</p><p>Minikube currently requires that you have VirtualBox installed, which you can download <a href=https://www.virtualbox.org/>here</a>.</p><p>_(This is for Mac, for Linux substitute “minikube-darwin-amd64” with “minikube-linux-amd64”)<em>curl -Lo minikube <a href=https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64>https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64</a> && chmod +x minikube && sudo mv minikube /usr/local/bin/</em></p><p>To start a Kubernetes cluster in Minikube, use the <code>minikube start</code> command:</p><pre><code>$ minikube start

Starting local Kubernetes cluster...

Kubernetes is available at https://192.168.99.100:443

Kubectl is now configured to use the cluster
</code></pre><p><img src=https://lh5.googleusercontent.com/UNRbuyrACtW32dxMehR7GaQlj4CaVxVmlw3UhTqzyIDBgENdT1PcXf-3RoW-T1PFhIQtBbIPq1p544NAKFMO_E_1BUx7MBpkRyw6URtv4W0xT-O4tyWDYJf3MYna6a_8cFJnVvXZ alt></p><p>At this point, you have a running single-node Kubernetes cluster on your laptop! Minikube also configures <code>kubectl</code> for you, so you're also ready to run containers with no changes.</p><p>Minikube creates a Host-Only network interface that routes to your node. To interact with running pods or services, you should send traffic over this address. To find out this address, you can use the <code>minikube ip</code> command:</p><p><img src=https://lh4.googleusercontent.com/Qm-FoMGXGTlyhiM9jzuH6HE3497ZH19gjDMZrkNVhrlJzi9KQXlGCPoWbss-Hxa3fSBTbgxVZYjUpK-EG4rSinHHGz-7xH9e0QsmE72gX6Mzn5FihvFBfeF6_pJugd1GT0Gzp5qb alt></p><p>Minikube also comes with the Kubernetes Dashboard. To open this up in your browser, you can use the built-in <code>minikube dashboard</code> command:</p><p><img src=https://lh5.googleusercontent.com/PZOe7HAMTJoO_U-r6mR8bXJc7pRIaw33BSQ_SafMY-DPSJB5tiw9SooUvCbtOCJEqQqvnHqngDfFJwWy9Oj3svyo8oTQnzy5srKwZEcBh7fm44n_9YImeJEGhvfNVnx0cfjZ7mcU alt></p><p><img src=https://lh3.googleusercontent.com/fshhlXr1e39gsMKWbVUGb7rrGcy4uP44ML3Jt7-Sr3ZryoMw802xpkAMaz7ayjQNGtAYl3wpKJgwfefuug1FWHbinr1usN9jwFIAJFKeVeZxaiKtalHXP322_D5otR0Asvw6MUD_ alt></p><p>In general, Minikube supports everything you would expect from a Kubernetes cluster. You can use <code>kubectl exec</code> to get a bash shell inside a pod in your cluster. You can use the <code>kubectl port-forward</code> and <code>kubectl proxy</code> commands to forward traffic from localhost to a pod or the API server.</p><p>Since Minikube is running locally instead of on a cloud provider, certain provider-specific features like LoadBalancers and PersistentVolumes will not work out-of-the-box. However, you can use NodePort LoadBalancers and HostPath PersistentVolumes.</p><p><strong>Architecture</strong></p><p>Minikube is built on top of Docker's <a href=https://github.com/docker/machine/tree/master/libmachine>libmachine</a>, and leverages the driver model to create, manage and interact with locally-run virtual machines.</p><p><a href=https://redspread.com/>RedSpread</a> was kind enough to donate their <a href=https://github.com/redspread/localkube>localkube</a> codebase to the Minikube repo, which we use to spin up a single-process Kubernetes cluster inside a VM. Localkube bundles etcd, DNS, the Kubelet and all the Kubernetes master components into a single Go binary, and runs them all via separate goroutines.</p><p><strong>Upcoming Features</strong></p><p>Minikube has been a lot of fun to work on so far, and we're always looking to improve Minikube to make the Kubernetes development experience better. If you have any ideas for features, don't hesitate to let us know in the <a href=https://github.com/kubernetes/minikube/issues>issue tracker</a>. </p><p>Here's a list of some of the things we're hoping to add to Minikube soon:</p><ul><li>Native hypervisor support for macOS and Windows</li><li>We're planning to remove the dependency on Virtualbox, and integrate with the native hypervisors included in macOS and Windows (Hypervisor.framework and Hyper-v, respectively).</li><li>Improved support for Kubernetes features</li><li>We're planning to increase the range of supported Kubernetes features, to include things like Ingress.</li><li>Configurable versions of Kubernetes</li><li>Today Minikube only supports Kubernetes 1.3. We're planning to add support for user-configurable versions of Kubernetes, to make it easier to match what you have running in production on your laptop.</li></ul><p><strong>Community</strong></p><p>We'd love to hear feedback on Minikube. To join the community:</p><ul><li>Post issues or feature requests on <a href=https://github.com/kubernetes/minikube>GitHub</a></li><li>Join us in the #minikube channel on <a href=https://kubernetes.slack.com/>Slack</a></li></ul><p>Please give Minikube a try, and let us know how it goes!</p><p><em>--Dan Lorenc, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d95ea49b72ae0d4ffcb02bbff05ffc22>rktnetes brings rkt container engine to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-07-11 class=text-muted>Monday, July 11, 2016</time></div><p><em>Editor’s note: this post is part of a <a href=https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3>series of in-depth articles</a> on what's new in Kubernetes 1.3 </em></p><p>As part of <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>Kubernetes 1.3</a>, we’re happy to report that our work to bring interchangeable container engines to Kubernetes is bearing early fruit. What we affectionately call “rktnetes” is included in the version 1.3 Kubernetes release, and is ready for development use. rktnetes integrates support for <a href=https://coreos.com/rkt/>CoreOS rkt</a> into Kubernetes as the container runtime on cluster nodes, and is now part of the mainline Kubernetes source code. Today it’s easier than ever for developers and ops pros with container portability in mind to try out running Kubernetes with a different container engine.</p><p>"We find CoreOS’s rkt a compelling container engine in Kubernetes because of how rkt is composed with the underlying systemd,” said Mark Petrovic, senior MTS and architect at Xoom, a PayPal service. “The rkt runtime assumes only the responsibility it needs to, then delegates to other system services where appropriate. This separation of concerns is important to us.”</p><h3 id=what-s-rktnetes>What’s rktnetes?</h3><p>rktnetes is the nickname given to the code that enables Kubernetes nodes to execute application containers with the rkt container engine, rather than with Docker. This change adds new abilities to Kubernetes, for instance running containers under flexible levels of isolation. rkt explores an alternative approach to container runtime architecture, aimed to reflect the Unix philosophy of cleanly separated, modular tools. Work done to support rktnetes also opens up future possibilities for Kubernetes, such as multiple container image format support, and the integration of other container runtimes tailored for specific use cases or platforms.</p><h3 id=why-does-kubernetes-need-rktnetes>Why does Kubernetes need rktnetes?</h3><p>rktnetes is about more than just rkt. It’s also about refining and exercising Kubernetes interfaces, and paving the way for other modular runtimes in the future. While the Docker container engine is well known, and is currently the default Kubernetes container runtime, a number of benefits derive from pluggable container environments. Some clusters may call for very specific container engine implementations, for example, and ensuring the Kubernetes design is flexible enough to support alternate runtimes, starting with rkt, helps keep the interfaces between components clean and simple.</p><h4 id=separation-of-concerns-decomposing-the-monolithic-container-daemon>Separation of concerns: Decomposing the monolithic container daemon</h4><p>The current container runtime used by Kubernetes imposes a number of design decisions. Experimenting with other container execution architectures is worthwhile in such a rapidly evolving space. Today, when Kubernetes sends a request to a node to start running a pod, it communicates through the kubelet on each node with the default container runtime’s central daemon, responsible for managing all of the node’s containers.</p><p>rkt does not implement a monolithic container management daemon. (It is worth noting that the <a href=https://blog.docker.com/2016/04/docker-engine-1-11-runc/>default container runtime is in the midst of refactoring its original monolithic architecture</a>.) The rkt design has from day one tried to apply the principle of modularity to the fullest, including reusing well-tested system components, rather than reimplementing them.</p><p>The task of building container images is abstracted away from the container runtime core in rkt, and implemented by an independent utility. The same approach is taken to ongoing container lifecycle management. A single binary, rkt, configures the environment and prepares container images for execution, then sets the container application and its isolation environment running. At this point, the rkt program has done its “one job”, and the container isolator takes over.</p><p>The API for querying container engine and pod state, used by Kubernetes to track cluster work on each node, is implemented in a separate service, isolating coordination and orchestration features from the core container runtime. While the API service does not fully implement all the API features of the current default container engine, it already helps isolate containers from failures and upgrades in the core runtime, and provides the read-only parts of the expected API for querying container metadata.</p><h4 id=modular-container-isolation-levels>Modular container isolation levels</h4><p>With rkt managing container execution, Kubernetes can take advantage of the CoreOS container engine’s modular <em>stage1</em> isolation mechanism. The typical container runs under rkt in a software-isolated environment constructed from Linux kernel namespaces, cgroups, and other facilities. Containers isolated in this common way nevertheless share a single kernel with all the other containers on a system, making for lightweight isolation of running apps.</p><p>However, rkt features pluggable isolation environments, referred to as stage1s, to change how containers are executed and isolated. For example, the <a href=https://coreos.com/rkt/docs/latest/running-fly-stage1.html>rkt fly stage1</a> runs containers in the host namespaces (PID, mount, network, etc), granting containers greater power on the host system. Fly is used for containerizing lower-level system and network software, like the kubelet itself. At the other end of the isolation spectrum, the <a href=https://coreos.com/rkt/docs/latest/running-lkvm-stage1.html>KVM stage1</a> runs standard app containers as individual virtual machines, each above its own Linux kernel, managed by the KVM hypervisor. This isolation level can be useful for high security and multi-tenant cluster workloads.</p><p><a href=https://1.bp.blogspot.com/-k3RRYf70fsg/V4a_-lVypxI/AAAAAAAAAl4/m9lVW0mxw7s35dzLlT4XJO5gdMzy_RBiQCLcB/s1600/rkt%2Bstages.png><img src=https://1.bp.blogspot.com/-k3RRYf70fsg/V4a_-lVypxI/AAAAAAAAAl4/m9lVW0mxw7s35dzLlT4XJO5gdMzy_RBiQCLcB/s640/rkt%2Bstages.png alt></a></p><p>Currently, rktnetes can use the KVM stage1 to execute all containers on a node with VM isolation by setting the kubelet’s --rkt-stage1-image option. Experimental work exists to choose the stage1 isolation regime on a per-pod basis with a Kubernetes annotation declaring the pod’s appropriate stage1. KVM containers and standard Linux containers can be mixed together in the same cluster.</p><h3 id=how-rkt-works-with-kubernetes>How rkt works with Kubernetes</h3><p>Kubernetes today talks to the default container engine over an API provided by the Docker daemon. rktnetes communicates with rkt a little bit differently. First, there is a distinction between how Kubernetes changes the state of a node’s containers – how it starts and stops pods, or reschedules them for failover or scaling – and how the orchestrator queries pod metadata for regular, read-only bookkeeping. Two different facilities implement these two different cases.</p><p><a href=https://3.bp.blogspot.com/-Agx6uMnddDc/V4bAA2YH_-I/AAAAAAAAAl8/PbKRFjVy0JMqyZ_OJ4oqMtGyTmlFTh0bQCEw/s1600/rktnetes%2B%25281%2529.png><img src=https://3.bp.blogspot.com/-Agx6uMnddDc/V4bAA2YH_-I/AAAAAAAAAl8/PbKRFjVy0JMqyZ_OJ4oqMtGyTmlFTh0bQCEw/s640/rktnetes%2B%25281%2529.png alt></a></p><h4 id=managing-microservice-lifecycles>Managing microservice lifecycles</h4><p>The kubelet on each cluster node communicates with rkt to <a href=https://coreos.com/rkt/docs/latest/subcommands/prepare.html>prepare</a> containers and their environments into pods, and with systemd, the linux service management framework, to invoke and manage the pod processes. Pods are then managed as systemd services, and the kubelet sends systemd commands over dbus to manipulate them. Lifecycle management, such as restarting failed pods and killing completed processes, is handled by systemd, at the kubelet’s behest.</p><h4 id=the-api-service-for-reading-pod-data>The API service for reading pod data</h4><p>A discrete <a href=https://coreos.com/rkt/docs/latest/subcommands/api-service.html>rkt api-service</a> implements the pod introspection mechanisms expected by Kubernetes. While each node’s kubelet uses systemd to start, stop, and restart pods as services, it contacts the API service to read container runtime metadata. This includes basic orchestration information such as the number of pods running on the node, the names and networks of those pods, and the details of pod configuration, resource limits and storage volumes (think of the information shown by the kubectl describe subcommand).</p><p>Pod logs, having been written to journal files, are made available for kubectl logs and other forensic subcommands by the API service as well, which reads from log files to provide pod log data to the kubelet for answering control plane requests.</p><p>This dual interface to the container environment is an area of very active development, and plans are for the API service to expand to provide methods for the pod manipulation commands. The underlying mechanism will continue to keep separation of concerns in mind, but will hide more of this from the kubelet. The methods the kubelet uses to control the rktnetes container engine will grow less different from the default container runtime interface over time.</p><h3 id=try-rktnetes>Try rktnetes</h3><p>So what can you do with rktnetes today? Currently, rktnetes passes all of <a href=http://storage.googleapis.com/kubernetes-test-history/static/suite-rktnetes:kubernetes-e2e-gce.html>the applicable Kubernetes “end-to-end” (aka “e2e”) tests</a>, provides standard metrics to cAdvisor, manages networks using <a href=https://github.com/containernetworking/cni>CNI</a>, handles per-container/pod logs, and automatically garbage collects old containers and images. Kubernetes running on rkt already provides more than the basics of a modular, flexible container runtime for Kubernetes clusters, and it is already a functional part of our development environment at CoreOS.</p><p>Developers and early adopters can follow the known issues in the <a href=/docs/getting-started-guides/rkt/notes/>rktnetes notes</a> to get an idea  of the wrinkles and bumps test-drivers can expect to encounter. This list groups the high-level pieces required to bring rktnetes to feature parity with the existing container runtime and API. We hope you’ll try out rktnetes in your Kubernetes clusters, too.</p><h4 id=use-rkt-with-kubernetes-today>Use rkt with Kubernetes Today</h4><p>The introductory guide <a href=/docs/getting-started-guides/rkt/><em>Running Kubernetes on rkt</em></a> walks through the steps to spin up a rktnetes cluster, from kubelet --container-runtime=rkt to networking and starting pods. This intro also sketches the configuration you’ll need to start a cluster on GCE with the Kubernetes kube-up.sh script.</p><p>Recent work aims to make rktnetes cluster creation much easier, too. While not yet merged, an <a href=https://github.com/coreos/coreos-kubernetes/pull/551>in-progress pull request creates a single rktnetes configuration toggle</a> to select rkt as the container engine when deploying a Kubernetes cluster with the <a href=https://github.com/coreos/coreos-kubernetes#kubernetes-on-coreos>coreos-kubernetes</a> configuration tools. You can also check out the <a href=https://github.com/coreos/rkt8s-workshop>rktnetes workshop project</a>, which launches a single-node rktnetes cluster on just about any developer workstation with one vagrant up command.</p><p>We’re excited to see the experiments the wider Kubernetes and CoreOS communities devise to put rktnetes to the test, and we welcome your input – and pull requests!</p><p><em>--Yifan Gu and Josh Wood, rktnetes Team, <a href=https://coreos.com/>CoreOS</a>. Twitter <a href=https://twitter.com/coreoslinux>@CoreOSLinux</a>.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-595bc7391d774dd446856ad8f4f770c2>Updates to Performance and Scalability in Kubernetes 1.3 -- 2,000 node 60,000 pod clusters</h1><div class="td-byline mb-4"><time datetime=2016-07-07 class=text-muted>Thursday, July 07, 2016</time></div><p>We are proud to announce that with the <a href=https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/>release of version 1.3</a>, Kubernetes now supports 2000-node clusters with even better end-to-end pod startup time. The latency of our API calls are within our one-second <a href=https://en.wikipedia.org/wiki/Service_level_objective>Service Level Objective (SLO)</a> and most of them are even an order of magnitude better than that. It is possible to run larger deployments than a 2,000 node cluster, but performance may be degraded and it may not meet our strict SLO.</p><p>In this blog post we discuss the detailed performance results from Kubernetes 1.3 and what changes we made from version 1.2 to achieve these results. We also describe Kubemark, a performance testing tool that we’ve integrated into our continuous testing framework to detect performance and scalability regressions.</p><p><strong>Evaluation Methodology</strong></p><p>We have described our test scenarios in a <a href=https://kubernetes.io/blog/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12>previous blog post</a>. The biggest change since the 1.2 release is that in our API responsiveness tests we now create and use multiple namespaces. In particular for the 2000-node/60000 pod cluster tests we create 8 namespaces. The change was done because we believe that users of such very large clusters are likely to use many namespaces, certainly at least 8 in the cluster in total.</p><p><strong>Metrics from Kubernetes 1.3</strong></p><p>So, what is the performance of Kubernetes version 1.3? The following graph shows the end-to-end pod startup latency with a 2000 and 1000 node cluster. For comparison we show the same metric from Kubernetes 1.2 with a 1000-node cluster.</p><p><img src=https://lh4.googleusercontent.com/muN6ySMhN7XhmNU_cuEu7CJbcnNuun_FeNidcvv1QVqtpWxTJUZVnKNDwXj9ttAsLBPDBlMi6l_-_sBxEWYvfK7SVp9bjxVa91VrR60v6Y8P8c5AQEl01Bt1cDTj4uVRPOUBn89e alt title=Wykres>
The next graphs show API response latency for a v1.3 2000-node cluster.</p><p><img src=https://lh5.googleusercontent.com/3wVH7grZXIlhtNNvzXXRMcqMtHhQUASnNSpu_EHOsQg4QrEAZvr_QeWmYWO0tLo3B-5uW1SThkod3eRauZcWprZn_Wlu14B1NSCRH3DI-IzqyLwC11IDfhNiskUqy4bOdHb9i1JY alt title=Wykres></p><p><img src=https://lh6.googleusercontent.com/U786KhDmaKjQPjPcN4bSLTgeAkdUp-X8sngo0pLVJzznb0ruo2elL10gjYnSaRq7EuCfvuJi-ab9PX0BloOArad-22uXVgPQ4kjq4cw2Zx1k0xsQl1FOLBPDbrRrMn9yX5NaEhap alt title=Wykres></p><p><strong>How did we achieve these improvements?</strong></p><p>The biggest change that we made for scalability in Kubernetes 1.3 was adding an efficient <a href=https://developers.google.com/protocol-buffers/>Protocol Buffer</a>-based serialization format to the API as an alternative to JSON. It is primarily intended for communication between Kubernetes control plane components, but all API server clients can use this format. All Kubernetes control plane components now use it for their communication, but the system continues to support JSON for backward compatibility.</p><p>We didn’t change the format in which we store cluster state in etcd to Protocol Buffers yet, as we’re still working on the upgrade mechanism. But we’re very close to having this ready, and we expect to switch the storage format to Protocol Buffers in Kubernetes 1.4. Our experiments show that this should reduce pod startup end-to-end latency by another 30%.</p><p><strong>How do we test Kubernetes at scale?</strong></p><p>Spawning clusters with 2000 nodes is expensive and time-consuming. While we need to do this at least once for each release to collect real-world performance and scalability data, we also need a lighter-weight mechanism that can allow us to quickly evaluate our ideas for different performance improvements, and that we can run continuously to detect performance regressions. To address this need we created a tool call “Kubemark.”</p><p><strong>What is “Kubemark”?</strong></p><p>Kubemark is a performance testing tool which allows users to run experiments on emulated clusters. We use it for measuring performance in large clusters.</p><p>A Kubemark cluster consists of two parts: a real master node running the normal master components, and a set of “hollow” nodes. The prefix “hollow” means an implementation/instantiation of a component with some “moving parts” mocked out. The best example is hollow-kubelet, which pretends to be an ordinary Kubelet, but doesn’t start any containers or mount any volumes. It just claims it does, so from master components’ perspective it behaves like a real Kubelet.</p><p>Since we want a Kubemark cluster to be as similar to a real cluster as possible, we use the real Kubelet code with an injected fake Docker client. Similarly hollow-proxy (KubeProxy equivalent) reuses the real KubeProxy code with injected no-op Proxier interface (to avoid mutating iptables).</p><p>Thanks to those changes</p><ul><li>many hollow-nodes can run on a single machine, because they are not modifying the environment in which they are running</li><li>without real containers running and the need for a container runtime (e.g. Docker), we can run up to 14 hollow-nodes on a 1-core machine.</li><li>yet hollow-nodes generate roughly the same load on the API server as their “whole” counterparts, so they provide a realistic load for performance testing [the only fundamental difference is that we are not simulating any errors that can happens in reality (e.g. failing containers) - adding support for this is a potential extension to the framework in the future]</li></ul><p><strong>How do we set up Kubemark clusters?</strong></p><p>To create a Kubemark cluster we use the power the Kubernetes itself gives us - we run Kubemark clusters on Kubernetes. Let’s describe this in detail.</p><p>In order to create a N-node Kubemark cluster, we:</p><ul><li>create a regular Kubernetes cluster where we can run N hollow-nodes [e.g. to create 2000-node Kubemark cluster, we create a regular Kubernetes cluster with 22 8-core nodes]</li><li>create a dedicated VM, where we start all master components for our Kubemark cluster (etcd, apiserver, controllers, scheduler, …). </li><li>schedule N “hollow-node” pods on the base Kubernetes cluster. Those hollow-nodes are configured to talk to the Kubemark API server running on the dedicated VM</li><li>finally, we create addon pods (currently just Heapster) by scheduling them on the base cluster and configuring them to talk to the Kubemark API server
Once this done, you have a usable Kubemark cluster that you can run your (performance) tests on. We have scripts for doing all of this on Google Compute Engine (GCE). For more details, take a look at our <a href=https://github.com/kubernetes/kubernetes/blob/release-1.3/docs/devel/kubemark-guide.md#starting-a-kubemark-cluster>guide</a>.</li></ul><p>One thing worth mentioning here is that while running Kubemark, underneath we’re also testing Kubernetes correctness. Obviously your Kubemark cluster will not work correctly if the base Kubernetes cluster under it doesn’t work. </p><p><strong>Performance measured in real clusters vs Kubemark</strong></p><p>Crucially, the performance of Kubemark clusters is mostly similar to the performance of real clusters. For the pod startup end-to-end latency, as shown in the graph below, the difference is negligible:</p><p><img src=https://lh6.googleusercontent.com/_pC-6DKVzZZoL7ek8sHhYqBi7Mmxw0aHU057RfYYam_qOIv0xtKc0dq6XfY9RXeoxMkLnYbg1RWwPAbwJEccAPIBEldwBMoFv8ZDcSiMFBhNuHxe9kSvN0UUHsVJTX4f7UH_APwi alt title=Wykres></p><p>For the API-responsiveness, the differences are higher, though generally less than 2x. However, trends are exactly the same: an improvement/regression in a real cluster is visible as a similar percentage drop/increase in metrics in Kubemark.</p><p><img src=https://lh3.googleusercontent.com/-2zrDvCks-LwStyskBlcIVUETPwEopcpvHGRxbaf0fIb0stsP-XuRo5PRs3dWO3qojcyf89QNzY5HIt5X0AuOKgMqOCl4r4gI2_h9cNre2RonNGyB8PvksBNOeONuwu6gXYGV4w- alt title=Wykres>
<strong>Conclusion</strong></p><p>We continue to improve the performance and scalability of Kubernetes. In this blog post we <br>showed that the 1.3 release scales to 2000 nodes while meeting our responsiveness SLOs<br>explained the major change we made to improve scalability from the 1.2 release, and <br>described Kubemark, our emulation framework that allows us to quickly evaluate the performance impact of code changes, both when experimenting with performance improvement ideas and to detect regressions as part of our continuous testing infrastructure.</p><p>Please join our community and help us build the future of Kubernetes! If you’re particularly interested in scalability, participate by:</p><ul><li>chatting with us on our <a href=https://kubernetes.slack.com/messages/sig-scale/>Slack channel</a></li><li>joining the scalability <a href=https://github.com/kubernetes/community/blob/master/README.md#special-interest-groups-sig>Special Interest Group</a>, which meets every Thursday at 9 AM Pacific Time on this <a href=https://plus.google.com/hangouts/_/google.com/k8scale-hangout>SIG-Scale Hangout</a></li></ul><p>For more information about the Kubernetes project, visit <a href=http://kubernetes.io/>kubernetes.io</a> and follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a>. </p><p><em>-- Wojciech Tyczynski, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-220eff9e67339951de66ffbac1eff1cd>Kubernetes 1.3: Bridging Cloud Native and Enterprise Workloads</h1><div class="td-byline mb-4"><time datetime=2016-07-06 class=text-muted>Wednesday, July 06, 2016</time></div><p>Nearly two years ago, when we officially kicked off the Kubernetes project, we wanted to simplify distributed systems management and provide the core technology required to everyone. The community’s response to this effort has blown us away. Today, thousands of customers, partners and developers are running clusters in production using Kubernetes and have joined the cloud native revolution. </p><p>Thanks to the help of over 800 contributors, we are pleased to announce today the availability of Kubernetes 1.3, our most robust and feature-rich release to date.</p><p>As our users scale their production deployments we’ve heard a clear desire to deploy services across cluster, zone and cloud boundaries. We’ve also heard a desire to run more workloads in containers, including stateful services. In this release, we’ve worked hard to address these two problems, while making it easier for new developers and enterprises to use Kubernetes to manage distributed systems at scale.</p><p>Product highlights in Kubernetes 1.3 include the ability to bridge services across multiple clouds (including on-prem), support for multiple node types, integrated support for stateful services (such as key-value stores and databases), and greatly simplified cluster setup and deployment on your laptop. Now, developers at organizations of all sizes can build production scale apps more easily than ever before.</p><p><strong>What’s new:</strong></p><ul><li><p><strong>Increased scale and automation</strong> - Customers want to scale their services up and down automatically in response to application demand. In 1.3 we have made it easier to autoscale clusters up and down while doubling the maximum number of nodes per cluster. Customers no longer need to think about cluster size, and can allow the underlying cluster to respond to demand.</p></li><li><p><strong>Cross-cluster federated services</strong> - Customers want their services to span one or more (possibly remote) clusters, and for them to be reachable in a consistent manner from both within and outside their clusters. Services that span clusters have higher availability, provide geographic distribution and enable hybrid and multi-cloud scenarios. Kubernetes 1.3 introduces cross-cluster service discovery so containers, and external clients can consistently resolve to services irrespective of whether they are running partially or completely in other clusters.</p></li><li><p><strong>Stateful applications</strong> - Customers looking to use containers for stateful workloads (such as databases or key value stores) will find a new ‘PetSet’ object with raft of alpha features, including:</p><ul><li>Permanent hostnames that persist across restarts</li><li>Automatically provisioned persistent disks per container that live beyond the life of a container</li><li>Unique identities in a group to allow for clustering and leader election</li><li>Initialization containers which are critical for starting up clustered applications</li></ul></li><li><p><strong>Ease of use for local development</strong> - Developers want an easy way to learn to use Kubernetes. In Kubernetes 1.3 we are introducing <a href=https://github.com/kubernetes/minikube>Minikube</a>, where with one command a developer can start a local Kubernetes cluster on their laptop that is API compatible with a full Kubernetes cluster. This enable developers to test locally, and push to their Kubernetes clusters when they are ready.</p></li><li><p><strong>Support for rkt and container standards OCI & CNI</strong> - Kubernetes is an extensible and modular orchestration platform. Part of what has made Kubernetes successful is our commitment to giving customers access to the latest container technologies that best suit their environment. In Kubernetes 1.3 we support emerging standards such as the Container Network Interface (<a href=https://github.com/containernetworking/cni>CNI</a>) natively, and have already taken steps to the Open Container Initiative (<a href=https://github.com/opencontainers>OCI</a>), which is still being ratified. We are also introducing <a href=https://github.com/coreos/rkt>rkt</a> as an alternative container runtime in Kubernetes node, with a first-class integration between rkt and the kubelet. This allows Kubernetes users to take advantage of some of rkt's unique features.</p></li><li><p><strong>Updated Kubernetes dashboard UI</strong> - Customers can now use the Kubernetes open source dashboard for the majority of interactions with their clusters, rather than having to use the CLI. The updated UI lets users control, edit and create all workload resources (including Deployments and PetSets).</p></li><li><p>And many more. For a complete list of updates, see the <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.3.0><em>release notes on GitHub</em></a>.</p></li></ul><p><strong>Community</strong></p><p>We could not have achieved this milestone without the tireless effort of countless people that are part of the Kubernetes community. We have <a href=https://github.com/kubernetes/community/blob/master/README.md#special-interest-groups-sig>19 different Special Interest Groups</a>, and over 100 meetups around the world. Kubernetes is a community project, built in the open, and it truly would not be possible without the over 233 person-years of effort the community has put in to date. Woot!</p><p><strong>Availability</strong></p><p>Kubernetes 1.3 is available for download at <a href=http://get.k8s.io/>get.k8s.io</a> and via the open source repository hosted on <a href=http://github.com/kubernetes/kubernetes>GitHub</a>. To get started with Kubernetes try our <a href=/docs/hellonode/>Hello World app</a>.</p><p>To learn the latest about the project, we encourage everyone to <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>join the weekly community meeting</a> or <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">watch a recorded hangout</a>. </p><p><strong>Connect</strong></p><p>We’d love to hear from you and see you participate in this growing community:</p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=https://stackoverflow.com/questions/tagged/kubernetes>Stackoverflow</a> </li><li>Connect with the community on <a href=http://slack.kubernetes.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates</li></ul><p>Thank you for your support! </p><p>-- Aparna Sinha, Product Manager, Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-139f28e64f23fe38bf154ae843d00991>Container Design Patterns</h1><div class="td-byline mb-4"><time datetime=2016-06-21 class=text-muted>Tuesday, June 21, 2016</time></div><p>Kubernetes automates deployment, operations, and scaling of applications, but our goals in the Kubernetes project extend beyond system management -- we want Kubernetes to help developers, too. Kubernetes should make it easy for them to write the distributed applications and services that run in cloud and datacenter environments. To enable this, Kubernetes defines not only an API for administrators to perform management actions, but also an API for containerized applications to interact with the management platform.</p><p>Our work on the latter is just beginning, but you can already see it manifested in a few features of Kubernetes. For example:</p><ul><li>The “<a href=/docs/api-reference/v1/definitions/#_v1_podspec>graceful termination</a>” mechanism provides a callback into the container a configurable amount of time before it is killed (due to a rolling update, node drain for maintenance, etc.). This allows the application to cleanly shut down, e.g. persist in-memory state and cleanly conclude open connections.</li><li><a href=/docs/user-guide/production-pods/#liveness-and-readiness-probes-aka-health-checks>Liveness and readiness probes</a> check a configurable application HTTP endpoint (other probe types are supported as well) to determine if the container is alive and/or ready to receive traffic. The response determines whether Kubernetes will restart the container, include it in the load-balancing pool for its Service, etc.</li><li><a href=/docs/user-guide/configmap/>ConfigMap</a> allows applications to read their configuration from a Kubernetes resource rather than using command-line flags.</li></ul><p>More generally, we see Kubernetes enabling a new generation of design patterns, similar to <a href=https://en.wikipedia.org/wiki/Object-oriented_programming#Design_patterns>object oriented design patterns</a>, but this time for containerized applications. That design patterns would emerge from containerized architectures is not surprising -- containers provide many of the same benefits as software objects, in terms of modularity/packaging, abstraction, and reuse. Even better, because containers generally interact with each other via HTTP and widely available data formats like JSON, the benefits can be provided in a language-independent way.</p><p>This week Kubernetes co-founder Brendan Burns is presenting a <a href=https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns><strong>paper</strong></a> outlining our thoughts on this topic at the <a href=https://www.usenix.org/conference/hotcloud16>8th Usenix Workshop on Hot Topics in Cloud Computing</a> (HotCloud ‘16), a venue where academic researchers and industry practitioners come together to discuss ideas at the forefront of research in private and public cloud technology. The paper describes three classes of patterns: management patterns (such as those described above), patterns involving multiple cooperating containers running on the same node, and patterns involving containers running across multiple nodes. We don’t want to spoil the fun of reading the paper, but we will say that you’ll see that the <a href=/docs/user-guide/pods/>Pod</a> abstraction is a key enabler for the last two types of patterns.</p><p>As the Kubernetes project continues to bring our decade of experience with <a href="https://queue.acm.org/detail.cfm?id=2898444">Borg</a> to the open source community, we aim not only to make application deployment and operations at scale simple and reliable, but also to make it easy to create “cloud-native” applications in the first place. Our work on documenting our ideas around design patterns for container-based services, and Kubernetes’s enabling of such patterns, is a first step in this direction. We look forward to working with the academic and practitioner communities to identify and codify additional patterns, with the aim of helping containers fulfill the promise of bringing increased simplicity and reliability to the entire software lifecycle, from development, to deployment, to operations.</p><p>To learn more about the Kubernetes project visit <a href=http://kubernetes.io/>kubernetes.io</a> or chat with us on Slack at <a href=http://slack.kubernetes.io/>slack.kubernetes.io</a>.</p><p>-<em>-Brendan Burns and David Oppenheimer, Software Engineers, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e713facfb8218a33c5a87cc80eaa761a>The Illustrated Children's Guide to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-06-09 class=text-muted>Thursday, June 09, 2016</time></div><p><em>Kubernetes is an open source project with a growing community. We love seeing the ways that our community innovates inside and on top of Kubernetes. Deis is an excellent example of company who understands the strategic impact of strong container orchestration. They contribute directly to the project; in associated subprojects; and, delightfully, with a creative endeavor to help our user community understand more about what Kubernetes is. Want to contribute to Kubernetes? One way is to get involved <a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Ahelp-wanted">here</a> and help us with code. But, please don’t consider that the only way to contribute. This little adventure that Deis takes us is an example of how open source isn’t only code. </em></p><p><em>Have your own Kubernetes story you’d like to tell, <a href=https://docs.google.com/a/google.com/forms/d/1cHiRdmBCEmUH9ekHY2G-KDySk5YXRzALHcMNgzwXtPM/viewform>let us know</a>!</em><br><em>-- @sarahnovotny Community Wonk, Kubernetes project.</em></p><p><em>Guest post is by Beau Vrolyk, CEO of Deis, the open source Kubernetes-native PaaS.</em></p><p>Over at <a href=https://deis.com/>Deis</a>, we’ve been busy building open source tools for Kubernetes. We’re just about to finish up moving our easy-to-use application platform to Kubernetes and couldn’t be happier with the results. In the Kubernetes project we’ve found not only a growing and vibrant community but also a well-architected system, informed by years of experience running containers at scale. </p><p>But that’s not all! As we’ve decomposed, ported, and reborn our PaaS as a Kubernetes citizen; we found a need for tools to help manage all of the ephemera that comes along with building and running Kubernetes-native applications. The result has been open sourced as <a href=https://github.com/kubernetes/helm>Helm</a> and we’re excited to see increasing adoption and growing excitement around the project.</p><p>There’s fun in the Deis offices too -- we like to add some character to our  architecture diagrams and pull requests. This time, literally. Meet Phippy--the intrepid little PHP app--and her journey to Kubernetes. What better way to talk to your parents, friends, and co-workers about this Kubernetes thing you keep going on about, than a little story time. We give to you The Illustrated Children's Guide to Kubernetes, conceived of and narrated by our own Matt Butcher and lovingly illustrated by Bailey Beougher. Join the fun on YouTube and tweet <a href=https://twitter.com/Opendeis>@opendeis</a> to win your own copy of the book or a squishy little Phippy of your own.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-34e25bc84f07afaa2906f5f35dd9f997>Bringing End-to-End Kubernetes Testing to Azure (Part 1)</h1><div class="td-byline mb-4"><time datetime=2016-06-06 class=text-muted>Monday, June 06, 2016</time></div><p><em>Today’s guest post is by Travis Newhouse, Chief Architect at AppFormix, writing about their experiences bringing Kubernetes to Azure.</em></p><p>At <a href=http://www.appformix.com/>AppFormix</a>, continuous integration testing is part of our culture. We see many benefits to running end-to-end tests regularly, including minimizing regressions and ensuring our software works together as a whole. To ensure a high quality experience for our customers, we require the ability to run end-to-end testing not just for our application, but for the entire orchestration stack. Our customers are adopting Kubernetes as their container orchestration technology of choice, and they demand choice when it comes to where their containers execute, from private infrastructure to public providers, including Azure. After several weeks of work, we are pleased to announce we are contributing a nightly, continuous integration job that executes e2e tests on the Azure platform. After running the e2e tests each night for only a few weeks, we have already found and fixed two issues in Kubernetes. We hope our contribution of an e2e job will help the community maintain support for the Azure platform as Kubernetes evolves.</p><p>In this blog post, we describe the journey we took to implement deployment scripts for the Azure platform. The deployment scripts are a prerequisite to the e2e test job we are contributing, as the scripts make it possible for our e2e test job to test the latest commits to the Kubernetes master branch. In a subsequent blog post, we will describe details of the e2e tests that will help maintain support for the Azure platform, and how to contribute federated e2e test results to the Kubernetes project.</p><p><strong>BACKGROUND</strong></p><p>While Kubernetes is designed to operate on any IaaS, and <a href=/docs/getting-started-guides/#table-of-solutions>solution guides</a> exist for many platforms including <a href=/docs/getting-started-guides/gce/>Google Compute Engine</a>, <a href=/docs/getting-started-guides/aws/>AWS</a>, <a href=/docs/getting-started-guides/coreos/azure/>Azure</a>, and <a href=/docs/getting-started-guides/rackspace/>Rackspace</a>, the Kubernetes project refers to these as “versioned distros,” as they are only tested against a particular binary release of Kubernetes. On the other hand, “development distros” are used daily by automated, e2e tests for the latest Kubernetes source code, and serve as gating checks to code submission.</p><p>When we first surveyed existing support for Kubernetes on Azure, we found documentation for running Kubernetes on Azure using CoreOS and Weave. The documentation includes <a href=/docs/getting-started-guides/coreos/azure/>scripts for deployment</a>, but the scripts do not conform to the cluster/kube-up.sh framework for automated cluster creation required by a “development distro.” Further, there did not exist a continuous integration job that utilized the scripts to validate Kubernetes using the end-to-end test scenarios (those found in test/e2e in the Kubernetes repository).</p><p>With some additional investigation into the project history (side note: git log --all --grep='azure' --oneline was quite helpful), we discovered that there previously existed a set of scripts that integrated with the cluster/kube-up.sh framework. These scripts were discarded on October 16, 2015 (<a href=https://github.com/kubernetes/kubernetes/pull/15790>commit 8e8437d</a>) because the scripts hadn’t worked since before Kubernetes version 1.0. With these commits as a starting point, we set out to bring the scripts up to date, and create a supported continuous integration job that will aid continued maintenance.</p><p><strong>CLUSTER DEPLOYMENT SCRIPTS</strong></p><p>To setup a Kubernetes cluster with Ubuntu VMs on Azure, we followed the groundwork laid by the previously abandoned commit, and tried to leverage the existing code as much as possible. The solution uses SaltStack for deployment and OpenVPN for networking between the master and the minions. SaltStack is also used for configuration management by several other solutions, such as AWS, GCE, Vagrant, and Vsphere. Resurrecting the discarded commit was a starting point, but we soon realized several key elements that needed attention:</p><ul><li>Install Docker and Kubernetes on the nodes using SaltStack</li><li>Configure authentication for services</li><li>Configure networking</li></ul><p>The cluster setup scripts ensure Docker is installed, copy the Kubernetes Docker images to the master and minions nodes, and load the images. On the master node, SaltStack launches kubelet, which in turn launches the following Kubernetes services running in containers: kube-api-server, kube-scheduler, and kube-controller-manager. On each of the minion nodes, SaltStack launches kubelet, which starts kube-proxy.</p><p>Kubernetes services must authenticate when communicating with each other. For example, minions register with the kube-api service on the master. On the master node, scripts generate a self-signed certificate and key that kube-api uses for TLS. Minions are configured to skip verification of the kube-api’s (self-signed) TLS certificate. We configure the services to use username and password credentials. The username and password are generated by the cluster setup scripts, and stored in the kubeconfig file on each node.</p><p>Finally, we implemented the networking configuration. To keep the scripts parameterized and minimize assumptions about the target environment, the scripts create a new Linux bridge device (cbr0), and ensure that all containers use that interface to access the network. To configure networking, we use OpenVPN to establish tunnels between master and minion nodes. For each minion, we reserve a /24 subnet to use for its pods. Azure assigned each node its own IP address. We also added the necessary routing table entries for this bridge to use OpenVPN interfaces. This is required to ensure pods in different hosts can communicate with each other. The routes on the master and minion are the following:</p><h6 id=master>master</h6><pre><code>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface

10.8.0.0        10.8.0.2        255.255.255.0   UG    0      0        0 tun0

10.8.0.2        0.0.0.0         255.255.255.255 UH    0      0        0 tun0

10.244.1.0      10.8.0.2        255.255.255.0   UG    0      0        0 tun0

10.244.2.0      10.8.0.2        255.255.255.0   UG    0      0        0 tun0

172.18.0.0      0.0.0.0         255.255.0.0     U     0      0        0 cbr0
</code></pre><h6 id=minion-1>minion-1</h6><pre><code>10.8.0.0        10.8.0.5        255.255.255.0   UG    0      0        0 tun0

10.8.0.5        0.0.0.0         255.255.255.255 UH    0      0        0 tun0

10.244.1.0      0.0.0.0         255.255.255.0   U     0      0        0 cbr0

10.244.2.0      10.8.0.5        255.255.255.0   UG    0      0        0 tun0
</code></pre><h6 id=minion-2>minion-2</h6><pre><code>10.8.0.0        10.8.0.9        255.255.255.0   UG    0      0        0 tun0

10.8.0.9        0.0.0.0         255.255.255.255 UH    0      0        0 tun0

10.244.1.0      10.8.0.9        255.255.255.0   UG    0      0        0 tun0

10.244.2.0      0.0.0.0         255.255.255.0   U     0      0        0 cbr0  
</code></pre><p><a href=https://3.bp.blogspot.com/-U2KYWNzJpFI/V3QMYbKRX8I/AAAAAAAAAks/SqEvCDJHJ8QtbB9hJVM8WAkFuAUlrFl8ACLcB/s1600/Kubernetes%2BBlog%2BPost%2B-%2BKubernetes%2Bon%2BAzure%2B%2528Part%2B1%2529.png><img src=https://3.bp.blogspot.com/-U2KYWNzJpFI/V3QMYbKRX8I/AAAAAAAAAks/SqEvCDJHJ8QtbB9hJVM8WAkFuAUlrFl8ACLcB/s400/Kubernetes%2BBlog%2BPost%2B-%2BKubernetes%2Bon%2BAzure%2B%2528Part%2B1%2529.png alt></a> |
| Figure 1 - OpenVPN network configuration |</p><p><strong>FUTURE WORK</strong> With the deployment scripts implemented, a subset of e2e test cases are passing on the Azure platform. Nightly results are published to the <a href=http://storage.googleapis.com/kubernetes-test-history/static/index.html>Kubernetes test history dashboard</a>. Weixu Zhuang made a <a href=https://github.com/kubernetes/kubernetes/pull/21207>pull request</a> on Kubernetes GitHub, and we are actively working with the Kubernetes community to merge the Azure cluster deployment scripts necessary for a nightly e2e test job. The deployment scripts provide a minimal working environment for Kubernetes on Azure. There are several next steps to continue the work, and we hope the community will get involved to achieve them.</p><ul><li>Only a subset of the e2e scenarios are passing because some cloud provider interfaces are not yet implemented for Azure, such as load balancer and instance information. To this end, we seek community input and help to define an Azure implementation of the cloudprovider interface (pkg/cloudprovider/). These interfaces will enable features such as Kubernetes pods being exposed to the external network and cluster DNS.</li><li>Azure has new APIs for interacting with the service. The submitted scripts currently use the Azure Service Management APIs, <a href=https://azure.microsoft.com/en-us/documentation/articles/azure-classic-rm/>which are deprecated</a>. The Azure Resource Manager APIs should be used in the deployment scripts.
The team at AppFormix is pleased to contribute support for Azure to the Kubernetes community. We look forward to feedback about how we can work together to improve Kubernetes on Azure.</li></ul><p><em>Editor's Note: Want to <em>contribute to</em> Kubernetes, get involved <a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Ahelp-wanted">here</a>. Have your own Kubernetes story you’d like to tell, <a href=https://docs.google.com/a/google.com/forms/d/1cHiRdmBCEmUH9ekHY2G-KDySk5YXRzALHcMNgzwXtPM/viewform>let us know</a>!</em></p><p>Part II is available <a href=https://kubernetes.io/blog/2016/07/bringing-end-to-end-kubernetes-testing-to-azure-2>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-66d9eafbf6937a1c1c0f4bbaf1d6edd4>Hypernetes: Bringing Security and Multi-tenancy to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-05-24 class=text-muted>Tuesday, May 24, 2016</time></div><p><em>Today’s guest post is written by Harry Zhang and Pengfei Ni, engineers at HyperHQ, describing a new hypervisor based container called HyperContainer</em></p><p>While many developers and security professionals are comfortable with Linux containers as an effective boundary, many users need a stronger degree of isolation, particularly for those running in a multi-tenant environment. Sadly, today, those users are forced to run their containers inside virtual machines, even one VM per container.</p><p>Unfortunately, this results in the loss of many of the benefits of a cloud-native deployment: slow startup time of VMs; a memory tax for every container; low utilization resulting in wasting resources.</p><p>In this post, we will introduce HyperContainer, a hypervisor based container and see how it naturally fits into the Kubernetes design, and enables users to serve their customers directly with virtualized containers, instead of wrapping them inside of full blown VMs.</p><p><strong>HyperContainer</strong></p><p><a href=http://hypercontainer.io/>HyperContainer</a> is a hypervisor-based container, which allows you to launch Docker images with standard hypervisors (KVM, Xen, etc.). As an open-source project, HyperContainer consists of an <a href=https://github.com/opencontainers/runtime-spec>OCI</a> compatible runtime implementation, named <a href=https://github.com/hyperhq/runv/>runV</a>, and a management daemon named <a href=https://github.com/hyperhq/hyperd>hyperd</a>. The idea behind HyperContainer is quite straightforward: to combine the best of both virtualization and container.</p><p>We can consider containers as two parts (as Kubernetes does). The first part is the container runtime, where HyperContainer uses virtualization to achieve execution isolation and resource limitation instead of namespaces and cgroups. The second part is the application data, where HyperContainer leverages Docker images. So in HyperContainer, virtualization technology makes it possible to build a fully isolated sandbox with an independent guest kernel (so things like <code>top</code> and /proc all work), but from developer’s view, it’s portable and behaves like a standard container.</p><p><strong>HyperContainer as Pod</strong></p><p>The interesting part of HyperContainer is not only that it is secure enough for multi-tenant environments (such as a public cloud), but also how well it fits into the Kubernetes philosophy.</p><p>One of the most important concepts in Kubernetes is Pods. The design of Pods is a lesson learned (<a href=http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf>Borg paper section 8.1</a>) from real world workloads, where in many cases people want an atomic scheduling unit composed of multiple containers (please check this <a href=https://github.com/kubernetes/examples/tree/master/staging/javaweb-tomcat-sidecar>example</a> for further information). In the context of Linux containers, a Pod wraps and encapsulates several containers into a logical group. But in HyperContainer, the hypervisor serves as a natural boundary, and Pods are introduced as first-class objects:</p><p><img src=https://lh6.googleusercontent.com/8DjNb9IE0HjinFxkaoGbPaaKbts5_Osbj-8NVWQMgY_8D32643Aum0SaMc2OedV2gECG3EXov8qj_f8XDe0IfpptZt61HxfJEonLo3RA5xkr5zSmd2nxqVc8yESc423nPEZTj1H3 alt></p><p>HyperContainer wraps a Pod of light-weight application containers and exposes the container interface at Pod level. Inside the Pod, a minimalist Linux kernel called HyperKernel is booted. This HyperKernel is built with a tiny Init service called HyperStart. It will act as the PID 1 process and creates the Pod, setup Mount namespace, and launch apps from the loaded images.</p><p>This model works nicely with Kubernetes. The integration of HyperContainer with Kubernetes, as we indicated in the title, is what makes up the <a href=https://github.com/hyperhq/hypernetes>Hypernetes</a> project.</p><p><strong>Hypernetes</strong></p><p>One of the best parts of Kubernetes is that it is designed to support multiple container runtimes, meaning users are not locked-in to a single vendor. We are very pleased to announce that we have already begun working with the Kubernetes team to integrate HyperContainer into Kubernetes upstream. This integration involves:</p><ol><li>container runtime optimizing and refactoring</li><li>new client-server mode runtime interface</li><li>containerd integration to support runV</li></ol><p>The OCI standard and kubelet’s multiple runtime architecture make this integration much easier even though HyperContainer is not based on Linux container technology stack.</p><p>On the other hand, in order to run HyperContainers in multi-tenant environment, we also created a new network plugin and modified an existing volume plugin. Since Hypernetes runs Pod as their own VMs, it can make use of your existing IaaS layer technologies for multi-tenant network and persistent volumes. The current Hypernetes implementation uses standard Openstack components.</p><p>Below we go into further details about how all those above are implemented.</p><p><strong>Identity and Authentication</strong></p><p>In Hypernetes we chose <a href=http://docs.openstack.org/developer/keystone/>Keystone</a> to manage different tenants and perform identification and authentication for tenants during any administrative operation. Since Keystone comes from the OpenStack ecosystem, it works seamlessly with the network and storage plugins we used in Hypernetes.</p><p><strong>Multi-tenant Network Model</strong></p><p>For a multi-tenant container cluster, each tenant needs to have strong network isolation from each other tenant. In Hypernetes, each tenant has its own Network. Instead of configuring a new network using OpenStack, which is complex, with Hypernetes, you just create a Network object like below.</p><pre><code>apiVersion: v1  
kind: Network  
metadata:  
  name: net1  
spec:  
  tenantID: 065f210a2ca9442aad898ab129426350  
  subnets:  
    subnet1:  
      cidr: 192.168.0.0/24  
      gateway: 192.168.0.1
</code></pre><p>Note that the tenantID is supplied by Keystone. This yaml will automatically create a new Neutron network with a default router and a subnet 192.168.0.0/24.</p><p>A Network controller will be responsible for the life-cycle management of any Network instance created by the user. This Network can be assigned to one or more Namespaces, and any Pods belonging to the same Network can reach each other directly through IP address.</p><pre><code>apiVersion: v1  
kind: Namespace  
metadata:  
  name: ns1  
spec:  
  network: net1
</code></pre><p>If a Namespace does not have a Network spec, it will use the default Kubernetes network model instead, including the default kube-proxy. So if a user creates a Pod in a Namespace with an associated Network, Hypernetes will follow the <a href=/docs/admin/network-plugins/>Kubernetes Network Plugin Model</a> to set up a Neutron network for this Pod. Here is a high level example:</p><p><img src=https://lh4.googleusercontent.com/ij88fHWT3wDSxDh4W7S0sARfjdRd5oTJTZGT_r8oQoqoGGjZWmHLJtPG8TT3U_tZ2rFqK7lwK56l3UIq3csSUxSdgGvfzORaAEAkl9fChxiLzVgz-mExTMi8sxUlfsesS59G0Fsa alt="A Hypernetes Network Workflow.png">{: HyperContainer wraps a Pod of li.big-img}</p><p>Hypernetes uses a standalone gRPC handler named kubestack to translate the Kubernetes Pod request into the Neutron network API. Moreover, kubestack is also responsible for handling another important networking feature: a multi-tenant Service proxy.</p><p>In a multi-tenant environment, the default iptables-based kube-proxy can not reach the individual Pods, because they are isolated into different networks. Instead, Hypernetes uses a <a href=https://github.com/hyperhq/hyperd/blob/2072dd8e28a02a25ae6a819f81029b47a579e683/servicediscovery/servicediscovery.go>built-in HAproxy in every HyperContainer</a> as the portal. This HAproxy will proxy all the Service instances in the namespace of that Pod. Kube-proxy will be responsible for updating these backend servers by following the standard OnServiceUpdate and OnEndpointsUpdate processes, so that users will not notice any difference. A downside of this method is that HAproxy has to listen to some specific ports which may conflicts with user’s containers.That’s why we are planning to use LVS to replace this proxy in the next release.</p><p>With the help of the Neutron based network plugin, the Hypernetes Service is able to provide an OpenStack load balancer, just like how the “external” load balancer does on GCE. When user creates a Service with external IPs, an OpenStack load balancer will be created and endpoints will be automatically updated through the kubestack workflow above.</p><p><strong>Persistent Storage</strong></p><p>When considering storage, we are actually building a tenant-aware persistent volume in Kubernetes. The reason we decided not to use existing Cinder volume plugin of Kubernetes is that its model does not work in the virtualization case. Specifically:</p><p>The Cinder volume plugin requires OpenStack as the Kubernetes provider.</p><p>The OpenStack provider will find on which VM the target Pod is running on</p><p>Cinder volume plugin will mount a Cinder volume to a path inside the host VM of Kubernetes.</p><p>The kubelet will bind mount this path as a volume into containers of target Pod.</p><p>But in Hypernetes, things become much simpler. Thanks to the physical boundary of Pods, HyperContainer can mount Cinder volumes directly as block devices into Pods, just like a normal VM. This mechanism eliminates extra time to query Nova to find out the VM of target Pod in the existing Cinder volume workflow listed above.</p><p>The current implementation of the Cinder plugin in Hypernetes is based on Ceph RBD backend, and it works the same as all other Kubernetes volume plugins, one just needs to remember to create the Cinder volume (referenced by volumeID below) beforehand.</p><pre><code>apiVersion: v1  
kind: Pod  
metadata:  
  name: nginx  
  labels:  
    app: nginx  
spec:  
  containers:  
  - name: nginx  
    image: nginx  
    ports:  
    - containerPort: 80  
    volumeMounts:  
    - name: nginx-persistent-storage  
      mountPath: /var/lib/nginx  
  volumes:  
  - name: nginx-persistent-storage  
    cinder:  
      volumeID: 651b2a7b-683e-47e1-bdd6-e3c62e8f91c0  
      fsType: ext4
</code></pre><p>So when the user provides a Pod yaml with a Cinder volume, Hypernetes will check if kubelet is using the Hyper container runtime. If so, the Cinder volume can be mounted directly to the Pod without any extra path mapping. Then the volume metadata will be passed to the Kubelet RunPod process as part of HyperContainer spec. Done!</p><p>Thanks to the plugin model of Kubernetes network and volume, we can easily build our own solutions above for HyperContainer though it is essentially different from the traditional Linux container. We also plan to propose these solutions to Kubernetes upstream by following the CNI model and volume plugin standard after the runtime integration is completed.</p><p>We believe all of these <a href=https://github.com/hyperhq/>open source projects</a> are important components of the container ecosystem, and their growth depends greatly on the open source spirit and technical vision of the Kubernetes team.</p><p><strong>Conclusion</strong></p><p>This post introduces some of the technical details about HyperContainer and the Hypernetes project. We hope that people will be interested in this new category of secure container and its integration with Kubernetes. If you are looking to try out Hypernetes and HyperContainer, we have just announced the public beta of our new secure container cloud service (<a href=https://hyper.sh/>Hyper_</a>), which is built on these technologies. But even if you are running on-premise, we believe that Hypernetes and HyperContainer will let you run Kubernetes in a more secure way.</p><p><em>~Harry Zhang and Pengfei Ni, engineers at HyperHQ</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-dfd7c2731ea9e6f52443db7ecb32aa82>CoreOS Fest 2016: CoreOS and Kubernetes Community meet in Berlin (& San Francisco)</h1><div class="td-byline mb-4"><time datetime=2016-05-03 class=text-muted>Tuesday, May 03, 2016</time></div><p><a href=https://coreos.com/fest/>CoreOS Fest 2016</a> will bring together the container and open source distributed systems community, including many thought leaders in the Kubernetes space. It is the second annual CoreOS community conference, held for the first time in Berlin on May 9th and 10th. CoreOS believes Kubernetes is the container orchestration component to deliver GIFEE (Google’s Infrastructure for Everyone Else).</p><p>At this year’s CoreOS Fest, there are tracks dedicated to Kubernetes where you’ll hear about various topics ranging from Kubernetes performance and scalability, continuous delivery and Kubernetes, rktnetes, stackanetes and more. In addition, there will be a variety of talks, from introductory workshops to deep-dives into all things containers and related software.</p><p>Don’t miss these great speaker sessions at the conference in <strong>Berlin</strong> :</p><ul><li><a href="https://coreosfest2016.sched.org/event/6ckp/kubernetes-performance-scalability-deep-dive?iframe=no&w=i:100;&sidebar=yes&bg=no">Kubernetes Performance & Scalability Deep-Dive</a> by Filip Grzadkowski, Senior Software Engineer at Google</li><li><a href=http://coreosfest2016.sched.org/event/6T0b/launching-a-complex-application-in-a-kubernetes-cloud>Launching a complex application in a Kubernetes cloud</a> by Thomas Fricke and Jannis Rake-Revelant, Operations & Infrastructure Lead, immmr Gmbh (a service developed by the Deutsche Telekom’s R&D department)</li><li><a href="https://coreosfest2016.sched.org/event/6db3/i-have-kubernetes-now-what?iframe=no&w=i:100;&sidebar=yes&bg=no">I have Kubernetes, now what?</a> by Gabriel Monroy, CTO of Engine Yard and creator of Deis</li><li><a href="https://coreosfest2016.sched.org/event/6YGg/when-rkt-meets-kubernetes-a-troubleshooting-tale?iframe=no&w=i:100;&sidebar=yes&bg=no">When rkt meets Kubernetes: a troubleshooting tale</a> by Luca Marturana, Software Engineer at Sysdig</li><li><a href="https://coreosfest2016.sched.org/event/6eSE/use-kubernetes-to-deploy-telecom-applications?iframe=no&w=i:100;&sidebar=yes&bg=no">Use Kubernetes to deploy telecom applications</a> by Victor Hu, Senior Engineer at Huawei Technologies</li><li><a href="https://coreosfest2016.sched.org/event/6qCs/continuous-delivery-kubernetes-and-you?iframe=no&w=i:100;&sidebar=yes&bg=no">Continuous Delivery, Kubernetes and You</a> by Micha Hernandez van Leuffen, CEO and founder of Wercker</li><li><a href="https://coreosfest2016.sched.org/event/6YJl/gifee-more-containers-more-problems?iframe=no&w=i:100;&sidebar=yes&bg=no">#GIFEE, More Containers, More Problems</a> by Ed Rooth, Head of Tectonic at CoreOS</li><li><a href="https://coreosfest2016.sched.org/event/6YH4/kubernetes-access-control-with-dex?iframe=no&w=i:100;&sidebar=yes&bg=no">Kubernetes Access Control with dex</a> by Eric Chiang, Software Engineer at CoreOS</li></ul><p>If you can’t make it to Berlin, Kubernetes is also a focal point at the <strong>CoreOS Fest <a href=https://www.eventbrite.com/e/coreos-fest-san-francisco-satellite-event-tickets-22705108591>San Francisco</a></strong><a href=https://www.eventbrite.com/e/coreos-fest-san-francisco-satellite-event-tickets-22705108591><strong>satellite event</strong></a>, a one day event dedicated to CoreOS and Kubernetes. In fact, Tim Hockin, senior staff engineer at Google and one of the creators of Kubernetes, will be kicking off the day with a keynote dedicated to Kubernetes updates.</p><p><strong>San Francisco</strong> sessions dedicated to Kubernetes include:</p><ul><li>Tim Hockin’s keynote address, Senior Staff Engineer at Google</li><li>When rkt meets Kubernetes: a troubleshooting tale by Loris Degioanni, CEO of Sysdig</li><li>rktnetes: what's new with container runtimes and Kubernetes by Derek Gonyeo, Software Engineer at CoreOS</li><li>Magical Security Sprinkles: Secure, Resilient Microservices on CoreOS and Kubernetes by Oliver Gould, CTO of Buoyant</li></ul><p><strong>Kubernetes Workshop in SF</strong> : <a href=https://www.eventbrite.com/e/getting-started-with-kubernetes-tickets-25180552711>Getting Started with Kubernetes</a>, hosted at Google San Francisco office (345 Spear St - 7th floor) by Google Developer Program Engineers Carter Morgan and Bill Prin on Tuesday May 10th from 9:00am to 1:00pm, lunch will be served afterwards. Limited seats, please <a href=https://www.eventbrite.com/e/getting-started-with-kubernetes-tickets-25180552711>RSVP for free here</a>.</p><p><strong>Get your tickets</strong> :</p><ul><li><a href=https://ti.to/coreos/coreos-fest-2016/en>CoreOS Fest - Berlin</a>, at the <a href="https://www.google.com/maps/place/bcc+Berlin+Congress+Center+GmbH/@52.5206732,13.4165195,15z/data=!4m2!3m1!1s0x0:0xd2a15220241f2080">Berlin Congress Center</a> (<a href=http://www.parkinn-berlin.de/>hotel option</a>)</li><li>satellite event in <a href=https://www.eventbrite.com/e/coreos-fest-san-francisco-satellite-event-tickets-22705108591>San Francisco</a>, at the <a href="https://www.google.com/maps/place/111+Minna+Gallery/@37.7873222,-122.3994124,15z/data=!4m2!3m1!1s0x0:0xb55875af8c0ca88b?sa=X&ved=0ahUKEwjZ8cPLtL7MAhVQ5GMKHa8bCM4Q_BIIdjAN">111 Minna Gallery</a></li></ul><p>Learn more at: <a href=https://coreos.com/fest/>coreos.com/fest/</a> and on Twitter <a href=https://twitter.com/coreosfest>@CoreOSFest</a> #CoreOSFest</p><p><em>-- Sarah Novotny, Kubernetes Community Manager</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a3e6efc3369171db872d37129309ee87>Introducing the Kubernetes OpenStack Special Interest Group</h1><div class="td-byline mb-4"><time datetime=2016-04-22 class=text-muted>Friday, April 22, 2016</time></div><p><em>Editor’s note: This week we’re featuring <a href=https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)>Kubernetes Special Interest Groups</a>; Today’s post is by the SIG-OpenStack team about their mission to facilitate ideas between the OpenStack and Kubernetes communities. </em></p><p>The community around the Kubernetes project includes a number of Special Interest Groups (SIGs) for the purposes of facilitating focused discussions relating to important subtopics between interested contributors. Today we would like to highlight the <a href=https://github.com/kubernetes/kubernetes/wiki/SIG-Openstack>Kubernetes OpenStack SIG</a> focused on the interaction between <a href=http://kubernetes.io/>Kubernetes</a> and <a href=http://www.openstack.org/>OpenStack</a>, the Open Source cloud computing platform.</p><p>There are two high level scenarios that are being discussed in the SIG:</p><ul><li>Using Kubernetes to manage containerized workloads running on top of OpenStack</li><li>Using Kubernetes to manage containerized OpenStack services themselves</li></ul><p>In both cases the intent is to help facilitate the inter-pollination of ideas between the growing Kubernetes and OpenStack communities. The OpenStack community itself includes a number of projects broadly aimed at assisting with both of these use cases including:</p><ul><li><a href=http://governance.openstack.org/reference/projects/kolla.html>Kolla</a> - Provides OpenStack service containers and deployment tooling for operating OpenStack clouds.</li><li><a href=http://governance.openstack.org/reference/projects/kuryr.html>Kuryr</a> - Provides bridges between container networking/storage framework models and OpenStack infrastructure services.</li><li><a href=http://governance.openstack.org/reference/projects/magnum.html>Magnum</a> - Provides containers as a service for OpenStack.</li><li><a href=http://governance.openstack.org/reference/projects/murano.html>Murano</a> - Provides an Application Catalog service for OpenStack including support for Kubernetes itself, and for containerized applications, managed by Kubernetes.</li></ul><p>There are also a number of example templates available to assist with using the OpenStack Orchestration service (<a href=http://governance.openstack.org/reference/projects/heat.html>Heat</a>) to deploy and configure either Kubernetes itself or offerings built around Kubernetes such as <a href=https://github.com/redhat-openstack/openshift-on-openstack/>OpenShift</a>. While each of these approaches has their own pros and cons the common theme is the ability, or potential ability, to use Kubernetes and where available leverage deeper integration between it and the OpenStack services themselves. </p><p>Current SIG participants represent a broad array of organizations including but not limited to: CoreOS, eBay, GoDaddy, Google, IBM, Intel, Mirantis, OpenStack Foundation, Rackspace, Red Hat, Romana, Solinea, VMware. </p><p>The SIG is currently working on <a href="https://docs.google.com/document/d/1wNl_xcITKwzUsFNRu5npUTJuh9pbJAdzzpG6Cd2Fcp0/edit?ts=57033dd6">collating information</a> about these approaches to help Kubernetes users navigate the OpenStack ecosystem along with feedback on which approaches to the requirements presented work best for operators. </p><p><strong>Kubernetes at OpenStack Summit Austin</strong></p><p>The <a href=https://www.openstack.org/summit/austin-2016/>OpenStack Summit</a> is in Austin from April 25th to 29th and is packed with sessions related to containers and container management using Kubernetes. If you plan on joining us in Austin you can review the <a href=https://www.openstack.org/summit/austin-2016/summit-schedule/>schedule</a> online where you will find a number of sessions, both in the form of presentations and hands on workshops, relating to <a href="https://www.openstack.org/summit/austin-2016/summit-schedule/global-search?t=Kubernetes">Kubernetes</a> and <a href="https://www.openstack.org/summit/austin-2016/summit-schedule/global-search?t=containers">containerization</a> at large. Folks from the Kubernetes OpenStack SIG are particularly keen to get the thoughts of operators in the “<a href=https://www.openstack.org/summit/austin-2016/summit-schedule/events/9500>Ops: Containers on OpenStack</a>” and “<a href=https://www.openstack.org/summit/austin-2016/summit-schedule/events/9501>Ops: OpenStack in Containers</a>” working sessions.</p><p>Kubernetes community experts will also be on hand in the Container Expert Lounge to answer your burning questions. You can find the lounge on the 4th floor of the Austin Convention Center.</p><p>Follow <a href=https://twitter.com/kubernetesio>@kubernetesio</a> and <a href="https://twitter.com/search?q=%23openstacksummit">#OpenStackSummit</a> to keep up with the latest updates on Kubernetes at OpenStack Summit throughout the week.</p><p><strong>Connect With Us</strong></p><p>If you’re interested in Kubernetes and OpenStack, there are several ways to participate:</p><ul><li>Email us at the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-openstack>SIG-OpenStack mailing list</a></li><li>Chat with us on the <a href=http://slack.k8s.io/>Kubernetes Slack</a>: <a href=https://kubernetes.slack.com/messages/sig-openstack/>#sig-openstack channel</a> and #openstack-kubernetes on freenode</li><li>Join our meeting occurring every second Tuesday at 2 PM PDT; attend via the zoom videoconference found in our <a href=https://docs.google.com/document/d/1iAQ3LSF_Ky6uZdFtEZPD_8i6HXeFxIeW4XtGcUJtPyU/edit#>meeting notes</a>.</li></ul><p><em>-- Steve Gordon, Principal Product Manager at Red Hat, and Ihor Dvoretskyi, OpenStack Operations Engineer at Mirantis</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-07647e8992c66dab6b47a8c680bb0ecd>SIG-UI: the place for building awesome user interfaces for Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-04-20 class=text-muted>Wednesday, April 20, 2016</time></div><p><em>Editor’s note: This week we’re featuring <a href=https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)>Kubernetes Special Interest Groups</a>; Today’s post is by the SIG-UI team describing their mission and showing the cool projects they work on.</em></p><p>Kubernetes has been handling production workloads for a long time now (see <a href=http://kubernetes.io/#talkToUs>case studies</a>). It runs on public, private and hybrid clouds as well as bare metal. It can handle all types of workloads (web serving, batch and mixed) and enable <a href="https://www.youtube.com/watch?v=9C6YeyyUUmI">zero-downtime rolling updates</a>. It abstracts service discovery, load balancing and storage so that applications running on Kubernetes aren’t restricted to a specific cloud provider or environment.</p><p>The abundance of features that Kubernetes offers is fantastic, but implementing a user-friendly, easy-to-use user interface is quite challenging. How shall all the features be presented to users? How can we gradually expose the Kubernetes concepts to newcomers, while empowering experts? There are lots of other challenges like these that we’d like to solve. This is why we created a special interest group for Kubernetes user interfaces.</p><p><strong>Meet SIG-UI: the place for building awesome user interfaces for Kubernetes</strong><br>The SIG UI mission is simple: we want to radically improve the user experience of all Kubernetes graphical user interfaces. Our goal is to craft UIs that are used by devs, ops and resource managers across their various environments, that are simultaneously intuitive enough for newcomers to Kubernetes to understand and use.</p><p>SIG UI members have been independently working on a variety of UIs for Kubernetes. So far, the projects we’ve seen have been either custom internal tools coupled to their company workflows, or specialized API frontends. We have realized that there is a need for a universal UI that can be used standalone or be a standard base for custom vendors. That’s how we started the <a href=http://github.com/kubernetes/dashboard>Dashboard UI</a> project. Version 1.0 has been recently released and is included with Kubernetes as a cluster addon. The Dashboard project was recently featured in a <a href="https://www.youtube.com/watch?v=sARH5zQhovE">talk at KubeCon EU</a>, and we have ambitious plans for the future!</p><p>| <img src=https://lh4.googleusercontent.com/jsHjTjFstXaq17Axu0xduW6Dd5g3EkEUmtStNsPmhvw5pxGuYxnhSRSkspHnpExKd0lBnhkD_F58sM7DVfjlYsGZLOYcKJghhK0cTxAdgk2Cun02RY-hSuUztugHJG8MmTmH8OPM alt> |
| Dashboard UI v1.0 home screen showing applications running in a Kubernetes cluster. |</p><p>Since the initial release of the Dashboard UI we have been thinking hard about what to do next and what users of UIs for Kubernetes think about our plans. We’ve had many internal discussions on this topic, but most importantly, reached out directly to our users. We created a questionnaire asking a few demographic questions as well as questions for prioritizing use cases. We received more than 200 responses from a wide spectrum of user types, which in turn helped to shape the Dashboard UI’s <a href=https://github.com/kubernetes/dashboard/blob/master/docs/devel/roadmap.md>current roadmap</a>. Our members from LiveWyer summarised the results in a <a href=http://static.lwy.io/img/kubernetes_dashboard_infographic.png>nice infographic</a>. </p><p><strong>Connect with us</strong></p><p>We believe that collaboration is the key to SIG UI success, so we invite everyone to connect with us. Whether you’re a Kubernetes user who wants to provide feedback, develop your own UIs, or simply want to collaborate on the Dashboard UI project, feel free to get in touch. There are many ways you can contact us:</p><ul><li>Email us at the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-ui>sig-ui mailing list</a></li><li>Chat with us on the <a href=http://slack.k8s.io/>Kubernetes Slack</a>: #<a href=https://kubernetes.slack.com/messages/sig-ui/>sig-ui channel</a></li><li>Join our meetings: biweekly on Wednesdays 9AM PT (US friendly) and weekly 10AM CET (Europe friendly). See the <a href="https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&ctz=Europe/Warsaw">SIG-UI calendar</a> for details. </li></ul><p><em>-- Piotr Bryk, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f5bc74c369fabdc9aabeb7ba9c4f3d42>SIG-ClusterOps: Promote operability and interoperability of Kubernetes clusters</h1><div class="td-byline mb-4"><time datetime=2016-04-19 class=text-muted>Tuesday, April 19, 2016</time></div><p><em>Editor’s note: This week we’re featuring <a href=https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)>Kubernetes Special Interest Groups</a>; Today’s post is by the SIG-ClusterOps team whose mission is to promote operability and interoperability of Kubernetes clusters -- to listen, help & escalate.</em></p><p>We think Kubernetes is an awesome way to run applications at scale! Unfortunately, there's a bootstrapping problem: we need good ways to build secure & reliable scale environments around Kubernetes. While some parts of the platform administration leverage the platform (cool!), there are fundamental operational topics that need to be addressed and questions (like upgrade and conformance) that need to be answered.</p><p><strong>Enter Cluster Ops SIG – the community members who work under the platform to keep it running.</strong></p><p>Our objective for Cluster Ops is to be a person-to-person community first, and a source of opinions, documentation, tests and scripts second. That means we dedicate significant time and attention to simply comparing notes about what is working and discussing real operations. Those interactions give us data to form opinions. It also means we can use real-world experiences to inform the project.</p><p>We aim to become the forum for operational review and feedback about the project. For Kubernetes to succeed, operators need to have a significant voice in the project by weekly participation and collecting survey data. We're not trying to create a single opinion about ops, but we do want to create a coordinated resource for collecting operational feedback for the project. As a single recognized group, operators are more accessible and have a bigger impact.</p><p><strong>What about real world deliverables?</strong></p><p>We've got plans for tangible results too. We’re already driving toward concrete deliverables like reference architectures, tool catalogs, community deployment notes and conformance testing. Cluster Ops wants to become the clearing house for operational resources. We're going to do it based on real world experience and battle tested deployments.</p><p><strong>Connect with us.</strong></p><p>Cluster Ops can be hard work – don't do it alone. We're here to listen, to help when we can and escalate when we can't. Join the conversation at:</p><ul><li>Chat with us on the <a href=https://kubernetes.slack.com/messages/sig-cluster-ops/>Cluster Ops Slack channel</a></li><li>Email us at the <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-ops>Cluster Ops SIG email list</a></li></ul><p>The Cluster Ops Special Interest Group meets weekly at 13:00PT on Thursdays, you can join us via the <a href=https://plus.google.com/hangouts/_/google.com/sig-cluster-ops>video hangout</a> and see latest <a href=https://docs.google.com/document/d/1IhN5v6MjcAUrvLd9dAWtKcGWBWSaRU8DNyPiof3gYMY/edit>meeting notes</a> for agendas and topics covered.</p><p><em>--Rob Hirschfeld, CEO, RackN </em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d3f23f912c594dbbccd2c55d5b380e84>SIG-Networking: Kubernetes Network Policy APIs Coming in 1.3</h1><div class="td-byline mb-4"><time datetime=2016-04-18 class=text-muted>Monday, April 18, 2016</time></div><p><em>Editor’s note: This week we’re featuring <a href=https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)>Kubernetes Special Interest Groups</a>; Today’s post is by the Network-SIG team describing network policy APIs coming in 1.3 - policies for security, isolation and multi-tenancy.</em></p><p>The <a href=https://kubernetes.slack.com/messages/sig-network/>Kubernetes network SIG</a> has been meeting regularly since late last year to work on bringing network policy to Kubernetes and we’re starting to see the results of this effort.</p><p>One problem many users have is that the open access network policy of Kubernetes is not suitable for applications that need more precise control over the traffic that accesses a pod or service. Today, this could be a multi-tier application where traffic is only allowed from a tier’s neighbor. But as new Cloud Native applications are built by composing microservices, the ability to control traffic as it flows among these services becomes even more critical.</p><p>In most IaaS environments (both public and private) this kind of control is provided by allowing VMs to join a ‘security group’ where traffic to members of the group is defined by a network policy or Access Control List (ACL) and enforced by a network packet filter.</p><p>The Network SIG started the effort by identifying <a href="https://docs.google.com/document/d/1blfqiH4L_fpn33ZrnQ11v7LcYP0lmpiJ_RaapAPBbNU/edit?pref=2&pli=1#">specific use case scenarios</a> that require basic network isolation for enhanced security. Getting the API right for these simple and common use cases is important because they are also the basis for the more sophisticated network policies necessary for multi-tenancy within Kubernetes.</p><p>From these scenarios several possible approaches were considered and a minimal <a href=https://docs.google.com/document/d/1qAm-_oSap-f1d6a-xRTj6xaH1sYQBfK36VyjB5XOZug/edit>policy specification</a> was defined. The basic idea is that if isolation were enabled on a per namespace basis, then specific pods would be selected where specific traffic types would be allowed.</p><p>The simplest way to quickly support this experimental API is in the form of a ThirdPartyResource extension to the API Server, which is possible today in Kubernetes 1.2.</p><p>If you’re not familiar with how this works, the Kubernetes API can be extended by defining ThirdPartyResources that create a new API endpoint at a specified URL.</p><h4 id=third-party-res-def-yaml>third-party-res-def.yaml</h4><pre><code>kind: ThirdPartyResource

apiVersion: extensions/v1beta1

metadata:

 &amp;nbsp;name: network-policy.net.alpha.kubernetes.io

description: &quot;Network policy specification&quot;

versions:

- name: v1alpha1
</code></pre><pre><code>$kubectl create -f third-party-res-def.yaml
</code></pre><p>This will create an API endpoint (one for each namespace):</p><pre><code>/net.alpha.kubernetes.io/v1alpha1/namespace/default/networkpolicys/
</code></pre><p>Third party network controllers can now listen on these endpoints and react as necessary when resources are created, modified or deleted. <em>Note: With the upcoming release of Kubernetes 1.3 - when the Network Policy API is released in beta form - there will be no need to create a ThirdPartyResource API endpoint as shown above.</em> </p><p>Network isolation is off by default so that all pods can communicate as they normally do. However, it’s important to know that once network isolation is enabled, all traffic to all pods, in all namespaces is blocked, which means that enabling isolation is going to change the behavior of your pods</p><p>Network isolation is enabled by defining the <em>network-isolation</em> annotation on namespaces as shown below:</p><pre><code>net.alpha.kubernetes.io/network-isolation: [on | off]
</code></pre><p>Once network isolation is enabled, explicit network policies <strong>must be applied</strong> to enable pod communication.</p><p>A policy specification can be applied to a namespace to define the details of the policy as shown below:</p><pre><code>POST /apis/net.alpha.kubernetes.io/v1alpha1/namespaces/tenant-a/networkpolicys/


{

&quot;kind&quot;: &quot;NetworkPolicy&quot;,

&quot;metadata&quot;: {

&quot;name&quot;: &quot;pol1&quot;

},

&quot;spec&quot;: {

&quot;allowIncoming&quot;: {

&quot;from&quot;: [

{ &quot;pods&quot;: { &quot;segment&quot;: &quot;frontend&quot; } }

],

&quot;toPorts&quot;: [

{ &quot;port&quot;: 80, &quot;protocol&quot;: &quot;TCP&quot; }

]

},

&quot;podSelector&quot;: { &quot;segment&quot;: &quot;backend&quot; }

}

}
</code></pre><p>In this example, the ‘ <strong>tenant-a</strong> ’ namespace would get policy ‘ <strong>pol1</strong> ’ applied as indicated. Specifically, pods with the <strong>segment</strong> label ‘ <strong>backend</strong> ’ would allow TCP traffic on port 80 from pods with the <strong>segment</strong> label ‘ <strong>frontend</strong> ’ to be received.</p><p>Today, <a href=http://romana.io/>Romana</a>, <a href=https://www.openshift.com/>OpenShift</a>, <a href=http://www.opencontrail.org/>OpenContrail</a> and <a href=http://projectcalico.org/>Calico</a> support network policies applied to namespaces and pods. Cisco and VMware are working on implementations as well. Both Romana and Calico demonstrated these capabilities with Kubernetes 1.2 recently at KubeCon. You can watch their presentations here: <a href="https://www.youtube.com/watch?v=f-dLKtK6qCs">Romana</a> (<a href=http://www.slideshare.net/RomanaProject/kubecon-london-2016-ronana-cloud-native-sdn>slides</a>), <a href="https://www.youtube.com/watch?v=p1zfh4N4SX0">Calico</a> (<a href=http://www.slideshare.net/kubecon/kubecon-eu-2016-secure-cloudnative-networking-with-project-calico>slides</a>). </p><p><strong>How does it work?</strong></p><p>Each solution has their their own specific implementation details. Today, they rely on some kind of on-host enforcement mechanism, but future implementations could also be built that apply policy on a hypervisor, or even directly by the network itself. </p><p>External policy control software (specifics vary across implementations) will watch the new API endpoint for pods being created and/or new policies being applied. When an event occurs that requires policy configuration, the listener will recognize the change and a controller will respond by configuring the interface and applying the policy.  The diagram below shows an API listener and policy controller responding to updates by applying a network policy locally via a host agent. The network interface on the pods is configured by a CNI plugin on the host (not shown).</p><p><img src=https://lh5.googleusercontent.com/zMEpLMYmask-B-rYWnbMyGb0M7YusPQFPS6EfpNOSLbkf-cM49V7rTDBpA6k9-Zdh2soMul39rz9rHFJfL-jnEn_mHbpg0E1WlM-wjU-qvQu9KDTQqQ9uBmdaeWynDDNhcT3UjX5 alt=controller.jpg></p><p>If you’ve been holding back on developing applications with Kubernetes because of network isolation and/or security concerns, these new network policies go a long way to providing the control you need. No need to wait until Kubernetes 1.3 since network policy is available now as an experimental API enabled as a ThirdPartyResource.</p><p>If you’re interested in Kubernetes and networking, there are several ways to participate - join us at:</p><ul><li>Our <a href=https://kubernetes.slack.com/messages/sig-network/>Networking slack channel</a> </li><li>Our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-network>Kubernetes Networking Special Interest Group</a> email list </li></ul><p>The Networking “Special Interest Group,” which meets bi-weekly at 3pm (15h00) Pacific Time at <a href=https://zoom.us/j/5806599998>SIG-Networking hangout</a>. </p><p><em>--Chris Marino, Co-Founder, Pani Networks</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4a815a0bb19673c92b920bef9ec0c84c>How to deploy secure, auditable, and reproducible Kubernetes clusters on AWS</h1><div class="td-byline mb-4"><time datetime=2016-04-15 class=text-muted>Friday, April 15, 2016</time></div><p><em>Today’s guest post is written by Colin Hom, infrastructure engineer at <a href=https://coreos.com/>CoreOS</a>, the company delivering Google’s Infrastructure for Everyone Else (#GIFEE) and running the world's containers securely on CoreOS Linux, Tectonic and Quay.</em></p><p><em>Join us at <a href=https://coreos.com/fest/>CoreOS Fest Berlin</a>, the Open Source Distributed Systems Conference, and learn more about CoreOS and Kubernetes.</em></p><p>At CoreOS, we're all about deploying Kubernetes in production at scale. Today we are excited to share a tool that makes deploying Kubernetes on Amazon Web Services (AWS) a breeze. Kube-aws is a tool for deploying auditable and reproducible Kubernetes clusters to AWS, currently used by CoreOS to spin up production clusters.</p><p>Today you might be putting the Kubernetes components together in a more manual way. With this helpful tool, Kubernetes is delivered in a streamlined package to save time, minimize interdependencies and quickly create production-ready deployments.</p><p>A simple templating system is leveraged to generate cluster configuration as a set of declarative configuration templates that can be version controlled, audited and re-deployed. Since the entirety of the provisioning is by <a href=https://aws.amazon.com/cloudformation/>AWS CloudFormation</a> and cloud-init, there’s no need for external configuration management tools on your end. Batteries included!</p><p>To skip the talk and go straight to the project, check out <a href=https://github.com/coreos/coreos-kubernetes/releases>the latest release of kube-aws</a>, which supports Kubernetes 1.2.x. To get your cluster running, <a href=https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html>check out the documentation</a>.</p><p><strong>Why kube-aws? Security, auditability and reproducibility</strong></p><p>Kube-aws is designed with three central goals in mind.</p><p><strong>Secure</strong> : TLS assets are encrypted via the <a href=https://aws.amazon.com/kms/>AWS Key Management Service (KMS)</a> before being embedded in the CloudFormation JSON. By managing <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html>IAM policy</a> for the KMS key independently, an operator can decouple operational access to the CloudFormation stack from access to the TLS secrets.</p><p><strong>Auditable</strong> : kube-aws is built around the concept of cluster assets. These configuration and credential assets represent the complete description of the cluster. Since KMS is used to encrypt TLS assets, you can feel free to check your unencrypted stack JSON into version control as well!</p><p><strong>Reproducible</strong> : The <em>--export</em> option packs your parameterized cluster definition into a single JSON file which defines a CloudFormation stack. This file can be version controlled and submitted directly to the CloudFormation API via existing deployment tooling, if desired.</p><p><strong>How to get started with kube-aws</strong></p><p>On top of this foundation, kube-aws implements features that make Kubernetes deployments on AWS easier to manage and more flexible. Here are some examples.</p><p><strong>Route53 Integration</strong> : Kube-aws can manage your cluster DNS records as part of the provisioning process.</p><p>cluster.yaml</p><pre><code>externalDNSName: my-cluster.kubernetes.coreos.com

createRecordSet: true

hostedZone: kubernetes.coreos.com

recordSetTTL: 300
</code></pre><p><strong>Existing VPC Support</strong> : Deploy your cluster to an existing VPC.</p><p>cluster.yaml</p><pre><code>vpcId: vpc-xxxxx

routeTableId: rtb-xxxxx
</code></pre><p><strong>Validation</strong> : Kube-aws supports validation of cloud-init and CloudFormation definitions, along with any external resources that the cluster stack will integrate with. For example, here’s a cloud-config with a misspelled parameter:</p><p>userdata/cloud-config-worker</p><pre><code>#cloud-config

coreos:

  flannel:  
    interrface: $private\_ipv4  
    etcd\_endpoints: {{ .ETCDEndpoints }}
</code></pre><p>$ kube-aws validate</p><p>> Validating UserData...<br>Error: cloud-config validation errors:<br>UserDataWorker: line 4: warning: unrecognized key "interrface"</p><p>To get started, check out the <a href=https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html>kube-aws documentation</a>.</p><p><strong>Future Work</strong></p><p>As always, the goal with kube-aws is to make deployments that are production ready. While we use kube-aws in production on AWS today, this project is pre-1.0 and there are a number of areas in which kube-aws needs to evolve.</p><p><strong>Fault tolerance</strong> : At CoreOS we believe Kubernetes on AWS is a potent platform for fault-tolerant and self-healing deployments. In the upcoming weeks, kube-aws will be rising to a new challenge: surviving the <a href=https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey>Chaos Monkey</a> – control plane and all!</p><p><strong>Zero-downtime updates</strong> : Updating CoreOS nodes and Kubernetes components can be done without downtime and without interdependency with the correct instance replacement strategy.</p><p>A <a href=https://github.com/coreos/coreos-kubernetes/issues/340>github issue</a> tracks the work towards this goal. We look forward to seeing you get involved with the project by filing issues or contributing directly.</p><p><em>Learn more about Kubernetes and meet the community at <a href=https://coreos.com/fest/>CoreOS Fest Berlin</a> - May 9-10, 2016</em></p><p><em>– Colin Hom, infrastructure engineer, CoreOS</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-584fa2494155bf80b285f6d9e0dd5284>Adding Support for Kubernetes in Rancher</h1><div class="td-byline mb-4"><time datetime=2016-04-08 class=text-muted>Friday, April 08, 2016</time></div><p><em>Today’s guest post is written by Darren Shepherd, Chief Architect at Rancher Labs, an open-source software platform for managing containers.</em></p><p>Over the last year, we’ve seen a tremendous increase in the number of companies looking to leverage containers in their software development and IT organizations. To achieve this, organizations have been looking at how to build a centralized container management capability that will make it simple for users to get access to containers, while centralizing visibility and control with the IT organization. In 2014 we started the open-source Rancher project to address this by building a management platform for containers.</p><p>Recently we shipped Rancher v1.0. With this latest release, <a href=http://www.rancher.com/>Rancher</a>, an open-source software platform for managing containers, now supports Kubernetes as a container orchestration framework when creating environments. Now, launching a Kubernetes environment with Rancher is fully automated, delivering a functioning cluster in just 5-10 minutes. </p><p>We created Rancher to provide organizations with a complete management platform for containers. As part of that, we’ve always supported deploying Docker environments natively using the Docker API and Docker Compose. Since its inception, we’ve been impressed with the operational maturity of Kubernetes, and with this release, we’re making it possible to deploy a variety of container orchestration and scheduling frameworks within the same management platform.</p><p>Adding Kubernetes gives users access to one of the fastest growing platforms for deploying and managing containers in production. We’ll provide first-class Kubernetes support in Rancher going forward and continue to support native Docker deployments. </p><p><strong>Bringing Kubernetes to Rancher</strong></p><p><img src=https://lh6.googleusercontent.com/bhmC1-XO5T-itFN3ZsCQmrxUSSEcnezaL-qch6ILWvJRnbhEBZZlAMEj-RcNgkM9XVEUzsRMsvDGc7u8f-M19Jdk_J0GCoO-gZTCZDtgkokgqNkCgP98o8W29xD0kmKiMPeLN-Tt alt="Kubernetes deployment-3.PNG"></p><p>Our platform was already extensible for a variety of different packaging formats, so we were optimistic about embracing Kubernetes. We were right, working with the Kubernetes project has been a fantastic experience as developers. The design of the project made this incredibly easy, and we were able to utilize plugins and extensions to build a distribution of Kubernetes that leveraged our infrastructure and application services. For instance, we were able to plug in Rancher’s software defined networking, storage management, load balancing, DNS and infrastructure management functions directly into Kubernetes, without even changing the code base.</p><p>Even better, we have been able to add a number of services around the core Kubernetes functionality. For instance, we implemented our popular <a href=https://github.com/rancher/community-catalog/tree/master/kubernetes-templates>application catalog on top of Kubernetes</a>. Historically we’ve used Docker Compose to define application templates, but with this release, we now support Kubernetes services, replication controllers and pods to deploy applications. With the catalog, users connect to a git repo and automate deployment and upgrade of an application deployed as Kubernetes services. Users then configure and deploy a complex multi-node enterprise application with one click of a button. Upgrades are fully automated as well, and pushed out centrally to users.</p><p><strong>Giving Back</strong></p><p>Like Kubernetes, Rancher is an open-source software project, free to use by anyone, and given to the community without any restrictions. You can find all of the source code, upcoming releases and issues for Rancher on <a href=http://www.github.com/rancher/rancher>GitHub</a>. We’re thrilled to be joining the Kubernetes community, and look forward to working with all of the other contributors. View a demo of the new Kubernetes support in Rancher <a href=http://rancher.com/kubernetes/>here</a>. </p><p><em>-- Darren Shepherd, Chief Architect, Rancher Labs</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f5d7064a37fa7662dc317c17c1ef7ed2>Container survey results - March 2016</h1><div class="td-byline mb-4"><time datetime=2016-04-08 class=text-muted>Friday, April 08, 2016</time></div><p>Last month, we had our third installment of our container survey and today we look at the results.  (raw data is available <a href="https://docs.google.com/spreadsheets/d/13356w6I2xxKnmjblFSsKGVANZGGlX2yFMzb8eOIe2Oo/edit?usp=sharing">here</a>)</p><p>Looking at the headline number, “how many people are using containers” we see a decrease in the number of people currently using containers from 89% to 80%.  Obviously, we can’t be certain for the cause of this decrease, but it’s my believe that the previous number was artificially high due to sampling biases and we did a better job getting a broader reach of participants in the March survey and so the March numbers more accurately represent what is going on in the world.</p><p>Along the lines of getting an unbiased sample, I’m excited to announce that going forward, we will be partnering with <a href=http://thenewstack.io/>The New Stack</a> <a href=http://thenewstack.io/>and the</a><a href=http://cncf.io/>Cloud Native Compute Foundation</a> to publicize and distribute this container survey.  This partnership will enable us to reach a broader audience than we are reaching and thus obtain a significantly more unbiased sample and representative portrayal of current container usage.  I’m really excited about this collaboration!</p><p>But without further ado, more on the data.</p><p>For the rest of the numbers, the March survey shows steady continuation of the numbers that we saw in February.  Most of the container usage is still in Development and Testing, though a solid majority (60%) are using it for production as well.  For the remaining folks using containers there continues to be a plan to bring containers to production as the “I am planning to” number for production use matches up nearly identically with the numbers for people currently in testing.</p><p>Physical and virtual machines continue to be the most popular places to deploy containers, though the March survey shows a fairly substantial drop (48% -> 35%) in people deploying to physical machines.</p><p>Likewise hosted container services show growth, with nearly every service showing some growth.  <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> continues to be the most popular in the survey, followed by the <a href=https://aws.amazon.com/ecs/>Amazon EC2 Container Service</a>.  It will be interesting to see how those numbers change as we move to the New Stack survey.</p><p>Finally, <a href=http://kubernetes.io/>Kubernetes</a> is still the favorite for container manager, with <a href=http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html>Bash scripts</a> are still in second place.  As with the container service provider numbers I’ll be quite interested to see what this looks like with a broader sample set.</p><p>Finally, the absolute use of containers appears to be ticking up.  The number of people running more than 250 containers has grown from 12% to nearly 20%.  And the number people running containers on 50 or more machines has grown from 10% to 18%.</p><p>As always, the raw data is available for you to analyze <a href="https://docs.google.com/spreadsheets/d/13356w6I2xxKnmjblFSsKGVANZGGlX2yFMzb8eOIe2Oo/edit?usp=sharing">here</a>.</p><p>--Brendan Burns, Software Engineer, Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-58e22d29b43afda73c42bce51426e19d>Configuration management with Containers</h1><div class="td-byline mb-4"><time datetime=2016-04-04 class=text-muted>Monday, April 04, 2016</time></div><p><em>Editor’s note: this is our seventh post in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2</em></p><p>A <a href=http://12factor.net/config>good practice</a> when writing applications is to separate application code from configuration. We want to enable application authors to easily employ this pattern within Kubernetes. While the Secrets API allows separating information like credentials and keys from an application, no object existed in the past for ordinary, non-secret configuration. In <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md/#v120>Kubernetes 1.2</a>, we've added a new API resource called ConfigMap to handle this type of configuration data.</p><h4 id=the-basics-of-configmap><strong>The basics of ConfigMap</strong></h4><p>The ConfigMap API is simple conceptually. From a data perspective, the ConfigMap type is just a set of key-value pairs. Applications are configured in different ways, so we need to be flexible about how we let users store and consume configuration data. There are three ways to consume a ConfigMap in a pod:</p><ul><li>Command line arguments</li><li>Environment variables</li><li>Files in a volume</li></ul><p>These different methods lend themselves to different ways of modeling the data being consumed. To be as flexible as possible, we made ConfigMap hold both fine- and/or coarse-grained data. Further, because applications read configuration settings from both environment variables and files containing configuration data, we built ConfigMap to support either method of access. Let’s take a look at an example ConfigMap that contains both types of configuration:</p><pre><code>apiVersion: v1

kind: ConfigMap

metadata:

  Name: example-configmap

data:

  # property-like keys

  game-properties-file-name: game.properties

  ui-properties-file-name: ui.properties

  # file-like keys

  game.properties: |

    enemies=aliens

    lives=3

    enemies.cheat=true

    enemies.cheat.level=noGoodRotten

    secret.code.passphrase=UUDDLRLRBABAS

    secret.code.allowed=true

    secret.code.lives=30

  ui.properties: |

    color.good=purple

    color.bad=yellow

    allow.textmode=true

    how.nice.to.look=fairlyNice
</code></pre><p>Users that have used Secrets will find it easy to begin using ConfigMap — they’re very similar. One major difference in these APIs is that Secret values are stored as byte arrays in order to support storing binaries like SSH keys. In JSON and YAML, byte arrays are serialized as base64 encoded strings. This means that it’s not easy to tell what the content of a Secret is from looking at the serialized form. Since ConfigMap is intended to hold only configuration information and not binaries, values are stored as strings, and thus are readable in the serialized form.</p><p>We want creating ConfigMaps to be as flexible as storing data in them. To create a ConfigMap object, we’ve added a kubectl command called <code>kubectl create configmap</code> that offers three different ways to specify key-value pairs:</p><ul><li>Specify literal keys and value</li><li>Specify an individual file</li><li>Specify a directory to create keys for each file</li></ul><p>These different options can be mixed, matched, and repeated within a single command:</p><pre><code>    $ kubectl create configmap my-config \

    --from-literal=literal-key=literal-value \

    --from-file=ui.properties \
    --from=file=path/to/config/dir
</code></pre><p>Consuming ConfigMaps is simple and will also be familiar to users of Secrets. Here’s an example of a Deployment that uses the ConfigMap above to run an imaginary game server:</p><pre><code>apiVersion: extensions/v1beta1

kind: Deployment

metadata:

  name: configmap-example-deployment

  labels:

    name: configmap-example-deployment

spec:

  replicas: 1

  selector:

    matchLabels:

      name: configmap-example

  template:

    metadata:

      labels:

        name: configmap-example

    spec:

      containers:

      - name: game-container

        image: imaginarygame

        command: [&quot;game-server&quot;, &quot;--config-dir=/etc/game/cfg&quot;]

        env:

        # consume the property-like keys in environment variables

        - name: GAME\_PROPERTIES\_NAME

          valueFrom:

            configMapKeyRef:

              name: example-configmap

              key: game-properties-file-name

        - name: UI\_PROPERTIES\_NAME

          valueFrom:

            configMapKeyRef:

              name: example-configmap

              key: ui-properties-file-name

        volumeMounts:

        - name: config-volume

          mountPath: /etc/game

      volumes:

      # consume the file-like keys of the configmap via volume plugin

      - name: config-volume

        configMap:

          name: example-configmap

          items:

          - key: ui.properties

            path: cfg/ui.properties

         - key: game.properties

           path: cfg/game.properties
      restartPolicy: Never
</code></pre><p>In the above example, the Deployment uses keys of the ConfigMap via two of the different mechanisms available. The property-like keys of the ConfigMap are used as environment variables to the single container in the Deployment template, and the file-like keys populate a volume. For more details, please see the <a href=/docs/user-guide/configmap/>ConfigMap docs</a>.</p><p>We hope that these basic primitives are easy to use and look forward to seeing what people build with ConfigMaps. Thanks to the community members that provided feedback about this feature. Special thanks also to Tamer Tas who made a great contribution to the proposal and implementation of ConfigMap.</p><p>If you’re interested in Kubernetes and configuration, you’ll want to participate in:</p><ul><li>Our Configuration <a href=https://kubernetes.slack.com/messages/sig-configuration/>Slack channel</a></li><li>Our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-config>Kubernetes Configuration Special Interest Group</a> email list</li><li>The Configuration “Special Interest Group,” which meets weekly on Wednesdays at 10am (10h00) Pacific Time at <a href=https://hangouts.google.com/hangouts/_/google.com/kube-sig-config>SIG-Config hangout</a></li></ul><p>And of course for more information about the project in general, go to <a href=http://www.kubernetes.io/>www.kubernetes.io</a> and follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a>.</p><p>-- <em>Paul Morie, Senior Software Engineer, Red Hat</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-aed7a51bb7920b1d7de6348f6e0c9e42>Using Deployment objects with Kubernetes 1.2</h1><div class="td-byline mb-4"><time datetime=2016-04-01 class=text-muted>Friday, April 01, 2016</time></div><p><em>Editor's note: this is the seventh post in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2</em></p><p>Kubernetes has made deploying and managing applications very straightforward, with most actions a single API or command line away, including rolling out new applications, canary testing and upgrading. So why would we need Deployments?</p><p>Deployment objects automate deploying and rolling updating applications. Compared with kubectl rolling-update, Deployment API is much faster, is declarative, is implemented server-side and has more features (for example, you can rollback to any previous revision even after the rolling update is done).</p><p>In today’s blogpost, we’ll cover how to use Deployments to:</p><ol><li>Deploy/rollout an application</li><li>Update the application declaratively and progressively, without a service outage</li><li>Rollback to a previous revision, if something’s wrong when you’re deploying/updating the application</li></ol><p><a href=https://4.bp.blogspot.com/-M9Xc21XYtLA/Vv7ImzURFxI/AAAAAAAACg0/jlHU3nJ-qYwC74DMiD-joaDPqQfebj3-g/s1600/image03.gif><img src=https://4.bp.blogspot.com/-M9Xc21XYtLA/Vv7ImzURFxI/AAAAAAAACg0/jlHU3nJ-qYwC74DMiD-joaDPqQfebj3-g/s640/image03.gif alt></a></p><p>Without further ado, let’s start playing around with Deployments!</p><h3 id=getting-started>Getting started</h3><p>If you want to try this example, basically you’ll need 3 things:</p><ol><li><strong>A running Kubernetes cluster</strong> : If you don’t already have one, check the <a href=/docs/getting-started-guides/>Getting Started guides</a> for a list of solutions on a range of platforms, from your laptop, to VMs on a cloud provider, to a rack of bare metal servers.</li><li><strong>Kubectl, the Kubernetes CLI</strong> : If you see a URL response after running kubectl cluster-info, you’re ready to go. Otherwise, follow the <a href=/docs/user-guide/prereqs/>instructions</a> to install and configure kubectl; or the <a href=https://cloud.google.com/container-engine/docs/before-you-begin>instructions for hosted solutions</a> if you have a Google Container Engine cluster.</li><li>The <a href=https://github.com/kubernetes/kubernetes.github.io/tree/master/docs/user-guide/update-demo>configuration files for this demo</a>.
If you choose not to run this example yourself, that’s okay. Just watch this <a href=https://youtu.be/eigalYy0v4w>video</a> to see what’s going on in each step.</li></ol><h3 id=diving-in>Diving in</h3><p>The configuration files contain a static website. First, we want to start serving its static content. From the root of the Kubernetes repository, run:</p><pre><code>$ kubectl proxy --www=docs/user-guide/update-demo/local/ &amp;  
</code></pre><p>Starting to serve on …</p><p>This runs a proxy on the default port 8001. You may now visit <a href=http://localhost:8001/static/>http://localhost:8001/static/</a> the demo website (and it should be a blank page for now). Now we want to run an app and show it on the website.</p><pre><code>$ kubectl run update-demo   
--image=gcr.io/google\_containers/update-demo:nautilus --port=80 -l name=update-demo  

deployment “update-demo” created  
</code></pre><p>This deploys 1 replica of an app with the image “update-demo:nautilus” and you can see it visually on <a href=http://localhost:8001/static/>http://localhost:8001/static/</a>.1</p><p><a href=https://3.bp.blogspot.com/-EYXhcEK1upw/Vv7JL4rOAtI/AAAAAAAACg4/uy9oKePGjA82xPHhX6ak2_NiHPZ3FU8gw/s1600/deployment-API-5.png><img src=https://3.bp.blogspot.com/-EYXhcEK1upw/Vv7JL4rOAtI/AAAAAAAACg4/uy9oKePGjA82xPHhX6ak2_NiHPZ3FU8gw/s640/deployment-API-5.png alt></a></p><p>The card showing on the website represents a Kubernetes pod, with the pod’s name (ID), status, image, and labels.</p><h3 id=getting-bigger>Getting bigger</h3><p>Now we want more copies of this app!<br>$ kubectl scale deployment/update-demo --replicas=4<br>deployment "update-demo" scaled</p><p><a href=https://1.bp.blogspot.com/-6YXQqogAGcY/Vv7JnU7g_FI/AAAAAAAAChE/00pqgQvUXkcgjPzi7NfDnSSRJeBUHFaGQ/s1600/deployment-API-2.png><img src=https://1.bp.blogspot.com/-6YXQqogAGcY/Vv7JnU7g_FI/AAAAAAAAChE/00pqgQvUXkcgjPzi7NfDnSSRJeBUHFaGQ/s640/deployment-API-2.png alt></a></p><h3 id=updating-your-application>Updating your application</h3><p>How about updating the app?</p><pre><code> $ kubectl edit deployment/update-demo  

 This opens up your default editor, and you can update the deployment on the fly. Find .spec.template.spec.containers[0].image and change nautilus to kitty. Save the file, and you’ll see:  

 deployment &quot;update-demo&quot; edited   
</code></pre><p>You’re now updating the image of this app from “update-demo:nautilus” to “update-demo:kitty”. Deployments allow you to update the app progressively, without a service outage.</p><p><a href=https://2.bp.blogspot.com/-x4FmFXdzw30/Vv7KAAQ21wI/AAAAAAAAChM/QWv8Y03lIsU4JBqjE3XFQU2EtzZgogylA/s1600/deployment-API-3.png><img src=https://2.bp.blogspot.com/-x4FmFXdzw30/Vv7KAAQ21wI/AAAAAAAAChM/QWv8Y03lIsU4JBqjE3XFQU2EtzZgogylA/s640/deployment-API-3.png alt></a></p><p>After a while, you’ll find the update seems stuck. What happened?</p><h3 id=debugging-your-rollout>Debugging your rollout</h3><p>If you look closer, you’ll find that the pods with the new “kitty” tagged image stays pending. The Deployment automatically stops the rollout if it’s failing. Let’s look at one of the new pod to see what happened:</p><pre><code>$ kubectl describe pod/update-demo-1326485872-a4key  
</code></pre><p>Looking at the events of this pod, you’ll notice that Kubernetes failed to pull the image because the “kitty” tag wasn’t found:</p><p>Failed to pull image "gcr.io/google_containers/update-demo:kitty": Tag kitty not found in repository gcr.io/google_containers/update-demo</p><h3 id=rolling-back>Rolling back</h3><p>Ok, now we want to undo the changes and then take our time to figure out which image tag we should use.</p><pre><code>$ kubectl rollout undo deployment/update-demo   
deployment &quot;update-demo&quot; rolled back  
</code></pre><p><a href=https://1.bp.blogspot.com/-6YXQqogAGcY/Vv7JnU7g_FI/AAAAAAAAChE/00pqgQvUXkcgjPzi7NfDnSSRJeBUHFaGQ/s1600/deployment-API-2.png><img src=https://1.bp.blogspot.com/-6YXQqogAGcY/Vv7JnU7g_FI/AAAAAAAAChE/00pqgQvUXkcgjPzi7NfDnSSRJeBUHFaGQ/s640/deployment-API-2.png alt></a></p><p>Everything’s back to normal, phew!</p><p>To learn more about rollback, visit <a href=/docs/user-guide/deployments/#rolling-back-a-deployment>rolling back a Deployment</a>.</p><h3 id=updating-your-application-for-real>Updating your application (for real)</h3><p>After a while, we finally figure that the right image tag is “kitten”, instead of “kitty”. Now change .spec.template.spec.containers[0].image tag from “nautilus“ to “kitten“.</p><pre><code>$ kubectl edit deployment/update-demo  
deployment &quot;update-demo&quot; edited  
</code></pre><p><a href=https://4.bp.blogspot.com/-u7qPUSQOMLE/Vv7JndUqKaI/AAAAAAAAChA/jHoysiDbnNQU2prPJn19ZFOtLiatzPsMg/s1600/deployment-API-1.png><img src=https://4.bp.blogspot.com/-u7qPUSQOMLE/Vv7JndUqKaI/AAAAAAAAChA/jHoysiDbnNQU2prPJn19ZFOtLiatzPsMg/s640/deployment-API-1.png alt></a></p><p>Now you see there are 4 cute kittens on the demo website, which means we’ve updated the app successfully! If you want to know the magic behind this, look closer at the Deployment:</p><pre><code>$ kubectl describe deployment/update-demo  
</code></pre><p><a href=https://1.bp.blogspot.com/-3U1OTNqdz1s/Vv7Kfw4uGYI/AAAAAAAAChU/CgF6Mv5J6b8_lANXkpEIFytRGo9x0Bn_A/s1600/deployment-API-6.png><img src=https://1.bp.blogspot.com/-3U1OTNqdz1s/Vv7Kfw4uGYI/AAAAAAAAChU/CgF6Mv5J6b8_lANXkpEIFytRGo9x0Bn_A/s640/deployment-API-6.png alt></a></p><p>From the events section, you’ll find that the Deployment is managing another resource called <a href=/docs/user-guide/replicasets/>Replica Set</a>, each controls the number of replicas of a different pod template. The Deployment enables progressive rollout by scaling up and down Replica Sets of new and old pod templates.</p><h3 id=conclusion>Conclusion</h3><p>Now, you’ve learned the basic use of Deployment objects:</p><ol><li>Deploy an app with a Deployment, using kubectl run</li><li>Updating the app by updating the Deployment with kubectl edit</li><li>Rolling back to a previously deployed app with kubectl rollout undo
But there’s so much more in Deployment that this article didn’t cover! To discover more, continue reading <a href=/docs/user-guide/deployments/>Deployment’s introduction</a>.</li></ol><p><strong><em>Note:</em></strong> <em>In Kubernetes 1.2, Deployment (beta release) is now feature-complete and enabled by default. For those of you who have tried Deployment in Kubernetes 1.1, please <strong>delete all Deployment 1.1 resources</strong> (including the Replication Controllers and Pods they manage) before trying out Deployments in 1.2. This is necessary because we made some non-backward-compatible changes to the API.</em></p><p>If you’re interested in Kubernetes and configuration, you’ll want to participate in:</p><ul><li>Our Configuration <a href=https://kubernetes.slack.com/messages/sig-configuration/>slack channel</a></li><li>Our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-config>Kubernetes Configuration Special Interest Group</a> email list</li><li>The Configuration “Special Interest Group,” which meets weekly on Wednesdays at 10am (10h00) Pacific Time at <a href=https://hangouts.google.com/hangouts/_/google.com/kube-sig-config>SIG-Config hangout</a>
And of course for more information about the project in general, go to <a href=http://www.kubernetes.io/>www.kubernetes.io</a>.</li></ul><p>-- <em>Janet Kuo, Software Engineer, Google</em></p><p><strong>1</strong> “kubectl run” outputs the type and name of the resource(s) it creates. In 1.2, it now creates a deployment resource. You can use that in subsequent commands, such as "kubectl get deployment ", or "kubectl expose deployment ". If you want to write a script to do that automatically, in a forward-compatible manner, use "-o name" flag with "kubectl run", and it will generate short output "deployments/", which can also be used on subsequent command lines. The "--generator" flag can be used with "kubectl run" to generate other types of resources, for example, set it to "run/v1" to create a Replication Controller, which was the default in 1.1 and 1.0, and to "run-pod/v1" to create a Pod, such as for --restart=Never pods.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7418a8c542b8eb8b0648387e10d74720>Kubernetes 1.2 and simplifying advanced networking with Ingress</h1><div class="td-byline mb-4"><time datetime=2016-03-31 class=text-muted>Thursday, March 31, 2016</time></div><p><em>Editor's note: This is the sixth post in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2.</em><br><em>Ingress is currently in beta and under active development.</em></p><p>In Kubernetes, Services and Pods have IPs only routable by the cluster network, by default. All traffic that ends up at an edge router is either dropped or forwarded elsewhere. In Kubernetes 1.2, we’ve made improvements to the Ingress object, to simplify allowing inbound connections to reach the cluster services. It can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, offer name based virtual hosting and lots more.</p><h3 id=ingress-controllers>Ingress controllers</h3><p>Today, with containers or VMs, configuring a web server or load balancer is harder than it should be. Most web server configuration files are very similar. There are some applications that have weird little quirks that tend to throw a wrench in things, but for the most part, you can apply the same logic to them and achieve a desired result. In Kubernetes 1.2, the Ingress resource embodies this idea, and an Ingress controller is meant to handle all the quirks associated with a specific "class" of Ingress (be it a single instance of a load balancer, or a more complicated setup of frontends that provide GSLB, CDN, DDoS protection etc). An Ingress Controller is a daemon, deployed as a Kubernetes Pod, that watches the ApiServer's /ingresses endpoint for updates to the <a href=/docs/user-guide/ingress/>Ingress resource</a>. Its job is to satisfy requests for ingress.</p><p>Your Kubernetes cluster must have exactly one Ingress controller that supports TLS for the following example to work. If you’re on a cloud-provider, first check the “kube-system” namespace for an Ingress controller RC. If there isn’t one, you can deploy the <a href=https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx>nginx controller</a>, or <a href=https://github.com/kubernetes/contrib/tree/master/ingress/controllers#writing-an-ingress-controller>write your own</a> in &lt; 100 lines of code.</p><p>Please take a minute to look over the known limitations of existing controllers (gce, nginx).</p><h3 id=tls-termination-and-http-load-balancing>TLS termination and HTTP load-balancing</h3><p>Since the Ingress spans Services, it’s particularly suited for load balancing and centralized security configuration. If you’re familiar with the go programming language, Ingress is like <a href=https://golang.org/pkg/net/http/#Server>net/http’s “Server”</a> for your entire cluster. The following example shows you how to configure TLS termination. Load balancing is not optional when dealing with ingress traffic, so simply creating the object will configure a load balancer.</p><p>First create a test Service. We’ll run a simple echo server for this example so you know exactly what’s going on. The source is <a href=https://github.com/kubernetes/contrib/tree/master/ingress/echoheaders>here</a>.</p><pre><code>$ kubectl run echoheaders   
--image=gcr.io/google\_containers/echoserver:1.3 --port=8080  
$ kubectl expose deployment echoheaders --target-port=8080   
--type=NodePort  
</code></pre><p>If you’re on a cloud-provider, make sure you can reach the Service from outside the cluster through its node port.</p><pre><code>$ NODE_IP=$(kubectl get node `kubectl get po -l run=echoheaders 
--template '{{range .items}}{{.spec.nodeName}}{{end}}'` --template
'{{range $i, $n := .status.addresses}}{{if eq $n.type 
&quot;ExternalIP&quot;}}{{$n.address}}{{end}}{{end}}')
$ NODE_PORT=$(kubectl get svc echoheaders --template '{{range $i, $e 
:= .spec.ports}}{{$e.nodePort}}{{end}}')
$ curl $NODE_IP:$NODE_PORT
</code></pre><p>This is a sanity check that things are working as expected. If the last step hangs, you might need a <a href=https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md#creating-the-firewall-rule-for-glbc-health-checks>firewall rule</a>.</p><p>Now lets create our TLS secret:</p><pre><code>$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout   

/tmp/tls.key -out /tmp/tls.crt -subj &quot;/CN=echoheaders/O=echoheaders&quot;

$ echo &quot;  
apiVersion: v1  
kind: Secret  
metadata:
  name: tls  
data:  
  tls.crt: `base64 -w 0 /tmp/tls.crt`  
  tls.key: `base64 -w 0 /tmp/tls.key`  
&quot; | kubectl create -f   
</code></pre><p>And the Ingress:</p><pre><code>$ echo &quot;

apiVersion: extensions/v1beta1

kind: Ingress

metadata:

  name: test

spec:

  tls:

  - secretName: tls
  backend:  
    serviceName: echoheaders  
    servicePort: 8080  
&quot; | kubectl create -f -  
</code></pre><p>You should get a load balanced IP soon:</p><pre><code>$ kubectl get ing   
NAME      RULE      BACKEND            ADDRESS         AGE  
test      -         echoheaders:8080   130.X.X.X     4m  
</code></pre><p>And if you wait till the Ingress controller marks your backends as healthy, you should see requests to that IP on :80 getting redirected to :443 and terminated using the given TLS certificates.</p><pre><code>$ curl 130.X.X.X  
\&lt;html\&gt;  
\&lt;head\&gt;\&lt;title\&gt;301 Moved Permanently\&lt;/title\&gt;\&lt;/head\&gt;\&lt;body bgcolor=&quot;white&quot;\&gt;\&lt;center\&gt;\&lt;h1\&gt;301 Moved Permanently\&lt;/h1\&gt;\&lt;/center\&gt;  
</code></pre><pre><code>$ curl https://130.X.X.X -kCLIENT VALUES:client\_address=10.48.0.1command=GETreal path=/  


$ curl 130.X.X.X -Lk

CLIENT VALUES:client\_address=10.48.0.1command=GETreal path=/
</code></pre><h3 id=future-work>Future work</h3><p>You can read more about the <a href=/docs/user-guide/ingress/>Ingress API</a> or controllers by following the links. The Ingress is still in beta, and we would love your input to grow it. You can contribute by writing controllers or evolving the API. All things related to the meaning of the word “<a href="https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=ingress%20meaning">ingress</a>” are in scope, this includes DNS, different TLS modes, SNI, load balancing at layer 4, content caching, more algorithms, better health checks; the list goes on.</p><p>There are many ways to participate. If you’re particularly interested in Kubernetes and networking, you’ll be interested in:</p><ul><li>Our <a href=https://kubernetes.slack.com/messages/sig-network/>Networking slack channel</a></li><li>Our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-network>Kubernetes Networking Special Interest Group</a> email list</li><li>The Big Data “Special Interest Group,” which meets biweekly at 3pm (15h00) Pacific Time at <a href=https://zoom.us/j/5806599998>SIG-Networking hangout</a></li></ul><p>And of course for more information about the project in general, go to<a href=http://kubernetes.io/>www.kubernetes.io</a></p><p>-- <em>Prashanth Balasubramanian, Software Engineer</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4f5d79599c93a734cf95c3da38dd3518>Using Spark and Zeppelin to process big data on Kubernetes 1.2</h1><div class="td-byline mb-4"><time datetime=2016-03-30 class=text-muted>Wednesday, March 30, 2016</time></div><p><em>Editor's note: this is the fifth post in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2 </em></p><p>With big data usage growing exponentially, many Kubernetes customers have expressed interest in running <a href=http://spark.apache.org/>Apache Spark</a> on their Kubernetes clusters to take advantage of the portability and flexibility of containers. Fortunately, with Kubernetes 1.2, you can now have a platform that runs Spark and Zeppelin, and your other applications side-by-side.</p><h3 id=why-zeppelin-nbsp>Why Zeppelin? </h3><p><a href=https://zeppelin.incubator.apache.org/>Apache Zeppelin</a> is a web-based notebook that enables interactive data analytics. As one of its backends, Zeppelin connects to Spark. Zeppelin allows the user to interact with the Spark cluster in a simple way, without having to deal with a command-line interpreter or a Scala compiler.</p><h3 id=why-kubernetes-nbsp>Why Kubernetes? </h3><p>There are many ways to run Spark outside of Kubernetes:</p><ul><li>You can run it using dedicated resources, in Standalone mode </li><li>You can run it on a <a href=https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html>YARN</a> cluster, co-resident with Hadoop and HDFS </li><li>You can run it on a <a href=http://mesos.apache.org/>Mesos</a> cluster alongside other Mesos applications </li></ul><p>So why would you run Spark on Kubernetes?</p><ul><li>A single, unified interface to your cluster: Kubernetes can manage a broad range of workloads; no need to deal with YARN/HDFS for data processing and a separate container orchestrator for your other applications. </li><li>Increased server utilization: share nodes between Spark and cloud-native applications. For example, you may have a streaming application running to feed a streaming Spark pipeline, or a nginx pod to serve web traffic — no need to statically partition nodes. </li><li>Isolation between workloads: Kubernetes' <a href=https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/proposals/resource-qos.md>Quality of Service</a> mechanism allows you to safely co-schedule batch workloads like Spark on the same nodes as latency-sensitive servers. </li></ul><h3 id=launch-spark-nbsp>Launch Spark </h3><p>For this demo, we’ll be using <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE), but this should work anywhere you have installed a Kubernetes cluster. First, create a Container Engine cluster with storage-full scopes. These Google Cloud Platform scopes will allow the cluster to write to a private Google Cloud Storage Bucket (we’ll get to why you need that later): </p><pre><code>$ gcloud container clusters create spark --scopes storage-full
--machine-type n1-standard-4
</code></pre><p><em>Note</em>: We’re using n1-standard-4 (which are larger than the default node size) to demonstrate some features of Horizontal Pod Autoscaling. However, Spark works just fine on the default node size of n1-standard-1.</p><p>After the cluster’s created, you’re ready to launch Spark on Kubernetes using the config files in the Kubernetes GitHub repo:</p><pre><code>$ git clone https://github.com/kubernetes/kubernetes.git
$ kubectl create -f kubernetes/examples/spark
</code></pre><p><code>‘kubernetes/examples/spark’</code> is a directory, so this command tells kubectl to create all of the Kubernetes objects defined in all of the YAML files in that directory. You don’t have to clone the entire repository, but it makes the steps of this demo just a little easier.</p><p>The pods (especially Apache Zeppelin) are somewhat large, so may take some time for Docker to pull the images. Once everything is running, you should see something similar to the following:</p><pre><code>$ kubectl get pods
NAME READY STATUS RESTARTS AGE
spark-master-controller-v4v4y 1/1 Running 0 21h
spark-worker-controller-7phix 1/1 Running 0 21h
spark-worker-controller-hq9l9 1/1 Running 0 21h
spark-worker-controller-vwei5 1/1 Running 0 21h
zeppelin-controller-t1njl 1/1 Running 0 21h
</code></pre><p>You can see that Kubernetes is running one instance of Zeppelin, one Spark master and three Spark workers.</p><h3 id=set-up-the-secure-proxy-to-zeppelin-nbsp>Set up the Secure Proxy to Zeppelin </h3><p>Next you’ll set up a secure proxy from your local machine to Zeppelin, so you can access the Zeppelin instance running in the cluster from your machine. (Note: You’ll need to change this command to the actual Zeppelin pod that was created on your cluster.)</p><pre><code>$ kubectl port-forward zeppelin-controller-t1njl 8080:8080
</code></pre><p>This establishes a secure link to the Kubernetes cluster and pod (<code>zeppelin-controller-t1njl</code>) and then forwards the port in question (8080) to local port 8080, which will allow you to use Zeppelin safely.</p><h3 id=now-that-i-have-zeppelin-up-and-running-what-do-i-do-with-it-nbsp>Now that I have Zeppelin up and running, what do I do with it? </h3><p>For our example, we’re going to show you how to build a simple movie recommendation model. This is based on the code <a href=http://spark.apache.org/docs/1.5.2/mllib-collaborative-filtering.html>on the Spark website</a>, modified slightly to make it interesting for Kubernetes. </p><p>Now that the secure proxy is up, visit <a href=http://localhost:8080/>http://localhost:8080/</a>. You should see an intro page like this:</p><p><a href=https://1.bp.blogspot.com/-rk6iWAauxGM/VvwPoE25QFI/AAAAAAAAA6c/NOBZzIWfTYEqJin-tWY1zrePPOqr3TZWQ/s1600/Spark2.png><img src=https://1.bp.blogspot.com/-rk6iWAauxGM/VvwPoE25QFI/AAAAAAAAA6c/NOBZzIWfTYEqJin-tWY1zrePPOqr3TZWQ/s640/Spark2.png alt></a></p><p>Click “Import note,” give it an arbitrary name (e.g. “Movies”), and click “Add from URL.” For a URL, enter:</p><pre><code>https://gist.githubusercontent.com/zmerlynn/875fed0f587d12b08ec9/raw/6
eac83e99caf712482a4937800b17bbd2e7b33c4/movies.json
</code></pre><p>Then click “Import Note.” This will give you a ready-made Zeppelin note for this demo. You should now have a “Movies” notebook (or whatever you named it). If you click that note, you should see a screen similar to this:</p><p><a href=https://2.bp.blogspot.com/-qyjtrUpXisg/VvwPvSPnWNI/AAAAAAAAA6g/Son7C2yWolk28KLSy63acGPnuFGjJEoeg/s1600/Spark1.png><img src=https://2.bp.blogspot.com/-qyjtrUpXisg/VvwPvSPnWNI/AAAAAAAAA6g/Son7C2yWolk28KLSy63acGPnuFGjJEoeg/s640/Spark1.png alt></a></p><p>You can now click the Play button, near the top-right of the PySpark code block, and you’ll create a new, in-memory movie recommendation model! In the Spark application model, Zeppelin acts as a <a href=https://spark.apache.org/docs/1.5.2/cluster-overview.html>Spark Driver Program</a>, interacting with the Spark cluster master to get its work done. In this case, the driver program that’s running in the Zeppelin pod fetches the data and sends it to the Spark master, which farms it out to the workers, which crunch out a movie recommendation model using the code from the driver. With a larger data set in Google Cloud Storage (GCS), it would be easy to pull the data from GCS as well. In the next section, we’ll talk about how to save your data to GCS.</p><h3 id=working-with-google-cloud-storage-optional-nbsp>Working with Google Cloud Storage (Optional) </h3><p>For this demo, we’ll be using Google Cloud Storage, which will let us store our model data beyond the life of a single pod. Spark for Kubernetes is built with the <a href=https://cloud.google.com/storage/>Google Cloud Storage</a> connector built-in. As long as you can access your data from a virtual machine in the Google Container Engine project where your Kubernetes nodes are running, you can access your data with the GCS connector on the Spark image.</p><p>If you want, you can change the variables at the top of the note so that the example will actually save and restore a model for the movie recommendation engine — just point those variables at a GCS bucket that you have access to. If you want to create a GCS bucket, you can do something like this on the command line:</p><pre><code>$ gsutil mb gs://my-spark-models
</code></pre><p>You’ll need to change this URI to something that is unique for you. This will create a bucket that you can use in the example above.</p><blockquote class="note callout"><div><strong>Note:</strong> Computing the model and saving it is much slower than computing the model and throwing it away. This is expected. However, if you plan to reuse a model, it’s faster to compute the model and save it and then restore it each time you want to use it, rather than throw away and recompute the model each time.</div></blockquote><h3 id=using-horizontal-pod-autoscaling-with-spark-optional-nbsp>Using Horizontal Pod Autoscaling with Spark (Optional) </h3><p>Spark is somewhat elastic to workers coming and going, which means we have an opportunity: we can use use <a href=/docs/user-guide/horizontal-pod-autoscaling/>Kubernetes Horizontal Pod Autoscaling</a> to scale-out the Spark worker pool automatically, setting a target CPU threshold for the workers and a minimum/maximum pool size. This obviates the need for having to configure the number of worker replicas manually.</p><p>Create the Autoscaler like this (note: if you didn’t change the machine type for the cluster, you probably want to limit the --max to something smaller): </p><pre><code>$ kubectl autoscale --min=1 --cpu-percent=80 --max=10 \
  rc/spark-worker-controller
</code></pre><p>To see the full effect of autoscaling, wait for the replication controller to settle back to one replica. Use <code>‘kubectl get rc’</code> and wait for the “replicas” column on spark-worker-controller to fall back to 1.</p><p>The workload we ran before ran too quickly to be terribly interesting for HPA. To change the workload to actually run long enough to see autoscaling become active, change the “rank = 100” line in the code to “rank = 200.” After you hit play, the Spark worker pool should rapidly increase to 20 pods. It will take up to 5 minutes after the job completes before the worker pool falls back down to one replica.</p><h3 id=conclusion>Conclusion</h3><p>In this article, we showed you how to run Spark and Zeppelin on Kubernetes, as well as how to use Google Cloud Storage to store your Spark model and how to use Horizontal Pod Autoscaling to dynamically size your Spark worker pool.</p><p>This is the first in a series of articles we’ll be publishing on how to run big data frameworks on Kubernetes — so stay tuned!</p><p>Please join our community and help us build the future of Kubernetes! There are many ways to participate. If you’re particularly interested in Kubernetes and big data, you’ll be interested in:</p><ul><li>Our <a href=https://kubernetes.slack.com/messages/sig-big-data/>Big Data slack channel</a></li><li>Our <a href=https://groups.google.com/forum/#!forum/kubernetes-sig-big-data>Kubernetes Big Data Special Interest Group email list</a></li><li>The Big Data “Special Interest Group,” which meets every Monday at 1pm (13h00) Pacific Time at <a href=https://plus.google.com/hangouts/_/google.com/big-data>SIG-Big-Data hangout </a>
And of course for more information about the project in general, go to <a href=http://www.kubernetes.io/>www.kubernetes.io</a>.</li></ul><p> -- <em>Zach Loafman, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-6e843f3c8bfc0bfa0651915953bf9a9e>AppFormix: Helping Enterprises Operationalize Kubernetes</h1><div class="td-byline mb-4"><time datetime=2016-03-29 class=text-muted>Tuesday, March 29, 2016</time></div><p><em>Today’s guest post is written Sumeet Singh, founder and CEO of <a href=http://www.appformix.com/>AppFormix</a>, a cloud infrastructure performance optimization service helping enterprise operators streamline their cloud operations on any OpenStack or Kubernetes cloud.</em></p><p>If you run clouds for a living, you’re well aware that the tools we've used since the client/server era for monitoring, analytics and optimization just don’t cut it when applied to the agile, dynamic and rapidly changing world of modern cloud infrastructure.</p><p>And, if you’re an operator of enterprise clouds, you know that implementing containers and container cluster management is all about giving your application developers a more agile, responsive and efficient cloud infrastructure. Applications are being rewritten and new ones developed – not for legacy environments where relatively static workloads are the norm, but for dynamic, scalable cloud environments. The dynamic nature of cloud native applications coupled with the shift to continuous deployment means that the demands placed by the applications on the infrastructure are constantly changing.</p><p>This shift necessitates infrastructure transparency and real-time monitoring and analytics. Without these key pieces, neither applications nor their underlying plumbing can deliver the low-latency user experience end users have come to expect.<br>  <br><strong>AppFormix Architectural Review</strong><br>From an operational standpoint, it is necessary to understand how applications are consuming infrastructure resources in order to maximize ROI and guarantee SLAs. AppFormix software empowers operators and developers to monitor, visualize, and control how physical resources are utilized by cloud workloads. </p><p>At the center of the software, the AppFormix Data Platform provides a distributed analysis engine that performs configurable, real-time evaluation of in-depth, high-resolution metrics. On each host, the resource-efficient AppFormix Agent collects and evaluates multi-layer metrics from the hardware, virtualization layer, and up to the application. Intelligent agents offer sub-second response times that make it possible to detect and solve problems before they start to impact applications and users. The raw data is associated with the elements that comprise a cloud-native environment: applications, virtual machines, containers, hosts. The AppFormix Agent then publishes metrics and events to a Data Manager that stores and forwards the data to Analytics modules. Events are based on predefined or dynamic conditions set by users or infrastructure operators to make sure that SLAs and policies are being met.</p><p>| <img src=https://lh3.googleusercontent.com/sPfaXresP1wDPPVERwQC1eZHDKtwrD1buAmMhLcWxwbnPmJIgJql1VIn7mNoh_QSPxcMTzjraQulg3pSta6OM9VvJn0hgrQKSteP8ijIp14E9JAzJnUd5Ds_rvHQwj4IHPQ7Jhsr alt> |
| Figure 1: Roll-up summary view of the Kubernetes cluster. Operators and Users can define their SLA policies and AppFormix provides with a real-time view of the health of all elements in the Kubernetes cluster.  |</p><p>| <img src=https://lh6.googleusercontent.com/0kOaFmyX8LWqbvGGuyFFl08uM_TC3_uFrwQslkEdKmBHZHzrSAsqU7bDb0w0cHDMLCWJa6uz9rfFtsf6BOvoKgNkpUeh7wwPTveC69X9JZru0VpwrT_hzACr0JADjgDV5EM0UVyc alt> |
| Figure 2: Real-Time visualization of telemetry from a Kubernetes node provides a quick overview of resource utilization on the host as well as resources consumed by the pods and containers. The user defined Labels make is easy to capture namespaces, and other metadata. |</p><p>Additional subsystems are the Policy Controller and Analytics. The Policy Controller manages policies for resource monitoring, analysis, and control. It also provides role-based access control. The Analytics modules analyze metrics and events produced by Data Platform, enabling correlation across multiple elements to provide higher-level information to operators and developers. The Analytics modules may also configure policies in Policy Controller in response to conditions in the infrastructure.</p><p>AppFormix organizes elements of cloud infrastructure around hosts and instances (either containers or virtual machines), and logical groups of such elements. AppFormix integrates with cloud platforms using Adapter modules that discover the physical and virtual elements in the environment and configure those elements into the Policy Controller.</p><p><strong>Integrating AppFormix with Kubernetes</strong><br>Enterprises often run many environments located on- or off-prem, as well as running different compute technologies (VMs, containers, bare metal). The analytics platform we’ve developed at AppFormix gives Kubernetes users a single pane of glass from which to monitor and manage container clusters in private and hybrid environments.</p><p>The AppFormix Kubernetes Adapter leverages the REST-based APIs of Kubernetes to discover nodes, pods, containers, services, and replication controllers. With the relational information about each element, Kubernetes Adapter is able to represent all of these elements in our system. A pod is a group of containers. A service and a replication controller are both different types of pod groups. In addition, using the watch endpoint, Kubernetes Adapter stays aware of changes to the environment.</p><p><strong>DevOps in the Enterprise with AppFormix</strong><br>With AppFormix, developers and operators can work collaboratively to optimize applications and infrastructure. Users can access a self-service IT experience that delivers visibility into CPU, memory, storage, and network consumption by each layer of the stack: physical hardware, platform, and application software. </p><ul><li><strong>Real-time multi-layer performance metrics</strong> - In real-time, developers can view multi-layer metrics that show container resource consumption in context of the physical node on which it executes. With this context, developers can determine if application performance is limited by the physical infrastructure, due to contention or resource exhaustion, or by application design.  </li><li><strong>Proactive resource control</strong> - AppFormix Health Analytics provides policy-based actions in response to conditions in the cluster. For example, when resource consumption exceeds threshold on a worker node, Health Analytics can remove the node from the scheduling pool by invoking Kubernetes REST APIs. This dynamic control is driven by real-time monitoring at each node.</li><li><strong>Capacity planning</strong> - Kubernetes will schedule workloads, but operators need to understand how the resources are being utilized. What resources have the most demand? How is demand trending over time? Operators can generate reports that provide necessary data for capacity planning.</li></ul><p>As you can see, we’re working hard to give Kubernetes users a useful, performant toolset for both OpenStack and Kubernetes environments that allows operators to deliver self-service IT to their application developers. We’re excited to be partner contributing to the Kubernetes ecosystem and community.</p><p><em>-- Sumeet Singh, Founder and CEO, AppFormix</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c304efe1c6fd9abd01050e643397611e>Building highly available applications using Kubernetes new multi-zone clusters (a.k.a. 'Ubernetes Lite')</h1><div class="td-byline mb-4"><time datetime=2016-03-29 class=text-muted>Tuesday, March 29, 2016</time></div><p><em>Editor's note: this is the third post in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2</em></p><h3 id=introduction-nbsp>Introduction </h3><p>One of the most frequently-requested features for Kubernetes is the ability to run applications across multiple zones. And with good reason — developers need to deploy applications across multiple domains, to improve availability in thxe advent of a single zone outage.</p><p><a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-even-more-performance-upgrades-plus-easier-application-deployment-and-management>Kubernetes 1.2</a>, released two weeks ago, adds support for running a single cluster across multiple failure zones (GCP calls them simply "zones," Amazon calls them "availability zones," here we'll refer to them as "zones"). This is the first step in a broader effort to allow federating multiple Kubernetes clusters together (sometimes referred to by the affectionate nickname "<a href=https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/federation.md>Ubernetes</a>"). This initial version (referred to as "Ubernetes Lite") offers improved application availability by spreading applications across multiple zones within a single cloud provider.</p><p>Multi-zone clusters are deliberately simple, and by design, very easy to use — no Kubernetes API changes were required, and no application changes either. You simply deploy your existing Kubernetes application into a new-style multi-zone cluster, and your application automatically becomes resilient to zone failures.</p><h3 id=now-into-some-details-nbsp>Now into some details . . .  </h3><p>Ubernetes Lite works by leveraging the Kubernetes platform’s extensibility through labels. Today, when nodes are started, labels are added to every node in the system. With Ubernetes Lite, the system has been extended to also add information about the zone it's being run in. With that, the scheduler can make intelligent decisions about placing application instances.</p><p>Specifically, the scheduler already spreads pods to minimize the impact of any single node failure. With Ubernetes Lite, via <code>SelectorSpreadPriority</code>, the scheduler will make a best-effort placement to spread across zones as well. We should note, if the zones in your cluster are heterogenous (e.g., different numbers of nodes or different types of nodes), you may not be able to achieve even spreading of your pods across zones. If desired, you can use homogenous zones (same number and types of nodes) to reduce the probability of unequal spreading.</p><p>This improved labeling also applies to storage. When persistent volumes are created, the <code>PersistentVolumeLabel</code> admission controller automatically adds zone labels to them. The scheduler (via the <code>VolumeZonePredicate</code> predicate) will then ensure that pods that claim a given volume are only placed into the same zone as that volume, as volumes cannot be attached across zones.</p><h3 id=walkthrough-nbsp>Walkthrough </h3><p>We're now going to walk through setting up and using a multi-zone cluster on both <a href=https://cloud.google.com/compute/>Google Compute Engine</a> (GCE) and Amazon EC2 using the default kube-up script that ships with Kubernetes. Though we highlight GCE and EC2, this functionality is available in any Kubernetes 1.2 deployment where you can make changes during cluster setup. This functionality will also be available in <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE) shortly.</p><h3 id=bringing-up-your-cluster-nbsp>Bringing up your cluster </h3><p>Creating a multi-zone deployment for Kubernetes is the same as for a single-zone cluster, but you’ll need to pass an environment variable (<code>"MULTIZONE”</code>) to tell the cluster to manage multiple zones. We’ll start by creating a multi-zone-aware cluster on GCE and/or EC2.</p><p>GCE:</p><pre><code>curl -sS https://get.k8s.io | MULTIZONE=true KUBERNETES_PROVIDER=gce
KUBE_GCE_ZONE=us-central1-a NUM_NODES=3 bash
</code></pre><p>EC2:</p><pre><code>curl -sS https://get.k8s.io | MULTIZONE=true KUBERNETES_PROVIDER=aws
KUBE_AWS_ZONE=us-west-2a NUM_NODES=3 bash
</code></pre><p>At the end of this command, you will have brought up a cluster that is ready to manage nodes running in multiple zones. You’ll also have brought up <code>NUM_NODES</code> nodes and the cluster's control plane (i.e., the Kubernetes master), all in the zone specified by <code>KUBE_{GCE,AWS}_ZONE</code>. In a future iteration of Ubernetes Lite, we’ll support a HA control plane, where the master components are replicated across zones. Until then, the master will become unavailable if the zone where it is running fails. However, containers that are running in all zones will continue to run and be restarted by Kubelet if they fail, thus the application itself will tolerate such a zone failure.</p><h3 id=nodes-are-labeled-nbsp>Nodes are labeled </h3><p>To see the additional metadata added to the node, simply view all the labels for your cluster (the example here is on GCE):</p><pre><code>$ kubectl get nodes --show-labels

NAME STATUS AGE LABELS
kubernetes-master Ready,SchedulingDisabled 6m        
beta.kubernetes.io/instance-type=n1-standard-1,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-master
kubernetes-minion-87j9 Ready 6m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv Ready 6m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q Ready 6m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-a12q
</code></pre><p>The scheduler will use the labels attached to each of the nodes (failure-domain.beta.kubernetes.io/region for the region, and failure-domain.beta.kubernetes.io/zone for the zone) in its scheduling decisions.</p><h3 id=add-more-nodes-in-a-second-zone-nbsp>Add more nodes in a second zone </h3><p>Let's add another set of nodes to the existing cluster, but running in a different zone (us-central1-b for GCE, us-west-2b for EC2). We run kube-up again, but by specifying <code>KUBE_USE_EXISTING_MASTER=1</code> kube-up will not create a new master, but will reuse one that was previously created.</p><p>GCE:</p><pre><code>KUBE_USE_EXISTING_MASTER=true MULTIZONE=true KUBERNETES_PROVIDER=gce
KUBE_GCE_ZONE=us-central1-b NUM_NODES=3 kubernetes/cluster/kube-up.sh
</code></pre><p>On EC2, we also need to specify the network CIDR for the additional subnet, along with the master internal IP address:</p><pre><code>KUBE_USE_EXISTING_MASTER=true MULTIZONE=true KUBERNETES_PROVIDER=aws
KUBE_AWS_ZONE=us-west-2b NUM_NODES=3 KUBE_SUBNET_CIDR=172.20.1.0/24
MASTER_INTERNAL_IP=172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre><p>View the nodes again; 3 more nodes will have been launched and labelled (the example here is on GCE):</p><pre><code>$ kubectl get nodes --show-labels

NAME STATUS AGE LABELS
kubernetes-master Ready,SchedulingDisabled 16m       
beta.kubernetes.io/instance-type=n1-standard-1,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-master
kubernetes-minion-281d Ready 2m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-b,kub
ernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-87j9 Ready 16m       
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv Ready 16m       
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q Ready 17m       
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-a12q
kubernetes-minion-pp2f Ready 2m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-b,kub
ernetes.io/hostname=kubernetes-minion-pp2f
kubernetes-minion-wf8i Ready 2m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-b,kub
ernetes.io/hostname=kubernetes-minion-wf8i
</code></pre><p>Let’s add one more zone:</p><p>GCE:</p><pre><code>KUBE_USE_EXISTING_MASTER=true MULTIZONE=true KUBERNETES_PROVIDER=gce
KUBE_GCE_ZONE=us-central1-f NUM_NODES=3 kubernetes/cluster/kube-up.sh
</code></pre><p>EC2:</p><pre><code>KUBE_USE_EXISTING_MASTER=true MULTIZONE=true KUBERNETES_PROVIDER=aws
KUBE_AWS_ZONE=us-west-2c NUM_NODES=3 KUBE_SUBNET_CIDR=172.20.2.0/24
MASTER_INTERNAL_IP=172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre><p>Verify that you now have nodes in 3 zones:</p><pre><code>kubectl get nodes --show-labels
</code></pre><p>Highly available apps, here we come.</p><h3 id=deploying-a-multi-zone-application-nbsp>Deploying a multi-zone application </h3><p>Create the guestbook-go example, which includes a ReplicationController of size 3, running a simple web app. Download all the files from <a href=https://github.com/kubernetes/examples/tree/master/guestbook-go>here</a>, and execute the following command (the command assumes you downloaded them to a directory named “guestbook-go”:</p><pre><code>kubectl create -f guestbook-go/
</code></pre><p>You’re done! Your application is now spread across all 3 zones. Prove it to yourself with the following commands:</p><pre><code>$ kubectl describe pod -l app=guestbook | grep Node
Node: kubernetes-minion-9vlv/10.240.0.5
Node: kubernetes-minion-281d/10.240.0.8
Node: kubernetes-minion-olsh/10.240.0.11

$ kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d 
kubernetes-minion-olsh --show-labels
NAME STATUS AGE LABELS
kubernetes-minion-9vlv Ready 34m       
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-a,kub
ernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-281d Ready 20m       
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-b,kub
ernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-olsh Ready 3m        
beta.kubernetes.io/instance-type=n1-standard-2,failure-domain.beta.kubernetes.
io/region=us-central1,failure-domain.beta.kubernetes.io/zone=us-central1-f,kub
ernetes.io/hostname=kubernetes-minion-olsh
</code></pre><p>Further, load-balancers automatically span all zones in a cluster; the guestbook-go example includes an example load-balanced service:</p><pre><code>$ kubectl describe service guestbook | grep LoadBalancer.Ingress
LoadBalancer Ingress: 130.211.126.21

ip=130.211.126.21

$ curl -s http://${ip}:3000/env | grep HOSTNAME
  &quot;HOSTNAME&quot;: &quot;guestbook-44sep&quot;,

$ (for i in `seq 20`; do curl -s http://${ip}:3000/env | grep HOSTNAME; done)  
</code></pre><pre><code>| sort | uniq
  &quot;HOSTNAME&quot;: &quot;guestbook-44sep&quot;,
  &quot;HOSTNAME&quot;: &quot;guestbook-hum5n&quot;,
  &quot;HOSTNAME&quot;: &quot;guestbook-ppm40&quot;,
</code></pre><p>The load balancer correctly targets all the pods, even though they’re in multiple zones.</p><h3 id=shutting-down-the-cluster-nbsp>Shutting down the cluster </h3><p>When you're done, clean up:</p><p>GCE:</p><pre><code>KUBERNETES_PROVIDER=gce KUBE_USE_EXISTING_MASTER=true 
KUBE_GCE_ZONE=us-central1-f kubernetes/cluster/kube-down.sh
KUBERNETES_PROVIDER=gce KUBE_USE_EXISTING_MASTER=true 
KUBE_GCE_ZONE=us-central1-b kubernetes/cluster/kube-down.sh
KUBERNETES_PROVIDER=gce KUBE_GCE_ZONE=us-central1-a 
kubernetes/cluster/kube-down.sh
</code></pre><p>EC2:</p><pre><code>KUBERNETES_PROVIDER=aws KUBE_USE_EXISTING_MASTER=true KUBE_AWS_ZONE=us-west-2c 
kubernetes/cluster/kube-down.sh
KUBERNETES_PROVIDER=aws KUBE_USE_EXISTING_MASTER=true KUBE_AWS_ZONE=us-west-2b 
kubernetes/cluster/kube-down.sh
KUBERNETES_PROVIDER=aws KUBE_AWS_ZONE=us-west-2a 
kubernetes/cluster/kube-down.sh
</code></pre><h3 id=conclusion-nbsp>Conclusion </h3><p>A core philosophy for Kubernetes is to abstract away the complexity of running highly available, distributed applications. As you can see here, other than a small amount of work at cluster spin-up time, all the complexity of launching application instances across multiple failure domains requires no additional work by application developers, as it should be. And we’re just getting started!</p><p>Please join our community and help us build the future of Kubernetes! There are many ways to participate. If you’re particularly interested in scalability, you’ll be interested in:</p><ul><li>Our federation <a href=https://kubernetes.slack.com/messages/sig-federation/>slack channel </a></li><li>The federation “Special Interest Group,” which meets every Thursday at 9:30 a.m. Pacific Time at <a href=https://plus.google.com/hangouts/_/google.com/ubernetes>SIG-Federation hangout </a></li></ul><p>And of course for more information about the project in general, go to <a href=http://www.kubernetes.io>www.kubernetes.io</a></p><p> -- <em>Quinton Hoole, Staff Software Engineer, Google, and Justin Santa Barbara</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d38901f59009438f4e70b9e6bf1222db>1000 nodes and beyond: updates to Kubernetes performance and scalability in 1.2</h1><div class="td-byline mb-4"><time datetime=2016-03-28 class=text-muted>Monday, March 28, 2016</time></div><p><em>Editor's note: this is the first in a <a href=https://kubernetes.io/blog/2016/03/five-days-of-kubernetes-12>series of in-depth posts</a> on what's new in Kubernetes 1.2</em></p><p>We're proud to announce that with the <a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-even-more-performance-upgrades-plus-easier-application-deployment-and-management>release of 1.2</a>, Kubernetes now supports 1000-node clusters, with a reduction of 80% in 99th percentile tail latency for most API operations. This means in just six months, we've increased our overall scale by 10 times while maintaining a great user experience — the 99th percentile pod startup times are less than 3 seconds, and 99th percentile latency of most API operations is tens of milliseconds (the exception being LIST operations, which take hundreds of milliseconds in very large clusters).</p><p>Words are fine, but nothing speaks louder than a demo. Check this out!</p><p>In the above video, you saw the cluster scale up to 10 M queries per second (QPS) over 1,000 nodes, including a rolling update, with zero downtime and no impact to tail latency. That’s big enough to be one of the top 100 sites on the Internet!</p><p>In this blog post, we’ll cover the work we did to achieve this result, and discuss some of our future plans for scaling even higher.</p><h3 id=methodology-nbsp>Methodology </h3><p>We benchmark Kubernetes scalability against the following Service Level Objectives (SLOs):</p><ol><li><strong>API responsiveness</strong> <sup><a href="https://www.blogger.com/blogger.g?blogID=112706738355446097#1">1</a></sup> 99% of all API calls return in less than 1s </li><li><strong>Pod startup time</strong> : 99% of pods and their containers (with pre-pulled images) start within 5s. 
We say Kubernetes scales to a certain number of nodes only if both of these SLOs are met. We continuously collect and report the measurements described above as part of the project test framework. This battery of tests breaks down into two parts: API responsiveness and Pod Startup Time.</li></ol><h3 id=api-responsiveness-for-user-level-abstractions-2-https-www-blogger-com-blogger-g-blogid-112706738355446097-2-nbsp>API responsiveness for user-level abstractions<a href="https://www.blogger.com/blogger.g?blogID=112706738355446097#2">2</a> </h3><p>Kubernetes offers high-level abstractions for users to represent their applications. For example, the ReplicationController is an abstraction representing a collection of <a href=/docs/user-guide/pods/>pods</a>. Listing all ReplicationControllers or listing all pods from a given ReplicationController is a very common use case. On the other hand, there is little reason someone would want to list all pods in the system — for example, 30,000 pods (1000 nodes with 30 pods per node) represent ~150MB of data (~5kB/pod * 30k pods). So this test uses ReplicationControllers.</p><p>For this test (assuming N to be number of nodes in the cluster), we:</p><ol><li><p>Create roughly 3xN ReplicationControllers of different sizes (5, 30 and 250 replicas), which altogether have 30xN replicas. We spread their creation over time (i.e. we don’t start all of them at once) and wait until all of them are running. </p></li><li><p>Perform a few operations on every ReplicationController (scale it, list all its instances, etc.), spreading those over time, and measuring the latency of each operation. This is similar to what a real user might do in the course of normal cluster operation. </p></li><li><p>Stop and delete all ReplicationControllers in the system. 
For results of this test see the “Metrics for Kubernetes 1.2” section below.</p></li></ol><p>For the v1.3 release, we plan to extend this test by also creating Services, Deployments, DaemonSets, and other API objects.</p><h3 id=pod-startup-end-to-end-latency-3-https-www-blogger-com-blogger-g-blogid-112706738355446097-3-nbsp>Pod startup end-to-end latency<a href="https://www.blogger.com/blogger.g?blogID=112706738355446097#3">3</a> </h3><p>Users are also very interested in how long it takes Kubernetes to schedule and start a pod. This is true not only upon initial creation, but also when a ReplicationController needs to create a replacement pod to take over from one whose node failed.</p><p>We (assuming N to be the number of nodes in the cluster):</p><ol><li><p>Create a single ReplicationController with 30xN replicas and wait until all of them are running. We are also running high-density tests, with 100xN replicas, but with fewer nodes in the cluster. </p></li><li><p>Launch a series of single-pod ReplicationControllers - one every 200ms. For each, we measure “total end-to-end startup time” (defined below). </p></li><li><p>Stop and delete all pods and replication controllers in the system. 
We define “total end-to-end startup time” as the time from the moment the client sends the API server a request to create a ReplicationController, to the moment when “running & ready” pod status is returned to the client via watch. That means that “pod startup time” includes the ReplicationController being created and in turn creating a pod, scheduler scheduling that pod, Kubernetes setting up intra-pod networking, starting containers, waiting until the pod is successfully responding to health-checks, and then finally waiting until the pod has reported its status back to the API server and then API server reported it via watch to the client.</p></li></ol><p>While we could have decreased the “pod startup time” substantially by excluding for example waiting for report via watch, or creating pods directly rather than through ReplicationControllers, we believe that a broad definition that maps to the most realistic use cases is the best for real users to understand the performance they can expect from the system.</p><h3 id=metrics-from-kubernetes-1-2-nbsp>Metrics from Kubernetes 1.2 </h3><p>So what was the result?We run our tests on Google Compute Engine, setting the size of the master VM based on the size of the Kubernetes cluster. In particular for 1000-node clusters we use a n1-standard-32 VM for the master (32 cores, 120GB RAM).</p><h4 id=api-responsiveness-nbsp>API responsiveness </h4><p>The following two charts present a comparison of 99th percentile API call latencies for the Kubernetes 1.2 release and the 1.0 release on 100-node clusters. (Smaller bars are better)</p><p><img src=https://lh5.googleusercontent.com/ea2yIJitkQn0aPP9TnamEo9YybBE-r9GfhtOcbu57wG1oZvIUD8rL5_crbQrlUob4oX4G8jY1F0W4Qx2U3B8dPwyhc2gcKGBIVbl_4lJg1xUg91-Kg5HSmrUj-g92gMx7WdmfZVx alt></p><p>We present results for LIST operations separately, since these latencies are significantly higher. Note that we slightly modified our tests in the meantime, so running current tests against v1.0 would result in higher latencies than they used to.</p><p><img src=https://lh6.googleusercontent.com/vMBFrgys0yEfFpftKN1Xg6iD5K9ONsBvYkLDe0lVQvtqYzmr8s7UG4O-hfIYd7otuqn1qeMFqfpHe0YyvOTYD8vS1A2M4JLf-Xi3dJZ6BEha9gM-bw5wkw3Ir5Wd2nnGuWEm8Egs alt></p><p>We also ran these tests against 1000-node clusters. Note: We did not support clusters larger than 100 on GKE, so we do not have metrics to compare these results to. However, customers have reported running on 1,000+ node clusters since Kubernetes 1.0.</p><p><img src=https://lh4.googleusercontent.com/tTQyEvEU2x8xFPKVMMc9fLxtugUduo4Vw2RdV4DLFTJr3wnGolBDUkB4vfk9hrPwRpw8usQK8cq0AlIn5pbIOwEtKmsa33OJOhiTx6oKmS4lAk6o9RkhwRQiT0b-_qhfJxiu2YOG alt></p><p>Since LIST operations are significantly larger, we again present them separately: All latencies, in both cluster sizes, are well within our 1 second SLO.</p><p><img src=https://lh4.googleusercontent.com/ch5-gMoeGWUDUjjMzurX-niiTMVg99wBx4rJI_tbeN-_3xSANB90BgfiNtP563a49TWQQOB-XXnoj3SOyxybQKMJmlhj5DUTGf47KNWJB3Ths_xLR9LCSjedksSjHYYWyXeb5eMb alt></p><h3 id=pod-startup-end-to-end-latency-nbsp>Pod startup end-to-end latency </h3><p>The results for “pod startup latency” (as defined in the “Pod-Startup end-to-end latency” section) are presented in the following graph. For reference we are presenting also results from v1.0 for 100-node clusters in the first part of the graph.</p><p><img src=https://lh4.googleusercontent.com/ecKvVkWZstjmKaitnEGHLFtauVdtG7v1zP1Zl0LZn05l47w7PgV_a0ufNWG-MSNFDWkbbvuSZJNNjLCmFFD_n-1JHbX4JeClteL6jMEqEnTY2y3TKyuKWQBNOXpK5J-zCKzk5Y5O alt></p><p>As you can see, we substantially reduced tail latency in 100-node clusters, and now deliver low pod startup latency up to the largest cluster sizes we have measured. It is noteworthy that the metrics for 1000-node clusters, for both API latency and pod startup latency, are generally better than those reported for 100-node clusters just six months ago!</p><h3 id=how-did-we-make-these-improvements-nbsp>How did we make these improvements? </h3><p>To make these significant gains in scale and performance over the past six months, we made a number of improvements across the whole system. Some of the most important ones are listed below.</p><ul><li>_ <strong>Created a “read cache” at the API server level </strong> _<br>(<a href=https://github.com/kubernetes/kubernetes/issues/15945>https://github.com/kubernetes/kubernetes/issues/15945</a> )</li></ul><p>Since most Kubernetes control logic operates on an ordered, consistent snapshot kept up-to-date by etcd watches (via the API server), a slight delay in that arrival of that data has no impact on the correct operation of the cluster. These independent controller loops, distributed by design for extensibility of the system, are happy to trade a bit of latency for an increase in overall throughput.</p><p>In Kubernetes 1.2 we exploited this fact to improve performance and scalability by adding an API server read cache. With this change, the API server’s clients can read data from an in-memory cache in the API server instead of reading it from etcd. The cache is updated directly from etcd via watch in the background. Those clients that can tolerate latency in retrieving data (usually the lag of cache is on the order of tens of milliseconds) can be served entirely from cache, reducing the load on etcd and increasing the throughput of the server. This is a continuation of an optimization begun in v1.1, where we added support for serving watch directly from the API server instead of etcd:<a href=https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/apiserver-watch.md>https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/apiserver-watch.md</a>. </p><p>Thanks to contributions from Wojciech Tyczynski at Google and Clayton Coleman and Timothy St. Clair at Red Hat, we were able to join careful system design with the unique advantages of etcd to improve the scalability and performance of Kubernetes. </p><ul><li><strong>Introduce a “Pod Lifecycle Event Generator” (PLEG) in the Kubelet</strong> (<a href=https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/pod-lifecycle-event-generator.md>https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/pod-lifecycle-event-generator.md</a>) </li></ul><p>Kubernetes 1.2 also improved density from a pods-per-node perspective — for v1.2 we test and advertise up to 100 pods on a single node (vs 30 pods in the 1.1 release). This improvement was possible because of diligent work by the Kubernetes community through an implementation of the Pod Lifecycle Event Generator (PLEG).</p><p>The Kubelet (the Kubernetes node agent) has a worker thread per pod which is responsible for managing the pod’s lifecycle. In earlier releases each worker would periodically poll the underlying container runtime (Docker) to detect state changes, and perform any necessary actions to ensure the node’s state matched the desired state (e.g. by starting and stopping containers). As pod density increased, concurrent polling from each worker would overwhelm the Docker runtime, leading to serious reliability and performance issues (including additional CPU utilization which was one of the limiting factors for scaling up).</p><p>To address this problem we introduced a new Kubelet subcomponent — the PLEG — to centralize state change detection and generate lifecycle events for the workers. With concurrent polling eliminated, we were able to lower the steady-state CPU usage of Kubelet and the container runtime by 4x. This also allowed us to adopt a shorter polling period, so as to detect and react to changes more quickly. 
<img src=https://lh4.googleusercontent.com/TFTdd6TXdbvIiN2yGWCrDDSxqcgt1Chqbs4kzxnQJSTYLtT0TGznNfW3s8ZELlnr2KmDZMP2_tpQa-d2SKRqDN2gnBIoDVaJQAtG-VlH1624mHFxi1fHSSBhY53noHLiDhmyKK1v alt> <img src=https://lh4.googleusercontent.com/sV7iqe-4aJsjcfXo2XenW3UhFkMNM-rVNMJAI00FNmD7dRg8wgK-NJQgRFtgeFsf5ekbNhoii6gSjupy8humK6jeHiSz93ZBmenHc72dYfqVRwgukWNH4LwmgcENRLyYx4lH1FOI alt></p><ul><li><p><strong>Improved scheduler throughput</strong> Kubernetes community members from CoreOS (Hongchao Deng and Xiang Li) helped to dive deep into the Kubernetes scheduler and dramatically improve throughput without sacrificing accuracy or flexibility. They cut total time to schedule 30,000 pods by nearly 1400%! You can read a great blog post on how they approached the problem here: <a href=https://coreos.com/blog/improving-kubernetes-scheduler-performance.html>https://coreos.com/blog/improving-kubernetes-scheduler-performance.html</a> </p></li><li><p><strong>A more efficient JSON parser</strong> Go’s standard library includes a flexible and easy-to-use JSON parser that can encode and decode any Go struct using the reflection API. But that flexibility comes with a cost — reflection allocates lots of small objects that have to be tracked and garbage collected by the runtime. Our profiling bore that out, showing that a large chunk of both client and server time was spent in serialization. Given that our types don’t change frequently, we suspected that a significant amount of reflection could be bypassed through code generation.</p></li></ul><p>After surveying the Go JSON landscape and conducting some initial tests, we found the <a href=https://github.com/ugorji/go>ugorji codec</a> library offered the most significant speedups - a 200% improvement in encoding and decoding JSON when using generated serializers, with a significant reduction in object allocations. After contributing fixes to the upstream library to deal with some of our complex structures, we switched Kubernetes and the go-etcd client library over. Along with some other important optimizations in the layers above and below JSON, we were able to slash the cost in CPU time of almost all API operations, especially reads. </p><ul><li><p>Other notable changes led to significant wins, including: </p><ul><li>Reducing the number of broken TCP connections, which were causing unnecessary new TLS sessions: <a href=https://github.com/kubernetes/kubernetes/issues/15664>https://github.com/kubernetes/kubernetes/issues/15664</a></li></ul></li></ul><p><img src=https://lh5.googleusercontent.com/JyjMps4dirKbPckos59nPIpX99nWyIFL0oWDQ2ykF888f9_N72FqmZsapU9qQf96p3nf1zEP-K2EWrHvMKiADCUuf9gM8tSpQihheHkA-Fa8TeTjksztrZlfmMlifq8okVUoVOWj alt=resync.png></p><ul><li>Improving the performance of ReplicationController in large clusters:<a href=https://github.com/kubernetes/kubernetes/issues/21672>https://github.com/kubernetes/kubernetes/issues/21672</a></li></ul><p>In both cases, the problem was debugged and/or fixed by Kubernetes community members, including Andy Goldstein and Jordan Liggitt from Red Hat, and Liang Mingqiang from NetEase. </p><h3 id=kubernetes-1-3-and-beyond-nbsp>Kubernetes 1.3 and Beyond </h3><p>Of course, our job is not finished. We will continue to invest in improving Kubernetes performance, as we would like it to scale to many thousands of nodes, just like Google’s <a href=http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf>Borg</a>. Thanks to our investment in testing infrastructure and our focus on how teams use containers in production, we have already identified the next steps on our path to improving scale. </p><p>On deck for Kubernetes 1.3: </p><ol><li><p> Our main bottleneck is still the API server, which spends the majority of its time just marshaling and unmarshaling JSON objects. We plan to <a href=https://github.com/kubernetes/kubernetes/pull/22600>add support for protocol buffers</a> to the API as an optional path for inter-component communication and for storing objects in etcd. Users will still be able to use JSON to communicate with the API server, but since the majority of Kubernetes communication is intra-cluster (API server to node, scheduler to API server, etc.) we expect a significant reduction in CPU and memory usage on the master. </p></li><li><p> Kubernetes uses labels to identify sets of objects; For example, identifying which pods belong to a given ReplicationController requires iterating over all pods in a namespace and choosing those that match the controller’s label selector. The addition of an efficient indexer for labels that can take advantage of the existing API object cache will make it possible to quickly find the objects that match a label selector, making this common operation much faster. </p></li><li><p>Scheduling decisions are based on a number of different factors, including spreading pods based on requested resources, spreading pods with the same selectors (e.g. from the same Service, ReplicationController, Job, etc.), presence of needed container images on the node, etc. Those calculations, in particular selector spreading, have many opportunities for improvement — see <a href=https://github.com/kubernetes/kubernetes/issues/22262>https://github.com/kubernetes/kubernetes/issues/22262</a> for just one suggested change. </p></li><li><p>We are also excited about the upcoming etcd v3.0 release, which was designed with Kubernetes use case in mind — it will both improve performance and introduce new features. Contributors from CoreOS have already begun laying the groundwork for moving Kubernetes to etcd v3.0 (see <a href=https://github.com/kubernetes/kubernetes/pull/22604>https://github.com/kubernetes/kubernetes/pull/22604</a>). 
While this list does not capture all the efforts around performance, we are optimistic we will achieve as big a performance gain as we saw going from Kubernetes 1.0 to 1.2. </p></li></ol><h3 id=conclusion-nbsp>Conclusion </h3><p>In the last six months we’ve significantly improved Kubernetes scalability, allowing v1.2 to run 1000-node clusters with the same excellent responsiveness (as measured by our SLOs) as we were previously achieving only on much smaller clusters. But that isn’t enough — we want to push Kubernetes even further and faster. Kubernetes v1.3 will improve the system’s scalability and responsiveness further, while continuing to add features that make it easier to build and run the most demanding container-based applications. </p><p>Please join our community and help us build the future of Kubernetes! There are many ways to participate. If you’re particularly interested in scalability, you’ll be interested in: </p><ul><li><p>Our <a href=https://kubernetes.slack.com/messages/sig-scale/>scalability slack channel</a></p></li><li><p>The scalability “Special Interest Group”, which meets every Thursday at 9 AM Pacific Time at <a href=https://plus.google.com/hangouts/_/google.com/k8scale-hangout>SIG-Scale hangout</a> 
 And of course for more information about the project in general, go to <a href=http://www.kubernetes.io/>www.kubernetes.io</a></p></li><li><p><em>Wojciech Tyczynski, Software Engineer, Google</em></p></li></ul><hr><p><a href=https://www.blogger.com/null><strong>1</strong></a>We exclude operations on “events” since these are more like system logs and are not required for the system to operate properly.<br><a href=https://www.blogger.com/null><strong>2</strong></a>This is test/e2e/load.go from the Kubernetes github repository.<br><a href=https://www.blogger.com/null><strong>3</strong></a>This is test/e2e/density.go test from the Kubernetes github repository <br><a href=https://www.blogger.com/null><strong>4</strong></a>We are looking into optimizing this in the next release, but for now using a smaller master can result in significant (order of magnitude) performance degradation. We encourage anyone running benchmarking against Kubernetes or attempting to replicate these findings to use a similarly sized master, or performance will suffer.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e4ab3e898fd10f3c8c0558d745f82a44>Five Days of Kubernetes 1.2</h1><div class="td-byline mb-4"><time datetime=2016-03-28 class=text-muted>Monday, March 28, 2016</time></div><p>The Kubernetes project has had some huge milestones over the past few weeks. We released <a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-even-more-performance-upgrades-plus-easier-application-deployment-and-management>Kubernetes 1.2</a>, had our <a href=https://kubecon.io/>first conference in Europe</a>, and were accepted into the <a href=https://cncf.io/>Cloud Native Computing Foundation</a>. While we catch our breath, we would like to take a moment to highlight some of the great work contributed by the community since our last milestone, just four months ago.</p><p>Our mission is to make building distributed systems easy and accessible for all. While Kubernetes 1.2 has LOTS of new features, there are a few that really highlight the strides we’re making towards that goal. Over the course of the next week, we’ll be publishing a series of in-depth posts covering what’s new, so come back daily this week to read about the new features that continue to make Kubernetes the easiest way to run containers at scale. Thanks, and stay tuned!</p><p>|
3/28
|
* <a href=https://kubernetes.io/blog/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12>1000 nodes and Beyond: Updates to Kubernetes performance and scalability in 1.2</a><br>* Guest post by Sysdig: <a href=https://kubernetes.io/blog/2016/03/how-container-metadata-changes-your-point-of-view>How container metadata changes your point of view</a> 
|
|
3/29
|
* <a href=https://kubernetes.io/blog/2016/03/building-highly-available-applications-using-kubernetes-new-multi-zone-clusters-aka-ubernetes-lite/>Building highly available applications using Kubernetes new multi-zone clusters (a.k.a. Ubernetes Lite")</a>
* Guest post by AppFormix: <a href=https://kubernetes.io/blog/2016/03/appformix-helping-enterprises>Helping Enterprises Operationalize Kubernetes</a>
|
|
3/30
|
* <a href=https://kubernetes.io/blog/2016/03/using-Spark-and-Zeppelin-to-process-Big-Data-on-Kubernetes>Using Spark and Zeppelin to process big data on Kubernetes 1.2</a>.  
|
|
3/31
|
* <a href=https://kubernetes.io/blog/2016/03/kubernetes-1-2-and-simplifying-advanced-networking-with-ingress/>Kubernetes 1.2 and simplifying advanced networking with Ingress</a>
|
|
4/1
|
* <a href=https://kubernetes.io/blog/2016/04/using-deployment-objects-with>Using Deployment Objects with Kubernetes 1.2</a>
|
|
BONUS
|
* ConfigMap API <a href=https://kubernetes.io/blog/2016/04/configuration-management-with-containers>Configuration management with Containers</a>
|</p><p>You can follow us on twitter here <a href=https://twitter.com/kubernetesio>@Kubernetesio</a></p><p><em>--David Aronchick, Senior Product Manager for Kubernetes, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b659de13c7fd55ca0915af25e1a707>How container metadata changes your point of view</h1><div class="td-byline mb-4"><time datetime=2016-03-28 class=text-muted>Monday, March 28, 2016</time></div><p><em>Today’s guest post is brought to you by Apurva Davé, VP of Marketing at Sysdig, who’ll discuss using Kubernetes metadata & Sysdig to understand what’s going on in your Kubernetes cluster. </em></p><p>Sure, metadata is a fancy word. It actually means “data that describes other data.” While that definition isn’t all that helpful, it turns out metadata itself is especially helpful in container environments. When you have any complex system, the availability of metadata helps you sort and process the variety of data coming out of that system, so that you can get to the heart of an issue with less headache.</p><p>In a Kubernetes environment, metadata can be a crucial tool for organizing and understanding the way containers are orchestrated across your many services, machines, availability zones or (in the future) multiple clouds. This metadata can also be consumed by other services running on top of your Kubernetes system and can help you manage your applications.</p><p>We’ll take a look at some examples of this below, but first...</p><h3 id=heading></h3><p>A quick intro to Kubernetes metadata 
Kubernetes metadata is abundant in the form of <a href=/docs/user-guide/labels/><em>labels</em></a> and <a href=/docs/user-guide/annotations/><em>annotations</em></a>. Labels are designed to be identifying metadata for your infrastructure, whereas annotations are designed to be non-identifying. For both, they’re simply generic key:value pairs that look like this:</p><pre><code>&quot;labels&quot;: {
  &quot;key1&quot; : &quot;value1&quot;,
  &quot;key2&quot; : &quot;value2&quot;
}
</code></pre><p>Labels are not designed to be unique; you can expect any number of objects in your environment to carry the same label, and you can expect that an object could have many labels.</p><p>What are some examples of labels you might use? Here are just a few. WARNING: Once you start, you might find more than a few ways to use this functionality!</p><ul><li>Environment: Dev, Prod, Test, UAT </li><li>Customer: Cust A, Cust B, Cust C </li><li>Tier: Frontend, Backend </li><li>App: Cache, Web, Database, Auth </li></ul><p>In addition to custom labels you might define, Kubernetes also automatically applies labels to your system with useful metadata. Default labels supply key identifying information about your entire Kubernetes hierarchy: Pods, Services, Replication Controllers,and Namespaces.</p><h3 id=putting-your-metadata-to-work-nbsp>Putting your metadata to work </h3><p>Once you spend a little time with Kubernetes, you’ll see that labels have one particularly powerful application that makes them essential:</p><p><strong>Kubernetes labels allows you to easily move between a “physical” view of your hosts and containers, and a “logical” view of your applications and micro-services. </strong></p><p>At its core, a platform like Kubernetes is designed to orchestrate the optimal use of underlying physical resources. This is a powerful way to consume private or public cloud resources very efficiently, and sometimes you need to visualize those physical resources. In reality, however, most of the time you care about the performance of the service first and foremost.</p><p>But in a Kubernetes world, achieving that high utilization means a service’s containers may be scattered all over the place! So how do you actually measure the service’s performance? That’s where the metadata comes in. With Kubernetes metadata, you can create a deep understanding of your service’s performance, regardless of where the underlying containers are physically located.</p><h3 id=paint-me-a-picture-nbsp>Paint me a picture </h3><p>Let’s look at a quick example to make this more concrete: monitoring your application. Let’s work with a small, 3 node deployment running on GKE. For visualizing the environment we’ll use Sysdig Cloud. Here’s a list of the nodes — note the “gke” prepended to the name of each host. We see some basic performance details like CPU, memory and network.</p><p><a href=https://1.bp.blogspot.com/-NSkvJcEj0L0/VvmM1eWSlLI/AAAAAAAAA5w/YupjdMPz8aEmXjSt8xyZJVOoa4osyLYBg/s1600/sysdig1.png><img src=https://1.bp.blogspot.com/-NSkvJcEj0L0/VvmM1eWSlLI/AAAAAAAAA5w/YupjdMPz8aEmXjSt8xyZJVOoa4osyLYBg/s640/sysdig1.png alt></a></p><p>Each of these hosts has a number of containers running on it. Drilling down on the hosts, we see the containers associated with each:</p><p><a href=https://2.bp.blogspot.com/-7hrB4V8zAkg/VvmJRpLcQQI/AAAAAAAAAYA/Fz7pul56ZQ8Xus6u4zHBFAwe8HJesyeRw/s1600/Kubernetes%2BMetadata%2BBlog%2B2.png><img src=https://2.bp.blogspot.com/-7hrB4V8zAkg/VvmJRpLcQQI/AAAAAAAAAYA/Fz7pul56ZQ8Xus6u4zHBFAwe8HJesyeRw/s640/Kubernetes%2BMetadata%2BBlog%2B2.png alt></a></p><p>Simply scanning this list of containers on a single host, I don’t see much organization to the responsibilities of these objects. For example, some of these containers run Kubernetes services (like kube-ui) and we presume others have to do with the application running (like javaapp.x).</p><p>Now let’s use some of the metadata provided by Kubernetes to take an application-centric view of the system. Let’s start by creating a hierarchy of components based on labels, in this order:</p><p><code>Kubernetes namespace -> replication controller -> pod -> container</code></p><p>This aggregates containers at corresponding levels based on the above labels. In the app UI below, this aggregation and hierarchy are shown in the grey “grouping” bar above the data about our hosts. As you can see, we have a “prod” namespace with a group of services (replication controllers) below it. Each of those replication controllers can then consist of multiple pods, which are in turn made up of containers.</p><p><a href=https://4.bp.blogspot.com/-7JuCC5kuF6U/VvmJzM4UYmI/AAAAAAAAAYE/iIhR19aVCpAaVFRKujflMo047PmzP0DpA/s1600/Kubernetes%2BMetadata%2BBlog%2B3.png><img src=https://4.bp.blogspot.com/-7JuCC5kuF6U/VvmJzM4UYmI/AAAAAAAAAYE/iIhR19aVCpAaVFRKujflMo047PmzP0DpA/s640/Kubernetes%2BMetadata%2BBlog%2B3.png alt></a></p><p>In addition to organizing containers via labels, this view also aggregates metrics across relevant containers, giving a singular view into the performance of a namespace or replication controller.</p><p><strong>In other words, with this aggregated view based on metadata, you can now start by monitoring and troubleshooting services, and drill into hosts and containers only if needed. </strong></p><p>Let’s do one more thing with this environment — let’s use the metadata to create a visual representation of services and the topology of their communications. Here you see our containers organized by services, but also a map-like view that shows you how these services relate to each other.</p><p><a href=https://1.bp.blogspot.com/-URGCJheccOE/Vvmeh7VnzgI/AAAAAAAAA6I/WIz3pmcrk9A5sgadIU5J8lVObg32HFlQQ/s1600/sysdig4.png><img src=https://1.bp.blogspot.com/-URGCJheccOE/Vvmeh7VnzgI/AAAAAAAAA6I/WIz3pmcrk9A5sgadIU5J8lVObg32HFlQQ/s640/sysdig4.png alt></a></p><p>The boxes represent services that are aggregates of containers (the number in the upper right of each box tells you how many containers), and the lines represent communications between services and their latencies.</p><p>This kind of view provides yet another logical, instead of physical, view of how these application components are working together. From here I can understand service performance, relationships and underlying resource consumption (CPU in this example).</p><h3 id=metadata-love-it-use-it-nbsp>Metadata: love it, use it </h3><p>This is a pretty quick tour of metadata, but I hope it inspires you to spend a little time thinking about the relevance to your own system and how you could leverage it. Here we built a pretty simple example — apps and services — but imagine collecting metadata across your apps, environments, software components and cloud providers. You could quickly assess performance differences across any slice of this infrastructure effectively, all while Kubernetes is efficiently scheduling resource usage.</p><p>Get started with metadata for visualizing these resources today, and in a followup post we’ll talk about the power of adaptive alerting based on metadata.</p><p><em>-- Apurva Davé is a closet Kubernetes fanatic, loves data, and oh yeah is also the VP of Marketing at Sysdig.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-0c81a9189333c197f7d01a8f12182057>Scaling neural network image classification using Kubernetes with TensorFlow Serving</h1><div class="td-byline mb-4"><time datetime=2016-03-23 class=text-muted>Wednesday, March 23, 2016</time></div><p>In 2011, Google developed an internal deep learning infrastructure called <a href=http://research.google.com/pubs/pub40565.html>DistBelief</a>, which allowed Googlers to build ever larger <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural networks</a> and scale training to thousands of cores. Late last year, Google <a href=http://googleresearch.blogspot.com/2015/11/tensorflow-googles-latest-machine_9.html>introduced TensorFlow</a>, its second-generation machine learning system. TensorFlow is general, flexible, portable, easy-to-use and, most importantly, developed with the open source community.</p><p><a href=https://4.bp.blogspot.com/-PDRpnk823Ps/VvHJH3vIyKI/AAAAAAAAA4g/adIWZPfa2W4ObtIaWNbhpl8UyIwk9R7xg/s1600/tensorflowserving-4.png><img src=https://4.bp.blogspot.com/-PDRpnk823Ps/VvHJH3vIyKI/AAAAAAAAA4g/adIWZPfa2W4ObtIaWNbhpl8UyIwk9R7xg/s320/tensorflowserving-4.png alt></a></p><p>The process of introducing machine learning into your product involves creating and training a model on your dataset, and then pushing the model to production to serve requests. In this blog post, we’ll show you how you can use <a href=http://kubernetes.io/>Kubernetes</a> with <a href=http://googleresearch.blogspot.com/2016/02/running-your-models-in-production-with.html>TensorFlow Serving</a>, a high performance, open source serving system for machine learning models, to meet the scaling demands of your application.</p><p>Let’s use image classification as an <a href=https://tensorflow.github.io/serving/serving_inception>example</a>. Suppose your application needs to be able to correctly identify an image across a set of categories. For example, given the cute puppy image below, your system should classify it as a retriever.</p><p>| <a href=https://3.bp.blogspot.com/-rUuOetJfoLc/VvHJHgDYusI/AAAAAAAAA4c/qO9xhVk4iH8EhrSqt3eZbqNGVQXH5fmCg/s1600/tensorflowserving-2.png><img src=https://3.bp.blogspot.com/-rUuOetJfoLc/VvHJHgDYusI/AAAAAAAAA4c/qO9xhVk4iH8EhrSqt3eZbqNGVQXH5fmCg/s320/tensorflowserving-2.png alt></a> |
| Image via <a href=https://commons.wikimedia.org/wiki/File:Golde33443.jpg>Wikipedia</a> |</p><p>You can implement image classification with TensorFlow using the <a href=http://googleresearch.blogspot.com/2016/03/train-your-own-image-classifier-with.html>Inception-v3 model</a> trained on the data from the <a href=http://www.image-net.org/>ImageNet dataset</a>. This dataset contains images and their labels, which allows the TensorFlow learner to train a model that can be used for by your application in production.</p><p><a href=https://4.bp.blogspot.com/-oaJYNPqiqIc/VvHJH2Z19cI/AAAAAAAAA4k/xq8m0kqRIOUewTZLDvzjPh6YLHG4MxdSQ/s1600/tensorflowserving-1.png><img src=https://4.bp.blogspot.com/-oaJYNPqiqIc/VvHJH2Z19cI/AAAAAAAAA4k/xq8m0kqRIOUewTZLDvzjPh6YLHG4MxdSQ/s640/tensorflowserving-1.png alt></a>
Once the model is trained and <a href=https://github.com/tensorflow/serving/blob/master/tensorflow_serving/session_bundle/exporter.py>exported</a>, <a href=https://tensorflow.github.io/serving/>TensorFlow Serving</a> uses the model to perform inference — predictions based on new data presented by its clients. In our example, clients submit image classification requests over <a href=http://www.grpc.io/>gRPC</a>, a high performance, open source RPC framework from Google.</p><p><a href=https://4.bp.blogspot.com/-g2S3V47h7BY/VvHJIkBlTiI/AAAAAAAAA4o/wISpFzB6kvIZxJHlnmM7-XYzZYl1YFfDA/s1600/tensorflowserving-5.png><img src=https://4.bp.blogspot.com/-g2S3V47h7BY/VvHJIkBlTiI/AAAAAAAAA4o/wISpFzB6kvIZxJHlnmM7-XYzZYl1YFfDA/s320/tensorflowserving-5.png alt></a></p><p>Inference can be very resource intensive. Our server executes the following TensorFlow graph to process every classification request it receives. The Inception-v3 model has over 27 million parameters and runs 5.7 billion floating point operations per inference.</p><p>| <a href=https://2.bp.blogspot.com/-Gcb6gxzqDkE/VvHJHE7yD3I/AAAAAAAAA4Y/4EZD83OV_8goqodV2pcaQKYeinokf9UuA/s1600/tensorflowserving-3.png><img src=https://2.bp.blogspot.com/-Gcb6gxzqDkE/VvHJHE7yD3I/AAAAAAAAA4Y/4EZD83OV_8goqodV2pcaQKYeinokf9UuA/s640/tensorflowserving-3.png alt></a> |
| Schematic diagram of Inception-v3 |</p><p>Fortunately, this is where Kubernetes can help us. Kubernetes distributes inference request processing across a cluster using its <a href=/docs/user-guide/load-balancer/>External Load Balancer</a>. Each <a href=/docs/user-guide/pods/>pod</a> in the cluster contains a <a href=https://tensorflow.github.io/serving/docker>TensorFlow Serving Docker image</a> with the TensorFlow Serving-based gRPC server and a trained Inception-v3 model. The model is represented as a <a href=https://github.com/tensorflow/serving/blob/master/tensorflow_serving/session_bundle/README.md>set of files</a> describing the shape of the TensorFlow graph, model weights, assets, and so on. Since everything is neatly packaged together, we can dynamically scale the number of replicated pods using the <a href=/docs/user-guide/replication-controller/operations/>Kubernetes Replication Controller</a> to keep up with the service demands.</p><p>To help you try this out yourself, we’ve written a <a href=https://tensorflow.github.io/serving/serving_inception>step-by-step tutorial</a>, which shows you how to create the TensorFlow Serving Docker container to serve the Inception-v3 image classification model, configure a Kubernetes cluster and run classification requests against it. We hope this will make it easier for you to integrate machine learning into your own applications and scale it with Kubernetes! To learn more about TensorFlow Serving, check out <a href=http://tensorflow.github.io/serving>tensorflow.github.io/serving</a>. </p><ul><li><em>Fangwei Li, Software Engineer, Google</em></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-59cd1c44d61e1d516de74b705e94160b>Kubernetes 1.2: Even more performance upgrades, plus easier application deployment and management</h1><div class="td-byline mb-4"><time datetime=2016-03-17 class=text-muted>Thursday, March 17, 2016</time></div><p>Today we released Kubernetes 1.2. This release represents significant improvements for large organizations building distributed systems. Now with over 680 unique contributors to the project, this release represents our largest yet.</p><p>From the beginning, our mission has been to make building distributed systems easy and accessible for all. With the Kubernetes 1.2 release we’ve made strides towards our goal by increasing scale, decreasing latency and overall simplifying the way applications are deployed and managed. Now, developers at organizations of all sizes can build production scale apps more easily than ever before. </p><h3 id=what-s-new-nbsp>What’s new: </h3><ul><li><p><strong>Significant scale improvements</strong>. Increased cluster scale by 400% to 1,000 nodes and 30,000 containers per cluster.</p></li><li><p><strong>Simplified application deployment and management</strong>. </p><ul><li>Dynamic Configuration (via the ConfigMap API) enables applications to pull their configuration when they run rather than packaging it in at build time. </li><li>Turnkey Deployments (via the Beta Deployment API) let you declare your application and Kubernetes will do the rest. It handles versioning, multiple simultaneous rollouts, aggregating status across all pods, maintaining application availability and rollback. </li></ul></li><li><p><strong>Automated cluster management</strong> :</p><ul><li>Improved reliability through cross-zone failover and multi-zone scheduling</li><li>Simplified One-Pod-Per-Node Applications (via the Beta DaemonSet API) allows you to schedule a service (such as a logging agent) that runs one, and only one, pod per node. </li><li>TLS and L7 support (via the Beta Ingress API) provides a straightforward way to integrate into custom networking environments by supporting TLS for secure communication and L7 for http-based traffic routing. </li><li>Graceful Node Shutdown (aka Node Drain) takes care of transitioning pods off a node and allowing it to be shut down cleanly. </li><li>Custom Metrics for Autoscaling now supports custom metrics, allowing you to specify a set of signals to indicate autoscaling pods. </li></ul></li><li><p><strong>New GUI</strong> allows you to get started quickly and enables the same functionality found in the CLI for a more approachable and discoverable interface.</p></li></ul><p><a href=https://1.bp.blogspot.com/-_xwIlw1gJo4/VusiOuHRzCI/AAAAAAAAA3s/NDN91tgdypQE7iBjzTCWlO7vzfDNt_guw/s1600/k8-1.2-release.png><img src=https://1.bp.blogspot.com/-_xwIlw1gJo4/VusiOuHRzCI/AAAAAAAAA3s/NDN91tgdypQE7iBjzTCWlO7vzfDNt_guw/s640/k8-1.2-release.png alt></a></p><ul><li><strong>And many more</strong>. For a complete list of updates, see the <a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.2.0>release notes on github</a>. </li></ul><h4 id=community-nbsp>Community </h4><p>All these improvements would not be possible without our enthusiastic and global community. The momentum is astounding. We’re seeing over 400 pull requests per week, a 50% increase since the previous 1.1 release. There are meetups and conferences discussing Kubernetes nearly every day, on top of the 85 Kubernetes related <a href=http://www.meetup.com/topics/kubernetes/>meetup groups</a> around the world. We’ve also seen significant participation in the community in the form of Special Interest Groups, with 18 active SIGs that cover topics from AWS and OpenStack to big data and scalability, to get involved <a href=https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)>join or start a new SIG</a>. Lastly, we’re proud that Kubernetes is the first project to be accepted to the Cloud Native Computing Foundation (CNCF), read more about the announcement <a href=https://cncf.io/news/announcement/2016/03/cloud-native-computing-foundation-accepts-kubernetes-first-hosted-projec-0>here</a>. </p><h4 id=documentation-nbsp>Documentation </h4><p>With Kubernetes 1.2 comes a relaunch of our website at <a href=http://kubernetes.io/>kubernetes.io</a>. We’ve slimmed down the docs contribution process so that all you have to do is fork/clone and send a PR. And the site works the same whether you’re staging it on your laptop, on github.io, or viewing it in production. It’s a pure GitHub Pages project; no scripts, no plugins. </p><p>From now on, our docs are at a new repo: <a href=https://github.com/kubernetes/kubernetes.github.io>https://github.com/kubernetes/kubernetes.github.io</a></p><p>To entice you even further to contribute, we’re also announcing our new bounty program. For every “bounty bug” you address with a merged pull request, we offer the listed amount in credit for Google Cloud Platform services. Just look for <a href="https://github.com/kubernetes/kubernetes.github.io/issues?q=is%3Aissue+is%3Aopen+label%3ABounty">bugs labeled “Bounty” in the new repo</a> for more details. </p><h4 id=roadmap-nbsp>Roadmap </h4><p>All of our work is done in the open, to learn the latest about the project j<a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>oin the weekly community meeting</a> or <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">watch a recorded hangout</a>. In keeping with our major release schedule of every three to four months, here are just a few items that are in development for <a href=https://github.com/kubernetes/kubernetes/wiki/Release-1.3>next release and beyond</a>: </p><ul><li>Improved stateful application support (aka Pet Set) </li><li>Cluster Federation (aka Ubernetes) </li><li>Even more (more!) performance improvements </li><li>In-cluster IAM </li><li>Cluster autoscaling </li><li>Scheduled job </li><li>Public dashboard that allows for nightly test runs across multiple cloud providers </li><li>Lots, lots more! 
Kubernetes 1.2 is available for download at <a href=http://get.k8s.io/>get.k8s.io</a> and via the open source repository hosted on <a href=https://github.com/kubernetes/kubernetes>GitHub</a>. To get started with Kubernetes try our new <a href=/docs/hellonode/>Hello World app</a>. </li></ul><h4 id=connect-nbsp>Connect </h4><p>We’d love to hear from you and see you participate in this growing community: </p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Post questions (or answer questions) on <a href=https://stackoverflow.com/questions/tagged/kubernetes>Stackoverflow</a> </li><li> Connect with the community on <a href=http://slack.kubernetes.io/>Slack</a> </li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates </li></ul><p>Thank you for your support! </p><p> - <em>David Aronchick, Senior Product Manager for Kubernetes, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d6b1b3e51eb1319f61cd392193402025>ElasticBox introduces ElasticKube to help manage Kubernetes within the enterprise</h1><div class="td-byline mb-4"><time datetime=2016-03-11 class=text-muted>Friday, March 11, 2016</time></div><p>Today’s guest post is brought to you by Brannan Matherson, from ElasticBox, who’ll discuss a new open source project to help standardize container deployment and management in enterprise environments. This highlights the advantages of authentication and user management for containerized applications</p><p>I’m delighted to share some exciting work that we’re doing at ElasticBox to contribute to the open source community regarding the rapidly changing advancements in container technologies. Our team is kicking off a new initiative called <a href=http://elastickube.com/>ElasticKube</a> to help solve the problem of challenging container management scenarios within the enterprise. This project is a native container management experience that is specific to Kubernetes and leverages automation to provision clusters for containerized applications based on the latest release of Kubernetes 1.2.  </p><p>I’ve talked to many enterprise companies, both large and small, and the plethora of cloud offering capabilities is often confusing and makes the evaluation process very difficult, so why Kubernetes? Of the large public cloud players - Amazon Web Services, Microsoft Azure, and Google Cloud Platform - Kubernetes is poised to take an innovative leadership role in framing the container management space. The Kubernetes platform does not restrict or dictate any given technical approach for containers, but encourages the community to collectively solve problems as this container market still takes form.  With a proven track record of supporting open source efforts, Kubernetes platform allows my team and me to actively contribute to this fundamental shift in the IT and developer world.</p><p>We’ve chosen Kubernetes, not just for the core infrastructure services, but also the agility of Kubernetes to leverage the cluster management layer across any cloud environment - GCP, AWS, Azure, vSphere, and Rackspace. Kubernetes also provides a huge benefit for users to run clusters for containers locally on many popular technologies such as: Docker, Vagrant (and VirtualBox), CoreOS, Mesos and more.  This amount of choice enables our team and many others in the community to consider solutions that will be viable for a wide range of enterprise scenarios. In the case of ElasticKube, we’re pleased with Kubernetes 1.2 which includes the full release of the deployment API. This provides the ability for us to perform seamless rolling updates of containerized applications that are running in production. In addition, we’ve been able to support new resource types like ConfigMaps and Horizontal Pod Autoscalers.</p><p>Fundamentally, ElasticKube delivers a web console for which compliments Kubernetes for users managing their clusters. The initial experience incorporates team collaboration, lifecycle management and reporting, so organizations can efficiently manage resources in a predictable manner. Users will see an ElasticKube portal that takes advantage of the infrastructure abstraction that enables users to run a container that has already been built. With ElasticKube assuming the cluster has been deployed, the overwhelming value is to provide visibility into who did what and define permissions for access to the cluster with multiple containers running on them. Secondly, by partitioning clusters into namespaces, authorization management is more effective. Finally, by empowering users to build a set of reusable templates in a modern portal, ElasticKube provides a vehicle for delivering a self-service template catalog that can be stored in GitHub (for instance, using Helm templates) and deployed easily.</p><p>ElasticKube enables organizations to accelerate adoption by developers, application operations and traditional IT operations teams and shares a mutual goal of increasing developer productivity, driving efficiency in container management and promoting the use of microservices as a modern application delivery methodology. When leveraging ElasticKube in your environment, users need to ensure the following technologies are configured appropriately to guarantee everything runs correctly:</p><ul><li>Configure Google Container Engine (GKE) for cluster installation and management</li><li>Use Kubernetes to provision the infrastructure and clusters for containers  </li><li>Use your existing tools of choice to actually build your containers</li><li>Use ElasticKube to run, deploy and manage your containers and services</li></ul><p><a href=http://cl.ly/0i3M2L3Q030z/Image%202016-03-11%20at%209.49.12%20AM.png><img src=https://cl.ly/0i3M2L3Q030z/Image%202016-03-11%20at%209.49.12%20AM.png alt></a></p><p>Getting Started with Kubernetes and ElasticKube</p><p>(this is a 3min walk through video with the following topics)</p><ol><li>Deploy ElasticKube to a Kubernetes cluster</li><li>Configuration</li><li>Admin: Setup and invite a user</li><li>Deploy an instance</li></ol><p>Hear What Others are Saying</p><p>“Kubernetes has provided us the level of sophistication required for enterprises to manage containers across complex networking environments and the appropriate amount of visibility into the application lifecycle.  Additionally, the community commitment and engagement has been exceptional, and we look forward to being a major contributor to this next wave of modern cloud computing and application management.”  </p><p><em>~Alberto Arias Maestro, Co-founder and Chief Technology Officer, ElasticBox</em></p><p><em>-- Brannan Matherson, Head of Product Marketing, ElasticBox</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-6f7e2e76af47eb65983039de87289b56>Kubernetes in the Enterprise with Fujitsu’s Cloud Load Control</h1><div class="td-byline mb-4"><time datetime=2016-03-11 class=text-muted>Friday, March 11, 2016</time></div><p>Today’s guest post is by Florian Walker, Product Manager at Fujitsu and working on Cloud Load Control, an offering focused on the usage of Kubernetes in an enterprise context. Florian tells us what potential Fujitsu sees in Kubernetes, and how they make it accessible to enterprises.</p><p>Earlier this year, Fujitsu released its Kubernetes-based offering Fujitsu ServerView<a href=http://www.fujitsu.com/software/clc/>Cloud Load Control</a> (CLC) to the public. Some might be surprised since Fujitsu’s reputation is not necessarily related to software development, but rather to hardware manufacturing and IT services. As a long-time member of the Linux foundation and founding member of the ​Open Container Initiative and the Cloud Native Computing Foundation, Fujitsu does not only build software, but is committed to open source software, and contributes to several projects, including Kubernetes. But we not only believe in Kubernetes as an open source project, we also chose it as the core of our offering, because it provides the best balance of feature set, resource requirements and complexity to run distributed applications at scale.</p><p>Today, we want to take you on a short tour explaining the background of our offering, why we think Kubernetes is the right fit for your customers and what value Cloud Load Control provides on top of it.
<strong>A long long time ago…</strong></p><p>In mid 2014 we looked at the challenges enterprises are facing in the context of digitization, where traditional enterprises experience that more and more competitors from the IT sector are pushing into the core of their markets. A big part of Fujitsu’s customers are such traditional businesses, so we considered how we could help them and came up with three basic principles:</p><ul><li>Decouple applications from infrastructure - Focus on where the value for the customer is: the application.</li><li>Decompose applications - Build applications from smaller, loosely coupled parts. Enable reconfiguration of those parts depending on the needs of the business. Also encourage innovation by low-cost experiments.</li><li>Automate everything - Fight the increasing complexity of the first two points by introducing a high degree of automation.</li></ul><p>We found that Linux containers themselves cover the first point and touch the second. But at this time there was little support for creating distributed applications and running them managed automatically. We found Kubernetes as the missing piece.
<strong>Not a free lunch</strong></p><p>The general approach of Kubernetes in managing containerized workload is convincing, but as we looked at it with the eyes of customers, we realized that it’s not a free lunch. Many  customers are medium-sized companies whose core business is often bound to strict data protection regulations. The top three requirements we identified are:</p><ul><li>On-premise deployments (with the option for hybrid scenarios)</li><li>Efficient operations as part of a (much) bigger IT infrastructure</li><li>Enterprise-grade support, potentially on global scale</li></ul><p>We created Cloud Load Control with these requirements in mind. It is basically a distribution of Kubernetes targeted for on-premise use, primarily focusing on operational aspects of container infrastructure. We are committed to work with the community, and contribute all relevant changes and extensions upstream to the Kubernetes project.
<strong>On-premise deployments</strong></p><p>As Kubernetes core developer Tim Hockin often puts it in his<a href=https://speakerdeck.com/thockin>talks</a>, Kubernetes is "a story with two parts" where setting up a Kubernetes cluster is not the easy part and often challenging due to variations in infrastructure. This is in particular true when it comes to production-ready deployments of Kubernetes. In the public cloud space, a customer could choose a service like Google Container Engine (GKE) to do this job. Since customers have less options on-premise, often they have to consider the deployment by themselves.</p><p>Cloud Load Control addresses these issues. It enables customers to reliably and readily provision a production grade Kubernetes clusters on their own infrastructure, with the following benefits:</p><ul><li>Proven setup process, lowers risk of problems while setting up the cluster</li><li>Reduction of provisioning time to minutes</li><li>Repeatable process, relevant especially for large, multi-tenant environments</li></ul><p>Cloud Load Control delivers these benefits for a range of platforms, starting from selected OpenStack distributions in the first versions of Cloud Load Control, and successively adding more platforms depending on customer demand.  We are especially excited about the option to remove the virtualization layer and support Kubernetes bare-metal on Fujitsu servers in the long run. By removing a layer of complexity, the total cost to run the system would be decreased and the missing hypervisor would increase performance.</p><p>Right now we are in the process of contributing a generic provider to set up Kubernetes on OpenStack. As a next step in driving multi-platform support, Docker-based deployment of Kubernetes seems to be crucial. We plan to contribute to this feature to ensure it is going to be Beta in Kubernetes 1.3.
<strong>Efficient operations</strong></p><p>Reducing operation costs is the target of any organization providing IT infrastructure. This can be achieved by increasing the efficiency of operations and helping operators to get their job done. Considering large-scale container infrastructures, we found it is important to differentiate between two types of operations:</p><ul><li>Platform-oriented, relates to the overall infrastructure, often including various systems, one of which might be Kubernetes.</li><li>Application-oriented, focusses rather on a single, or a small set of applications deployed on Kubernetes.</li></ul><p>Kubernetes is already great for the application-oriented part. Cloud Load Control was created to help platform-oriented operators to efficiently manage Kubernetes as part of the overall infrastructure and make it easy to execute Kubernetes tasks relevant to them.</p><p>The first version of Cloud Load Control provides a user interface integrated in the OpenStack Horizon dashboard which enables the Platform ops to create and manage their Kubernetes clusters.</p><p><img src=https://lh3.googleusercontent.com/s_ZBCL1arPc3SiO2vW6OYcNIp0ZPPoNboFQX1ly0ZB_m8LTJ5krzQZjR9_xyHBHc6k6KRHpTmzmoidUqhDiV4f6SMRR7wmb0-9CgXo1TRQQFa-4mwlOfri6QieHPYdHVg2B0K2oE alt></p><p>Clusters are treated as first-class citizens of OpenStack. Their creation is as simple as the creation of a virtual machine. Operators do not need to learn a new system or method of provisioning, and the self-service approach enables large organizations to rapidly provide the Kubernetes infrastructure to their tenants.</p><p>An intuitive UI is crucial for the simplification of operations. This is why we heavily contributed to the<a href=https://github.com/kubernetes/dashboard>Kubernetes Dashboard</a> project and ship it in Cloud Load Control. Especially for operators who don’t know the Kubernetes CLI by heart, because they have to care about other systems too, a great UI is perfectly suitable to get typical operational tasks done, such as checking the health of the system or deploying a new application.</p><p>Monitoring is essential. With the dashboard, it is possible to get insights on cluster level. To ensure that the OpenStack operators have a deep understanding of their platform, we will soon add an integration with<a href=https://wiki.openstack.org/wiki/Monasca>Monasca</a>, OpenStack’s monitoring-as-a-service project, so metrics of Kubernetes can be analyzed together with OpenStack metrics from a single point of access.
<strong>Quality and enterprise-grade support</strong></p><p>As a Japanese company, quality and customer focus have the highest priority in every product and service we ship. This is where the actual value of Cloud Cloud Control comes from: it provides a specific version of the open source software which has been intensively tested and hardened to ensure stable operations on a particular set of platforms.</p><p>Acknowledging that container technology and Kubernetes is new territory for a lot of enterprises, expert assistance is the key for setting up and running a production-grade container infrastructure. Cloud Load Control comes with a support service leveraging Fujitsu’s proven support structure. This enables support also for customers operating Kubernetes in different regions of the world, like Europe and Japan, as part of the same offering.
<strong>Conclusion</strong></p><p>2014 seems to be light years away, we believe the decision for Kubernetes was the right one. It is built from the ground-up to enable the creation of container-based, distributed applications, and best supports this use case.</p><p>With Cloud Load Control, we’re excited to enable enterprises to run Kubernetes in production environments and to help their operators to efficiently use it, so DevOps teams can build awesome applications on top of it.</p><p><em>-- Florian Walker, Product Manager, FUJITSU</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8d7eb4d2c3ea7e0c8a7600c4633741fb>Kubernetes Community Meeting Notes - 20160225</h1><div class="td-byline mb-4"><time datetime=2016-03-01 class=text-muted>Tuesday, March 01, 2016</time></div><h5 id=february-25th-redspread-demo-1-2-update-and-planning-1-3-newbie-introductions-sig-networking-and-a-shout-out-to-coreos-blog-post>February 25th - Redspread demo, 1.2 update and planning 1.3, newbie introductions, SIG-networking and a shout out to CoreOS blog post.</h5><p>The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project's status via videoconference. Here are the notes from the latest meeting.</p><p>Note taker: [Ilan Rabinovich]</p><ul><li>Quick call out for sharing presentations/slides [JBeda]</li><li>Demo (10 min):<a href=https://redspread.com/> Redspread</a> [Mackenzie Burnett, Dan Gillespie]</li><li>1.2 Release Watch [T.J. Goltermann]<ul><li>currently about 80 issues in the queue that need to be addressed before branching.<ul><li>currently looks like March 7th may slip to later in the week, but up in the air until flakey tests are resolved.</li><li>non-1.2 changes may be delayed in review/merging until 1.2 stabilization work completes.</li></ul></li><li>1.3 release planning</li></ul></li><li>Newbie Introductions</li><li>SIG Reports -<ul><li>Networking [Tim Hockin]</li><li>Scale [Bob Wise]</li><li>meeting last Friday went very well. Discussed charter AND a working deployment<ul><li>moved meeting to Thursdays @ 1 (so in 3 hours!)</li><li>Rob is posting a Cluster Ops announce on TheNewStack to recruit more members</li></ul></li></ul></li><li>GSoC participation -- no application submitted. [Sarah Novotny]</li><li>Brian Grant has offered to review PRs that need attention for 1.2</li><li>Dynamic Provisioning<ul><li>Currently overlaps a bit with the ubernetes work</li><li>PR in progress.</li><li>Should work in 1.2, but being targeted more in 1.3</li></ul></li><li>Next meeting is March 3rd.<ul><li>Demo from Weave on Kubernetes Anywhere</li><li>Another Kubernetes 1.2 update</li><li>Update from CNCF update</li><li>1.3 commitments from google</li></ul></li><li>No meeting on March 10th.</li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you're really excited, you can do all of the above and join us for the next community conversation — March 3rd, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p><p>The full recording is available on YouTube in the growing archive of <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">Kubernetes Community Meetings</a>. <em>-- Kubernetes Community</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-65a3c01be5e43cd74d91ae7b341ba25d>State of the Container World, February 2016</h1><div class="td-byline mb-4"><time datetime=2016-03-01 class=text-muted>Tuesday, March 01, 2016</time></div><p>Hello, and welcome to the second installment of the Kubernetes state of the container world survey. At the beginning of February we sent out a survey about people’s usage of containers, and wrote about the <a href=https://kubernetes.io/blog/2016/02/state-of-container-world-january-2016>results from the January survey</a>. Here we are again, as before, while we try to reach a large and representative set of respondents, this survey was publicized across the social media account of myself and others on the Kubernetes team, so I expect some pro-container and Kubernetes bias in the data.We continue to try to get as large an audience as possible, and in that vein, please go and take the <a href=https://docs.google.com/a/google.com/forms/d/1hlOEyjuN4roIbcAAUbDhs7xjNMoM8r-hqtixf6zUsp4/viewform>March survey</a> and share it with your friends and followers everywhere! Without further ado, the numbers...</p><h2 id=containers-continue-to-gain-ground>Containers continue to gain ground</h2><p>In January, 71% of respondents were currently using containers, in February, 89% of respondents were currently using containers. The percentage of users not even considering containers also shrank from 4% in January to a surprising 0% in February. Will see if that holds consistent in March.Likewise, the usage of containers continued to march across the dev/canary/prod lifecycle. In all parts of the lifecycle, container usage increased:</p><ul><li>Development: 80% -> 88%</li><li>Test: 67% -> 72%</li><li>Pre production: 41% -> 55%</li><li>Production: 50% -> 62%</li></ul><p>What is striking in this is that pre-production growth continued, even as workloads were clearly transitioned into true production. Likewise the share of people considering containers for production rose from 78% in January to 82% in February. Again we’ll see if the trend continues into March.</p><h2 id=container-and-cluster-sizes>Container and cluster sizes</h2><p>We asked some new questions in the survey too, around container and cluster sizes, and there were some interesting numbers:</p><p>How many containers are you running?</p><p><img src=https://lh6.googleusercontent.com/Ug0Bzcj6LZ__KYwUsHgMB5MFGnRHhexu6YKPaooShWCCpfYsCiynpod5cTZR_WnQdm4ox3GcHjMuGkfG863C3aiMy-sP-mX2vWJCv5gY3JzjOvCbzIvz0_pNZJSlHieTNWZZRJCv alt="Screen Shot 2016-02-29 at 9.27.01 AM.png"></p><p>How many machines are you running containers on?</p><p><img src=https://lh5.googleusercontent.com/3wek678JBM05-9wllCpRjP0QQHl5qWfAVbW1dA5XqVMtf1JlLm2PW82-rrhOOSs_owGUAXOyG3eC53pd9qPTuedXukqmwC9zDOJoA7xeKeggMp3snapK9q_cWjbLDxrBLIhJHkTK alt="Screen Shot 2016-02-29 at 9.27.15 AM.png"></p><p>So while container usage continues to grow, the size and scope continues to be quite modest, with more than 50% of users running fewer than 50 containers on fewer than 10 machines.</p><h2 id=things-stay-the-same>Things stay the same</h2><p>Across the orchestration space, things seemed pretty consistent between January and February (Kubernetes is quite popular with folks (54% -> 57%), though again, please see the note at the top about the likely bias in our survey population. Shell scripts likewise are also quite popular and holding steady. You all certainly love your Bash (don’t worry, we do too ;)
Likewise people continue to use cloud services both in raw IaaS form (10% on GCE, 30% on EC2, 2% on Azure) as well as cloud container services (16% for GKE, 11% on ECS, 1% on ACS). Though the most popular deployment target by far remains your laptop/desktop at ~53%.</p><h2 id=raw-data>Raw data</h2><p>As always, the complete raw data is available in a spreadsheet <a href="https://docs.google.com/spreadsheets/d/126nnv9Q9avxDvC82irJGUDK3UODokILZOQe5X_WB9VQ/edit?usp=sharing">here</a>.</p><h2 id=conclusions>Conclusions</h2><p>Containers continue to gain in popularity and usage. The world of orchestration is somewhat stabilizing, and cloud services continue to be a common place to run containers, though your laptop is even more popular.</p><p>And if you are just getting started with containers (or looking to move beyond your laptop) please visit us at <a href=http://kubernetes.io/>kubernetes.io</a> and <a href=https://cloud.google.com/container-engine/>Google Container Engine</a>. ‘Till next month, please get your friends, relatives and co-workers to take our <a href=https://docs.google.com/a/google.com/forms/d/1hlOEyjuN4roIbcAAUbDhs7xjNMoM8r-hqtixf6zUsp4/viewform>March survey</a>!</p><p>Thanks!</p><p><em>-- Brendan Burns, Software Engineer, Google</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a59c3151a2598aa5539dd1e5f8b30666>KubeCon EU 2016: Kubernetes Community in London</h1><div class="td-byline mb-4"><time datetime=2016-02-24 class=text-muted>Wednesday, February 24, 2016</time></div><p>KubeCon EU 2016 is the inaugural <a href=http://kubernetes.io/>European Kubernetes</a> community conference that follows on the American launch in November 2015. KubeCon is fully dedicated to education and community engagement for<a href=http://kubernetes.io/>Kubernetes</a> enthusiasts, production users and the surrounding ecosystem.</p><p>Come join us in London and hang out with hundreds from the Kubernetes community and experience a wide variety of deep technical expert talks and use cases.</p><p>Don’t miss these great speaker sessions at the conference:</p><ul><li><p>“Kubernetes Hardware Hacks: Exploring the Kubernetes API Through Knobs, Faders, and Sliders” by Ian Lewis and Brian Dorsey, Developer Advocate, Google -* <a href=http://sched.co/6Bl3>http://sched.co/6Bl3</a></p></li><li><p>“rktnetes: what's new with container runtimes and Kubernetes” by Jonathan Boulle, Developer and Team Lead at CoreOS -* <a href=http://sched.co/6BY7>http://sched.co/6BY7</a></p></li><li><p>“Kubernetes Documentation: Contributing, fixing issues, collecting bounties” by John Mulhausen, Lead Technical Writer, Google -* <a href=http://sched.co/6BUP>http://sched.co/6BUP</a> </p></li><li><p>“<a href="https://kubeconeurope2016.sched.org/event/6BYC/what-is-openstacks-role-in-a-kubernetes-world?iframe=yes&w=i:0;&sidebar=yes&bg=no#?iframe=yes&w=i:100;&sidebar=yes&bg=no">What is OpenStack's role in a Kubernetes world?</a>” By Thierry Carrez, Director of Engineering, OpenStack Foundation -* <a href=http://sched.co/6BYC>http://sched.co/6BYC</a></p></li><li><p>“A Practical Guide to Container Scheduling” by Mandy Waite, Developer Advocate, Google -* <a href=http://sched.co/6BZa>http://sched.co/6BZa</a></p></li><li><p>“<a href="https://kubeconeurope2016.sched.org/event/67f2/kubernetes-in-production-in-the-new-york-times-newsroom?iframe=yes&w=i:0;&sidebar=yes&bg=no#?iframe=yes&w=i:100;&sidebar=yes&bg=no">Kubernetes in Production in The New York Times newsroom</a>” Eric Lewis, Web Developer, New York Times -* <a href=http://sched.co/67f2>http://sched.co/67f2</a></p></li><li><p>“<a href="https://kubeconeurope2016.sched.org/event/6Bc9/creating-an-advanced-load-balancing-solution-for-kubernetes-with-nginx?iframe=yes&w=i:0;&sidebar=yes&bg=no#?iframe=yes&w=i:100;&sidebar=yes&bg=no">Creating an Advanced Load Balancing Solution for Kubernetes with NGINX</a>” by Andrew Hutchings, Technical Product Manager, NGINX -* <a href=http://sched.co/6Bc9>http://sched.co/6Bc9</a></p></li><li><p>And many more <a href=http://kubeconeurope2016.sched.org/>http://kubeconeurope2016.sched.org/</a></p></li></ul><p>Get your KubeCon EU <a href=https://ti.to/kubecon/kubecon-eu-2016>tickets here</a>.</p><p>Venue Location: CodeNode * 10 South Pl, London, United Kingdom<br>Accommodations: <a href=https://skillsmatter.com/contact-us#hotels>hotels</a><br>Website: <a href=https://www.kubecon.io/>kubecon.io</a><br>Twitter: <a href=https://twitter.com/kubeconio>@KubeConio</a> #KubeCon
Google is a proud Diamond sponsor of KubeCon EU 2016. Come to London next month, March 10th & 11th, and visit booth #13 to learn all about Kubernetes, Google Container Engine (GKE) and Google Cloud Platform!</p><p><em>KubeCon is organized by KubeAcademy, LLC, a community-driven group of developers focused on the education of developers and the promotion of Kubernetes.</em></p><p>-* Sarah Novotny, Kubernetes Community Manager, Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5c389f15f0500871a7419690797bb894>Kubernetes Community Meeting Notes - 20160218</h1><div class="td-byline mb-4"><time datetime=2016-02-23 class=text-muted>Tuesday, February 23, 2016</time></div><h5 id=february-18th-kmachine-demo-clusterops-sig-formed-new-k8s-io-website-preview-1-2-update-and-planning-1-3>February 18th - kmachine demo, clusterops SIG formed, new k8s.io website preview, 1.2 update and planning 1.3</h5><p>The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project's status via videoconference. Here are the notes from the latest meeting.</p><ul><li>Note taker: Rob Hirschfeld</li><li>Demo (10 min): <a href=https://github.com/skippbox/kmachine>kmachine</a> [Sebastien Goasguen]<ul><li>started :01 intro video</li><li>looking to create mirror of Docker tools for Kubernetes (similar to machine, compose, etc)</li><li>kmachine (forked from Docker Machine, so has the same endpoints)</li></ul></li><li>Use Case (10 min): started at :15</li><li>SIG Report starter<ul><li>Cluster Ops launch meeting Friday (<a href=https://docs.google.com/document/d/1IhN5v6MjcAUrvLd9dAWtKcGWBWSaRU8DNyPiof3gYMY/edit#>doc</a>). [Rob Hirschfeld]</li></ul></li><li>Time Zone Discussion [:22]<ul><li>This timezone does not work for Asia.</li><li>Considering rotation - once per month</li><li>Likely 5 or 6 PT</li><li>Rob suggested moving the regular meeting up a little</li></ul></li><li>k8s.io website preview [John Mulhausen] [:27]<ul><li>using github for docs. you can fork and do a pull request against the site</li><li>will be its own kubernetes organization BUT not in the code repo</li><li>Google will offer a "doc bounty" where you can get GCP credits for working on docs</li><li>Uses Jekyll to generate the site (e.g. the ToC)</li><li>Principle will be to 100% GitHub Pages; no script trickery or plugins, just fork/clone, edit, and push</li><li>Hope to launch at Kubecon EU</li><li>Home Page Only Preview: <a href=http://kub.unitedcreations.xyz>http://kub.unitedcreations.xyz</a></li></ul></li><li>1.2 Release Watch [T.J. Goltermann] [:38]</li><li>1.3 Planning update [T.J. Goltermann]</li><li>GSoC participation -- deadline 2/19 [Sarah Novotny]</li><li>March 10th meeting? [Sarah Novotny]</li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you're really excited, you can do all of the above and join us for the next community conversation — February 25th, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p><p>"https://youtu.be/L5BgX2VJhlY?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ"</p><p><em>-- Kubernetes Community</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-cff3eab8d736e2f13bf6269240002c94>Kubernetes Community Meeting Notes - 20160211</h1><div class="td-byline mb-4"><time datetime=2016-02-16 class=text-muted>Tuesday, February 16, 2016</time></div><h5 id=february-11th-pangaea-demo-aws-sig-formed-release-automation-and-documentation-team-introductions-1-2-update-and-planning-1-3>February 11th - Pangaea Demo, #AWS SIG formed, release automation and documentation team introductions. 1.2 update and planning 1.3.</h5><p>The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project's status via videoconference. Here are the notes from the latest meeting.</p><p>Note taker: Rob Hirschfeld</p><ul><li><p>Demo: <a href=http://hasura.io/blog/pangaea-point-and-shoot-kubernetes/>Pangaea</a> [Shahidh K Muhammed, Tanmai Gopal, and Akshaya Acharya]</p><ul><li>Microservices packages</li><li>Focused on Application developers</li><li>Demo at recording +4 minutes</li><li>Single node kubernetes cluster — runs locally using Vagrant CoreOS image</li><li>Single user/system cluster allows use of DNS integration (unlike Compose)</li><li>Can run locally or in cloud</li><li><em>SIG Report:</em><ul><li>Release Automation and an introduction to David McMahon</li><li>Docs and k8s website redesign proposal and an introduction to John Mulhausen</li><li>This will allow the system to build docs correctly from GitHub w/ minimal effort</li><li>Will be check-in triggered</li><li>Getting website style updates</li><li>Want to keep authoring really light</li><li>There will be some automated checks</li><li>Next week: preview of the new website during the community meeting</li></ul></li></ul></li><li><p>[@goltermann] 1.2 Release Watch (time +34 minutes)</p><ul><li>code slush * no major features or refactors accepted</li><li>discussion about release criteria: we will hold release date for bugs</li></ul></li><li><p>Testing flake surge is over (one time event and then maintain test stability)</p></li><li><p>1.3 Planning (time +40 minutes)</p><ul><li>working to cleanup the GitHub milestones — they should be a source of truth. you can use GitHub for bug reporting</li><li>push off discussion while 1.2 crunch is under</li><li>Framework<ul><li>dates</li><li>prioritization</li><li>feedback</li></ul></li><li>Design Review meetings</li><li>General discussion about the PRD process — still at the beginning states</li><li>Working on a contributor conference</li><li>Rob suggested tracking relationships between PRD/Mgmr authors</li><li>PLEASE DO REVIEWS — talked about the way people are authorized to +2 reviews.</li></ul></li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel,</a> taking a look at the <a href=https://github.com/kubernetes/>Kubernetes</a> project on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you're really excited, you can do all of the above and join us for the next community conversation — February 18th, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p><p>The full recording is available on YouTube in the growing archive of <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">Kubernetes Community Meetings</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9e9c02244708058c8ef00d746c53046e>ShareThis: Kubernetes In Production</h1><div class="td-byline mb-4"><time datetime=2016-02-11 class=text-muted>Thursday, February 11, 2016</time></div><p>Today’s guest blog post is by Juan Valencia, Technical Lead at ShareThis, a service that helps website publishers drive engagement and consumer sharing behavior across social networks.</p><p>ShareThis has grown tremendously since its first days as a tiny widget that allowed you to share to your favorite social services. It now serves over 4.5 million domains per month, helping publishers create a more authentic digital experience.</p><p>Fast growth came with a price. We leveraged technical debt to scale fast and to grow our products, particularly when it came to infrastructure. As our company expanded, the infrastructure costs mounted as well - both in terms of inefficient utilization and in terms of people costs. About 1 year ago, it became clear something needed to change.</p><h3 id=tl-drkubernetes-has-been-a-key-component-for-us-to-reduce-technical-debt-in-our-infrastructure-by>TL;DRKubernetes has been a key component for us to reduce technical debt in our infrastructure by:</h3><ul><li>Fostering the Adoption of Docker</li><li>Simplifying Container Management</li><li>Onboarding Developers On Infrastructure</li><li>Unlocking Continuous Integration and Delivery
We accomplished this by radically adopting Kubernetes and switching our DevOps team to a Cloud Platform team that worked in terms of containers and microservices. This included creating some tools to get around our own legacy debt.</li></ul><h3 id=the-problem>The Problem</h3><p>Alas, the cloud was new and we were young. We started with a traditional data-center mindset.  We managed all of our own services: MySQL, Cassandra, Aerospike, Memcache, you name it.  We set up VM’s just like you would traditional servers, installed our applications on them, and managed them in Nagios or Ganglia.</p><p>Unfortunately, this way of thinking was antithetical to a cloud-centric approach. Instead of thinking in terms of services, we were thinking in terms of servers. Instead of using modern cloud approaches such as autoscaling, microservices, or even managed VM’s, we were thinking in terms of scripted setups, server deployments, and avoiding vendor lock-in.</p><p>These ways of thinking were not bad per se, they were simply inefficient. They weren’t taking advantage of the changes to the cloud that were happening very quickly. It also meant that when changes needed to take place, we were treating those changes as big slow changes to a datacenter, rather than small fast changes to the cloud.</p><h3 id=the-solution>The Solution</h3><h4 id=kubernetes-as-a-tool-to-foster-docker-adoption>Kubernetes As A Tool To Foster Docker Adoption</h4><p>As Docker became more of a force in our industry, engineers at ShareThis also started experimenting with it to good effect. It soon became obvious that we needed to have a working container for every app in our company just so we could simplify testing in our development environment.</p><p>Some apps moved quickly into Docker because they were simple and had few dependencies.  For those that had small dependencies, we were able to manage using Fig (Fig was the original name of Docker Compose). Still, many of our data pipelines or interdependent apps were too gnarly to be directly dockerized. We still wanted to do it, but Docker was not enough.</p><p>In late 2015, we were frustrated enough with our legacy infrastructure that we finally bit the bullet. We evaluated Docker’s tools, ECS, Kubernetes, and Mesosphere. It was quickly obvious that Kubernetes was in a more stable and user friendly state than its competitors for our infrastructure. As a company, we could solidify our infrastructure on Docker by simply setting the goal of having all of our infrastructure on Kubernetes.</p><p>Engineers were skeptical at first. However, once they saw applications scale effortlessly into hundreds of instances per application, they were hooked. Now, not only was there the pain points driving us forward into Docker and by extension Kubernetes, but there was genuine excitement for the technology pulling us in. This has allowed us to make an incredibly difficult migration fairly quickly. We now run Kubernetes in multiple regions on about 65 large VMs and increasing to over 100 in the next couple months. Our Kubernetes cluster currently processes 800 million requests per day with the plan to process over 2 billion requests per day in the coming months.</p><h4 id=kubernetes-as-a-tool-to-manage-containers>Kubernetes As A Tool To Manage Containers</h4><p>Our earliest use of Docker was promising for development, but not so much so for production. The biggest friction point was the inability to manage Docker components at scale. Knowing which containers were running where, what version of a deployment was running, what state an app was in, how to manage subnets and VPCs, etc, plagued any chance of it going to production. The tooling required would have been substantial.</p><p>When you look at Kubernetes, there are several key features that were immediately attractive:</p><ul><li>It is easy to install on AWS (where all our apps were running)</li><li>There is a direct path from a Dockerfile to a replication controller through a yaml/json file</li><li>Pods are able to scale in number easily</li><li>We can easily scale the number of VM’s running on AWS in a Kubernetes cluster</li><li>Rolling deployments and rollback are built into the tooling</li><li>Each pod gets monitored through health checks</li><li>Service endpoints are managed by the tool</li><li>There is an active and vibrant community</li></ul><p>Unfortunately, one of the biggest pain points was that the tooling didn’t solve our existing legacy infrastructure, it just provided an infrastructure to move onto. There were still a variety of network quirks which disallowed us from directly moving our applications onto a new VPC. In addition, the reworking of so many applications required developers to jump onto problems that have classically been solved by sys admins and operations teams.</p><h4 id=kubernetes-as-a-tool-for-onboarding-developers-on-infrastructure>Kubernetes As A Tool For Onboarding Developers On Infrastructure</h4><p>When we decided to make the switch from what was essentially a Chef-run setup to Kubernetes, I do not think we understood all of the pain points that we would hit.  We ran our servers in a variety of different ways in a variety of different network configurations that were considerably different than the clean setup that you find on a fresh Kubernetes VPC.  </p><p>In production we ran in both AWS VPCs and AWS classic across multiple regions. This means that we managed several subnets with different access controls across different applications. Our most recent applications were also very secure, having no public endpoints. This meant that we had a combination of VPC peering, network address translation (NAT), and proxies running in varied configurations.</p><p>In the Kubernetes world, there’s only the VPC.  All the pods can theoretically talk to each other, and services endpoints are explicitly defined. It’s easy for the developer to gloss over some of the details and it removes the need for operations (mostly).  </p><p>We made the decision to convert all of our infrastructure / DevOps developers into application developers (really!). We had already started hiring them on the basis of their development skills rather than their operational skills anyway, so perhaps that is not as wild as it sounds.</p><p>We then made the decision to onboard our entire engineering organization onto Operations. Developers are flexible, they enjoy challenges, and they enjoy learning. It was remarkable.  After 1 month, our organization went from having a few DevOps folks, to having every engineer capable of modifying our architecture.</p><p>The training ground for onboarding on networking, productionization, problem solving, root cause analysis, etc, was getting Kubernetes into prod at scale. After the first month, I was biting my nails and worrying about our choices. After 2 months, it looked like it might some day be viable. After 3 months, we were deploying 10 times per week. After 4 months, 40 apps per week. Only 30% of our apps have been migrated, yet the gains are not only remarkable, they are astounding. Kubernetes allowed us to go from an infrastructure-is-slowing-us-down-ugh! organization, to an infrastructure-is-speeding-us-up-yay! organization.</p><h4 id=kubernetes-as-a-means-to-unlock-continuous-integration-and-delivery>Kubernetes As A Means To Unlock Continuous Integration And Delivery</h4><p>How did we get to 40+ deployments per week? Put simply, continuous integration and deployment (CI/CD) came as a byproduct of our migration. Our first application in Kubernetes was Jenkins, and every app that went in also was added to Jenkins. As we moved forward, we made Jenkins more automatic until pods were being added and taken from Kubernetes faster than we could keep track.  </p><p>Interestingly, our problems with scaling are now about wanting to push out too many changes at once and people having to wait until their turn. Our goal is to get 100 deployments per week through the new infrastructure. This is achievable if we can continue to execute on our migration and on our commitment to a CI/CD process on Kubernetes and Jenkins.</p><h3 id=next-steps>Next Steps</h3><p>We need to finish our migration. At this point the problems are mostly solved, the biggest difficulties are in the tedium of the task at hand. To move things out of our legacy infrastructure meant changing the network configurations to allow access to and from the Kubernetes VPC and across the regions. This is still a very real pain, and one we continue to address.  </p><p>Some services do not play well in Kubernetes -- think stateful distributed databases. Luckily, we can usually migrate those to a 3rd party who will manage it for us. At the end of this migration, we will only be running pods on Kubernetes. Our infrastructure will become much simpler.</p><p>All these changes do not come for free; committing our entire infrastructure to Kubernetes means that we need to have Kubernetes experts.  Our team has been unblocked in terms of infrastructure and they are busy adding business value through application development (as they should). However, we do not (yet) have committed engineers to stay up to date with changes to Kubernetes and cloud computing.  </p><p>As such, we have transferred one engineer to a new “cloud platform team” and will hire a couple of others (have I mentioned <a href=http://www.sharethis.com/hiring.html>we’re hiring</a>!). They will be responsible for developing tools that we can use to interface well with Kubernetes and manage all of our cloud resources. In addition, they will be working in the Kubernetes source code, part of Kubernetes SIGs, and ideally, pushing code into the open source project.</p><h3 id=summary>Summary</h3><p>All in all, while the move to Kubernetes initially seemed daunting, it was far less complicated and disruptive than we thought. And the reward at the other end was a company that could respond as fast as our customers wanted.<em>Editor's note: at a recent Kubernetes meetup, the team at ShareThis gave a talk about their production use of Kubernetes. Video is embedded below.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-ca2bb4aa037f8b34a465c33394c21026>Kubernetes Community Meeting Notes - 20160204</h1><div class="td-byline mb-4"><time datetime=2016-02-09 class=text-muted>Tuesday, February 09, 2016</time></div><h4 id=february-4th-rkt-demo-congratulations-on-the-1-0-coreos-ebay-puts-k8s-on-openstack-and-considers-openstack-on-k8s-sigs-and-flaky-test-surge-makes-progress>February 4th - rkt demo (congratulations on the 1.0, CoreOS!), eBay puts k8s on Openstack and considers Openstack on k8s, SIGs, and flaky test surge makes progress.</h4><p>The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project's status via a videoconference. Here are the notes from the latest meeting.</p><ul><li>Note taker: Rob Hirschfeld</li><li>Demo (20 min): CoreOS rkt + Kubernetes [Shaya Potter]<ul><li>expect to see integrations w/ rkt & k8s in the coming months ("rkt-netes"). not integrated into the v1.2 release.</li><li>Shaya gave a demo (8 minutes into meeting for video reference)<ul><li><p>CLI of rkt shown spinning up containers</p></li><li><p>[note: audio is garbled at points]</p></li><li><p>Discussion about integration w/ k8s & rkt</p></li><li><p>rkt community sync next week: <a href=https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY>https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY</a></p></li><li><p>Dawn Chen:</p><ul><li>The remaining issues of integrating rkt with kubernetes: 1) cadivsor 2) DNS 3) bugs related to logging</li><li>But need more work on e2e test suites</li></ul></li></ul></li></ul></li><li>Use Case (10 min): eBay k8s on OpenStack and OpenStack on k8s [Ashwin Raveendran]<ul><li>eBay is currently running Kubernetes on OpenStack</li><li>Goal for eBay is to manage the OpenStack control plane w/ k8s. Goal would be to achieve upgrades</li><li>OpenStack Kolla creates containers for the control plane. Uses Ansible+Docker for management of the containers.</li><li>Working on k8s control plan management - Saltstack is proving to be a management challenge at the scale they want to operate. Looking for automated management of the k8s control plane.</li></ul></li><li>SIG Report</li><li>Testing update [Jeff, Joe, and Erick]<ul><li>Working to make the workflow about contributing to K8s easier to understanding<ul><li><a href=https://github.com/kubernetes/kubernetes/pull/19714>pull/19714</a> has flow chart of the bot flow to help users understand</li></ul></li><li>Need a consistent way to run tests w/ hacking config scripts (you have to fake a Jenkins process right now)</li><li>Want to create necessary infrastructure to make test setup less flaky</li><li>want to decouple test start (single or full) from Jenkins</li><li>goal is to get to point where you have 1 script to run that can be pointed to any cluster</li><li>demo included Google internal views - working to try get that external.</li><li>want to be able to collect test run results</li><li>Bob Wise calls for testing infrastructure to be a blocker on v1.3</li><li>Long discussion about testing practices…<ul><li>consensus that we want to have tests work over multiple platforms.</li><li>would be helpful to have a comprehensive state dump for test reports</li><li>"phone-home" to collect stack traces - should be available</li></ul></li></ul></li><li>1.2 Release Watch</li><li>CoC [Sarah]</li><li>GSoC [Sarah]</li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you're really excited, you can do all of the above and join us for the next community conversation — February 11th, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p><p>"https://youtu.be/IScpP8Cj0hw?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ"</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9393cc8cec34b63465f7932dca6f62eb>Kubernetes Community Meeting Notes - 20160128</h1><div class="td-byline mb-4"><time datetime=2016-02-02 class=text-muted>Tuesday, February 02, 2016</time></div><h5 id=january-28-1-2-release-update-deis-demo-flaky-test-surge-and-sigs>January 28 - 1.2 release update, Deis demo, flaky test surge and SIGs</h5><p>The Kubernetes contributing community meets once a week to discuss the project's status via a videoconference. Here are the notes from the latest meeting.</p><p>Note taker: Erin Boyd</p><ul><li>Discuss process around code freeze/code slush (TJ Goltermann)<ul><li>Code wind down was happening during holiday (for 1.1)</li><li>Releasing ~ every 3 months</li><li>Build stability is still missing</li><li>Issue on Transparency (Bob Wise)<ul><li>Email from Sarah for call to contribute (Monday, January 25)<ul><li>Concern over publishing dates / understanding release schedule /etc…</li></ul></li></ul></li><li>Release targeted for early March<ul><li>Where does one find information on the release schedule with the committed features?<ul><li>For 1.2 - Send email / Slack to TJ</li><li>For 1.3 - Working on better process to communicate to the community<ul><li>Twitter</li><li>Wiki</li><li>GitHub Milestones</li></ul></li></ul></li></ul></li><li>How to better communicate issues discovered in the SIG<ul><li>AI: People need to email the kubernetes-dev@ mailing list with summary of findings</li><li>AI: Each SIG needs a note taker</li></ul></li></ul></li><li>Release planning vs Release testing<ul><li>Testing SIG lead Ike McCreery<ul><li>Also part of the testing infrastructure team at Google</li><li>Community being able to integrate into the testing framework<ul><li>Federated testing</li></ul></li></ul></li><li>Release Manager = David McMahon<ul><li>Request to  introduce him to the community meeting</li></ul></li></ul></li><li>Demo: Jason Hansen Deis<ul><li>Implemented simple REST API to interact with the platform</li><li>Deis managed application (deployed via)<ul><li>Source -> image</li><li>Rolling upgrades -> Rollbacks</li><li>AI: Jason will provide the slides & notes<ul><li>Slides: <a href=https://speakerdeck.com/slack/kubernetes-community-meeting-demo-january-28th-2016>https://speakerdeck.com/slack/kubernetes-community-meeting-demo-january-28th-2016</a></li><li>Alpha information: <a href=https://groups.google.com/forum/#!topic/deis-users/Qhia4DD2pv4>https://groups.google.com/forum/#!topic/deis-users/Qhia4DD2pv4</a></li></ul></li><li>Adding an administrative component (dashboard)</li><li>Helm wraps kubectl</li></ul></li></ul></li><li>Testing<ul><li>Called for community interaction</li><li>Need to understand friction points from community<ul><li>Better documentation</li><li>Better communication on how things “should work"</li></ul></li><li>Internally, Google is having daily calls to resolve test flakes</li><li>Started up SIG testing meetings (Tuesday at 10:30 am PT)</li><li>Everyone wants it, but no one want to pony up the time to make it happen<ul><li>Google is dedicating headcount to it (3-4 people, possibly more)</li></ul></li><li><a href="https://groups.google.com/forum/?hl=en#%21forum/kubernetes-sig-testing">https://groups.google.com/forum/?hl=en#!forum/kubernetes-sig-testing</a></li></ul></li><li>Best practices for labeling<ul><li>Are there tools built on top of these to leverage</li><li>AI: Generate artifact for labels and what they do (Create doc)<ul><li>Help Wanted Label - good for new community members</li><li>Classify labels for team and area<ul><li>User experience, test infrastructure, etc..</li></ul></li></ul></li></ul></li><li>SIG Config (not about deployment)<ul><li>Any interest in ansible, etc.. type</li></ul></li><li>SIG Scale meeting (Bob Wise & Tim StClair)<ul><li>Tests related to performance SLA get relaxed in order to get the tests to pass<ul><li>exposed process issues</li><li>AI: outline of a proposal for a notice policy if things are being changed that are critical to the system (Bob Wise/Samsung)<ul><li>Create a Best Practices of set of constants into well documented place</li></ul></li></ul></li></ul></li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you’re really excited, you can do all of the above and join us for the next community conversation — February 4th, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p><p>The full recording is available on YouTube in the growing archive of <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">Kubernetes Community Meetings</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7b6c140fad77ba1f495571539d32b7aa>State of the Container World, January 2016</h1><div class="td-byline mb-4"><time datetime=2016-02-01 class=text-muted>Monday, February 01, 2016</time></div><p>At the start of the new year, we sent out a survey to gauge the state of the container world. We’re ready to send the <a href=https://docs.google.com/forms/d/13yxxBqb5igUhwrrnDExLzZPjREiCnSs-AH-y4SSZ-5c/viewform>February edition</a>, but before we do, let’s take a look at the January data from the 119 responses (thank you for participating!).</p><p>A note about these numbers: First, you may notice that the numbers don’t add up to 100%, the choices were not exclusive in most cases and so percentages given are the percentage of all respondents who selected a particular choice. Second, while we attempted to reach a broad cross-section of the cloud community, the survey was initially sent out via Twitter to followers of <a href=https://twitter.com/brendandburns>@brendandburns</a>, <a href=https://twitter.com/kelseyhightower>@kelseyhightower</a>, <a href=https://twitter.com/sarahnovotny>@sarahnovotny</a>, <a href=https://twitter.com/juliaferraioli>@juliaferraioli</a>, <a href=https://twitter.com/thagomizer_rb>@thagomizer_rb</a>, so the audience is likely not a perfect cross-section. We’re working to broaden our sample size (have I mentioned our February survey? <a href=https://docs.google.com/forms/d/13yxxBqb5igUhwrrnDExLzZPjREiCnSs-AH-y4SSZ-5c/viewform>Come take it now</a>).</p><h4 id=now-without-further-ado-the-data>Now, without further ado, the data:</h4><p>First off, lots of you are using containers! 71% are currently using containers, while 24% of you are considering using them soon. Obviously this indicates a somewhat biased sample set. Numbers for container usage in the broader community vary, but are definitely lower than 71%.  Consequently, take all of the rest of these numbers with a grain of salt.</p><p>So what are folks using containers for? More than 80% of respondents are using containers for development, while only 50% are using containers for production. But you plan to move to production soon, as 78% of container users said that you were planning on moving to production sometime soon.</p><p>Where do you deploy containers? Your laptop was the clear winner here, with 53% of folks deploying to laptops. Next up was 44% of people running on their own VMs (Vagrant? OpenStack? we’ll try dive into this in the February survey), followed by 33% of folks running on physical infrastructure, and 31% on public cloud VMs.</p><p>And how are you deploying containers? 54% of you are using Kubernetes, awesome to see, though likely somewhat biased by the sample set (see the notes above), possibly more surprising, 45% of you are using shell scripts. Is it because of the extensive (and awesome) Bash scripting going on in the Kubernetes repository? Go on, you can tell me the truth…  Rounding out the numbers, 25% are using CAPS (Chef/Ansible/Puppet/Salt) systems, and roughly 13% are using Docker Swarm, Mesos or other systems.</p><p>Finally, we asked people for free-text answers about the challenges of working with containers. Some of the most interesting answers are grouped and reproduced here:</p><h6 id=development-complexity>Development Complexity</h6><ul><li>“Silo'd development environments / workflows can be fragmented, ease of access to tools like logs is available when debugging containers but not intuitive at times, massive amounts of knowledge is required to grasp the whole infrastructure stack and best practices from say deploying / updating kubernetes, to underlying networking etc.”</li><li>“Migrating developer workflow. People uninitiated with containers, volumes, etc just want to work.”</li></ul><h6 id=security>Security</h6><ul><li>“Network Security”</li><li>“Secrets”</li></ul><h6 id=immaturity>Immaturity</h6><ul><li>“Lack of a comprehensive non-proprietary standard (i.e. non-Docker) like e.g runC / OCI”</li><li>“Still early stage with few tools and many missing features.”</li><li>“Poor CI support, a lot of tooling still in very early days.”</li><li>"We've never done it that way before."</li></ul><h6 id=complexity>Complexity</h6><ul><li>“Networking support, providing ip per pod on bare metal for kubernetes”</li><li>“Clustering is still too hard”</li><li>“Setting up Mesos and Kubernetes too damn complicated!!”</li></ul><h6 id=data>Data</h6><ul><li>“Lack of flexibility of volumes (which is the same problem with VMs, physical hardware, etc)”</li><li>“Persistency”</li><li>“Storage”</li><li>“Persistent Data”</li></ul><p><em>Download the full survey results <a href="https://docs.google.com/spreadsheets/d/18wZe7wEDvRuT78CEifs13maXoSGem_hJvbOSmsuJtkA/pub?gid=530616014&single=true&output=csv">here</a> (CSV file).</em></p><p>_Up-- Brendan Burns, Software Engineer, Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eace1ed65ee850592eaf33f599130b33>Kubernetes Community Meeting Notes - 20160114</h1><div class="td-byline mb-4"><time datetime=2016-01-28 class=text-muted>Thursday, January 28, 2016</time></div><h5 id=january-14-rackn-demo-testing-woes-and-kubecon-eu-cfp>January 14 - RackN demo, testing woes, and KubeCon EU CFP.</h5><hr><h2 id=note-taker-joe-beda>Note taker: Joe Beda</h2><ul><li><p>Demonstration: Automated Deploy on Metal, AWS and others w/ Digital Rebar, Rob Hirschfeld and Greg Althaus from RackN</p><ul><li><p>Greg Althaus. CTO. Digital Rebar is the product. Bare metal provisioning tool.</p></li><li><p>Detect hardware, bring it up, configure raid, OS and get workload deployed.</p></li><li><p>Been working on Kubernetes workload.</p></li><li><p>Seeing trend to start in cloud and then move back to bare metal.</p></li><li><p>New provider model to use provisioning system on both cloud and bare metal.</p></li><li><p>UI, REST API, CLI</p></li><li><p>Demo: Packet -- bare metal as a service</p><ul><li><p>4 nodes running grouped into a "deployment"</p></li><li><p>Functional roles/operations selected per node.</p></li><li><p>Decomposed the kubernetes bring up into units that can be ordered and synchronized. Dependency tree -- things like wait for etcd to be up before starting k8s master.</p></li><li><p>Using the Ansible playbook under the covers.</p></li><li><p>Demo brings up 5 more nodes -- packet will build those nodes</p></li><li><p>Pulled out basic parameters from the ansible playbook. Things like the network config, dns set up, etc.</p></li><li><p>Hierarchy of roles pulls in other components -- making a node a master brings in a bunch of other roles that are necessary for that.</p></li><li><p>Has all of this combined into a command line tool with a simple config file.</p></li></ul></li><li><p>Forward: extending across multiple clouds for test deployments. Also looking to create split/replicated across bare metal and cloud.</p></li><li><p>Q: secrets?<br>A: using ansible playbooks. Builds own certs and then distributes them. Wants to abstract them out and push that stuff upstream.</p></li><li><p>Q: Do you support bringing up from real bare metal with PXE boot?<br>A: yes -- will discover bare metal systems and install OS, install ssh keys, build networking, etc.</p></li></ul></li><li><p>[from SIG-scalability] Q: What is the status of moving to golang 1.5?<br>A: At HEAD we are 1.5 but will support 1.4 also. Some issues with flakiness but looks like things are stable now.</p><ul><li><p>Also looking to use the 1.5 vendor experiment. Move away from godep. But can't do that until 1.5 is the baseline.</p></li><li><p>Sarah: one of the things we are working on is rewards for doing stuff like this. Cloud credits, tshirts, poker chips, ponies.</p></li></ul></li><li><p>[from SIG-scalability] Q: What is the status of cleaning up the jenkins based submit queue? What can the community do to help out?<br>A: It has been rocky the last few days. There should be issues associated with each of these. There is a <a href=https://github.com/kubernetes/kubernetes/labels/kind%2Fflake>flake label</a> on those issues.</p><ul><li><p>Still working on test federation. More test resources now. Happening slowly but hopefully faster as new people come up to speed. Will be great to having lots of folks doing e2e tests on their environments.</p></li><li><p>Erick Fjeta is the new test lead</p></li><li><p>Brendan is happy to help share details on Jenkins set up but that shouldn't be necessary.</p></li><li><p>Federation may use Jenkins API but doesn't require Jenkins itself.</p></li><li><p>Joe bitches about the fact that running the e2e tests in the way Jenkins is tricky. Brendan says it should be runnable easily. Joe will take another look.</p></li><li><p>Conformance tests? etune did this but he isn't here. - revisit 20150121</p></li></ul></li><li><pre><code>* March 10-11 in London.  Venue to be announced this week.
</code></pre><ul><li><p>Please send talks! CFP deadline looks to be Feb 5.</p></li><li><p>Lots of excitement. Looks to be 700-800 people. Bigger than SF version (560 ppl).</p></li><li><p>Buy tickets early -- early bird prices will end soon and price will go up 100 GBP.</p></li><li><p>Accommodations provided for speakers?</p></li><li><p>Q from Bob @ Samsung: Can we get more warning/planning for stuff like this:</p><ul><li><p>A: Sarah -- I don't hear about this stuff much in advance but will try to pull together a list. Working to make the events page on kubernetes.io easier to use.</p></li><li><p>A: JJ -- we'll make sure we give more info earlier for the next US conf.</p></li></ul></li></ul></li><li><p>Scale tests [Rob Hirschfeld from RackN] -- if you want to help coordinate on scale tests we'd love to help.</p><ul><li><p>Bob invited Rob to join the SIG-scale group.</p></li><li><p>There is also a big bare metal cluster through the CNCF (from Intel) that will be useful too. No hard dates yet on that.</p></li></ul></li><li><p>Notes/video going to be posted on k8s blog. (Video for 20150114 wasn't recorded. Fail.)</p></li></ul><p>To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you're really excited, you can do all of the above and join us for the next community conversation - January 27th, 2016. Please add yourself or a topic you want to know about to the <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#>agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a053c79699463dc190af082ab57521dd>Kubernetes Community Meeting Notes - 20160121</h1><div class="td-byline mb-4"><time datetime=2016-01-28 class=text-muted>Thursday, January 28, 2016</time></div><h4 id=january-21-configuration-federation-and-testing-oh-my-nbsp>January 21 - Configuration, Federation and Testing, oh my. </h4><p>Note taker: Rob Hirshfeld</p><ul><li><p>Use Case (10 min): <a href="https://docs.google.com/a/google.com/presentation/d/1MEI97efplr3f-GDX1GcWGfkEuGKKV-4niu27kHOeMLk/edit?usp=sharing_eid&ts=56a114f8">SFDC Paul Brown</a></p></li><li><p>SIG Report - SIG-config and the story of <a href=https://github.com/kubernetes/kubernetes/pull/18215>#18215</a>.</p><ul><li>Application config IN K8s not deployment of K8s</li><li>Topic has been reuse of configuration,specifically parameterization(aka templates). Needs:<ul><li>include scoping(cluster namespace)</li><li>slight customization (naming changes, but not major config)</li><li>multiple positions on how todo this including allowing external or simple extensions</li></ul></li><li>PetSet creates instances w/stable namespace</li></ul></li><li><p>Workflow proposal</p><ul><li>Distributed Chron. Challenge is that configs need to create multiple objects in sequence</li><li>Trying to figure out how balance the many config options out there (compose, terraform,ansible/etc)</li><li>Goal is to “meet people where they are” to keep it simple</li><li>Q: is there an opinion for the keystore sizing<ul><li>large size / data blob would not be appropriate</li><li>you can pull data(config) from another store for larger objects</li></ul></li></ul></li><li><p>SIG Report - SIG-federation - progress on Ubernetes-Lite & Ubernetes design</p></li><li><p>Goal is to be able to have a cluster manager, so you can federate clusters. They will automatically distribute the pods.</p></li><li><p>Plan is to use the same API for the master cluster</p></li><li><p><a href=https://youtu.be/L2ZK24JojB4>Quinton's Kubernetes Talk</a></p></li><li><p><a href=https://github.com/kubernetes/kubernetes/pull/19313>Design for Kubernetes:</a></p></li><li><p>Conformance testing Q+A Isaac Hollander McCreery</p><ul><li>status on conformance testing for release process</li><li>expect to be forward compatible but not backwards</li><li>is there interest for a sig-testing meeting</li><li>testing needs to a higher priority for the project</li><li>lots of focus on trying to make this a higher priority
To get involved in the Kubernetes community consider joining our <a href=http://slack.k8s.io/>Slack channel</a>, taking a look at the <a href=https://github.com/kubernetes/>Kubernetes project</a> on GitHub, or join the <a href=https://groups.google.com/forum/#!forum/kubernetes-dev>Kubernetes-dev Google group</a>. If you’re really excited, you can do all of the above and join us for the next community conversation -- January 27th, 2016. Please add yourself or a topic you want to know about to <a href=https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit>the agenda</a> and get a calendar invitation by joining <a href=https://groups.google.com/forum/#!forum/kubernetes-community-video-chat>this group</a>.</li></ul></li></ul><p>Still want more Kubernetes? Check out the <a href="https://www.youtube.com/watch?v=izQLFx_6kwY&feature=youtu.be&list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">recording</a> of this meeting and the growing of the archive of <a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ">Kubernetes Community Meetings</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bf667c51e2011c6904d9968c4e0e4850>Why Kubernetes doesn’t use libnetwork</h1><div class="td-byline mb-4"><time datetime=2016-01-14 class=text-muted>Thursday, January 14, 2016</time></div><p>Kubernetes has had a very basic form of network plugins since before version 1.0 was released — around the same time as Docker's <a href=https://github.com/docker/libnetwork>libnetwork</a> and Container Network Model (<a href=https://github.com/docker/libnetwork/blob/master/docs/design.md>CNM</a>) was introduced. Unlike libnetwork, the Kubernetes plugin system still retains its "alpha" designation. Now that Docker's network plugin support is released and supported, an obvious question we get is why Kubernetes has not adopted it yet. After all, vendors will almost certainly be writing plugins for Docker — we would all be better off using the same drivers, right?</p><p>Before going further, it's important to remember that Kubernetes is a system that supports multiple container runtimes, of which Docker is just one. Configuring networking is a facet of each runtime, so when people ask "will Kubernetes support CNM?" what they really mean is "will kubernetes support CNM drivers with the Docker runtime?" It would be great if we could achieve common network support across runtimes, but that’s not an explicit goal.</p><p>Indeed, Kubernetes has not adopted CNM/libnetwork for the Docker runtime. In fact, we’ve been investigating the alternative Container Network Interface (<a href=https://github.com/appc/cni/blob/master/SPEC.md>CNI</a>) model put forth by CoreOS and part of the App Container (<a href=https://github.com/appc>appc</a>) specification. Why? There are a number of reasons, both technical and non-technical.</p><p>First and foremost, there are some fundamental assumptions in the design of Docker's network drivers that cause problems for us.</p><p>Docker has a concept of "local" and "global" drivers. Local drivers (such as "bridge") are machine-centric and don’t do any cross-node coordination. Global drivers (such as "overlay") rely on <a href=https://github.com/docker/libkv>libkv</a> (a key-value store abstraction) to coordinate across machines. This key-value store is a another plugin interface, and is very low-level (keys and values, no semantic meaning). To run something like Docker's overlay driver in a Kubernetes cluster, we would either need cluster admins to run a whole different instance of <a href=https://github.com/hashicorp/consul>consul</a>, <a href=https://github.com/coreos/etcd>etcd</a> or <a href=https://zookeeper.apache.org/>zookeeper</a> (see <a href=https://docs.docker.com/engine/userguide/networking/get-started-overlay/>multi-host networking</a>), or else we would have to provide our own libkv implementation that was backed by Kubernetes.</p><p>The latter sounds attractive, and we tried to implement it, but the libkv interface is very low-level, and the schema is defined internally to Docker. We would have to either directly expose our underlying key-value store or else offer key-value semantics (on top of our structured API which is itself implemented on a key-value system). Neither of those are very attractive for performance, scalability and security reasons. The net result is that the whole system would significantly be more complicated, when the goal of using Docker networking is to simplify things.</p><p>For users that are willing and able to run the requisite infrastructure to satisfy Docker global drivers and to configure Docker themselves, Docker networking should "just work." Kubernetes will not get in the way of such a setup, and no matter what direction the project goes, that option should be available. For default installations, though, the practical conclusion is that this is an undue burden on users and we therefore cannot use Docker's global drivers (including "overlay"), which eliminates a lot of the value of using Docker's plugins at all.</p><p>Docker's networking model makes a lot of assumptions that aren’t valid for Kubernetes. In docker versions 1.8 and 1.9, it includes a fundamentally flawed implementation of "discovery" that results in corrupted <code>/etc/hosts</code> files in containers (<a href=https://github.com/docker/docker/issues/17190>docker #17190</a>) — and this cannot be easily turned off. In version 1.10 Docker is planning to <a href=https://github.com/docker/docker/issues/17195>bundle a new DNS server</a>, and it’s unclear whether this will be able to be turned off. Container-level naming is not the right abstraction for Kubernetes — we already have our own concepts of service naming, discovery, and binding, and we already have our own DNS schema and server (based on the well-established <a href=https://github.com/skynetservices/skydns>SkyDNS</a>). The bundled solutions are not sufficient for our needs but are not disableable.</p><p>Orthogonal to the local/global split, Docker has both in-process and out-of-process ("remote") plugins. We investigated whether we could bypass libnetwork (and thereby skip the issues above) and drive Docker remote plugins directly. Unfortunately, this would mean that we could not use any of the Docker in-process plugins, "bridge" and "overlay" in particular, which again eliminates much of the utility of libnetwork.</p><p>On the other hand, CNI is more philosophically aligned with Kubernetes. It's far simpler than CNM, doesn't require daemons, and is at least plausibly cross-platform (CoreOS’s <a href=https://coreos.com/rkt/docs/>rkt</a> container runtime supports it). Being cross-platform means that there is a chance to enable network configurations which will work the same across runtimes (e.g. Docker, Rocket, Hyper). It follows the UNIX philosophy of doing one thing well.</p><p>Additionally, it's trivial to wrap a CNI plugin and produce a more customized CNI plugin — it can be done with a simple shell script. CNM is much more complex in this regard. This makes CNI an attractive option for rapid development and iteration. Early prototypes have proven that it's possible to eject almost 100% of the currently hard-coded network logic in kubelet into a plugin.</p><p>We investigated <a href=https://groups.google.com/g/kubernetes-sig-network/c/5MWRPxsURUw>writing a "bridge" CNM driver</a> for Docker that ran CNI drivers. This turned out to be very complicated. First, the CNM and CNI models are very different, so none of the "methods" lined up. We still have the global vs. local and key-value issues discussed above. Assuming this driver would declare itself local, we have to get info about logical networks from Kubernetes.</p><p>Unfortunately, Docker drivers are hard to map to other control planes like Kubernetes. Specifically, drivers are not told the name of the network to which a container is being attached — just an ID that Docker allocates internally. This makes it hard for a driver to map back to any concept of network that exists in another system.</p><p>This and other issues have been brought up to Docker developers by network vendors, and are usually closed as "working as intended" (<a href=https://github.com/docker/libnetwork/issues/139>libnetwork #139</a>, <a href=https://github.com/docker/libnetwork/issues/486>libnetwork #486</a>, <a href=https://github.com/docker/libnetwork/pull/514>libnetwork #514</a>, <a href=https://github.com/docker/libnetwork/issues/865>libnetwork #865</a>, <a href=https://github.com/docker/docker/issues/18864>docker #18864</a>), even though they make non-Docker third-party systems more difficult to integrate with. Throughout this investigation Docker has made it clear that they’re not very open to ideas that deviate from their current course or that delegate control. This is very worrisome to us, since Kubernetes complements Docker and adds so much functionality, but exists outside of Docker itself.</p><p>For all of these reasons we have chosen to invest in CNI as the Kubernetes plugin model. There will be some unfortunate side-effects of this. Most of them are relatively minor (for example, <code>docker inspect</code> will not show an IP address), but some are significant. In particular, containers started by <code>docker run</code> might not be able to communicate with containers started by Kubernetes, and network integrators will have to provide CNI drivers if they want to fully integrate with Kubernetes. On the other hand, Kubernetes will get simpler and more flexible, and a lot of the ugliness of early bootstrapping (such as configuring Docker to use our bridge) will go away.</p><p>As we proceed down this path, we’ll certainly keep our eyes and ears open for better ways to integrate and simplify. If you have thoughts on how we can do that, we really would like to hear them — find us on <a href=http://slack.k8s.io/>slack</a> or on our <a href=https://groups.google.com/g/kubernetes-sig-network>network SIG mailing-list</a>.</p><p>Tim Hockin, Software Engineer, Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f2fbd7b648c34ea2b5bc519080891738>Simple leader election with Kubernetes and Docker</h1><div class="td-byline mb-4"><time datetime=2016-01-11 class=text-muted>Monday, January 11, 2016</time></div><h4 id=overview>Overview</h4><p>Kubernetes simplifies the deployment and operational management of services running on clusters. However, it also simplifies the development of these services. In this post we'll see how you can use Kubernetes to easily perform leader election in your distributed application. Distributed applications usually replicate the tasks of a service for reliability and scalability, but often it is necessary to designate one of the replicas as the leader who is responsible for coordination among all of the replicas.</p><p>Typically in leader election, a set of candidates for becoming leader is identified. These candidates all race to declare themselves the leader. One of the candidates wins and becomes the leader. Once the election is won, the leader continually "heartbeats" to renew their position as the leader, and the other candidates periodically make new attempts to become the leader. This ensures that a new leader is identified quickly, if the current leader fails for some reason.</p><p>Implementing leader election usually requires either deploying software such as ZooKeeper, etcd or Consul and using it for consensus, or alternately, implementing a consensus algorithm on your own. We will see below that Kubernetes makes the process of using leader election in your application significantly easier.</p><h4 id=implementing-leader-election-in-kubernetes>Implementing leader election in Kubernetes</h4><p>The first requirement in leader election is the specification of the set of candidates for becoming the leader. Kubernetes already uses <em>Endpoints</em> to represent a replicated set of pods that comprise a service, so we will re-use this same object. (aside: You might have thought that we would use <em>ReplicationControllers</em>, but they are tied to a specific binary, and generally you want to have a single leader even if you are in the process of performing a rolling update)</p><p>To perform leader election, we use two properties of all Kubernetes API objects:</p><ul><li>ResourceVersions - Every API object has a unique ResourceVersion, and you can use these versions to perform compare-and-swap on Kubernetes objects</li><li>Annotations - Every API object can be annotated with arbitrary key/value pairs to be used by clients.</li></ul><p>Given these primitives, the code to use master election is relatively straightforward, and you can find it <a href=https://github.com/kubernetes/contrib/pull/353>here</a>. Let's run it ourselves.</p><pre><code>$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example
</code></pre><p>This creates a leader election set with 3 replicas:</p><pre><code>$ kubectl get pods
NAME                   READY     STATUS    RESTARTS   AGE
leader-elector-inmr1   1/1       Running   0          13s
leader-elector-qkq00   1/1       Running   0          13s
leader-elector-sgwcq   1/1       Running   0          13s
</code></pre><p>To see which pod was chosen as the leader, you can access the logs of one of the pods, substituting one of your own pod's names in place of</p><pre><code>${pod_name}, (e.g. leader-elector-inmr1 from the above)

$ kubectl logs -f ${name}
leader is (leader-pod-name)
</code></pre><p>… Alternately, you can inspect the endpoints object directly:</p><p><em>'example' is the name of the candidate set from the above kubectl run … command</em></p><pre><code>$ kubectl get endpoints example -o yaml
</code></pre><p>Now to validate that leader election actually works, in a different terminal, run:</p><pre><code>$ kubectl delete pods (leader-pod-name)
</code></pre><p>This will delete the existing leader. Because the set of pods is being managed by a replication controller, a new pod replaces the one that was deleted, ensuring that the size of the replicated set is still three. Via leader election one of these three pods is selected as the new leader, and you should see the leader failover to a different pod. Because pods in Kubernetes have a <em>grace period</em> before termination, this may take 30-40 seconds.</p><p>The leader-election container provides a simple webserver that can serve on any address (e.g. http://localhost:4040). You can test this out by deleting the existing leader election group and creating a new one where you additionally pass in a --http=(host):(port) specification to the leader-elector image. This causes each member of the set to serve information about the leader via a webhook.</p><pre><code># delete the old leader elector group
$ kubectl delete rc leader-elector

# create the new group, note the --http=localhost:4040 flag
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example --http=0.0.0.0:4040

# create a proxy to your Kubernetes api server
$ kubectl proxy
</code></pre><p>You can then access:</p><p>http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/</p><p>And you will see:</p><pre><code>{&quot;name&quot;:&quot;(name-of-leader-here)&quot;}
</code></pre><h4 id=leader-election-with-sidecars>Leader election with sidecars</h4><p>Ok, that's great, you can do leader election and find out the leader over HTTP, but how can you use it from your own application? This is where the notion of sidecars come in. In Kubernetes, Pods are made up of one or more containers. Often times, this means that you add sidecar containers to your main application to make up a Pod. (for a much more detailed treatment of this subject see my earlier blog post).</p><p>The leader-election container can serve as a sidecar that you can use from your own application. Any container in the Pod that's interested in who the current master is can simply access http://localhost:4040 and they'll get back a simple JSON object that contains the name of the current master. Since all containers in a Pod share the same network namespace, there's no service discovery required!</p><p>For example, here is a simple Node.js application that connects to the leader election sidecar and prints out whether or not it is currently the master. The leader election sidecar sets its identifier to <code>hostname</code> by default.</p><pre><code>var http = require('http');
// This will hold info about the current master
var master = {};

  // The web handler for our nodejs application
  var handleRequest = function(request, response) {
    response.writeHead(200);
    response.end(&quot;Master is &quot; + master.name);
  };

  // A callback that is used for our outgoing client requests to the sidecar
  var cb = function(response) {
    var data = '';
    response.on('data', function(piece) { data = data + piece; });
    response.on('end', function() { master = JSON.parse(data); });
  };

  // Make an async request to the sidecar at http://localhost:4040
  var updateMaster = function() {
    var req = http.get({host: 'localhost', path: '/', port: 4040}, cb);
    req.on('error', function(e) { console.log('problem with request: ' + e.message); });
    req.end();
  };

  / / Set up regular updates
  updateMaster();
  setInterval(updateMaster, 5000);

  // set up the web server
  var www = http.createServer(handleRequest);
  www.listen(8080);
</code></pre><p>Of course, you can use this sidecar from any language that you choose that supports HTTP and JSON.</p><h4 id=conclusion>Conclusion</h4><p>Hopefully I've shown you how easy it is to build leader election for your distributed application using Kubernetes. In future installments we'll show you how Kubernetes is making building distributed systems even easier. In the meantime, head over to <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> or <a href=http://kubernetes.io/>kubernetes.io</a> to get started with Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1718a0c9c9389c18359c3cfc4c12df57>Creating a Raspberry Pi cluster running Kubernetes, the installation (Part 2)</h1><div class="td-byline mb-4"><time datetime=2015-12-22 class=text-muted>Tuesday, December 22, 2015</time></div><p>At Devoxx Belgium and Devoxx Morocco, <a href=https://twitter.com/saturnism>Ray Tsang</a> and I (<a href=https://twitter.com/ArjenWassink>Arjen Wassink</a>) showed a Raspberry Pi cluster we built at Quintor running HypriotOS, Docker and Kubernetes. While we received many compliments on the talk, the most common question was about how to build a Pi cluster themselves! We’ll be doing just that, in two parts. The <a href=https://kubernetes.io/blog/2015/11/creating-a-Raspberry-Pi-cluster-running-Kubernetes-the-shopping-list-Part-1>first part covered the shopping list for the cluster</a>, and this second one will show you how to get kubernetes up and running . . .</p><p>Now you got your Raspberry Pi Cluster all setup, it is time to run some software on it. As mentioned in the previous blog I based this tutorial on the Hypriot linux distribution for the ARM processor. Main reason is the bundled support for Docker. I used <a href=http://downloads.hypriot.com/hypriot-rpi-20151004-132414.img.zip>this version of Hypriot</a> for this tutorial, so if you run into trouble with other versions of Hypriot, please consider the version I’ve used.</p><p>First step is to make sure every Pi has Hypriot running, if not yet please check the <a href=http://blog.hypriot.com/getting-started-with-docker-on-your-arm-device/>getting started guide</a> of them. Also hook up the cluster switch to a network so that Internet is available and every Pi get an IP-address assigned via DHCP. Because we will be running multiple Pi’s it is practical to give each Pi a unique hostname. I renamed my Pi’s to rpi-master, rpi-node-1, rpi-node-2, etc for my convenience. Note that on Hypriot the hostname is set by editing the /boot/occidentalis.txt file, not the /etc/hostname. You could also set the hostname using the Hypriot flash tool.</p><p>The most important thing about running software on a Pi is the availability of an ARM distribution. Thanks to <a href=https://twitter.com/brendandburns>Brendan Burns</a>, there are Kubernetes components for ARM available in the <a href=https://cloud.google.com/container-registry/docs/>Google Cloud Registry</a>. That’s great. The second hurdle is how to install Kubernetes. There are two ways; directly on the system or in a Docker container. Although the container support has an experimental status, I choose to go for that because it makes it easier to install Kubernetes for you. Kubernetes requires several processes (etcd, flannel, kubectl, etc) to run on a node, which should be started in a specific order. To ease that, systemd services are made available to start the necessary processes in the right way. Also the systemd services make sure that Kubernetes is spun up when a node is (re)booted. To make the installation real easy I created an simple install script for the master node and the worker nodes. All is available at <a href=https://github.com/awassink/k8s-on-rpi>GitHub</a>. So let’s get started now!</p><h3 id=installing-the-kubernetes-master-node>Installing the Kubernetes master node</h3><p>First we will be installing Kubernetes on the master node and add the worker nodes later to the cluster. It comes basically down to getting the git repository content and executing the installation script.</p><pre><code>$ curl -L -o k8s-on-rpi.zip https://github.com/awassink/k8s-on-rpi/archive/master.zip

$ apt-get update

$ apt-get install unzip

$ unzip k8s-on-rpi.zip

$ k8s-on-rpi-master/install-k8s-master.sh
</code></pre><p>The install script will install five services:</p><ul><li>docker-bootstrap.service - is a separate Docker daemon to run etcd and flannel because flannel needs to be running before the standard Docker daemon (docker.service) because of network configuration.</li><li>k8s-etcd.service - is the etcd service for storing flannel and kubelet data.</li><li>k8s-flannel.service - is the flannel process providing an overlay network over all nodes in the cluster.</li><li>docker.service - is the standard Docker daemon, but with flannel as a network bridge. It will run all Docker containers.</li><li>k8s-master.service - is the kubernetes master service providing the cluster functionality.</li></ul><p>The basic details of this installation procedure is also documented in the <a href=https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/getting-started-guides/docker-multinode/master.md>Getting Started Guide</a> of Kubernetes. Please check it to get more insight on how a multi node Kubernetes cluster is setup.</p><p>Let’s check if everything is working correctly. Two docker daemon processes must be running.</p><pre><code>$ ps -ef|grep docker
root       302     1  0 04:37 ?        00:00:14 /usr/bin/docker daemon -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-bootstrap.pid --storage-driver=overlay --storage-opt dm.basesize=10G --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-bootstrap

root       722     1 11 04:38 ?        00:16:11 /usr/bin/docker -d -bip=10.0.97.1/24 -mtu=1472 -H fd:// --storage-driver=overlay -D
</code></pre><p>The etcd and flannel containers must be up.</p><pre><code>$ docker -H unix:///var/run/docker-bootstrap.sock ps

CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES

4855cc1450ff        andrewpsuedonym/flanneld     &quot;flanneld --etcd-endp&quot;   2 hours ago         Up 2 hours                              k8s-flannel

ef410b986cb3        andrewpsuedonym/etcd:2.1.1   &quot;/bin/etcd --addr=127&quot;   2 hours ago         Up 2 hours                              k8s-etcd


The hyperkube kubelet, apiserver, scheduler, controller and proxy must be up.

$ docker ps

CONTAINER ID        IMAGE                                           COMMAND                  CREATED             STATUS              PORTS               NAMES

a17784253dd2        gcr.io/google\_containers/hyperkube-arm:v1.1.2   &quot;/hyperkube controller&quot;   2 hours ago         Up 2 hours                              k8s\_controller-manager.7042038a\_k8s-master-127.0.0.1\_default\_43160049df5e3b1c5ec7bcf23d4b97d0\_2174a7c3

a0fb6a169094        gcr.io/google\_containers/hyperkube-arm:v1.1.2   &quot;/hyperkube scheduler&quot;   2 hours ago         Up 2 hours                              k8s\_scheduler.d905fc61\_k8s-master-127.0.0.1\_default\_43160049df5e3b1c5ec7bcf23d4b97d0\_511945f8

d93a94a66d33        gcr.io/google\_containers/hyperkube-arm:v1.1.2   &quot;/hyperkube apiserver&quot;   2 hours ago         Up 2 hours                              k8s\_apiserver.f4ad1bfa\_k8s-master-127.0.0.1\_default\_43160049df5e3b1c5ec7bcf23d4b97d0\_b5b4936d

db034473b334        gcr.io/google\_containers/hyperkube-arm:v1.1.2   &quot;/hyperkube kubelet -&quot;   2 hours ago         Up 2 hours                              k8s-master

f017f405ff4b        gcr.io/google\_containers/hyperkube-arm:v1.1.2   &quot;/hyperkube proxy --m&quot;   2 hours ago         Up 2 hours                              k8s-master-proxy
</code></pre><h3 id=deploying-the-first-pod-and-service-on-the-cluster>Deploying the first pod and service on the cluster</h3><p>When that’s looking good we’re able to access the master node of the Kubernetes cluster with kubectl. Kubectl for ARM can be downloaded from googleapis storage. kubectl get nodes shows which cluster nodes are registered with its status. The master node is named 127.0.0.1.</p><pre><code>$ curl -fsSL -o /usr/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/arm/kubectl

$ kubectl get nodes

NAME              LABELS                                   STATUS    AGE

127.0.0.1         kubernetes.io/hostname=127.0.0.1         Ready      1h


An easy way to test the cluster is by running a busybox docker image for ARM. kubectl run can be used to run the image as a container in a pod. kubectl get pods shows the pods that are registered with its status.

$ kubectl run busybox --image=hypriot/rpi-busybox-httpd

$ kubectl get pods -o wide

NAME                   READY     STATUS    RESTARTS   AGE       NODE

busybox-fry54          1/1       Running   1          1h        127.0.0.1

k8s-master-127.0.0.1   3/3       Running   6          1h        127.0.0.1
</code></pre><p>Now the pod is running but the application is not generally accessible. That can be achieved by creating a service. The cluster IP-address is the IP-address the service is avalailable within the cluster. Use the IP-address of your master node as external IP and the service becomes available outside of the cluster (e.g. at http://192.168.192.161 in my case).</p><pre><code>$ kubectl expose rc busybox --port=90 --target-port=80 --external-ip=\&lt;ip-address-master-node\&gt;

$ kubectl get svc

NAME         CLUSTER\_IP   EXTERNAL\_IP       PORT(S)   SELECTOR      AGE

busybox      10.0.0.87    192.168.192.161   90/TCP    run=busybox   1h

kubernetes   10.0.0.1     \&lt;none\&gt;            443/TCP   \&lt;none\&gt;        2h

$ curl http://10.0.0.87:90/
</code></pre><pre><code>\&lt;html\&gt;

\&lt;head\&gt;\&lt;title\&gt;Pi armed with Docker by Hypriot\&lt;/title\&gt;

  \&lt;body style=&quot;width: 100%; background-color: black;&quot;\&gt;

    \&lt;div id=&quot;main&quot; style=&quot;margin: 100px auto 0 auto; width: 800px;&quot;\&gt;

      \&lt;img src=&quot;pi\_armed\_with\_docker.jpg&quot; alt=&quot;pi armed with docker&quot; style=&quot;width: 800px&quot;\&gt;

    \&lt;/div\&gt;

  \&lt;/body\&gt;

\&lt;/html\&gt;
</code></pre><h3 id=installing-the-kubernetes-worker-nodes>Installing the Kubernetes worker nodes</h3><p>The next step is installing Kubernetes on each worker node and add it to the cluster. This also comes basically down to getting the git repository content and executing the installation script. Though in this installation the k8s.conf file needs to be copied on forehand and edited to contain the IP-address of the master node.</p><pre><code>$ curl -L -o k8s-on-rpi.zip https://github.com/awassink/k8s-on-rpi/archive/master.zip

$ apt-get update

$ apt-get install unzip

$ unzip k8s-on-rpi.zip

$ mkdir /etc/kubernetes

$ cp k8s-on-rpi-master/rootfs/etc/kubernetes/k8s.conf /etc/kubernetes/k8s.conf
</code></pre><h3 id=change-the-ip-address-in-etc-kubernetes-k8s-conf-to-match-the-master-node>Change the ip-address in /etc/kubernetes/k8s.conf to match the master node</h3><pre><code>$ k8s-on-rpi-master/install-k8s-worker.sh
</code></pre><p>The install script will install four services. These are the quite similar to ones on the master node, but with the difference that no etcd service is running and the kubelet service is configured as worker node.</p><p>Once all the services on the worker node are up and running we can check that the node is added to the cluster on the master node.</p><pre><code>$ kubectl get nodes

NAME              LABELS                                   STATUS    AGE

127.0.0.1         kubernetes.io/hostname=127.0.0.1         Ready     2h

192.168.192.160   kubernetes.io/hostname=192.168.192.160   Ready     1h

$ kubectl scale --replicas=2 rc/busybox

$ kubectl get pods -o wide

NAME                   READY     STATUS    RESTARTS   AGE       NODE

busybox-fry54          1/1       Running   1          1h        127.0.0.1

busybox-j2slu          1/1       Running   0          1h        192.168.192.160

k8s-master-127.0.0.1   3/3       Running   6          2h        127.0.0.1
</code></pre><h3 id=enjoy-your-kubernetes-cluster>Enjoy your Kubernetes cluster!</h3><p>Congratulations! You now have your Kubernetes Raspberry Pi cluster running and can start playing with Kubernetes and start learning. Checkout the <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/README.md>Kubernetes User Guide</a> to find out what you all can do. And don’t forget to pull some plugs occasionally like Ray and I do :-)</p><p>Arjen Wassink, Java Architect and Team Lead, Quintor</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9ead5345aff00f9fdaf83a1f428f0559>Managing Kubernetes Pods, Services and Replication Controllers with Puppet</h1><div class="td-byline mb-4"><time datetime=2015-12-17 class=text-muted>Thursday, December 17, 2015</time></div><p><em>Today’s guest post is written by Gareth Rushgrove, Senior Software Engineer at Puppet Labs, a leader in IT automation. Gareth tells us about a new Puppet module that helps manage resources in Kubernetes. </em></p><p>People familiar with <a href=https://github.com/puppetlabs/puppet>Puppet</a> might have used it for managing files, packages and users on host computers. But Puppet is first and foremost a configuration management tool, and config management is a much broader discipline than just managing host-level resources. A good definition of configuration management is that it aims to solve four related problems: identification, control, status accounting and verification and audit. These problems exist in the operation of any complex system, and with the new <a href=https://forge.puppetlabs.com/garethr/kubernetes>Puppet Kubernetes module</a> we’re starting to look at how we can solve those problems for Kubernetes.</p><h3 id=the-puppet-kubernetes-module>The Puppet Kubernetes Module</h3><p>The Puppet Kubernetes module currently assumes you already have a Kubernetes cluster <a href=/docs/tutorials/kubernetes-basics/>up and running</a>;Its focus is on managing the resources in Kubernetes, like Pods, Replication Controllers and Services, not (yet) on managing the underlying kubelet or etcd services. Here’s a quick snippet of code describing a Pod in Puppet’s DSL.</p><pre><code>kubernetes_pod { 'sample-pod':
  ensure =&gt; present,
  metadata =&gt; {
    namespace =&gt; 'default',
  },
  spec =&gt; {
    containers =&gt; [{
      name =&gt; 'container-name',
      image =&gt; 'nginx',
    }]
  },
</code></pre><p>}</p><p>If you’re familiar with the YAML file format, you’ll probably recognise the structure immediately. The interface is intentionally identical to aid conversion between different formats — in fact, the code powering this is autogenerated from the Kubernetes API Swagger definitions. Running the above code, assuming we save it as pod.pp, is as simple as:</p><pre><code>puppet apply pod.pp
</code></pre><p>Authentication uses the standard kubectl configuration file. You can find complete <a href=https://github.com/garethr/garethr-kubernetes/blob/master/README.md>installation instructions in the module's README</a>.</p><p>Kubernetes has several resources, from Pods and Services to Replication Controllers and Service Accounts. You can see an example of the module managing these resources in the <a href=https://puppetlabs.com/blog/kubernetes-guestbook-example-puppet>Kubernetes guestbook sample in Puppet</a> post. This demonstrates converting the canonical hello-world example to use Puppet code.</p><p>One of the main advantages of using Puppet for this, however, is that you can create your own higher-level and more business-specific interfaces to Kubernetes-managed applications. For instance, for the guestbook, you could create something like the following:</p><pre><code>guestbook { 'myguestbook':
  redis_slave_replicas =&gt; 2,
  frontend_replicas =&gt; 3,
  redis_master_image =&gt; 'redis',
  redis_slave_image =&gt; 'gcr.io/google_samples/gb-redisslave:v1',
  frontend_image =&gt; 'gcr.io/google_samples/gb-frontend:v3',     
}
</code></pre><p>You can read more about using Puppet’s defined types, and see lots more code examples, in the Puppet blog post, <a href=https://puppetlabs.com/blog/building-your-own-abstractions-kubernetes-puppet>Building Your Own Abstractions for Kubernetes in Puppet</a>.</p><h3 id=conclusions>Conclusions</h3><p>The advantages of using Puppet rather than just the standard YAML files and kubectl are:</p><ul><li>The ability to create your own abstractions to cut down on repetition and craft higher-level user interfaces, like the guestbook example above. </li><li>Use of Puppet’s development tools for validating code and for writing unit tests. </li><li>Integration with other tools such as Puppet Server, for ensuring that your model in code matches the state of your cluster, and with PuppetDB for storing reports and tracking changes.</li><li>The ability to run the same code repeatedly against the Kubernetes API, to detect any changes or remediate configuration drift. </li></ul><p>It’s also worth noting that most large organisations will have very heterogenous environments, running a wide range of software and operating systems. Having a single toolchain that unifies those discrete systems can make adopting new technology like Kubernetes much easier.</p><p>It’s safe to say that Kubernetes provides an excellent set of primitives on which to build cloud-native systems. And with Puppet, you can address some of the operational and configuration management issues that come with running any complex system in production. <a href=mailto:gareth@puppetlabs.com>Let us know</a> what you think if you try the module out, and what else you’d like to see supported in the future.</p><p> - Gareth Rushgrove, Senior Software Engineer, Puppet Labs</p></div><div class=td-content style=page-break-before:always><h1 id=pg-83aeab877e6fb05786088a780849b599>How Weave built a multi-deployment solution for Scope using Kubernetes</h1><div class="td-byline mb-4"><time datetime=2015-12-12 class=text-muted>Saturday, December 12, 2015</time></div><p><em>Today we hear from Peter Bourgon, Software Engineer at Weaveworks, a company that provides software for developers to network, monitor and control microservices-based apps in docker containers. Peter tells us what was involved in selecting and deploying Kubernetes </em></p><p>Earlier this year at Weaveworks we launched <a href=http://weave.works/product/scope/index.html>Weave Scope</a>, an open source solution for visualization and monitoring of containerised apps and services. Recently we released a hosted Scope service into an <a href=http://blog.weave.works/2015/10/08/weave-the-fastest-path-to-docker-on-amazon-ec2-container-service/>Early Access Program</a>. Today, we want to walk you through how we initially prototyped that service, and how we ultimately chose and deployed Kubernetes as our platform.</p><h5 id=a-cloud-native-architecture-nbsp>A cloud-native architecture </h5><p>Scope already had a clean internal line of demarcation between data collection and user interaction, so it was straightforward to split the application on that line, distribute probes to customers, and host frontends in the cloud. We built out a small set of microservices in the <a href=http://12factor.net/>12-factor model</a>, which includes:</p><ul><li>A users service, to manage and authenticate user accounts </li><li>A provisioning service, to manage the lifecycle of customer Scope instances </li><li>A UI service, hosting all of the fancy HTML and JavaScript content </li><li>A frontend service, to route requests according to their properties </li><li>A monitoring service, to introspect the rest of the system </li></ul><p>All services are built as Docker images, <a href=https://medium.com/@kelseyhightower/optimizing-docker-images-for-static-binaries-b5696e26eb07#.qqjkud6i0>FROM scratch</a> where possible. We knew that we wanted to offer at least 3 deployment environments, which should be as near to identical as possible. </p><ul><li>An "Airplane Mode" local environment, on each developer's laptop </li><li>A development or staging environment, on the same infrastructure that hosts production, with different user credentials </li><li>The production environment itself </li></ul><p>These were our application invariants. Next, we had to choose our platform and deployment model.</p><h5 id=our-first-prototype-nbsp>Our first prototype </h5><p>There are a seemingly infinite set of choices, with an infinite set of possible combinations. After surveying the landscape in mid-2015, we decided to make a prototype with</p><ul><li><a href=https://aws.amazon.com/ec2/>Amazon EC2</a> as our cloud platform, including RDS for persistence </li><li><a href=https://docs.docker.com/swarm/>Docker Swarm</a> as our "scheduler" </li><li><a href=https://consul.io/>Consul</a> for service discovery when bootstrapping Swarm </li><li><a href=https://www.weave.works/oss/net/>Weave Net</a> for our network and service discovery for the application itself </li><li><a href=https://terraform.io/>Terraform</a> as our provisioner </li></ul><p>This setup was fast to define and fast to deploy, so it was a great way to validate the feasibility of our ideas. But we quickly hit problems. </p><ul><li>Terraform's support for <a href=https://terraform.io/docs/providers/docker>Docker as a provisioner</a> is barebones, and we uncovered <a href=https://github.com/hashicorp/terraform/issues/3526>some bugs</a> when trying to use it to drive Swarm. </li><li>Largely as a consequence of the above, managing a zero-downtime deploy of Docker containers with Terraform was very difficult. </li><li>Swarm's <em>raison d'être</em> is to abstract the particulars of multi-node container scheduling behind the familiar Docker CLI/API commands. But we concluded that the API is insufficiently expressive for the kind of operations that are necessary at scale in production. </li><li>Swarm provides no fault tolerance in the case of e.g. node failure. </li></ul><p>We also made a number of mistakes when designing our workflow.</p><ul><li>We tagged each container with its target environment at build time, which simplified our Terraform definitions, but effectively forced us to manage our versions via image repositories. That responsibility belongs in the scheduler, not the artifact store. </li><li>As a consequence, every deploy required artifacts to be pushed to all hosts. This made deploys slow, and rollbacks unbearable. </li><li>Terraform is designed to provision infrastructure, not cloud applications. The process is slower and more deliberate than we’d like. Shipping a new version of something to prod took about 30 minutes, all-in. </li></ul><p>When it became clear that the service had potential, we re-evaluated the deployment model with an eye towards the long-term.</p><h5 id=rebasing-on-kubernetes-nbsp>Rebasing on Kubernetes </h5><p>It had only been a couple of months, but a lot had changed in the landscape.</p><ul><li>HashiCorp released <a href=https://nomadproject.io/>Nomad</a> </li><li><a href=https://kubernetes.io/>Kubernetes</a> hit 1.0 </li><li>Swarm was soon to hit 1.0 </li></ul><p>While many of our problems could be fixed without making fundamental architectural changes, we wanted to capitalize on the advances in the industry, by joining an existing ecosystem, and leveraging the experience and hard work of its contributors. </p><p>After some internal deliberation, we did a small-scale audition of Nomad and Kubernetes. We liked Nomad a lot, but felt it was just too early to trust it with our production service. Also, we found the Kubernetes developers to be the most responsive to issues on GitHub. So, we decided to go with Kubernetes.</p><h5 id=local-kubernetes-nbsp>Local Kubernetes </h5><p>First, we would replicate our Airplane Mode local environment with Kubernetes. Because we have developers on both Mac and Linux laptops, it’s important that the local environment is containerised. So, we wanted the Kubernetes components themselves (kubelet, API server, etc.) to run in containers.</p><p>We encountered two main problems. First, and most broadly, creating Kubernetes clusters from scratch is difficult, as it requires deep knowledge of how Kubernetes works, and quite some time to get the pieces to fall in place together. <a href=http://local-cluster-up.sh/>local-cluster-up.sh</a> seems like a Kubernetes developer’s tool and didn’t leverage containers, and the third-party solutions we found, like <a href=https://github.com/rimusz/coreos-osx-kubernetes-solo>Kubernetes Solo</a>, require a dedicated VM or are platform-specific.</p><p>Second, containerised Kubernetes is still missing several important pieces. Following the <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/docker.md>official Kubernetes Docker guide</a> yields a barebones cluster without certificates or service discovery. We also encountered a couple of usability issues (<a href=https://github.com/kubernetes/kubernetes/issues/16586>#16586</a>, <a href=https://github.com/kubernetes/kubernetes/issues/17157>#17157</a>), which we resolved by <a href=https://github.com/kubernetes/kubernetes/pull/17159>submitting a patch</a> and building our own <a href=https://hub.docker.com/r/2opremio/hyperkube/>hyperkube image</a> from master.</p><p>In the end, we got things working by creating our own provisioning script. It needs to do things like <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/admin/authentication.md#creating-certificates>generate the PKI keys and certificates</a> and <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns>provision the DNS add-on</a>, which took a few attempts to get right. We’ve also learned of a <a href=https://github.com/kubernetes/kubernetes/commit/ce90b83689f08cb5ebb6b632dab7f95a48060425>commit to add certificate generation to the Docker build</a>, so things will likely get easier in the near term.</p><h5 id=kubernetes-on-aws-nbsp>Kubernetes on AWS </h5><p>Next, we would deploy Kubernetes to AWS, and wire it up with the other AWS components. We wanted to stand up the service in production quickly, and we only needed to support Amazon, so we decided to do so without Weave Net and to use a pre-existing provisioning solution. But we’ll definitely revisit this decision in the near future, leveraging Weave Net via Kubernetes plugins.</p><p>Ideally we would have used Terraform resources, and we found a couple: <a href=https://github.com/Samsung-AG/kraken>kraken</a> (using Ansible), <a href=https://github.com/kelseyhightower/kubestack>kubestack</a> (coupled to GCE), <a href=https://github.com/bakins/kubernetes-coreos-terraform>kubernetes-coreos-terraform</a> (outdated Kubernetes) and <a href=https://github.com/coreos/coreos-kubernetes>coreos-kubernetes</a>. But they all build on CoreOS, which was an extra moving part we wanted to avoid in the beginning. (On our next iteration, we’ll probably audition CoreOS.) If you use Ansible, there are <a href=https://github.com/kubernetes/contrib/tree/master/ansible>playbooks available</a> in the main repo. There are also community-drive <a href=https://github.com/evilmartians/chef-kubernetes>Chef cookbooks</a> and <a href=https://forge.puppetlabs.com/cristifalcas/kubernetes>Puppet modules</a>. I’d expect the community to grow quickly here.</p><p>The only other viable option seemed to be kube-up, which is a collection of scripts that provision Kubernetes onto a variety of cloud providers. By default, kube-up onto AWS puts the master and minion nodes into their own VPC, or Virtual Private Cloud. But our RDS instances were provisioned in the region-default VPC, which meant that communication from a Kubernetes minion to the DB would be possible only via <a href=http://ben.straub.cc/2015/08/19/kubernetes-aws-vpc-peering/>VPC peering</a> or by opening the RDS VPC's firewall rules manually.</p><p>To get traffic to traverse a VPC peer link, your destination IP needs to be in the target VPC's private address range. But <a href="https://forums.aws.amazon.com/thread.jspa?messageID=681125">it turns out</a> that resolving the RDS instance's hostname from anywhere outside the same VPC will yield the public IP. And performing the resolution is important, because RDS reserves the right to change the IP for maintenance. This wasn't ever a concern in the previous infrastructure, because our Terraform scripts simply placed everything in the same VPC. So I thought I'd try the same with Kubernetes; the kube-up script ostensibly supports installing to an existing VPC by specifying a VPC_ID environment variable, so I tried installing Kubernetes to the RDS VPC. kube-up appeared to succeed, but <a href=https://github.com/kubernetes/kubernetes/issues/17647>service integration via ELBs broke</a> and<a href=https://github.com/kubernetes/kubernetes/issues/17219>teardown via kube-down stopped working</a>. After some time, we judged it best to let kube-up keep its defaults, and poked a hole in the RDS VPC.</p><p>This was one hiccup among several that we encountered. Each one could be fixed in isolation, but the inherent fragility of using a shell script to provision remote state seemed to be the actual underlying cause. We fully expect the Terraform, Ansible, Chef, Puppet, etc. packages to continue to mature, and hope to switch soon.</p><p>Provisioning aside, there are great things about the Kubernetes/AWS integration. For example, Kubernetes <a href=http://kubernetes.io/v1.1/docs/user-guide/services.html>services</a> of the correct type automatically generate ELBs, and Kubernetes does a great job of lifecycle management there. Further, the Kubernetes domain model—services, <a href=http://kubernetes.io/v1.1/docs/user-guide/pods.html>pods</a>, <a href=http://kubernetes.io/v1.1/docs/user-guide/replication-controller.html>replication controllers</a>, the <a href=http://kubernetes.io/v1.1/docs/user-guide/labels.html>labels and selector model</a>, and so on—is coherent, and seems to give the user the right amount of expressivity, though the definition files do <a href=https://github.com/kubernetes/kubernetes/blob/643cb7a1c7499df4e569f4f0fbd3b18c0c4e63ce/examples/guestbook/redis-master-controller.yaml>tend to stutter needlessly</a>. The kubectl tool is good, albeit <a href=http://i.imgur.com/nEyTWej.png>daunting at first glance</a>. The <a href=http://kubernetes.io/v1.1/docs/user-guide/update-demo/README.html>rolling-update</a> command in particular is brilliant: exactly the semantics and behavior I'd expect from a system like this. Indeed, once Kubernetes was up and running, <em>it just worked</em>, and exactly as I expected it to. That’s a huge thing.</p><h5 id=conclusions-nbsp>Conclusions </h5><p>After a couple weeks of fighting with the machines, we were able to resolve all of our integration issues, and have rolled out a reasonably robust Kubernetes-based system to production.</p><ul><li><strong>Provisioning Kubernetes is difficult</strong> , owing to a complex architecture and young provisioning story. This shows all signs of improving. </li><li>Kubernetes’ non-optional <strong>security model takes time to get right</strong>. </li><li>The Kubernetes <strong>domain language is a great match</strong> to the problem domain. </li><li>We have <strong>a lot more confidence</strong> in operating our application (It's a lot faster, too.). </li><li>And we're <strong>very happy to be part of a growing Kubernetes userbase</strong> , contributing issues and patches as we can and benefitting from the virtuous cycle of open-source development that powers the most exciting software being written today. 
 - Peter Bourgon, Software Engineer at Weaveworks</li></ul><p><em>Weave Scope is an open source solution for visualization and monitoring of containerised apps and services. For a hosted Scope service, request an invite to Early Access program at scope.weave.works.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-cf6e9f299e66c20fe0263f3d4484de3c>Creating a Raspberry Pi cluster running Kubernetes, the shopping list (Part 1)</h1><div class="td-byline mb-4"><time datetime=2015-11-25 class=text-muted>Wednesday, November 25, 2015</time></div><p>At Devoxx Belgium and Devoxx Morocco, Ray Tsang and I showed a Raspberry Pi cluster we built at Quintor running HypriotOS, Docker and Kubernetes. For those who did not see the talks, you can check out <a href="https://www.youtube.com/watch?v=AAS5Mq9EktI">an abbreviated version of the demo</a> or the full talk by Ray on <a href="https://www.youtube.com/watch?v=kT1vmK0r184">developing and deploying Java-based microservices</a> in Kubernetes. While we received many compliments on the talk, the most common question was about how to build a Pi cluster themselves! We’ll be doing just that, in two parts. This first post will cover the shopping list for the cluster, and the second will show you how to get it up and running . . .</p><h3 id=wait-why-the-heck-build-a-raspberry-pi-cluster-running-kubernetes-nbsp>Wait! Why the heck build a Raspberry Pi cluster running Kubernetes? </h3><p>We had two big reasons to build the Pi cluster at Quintor. First of all we wanted to experiment with container technology at scale on real hardware. You can try out container technology using virtual machines, but Kubernetes runs great on bare metal too. To explore what that’d be like, we built a Raspberry Pi cluster just like we would build a cluster of machines in a production datacenter. This allowed us to understand and simulate how Kubernetes would work when we move it to our data centers.</p><p>Secondly, we did not want to blow the budget to do this exploration. And what is cheaper than a Raspberry Pi! If you want to build a cluster comprising many nodes, each node should have a good cost to performance ratio. Our Pi cluster has 20 CPU cores, which is more than many servers, yet cost us less than $400. Additionally, the total power consumption is low and the form factor is small, which is great for these kind of demo systems.</p><p>So, without further ado, let’s get to the hardware.</p><h3 id=the-shopping-list>The Shopping List:</h3><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>5</td><td>Raspberry Pi 2 model B</td><td><a href=https://www.raspberrypi.org/products/raspberry-pi-2-model-b/>~$200</a></td></tr><tr><td>5</td><td>16 GB micro SD-card class 10</td><td>~ $45</td></tr><tr><td>1</td><td>D-Link Switch GO-SW-8E 8-Port</td><td><a href=http://www.dlink.com/uk/en/home-solutions/connect/go/go-sw-8e>~$15</a></td></tr><tr><td>1</td><td>Anker 60W 6-Port PowerPort USB Charger (white)</td><td><a href=http://www.ianker.com/product/A2123122>~$35</a></td></tr><tr><td>3</td><td>ModMyPi Multi-Pi Stackable Raspberry Pi Case</td><td><a href=http://www.modmypi.com/raspberry-pi/cases/multi-pi-stacker/multi-pi-stackable-raspberry-pi-case>~$60</a></td></tr><tr><td>1</td><td>ModMyPi Multi-Pi Stackable Raspberry Pi Case - Bolt Pack</td><td><a href=http://www.modmypi.com/raspberry-pi/cases/multi-pi-stacker/multi-pi-stackable-raspberry-pi-case-bolt-pack>~$7</a></td></tr><tr><td>5</td><td>Micro USB cable (white) 1ft long</td><td>~ $10</td></tr><tr><td>5</td><td>UTP cat5 cable (white) 1ft long</td><td>~ $10</td></tr></tbody></table><br>For a total of approximately $380 you will have a building set to create a Raspberry Pi cluster like we built! [1](#1)<h3 id=some-of-our-considerations-nbsp>Some of our considerations </h3><p>We used the Raspberry Pi 2 model B boards in our cluster rather than the Pi 1 boards because of the CPU power (quadcore @ 900MHz over a dualcore @ 700MHz) and available memory (1 GB over 512MB). These specs allowed us to run multiple containers on each Pi to properly experiment with Kubernetes.</p><p>We opted for a 16GB SD-card in each Pi to be at the save side on filesystem storage. In hindsight, 8GB seemed to be enough.</p><p>Note the GeauxRobot Stackable Case looks like an alternative for the ModMyPi Stackable Case, but it’s smaller which can cause a problem fitting in the Anker USB Adapter and placing the D-Link Network Switch. So, we stuck with the ModMyPi case.</p><h3 id=putting-it-together-nbsp>Putting it together </h3><p>Building the Raspberry Pi cluster is pretty straight forward. Most of the work is putting the stackable casing together and mounting the Pi boards on the plexiglass panes. We mounted the network switch and USB Adapter using double side foam tape, which feels strong enough for most situations. Finally, we connected the USB and UTP cables. Next, we installed HypriotOS on every Pi. HypriotOS is a Raspbian based Linux OS for Raspberry Pi’s extended with Docker support. The Hypriot team has an excellent tutorial on <a href=http://blog.hypriot.com/getting-started-with-docker-on-your-arm-device/>Getting started with Docker on your Raspberry Pi</a>. Follow this tutorial to get Linux and Docker running on all Pi’s.</p><p>With that, you’re all set! Next up will be running Kubernetes on the Raspberry Pi cluster. We’ll be covering this the <a href=https://kubernetes.io/blog/2015/12/creating-raspberry-pi-cluster-running>next post</a>, so stay tuned!</p><p>Arjen Wassink, Java Architect and Team Lead, Quintor</p><p>** ## [1] ## **
**[1] **To save ~$90 by making a stack of four Pi’s (instead of five). This also means you can use a 5-Port Anker USB Charger instead of the 6-Port one.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0949771c0825e8ab602bc8ccff39f4ab>Monitoring Kubernetes with Sysdig</h1><div class="td-byline mb-4"><time datetime=2015-11-19 class=text-muted>Thursday, November 19, 2015</time></div><p><em>Today we’re sharing a guest post by Chris Crane from Sysdig about their monitoring integration into Kubernetes. </em></p><p>Kubernetes offers a full environment to write scalable and service-based applications. It takes care of things like container grouping, discovery, load balancing and healing so you don’t have to worry about them. The design is elegant, scalable and the APIs are a pleasure to use.</p><p>And like any new infrastructure platform, if you want to run Kubernetes in production, you’re going to want to be able to monitor and troubleshoot it. We’re big fans of Kubernetes here at Sysdig, and, well: we’re here to help.</p><p>Sysdig offers native visibility into Kubernetes across the full Sysdig product line. That includes <a href=http://www.sysdig.org/>sysdig</a>, our open source, CLI system exploration tool, and <a href=https://sysdig.com/>Sysdig Cloud</a>, the first and only monitoring platform designed from the ground up to support containers and microservices.</p><p>At a high level, Sysdig products are aware of the entire Kubernetes cluster hierarchy, including <strong>namespaces, services, replication controllers</strong> and <strong>labels</strong>. So all of the rich system and application data gathered is now available in the context of your Kubernetes infrastructure. What does this mean for you? In a nutshell, we believe Sysdig can be your go-to tool for making Kubernetes environments significantly easier to monitor and troubleshoot!</p><p>In this post I will quickly preview the Kubernetes visibility in both open source sysdig and Sysdig Cloud, and show off a couple interesting use cases. Let’s start with the open source solution.</p><h3 id=exploring-a-kubernetes-cluster-with-csysdig-nbsp>Exploring a Kubernetes Cluster with csysdig </h3><p>The easiest way to take advantage of sysdig’s Kubernetes support is by launching csysdig, the sysdig ncurses UI:</p><p><code> > csysdig -k http://127.0.0.1:8080</code><br>*Note: specify the address of your Kubernetes API server with the -k command, and sysdig will poll all the relevant information, leveraging both the standard and the watch API.</p><p>Now that csysdig is running, hit F2 to bring up the views panel, and you'll notice the presence of a bunch of new views. The <strong>k8s Namespaces</strong> view can be used to see the list of namespaces and observe the amount of CPU, memory, network and disk resources each of them is using on this machine:</p><p><a href=https://2.bp.blogspot.com/-9kXfpo76r0k/Vkz8AkpctEI/AAAAAAAAAss/yvf9oc759Wg/s1600/sisdig%2B6.png><img src=https://2.bp.blogspot.com/-9kXfpo76r0k/Vkz8AkpctEI/AAAAAAAAAss/yvf9oc759Wg/s640/sisdig%2B6.png alt></a></p><p>Similarly, you can select <strong>k8s Services</strong> to see the same information broken up by service:</p><p><a href=https://2.bp.blogspot.com/-Ya1W3Z_ETcs/Vkz8AN3XtfI/AAAAAAAAAs8/HNv_TvHpfHU/s1600/sisdig%2B2.png><img src=https://2.bp.blogspot.com/-Ya1W3Z_ETcs/Vkz8AN3XtfI/AAAAAAAAAs8/HNv_TvHpfHU/s640/sisdig%2B2.png alt></a></p><p>or <strong>k8s Controllers</strong> to see the replication controllers:</p><p><a href=https://3.bp.blogspot.com/-gGkgXRC5P6g/Vkz8A1RVyAI/AAAAAAAAAtQ/SFlHQeNrDjQ/s1600/sysdig%2B1.png><img src=https://3.bp.blogspot.com/-gGkgXRC5P6g/Vkz8A1RVyAI/AAAAAAAAAtQ/SFlHQeNrDjQ/s640/sysdig%2B1.png alt></a></p><p>or <strong>k8s Pods</strong> to see the list of pods running on this machine and the resources they use:</p><p><a href=https://3.bp.blogspot.com/-PrDfWzi9F3c/Vkz8H6rPlII/AAAAAAAAAtc/f46tE6EKvoo/s1600/sisdig%2B7.png><img src=https://3.bp.blogspot.com/-PrDfWzi9F3c/Vkz8H6rPlII/AAAAAAAAAtc/f46tE6EKvoo/s640/sisdig%2B7.png alt></a></p><h3 id=drill-down-based-navigation-nbsp>Drill Down-Based Navigation </h3><p>A cool feature in csysdig is the ability to drill down: just select an element, click on enter and – boom – now you're looking inside it. Drill down is also aware of the Kubernetes hierarchy, which means I can start from a service, get the list of its pods, see which containers run inside one of the pods, and go inside one of the containers to explore files, network connections, processes or even threads. Check out the video below.</p><p><a href=https://1.bp.blogspot.com/-lQ-P2gLywlY/Vkz9MOoTgGI/AAAAAAAAAtk/UB6pW7sUbQA/s1600/image09.gif><img src=https://1.bp.blogspot.com/-lQ-P2gLywlY/Vkz9MOoTgGI/AAAAAAAAAtk/UB6pW7sUbQA/s640/image09.gif alt></a></p><h3 id=actions-nbsp>Actions! </h3><p>One more thing about csysdig. As <a href=https://sysdig.com/csysdigs-hotkeys-turning-csysdig-into-a-control-panel-for-processes-connections-and-containers/>recently announced</a>, csysdig also offers “control panel” functionality, making it possible to use hotkeys to execute command lines based on the element currently selected. So we made sure to enrich the Kubernetes views with a bunch of useful hotkeys. For example, you can delete a namespace or a service by pressing "x," or you can describe them by pressing "d."</p><p>My favorite hotkeys, however, are "f," to follow the logs that a pod is generating, and "b," which leverages <code>kubectl</code> exec to give you a shell inside a pod. Being brought into a bash prompt for the pod you’re observing is really useful and, frankly, a bit magic. :-)</p><p>So that’s a quick preview of Kubernetes in sysdig. Note though, that all of this functionality is only for a single machine. What happens if you want to monitor a distributed Kubernetes cluster? Enter Sysdig Cloud.</p><h3 id=monitoring-kubernetes-with-sysdig-cloud-nbsp>Monitoring Kubernetes with Sysdig Cloud </h3><p>Let’s start with a quick review of Kubernetes’ architecture. From the physical/infrastructure point of view, a Kubernetes cluster is made up of a set of <strong>minion</strong> machines overseen by a <strong>master</strong> machine. The master’s tasks include orchestrating containers across minions, keeping track of state and exposing cluster control through a REST API and a UI.</p><p>On the other hand, from the logical/application point of view, Kubernetes clusters are arranged in the hierarchical fashion shown in this picture:</p><p><a href=https://1.bp.blogspot.com/-p_x0bLRdFJo/Vkz8IPR5q4I/AAAAAAAAAtg/D9UU2MfPmcI/s1600/sisdig%2B4.png><img src=https://1.bp.blogspot.com/-p_x0bLRdFJo/Vkz8IPR5q4I/AAAAAAAAAtg/D9UU2MfPmcI/s640/sisdig%2B4.png alt></a></p><ul><li>All containers run inside <strong>pods</strong>. A pod can host a single container, or multiple cooperating containers; in the latter case, the containers in the pod are guaranteed to be co-located on the same machine and can share resources. </li><li>Pods typically sit behind <strong>services</strong> , which take care of balancing the traffic, and also expose the set of pods as a single discoverable IP address/port. </li><li>Services are scaled horizontally by <strong>replication controllers</strong> (“RCs”) which create/destroy pods for each service as needed. </li><li><strong>Namespaces</strong> are virtual clusters that can include one or more services. </li></ul><p>So just to be clear, multiple services and even multiple namespaces can be scattered across the same physical infrastructure.</p><p>After talking to hundreds of Kubernetes users, it seems that the typical cluster administrator is often interested in looking at things from the physical point of view, while service/application developers tend to be more interested in seeing things from the logical point of view. </p><p>With both these use cases in mind, Sysdig Cloud’s support for Kubernetes works like this: </p><ul><li>By automatically connecting to a Kubernetes’ cluster API Server and querying the API (both the regular and the watch API), Sysdig Cloud is able to infer both the physical and the logical structure of your microservice application. </li><li>In addition, we transparently extract important metadata such as labels. </li><li>This information is combined with our patent-pending ContainerVision technology, which makes it possible to inspect applications running inside containers without requiring any instrumentation of the container or application. 
Based on this, Sysdig Cloud can provide rich visibility and context from both an <strong>infrastructure-centric</strong> and an <strong>application-centric</strong> point of view. Best of both worlds! Let’s check out what this actually looks like.</li></ul><p>One of the core features of Sysdig Cloud is groups, which allow you to define the hierarchy of metadata for your applications and infrastructure. By applying the proper groups, you can explore your containers based on their physical hierarchy (for example, physical cluster > minion machine > pod > container) or based on their logical microservice hierarchy (for example, namespace > replication controller > pod > container – as you can see in this example). </p><p>If you’re interested in the utilization of your underlying physical resource – e.g., identifying noisy neighbors – then the physical hierarchy is great. But if you’re looking to explore the performance of your applications and microservices, then the logical hierarchy is often the best place to start. </p><p><a href=https://4.bp.blogspot.com/-80u3oSEi_Fw/Vkz8AZgE6eI/AAAAAAAAAtE/3iRDMJKBNmc/s1600/sisdig%2B5.png><img src=https://4.bp.blogspot.com/-80u3oSEi_Fw/Vkz8AZgE6eI/AAAAAAAAAtE/3iRDMJKBNmc/s640/sisdig%2B5.png alt></a></p><p>For example: here you can see the overall performance of our WordPress service: </p><p><a href=https://4.bp.blogspot.com/-QAsedrM2UxI/Vkz8Aas-26I/AAAAAAAAAtM/9B7Z33vUQrg/s1600/sisdig%2B3.png><img src=https://4.bp.blogspot.com/-QAsedrM2UxI/Vkz8Aas-26I/AAAAAAAAAtM/9B7Z33vUQrg/s640/sisdig%2B3.png alt></a></p><p>Keep in mind that the pods implementing this service are scattered across multiple machines, but we can still total request counts, response times and URL statistics aggregated together for this service. And don’t forget: this doesn’t require any configuration or instrumentation of wordpress, apache, or the underlying containers! </p><p>And from this view, I can now easily create alerts for these service-level metrics, and I can dig down into any individual container for deep inspection - down to the process level  – whenever I want, including back in time! </p><h3 id=visualizing-your-kubernetes-services-nbsp>Visualizing Your Kubernetes Services </h3><p>We’ve also included Kubernetes awareness in Sysdig Cloud’s famous topology view, at both the physical and logical level. </p><p><a href=https://2.bp.blogspot.com/-2is-UJatmPk/Vk0AtdfvYvI/AAAAAAAAAt0/9SEsl2LCpYI/s1600/image02.gif><img src=https://2.bp.blogspot.com/-2is-UJatmPk/Vk0AtdfvYvI/AAAAAAAAAt0/9SEsl2LCpYI/s640/image02.gif alt></a></p><p><a href=https://2.bp.blogspot.com/-hGQtaIV9XTA/Vk0RnwtlcGI/AAAAAAAAAuM/7ndiyAWpSvU/s1600/image08.gif><img src=https://2.bp.blogspot.com/-hGQtaIV9XTA/Vk0RnwtlcGI/AAAAAAAAAuM/7ndiyAWpSvU/s640/image08.gif alt></a></p><p>The two pictures below show the exact same infrastructure and services. But the first one depicts the physical hierarchy, with a master node and three minion nodes; while the second one groups containers into namespaces, services and pods, while abstracting the physical location of the containers. </p><p>Hopefully it’s self-evident how much more natural and intuitive the second (services-oriented) view is. The structure of the application and the various dependencies are immediately clear. The interactions between various microservices become obvious, despite the fact that these microservices are intermingled across our machine cluster! </p><h3 id=conclusion-nbsp>Conclusion </h3><p>I’m pretty confident that what we’re delivering here represents a huge leap in visibility into Kubernetes environments and it won’t disappoint you. I also hope it can be a useful tool enabling you to use Kubernetes in production with a little more peace of mind. Thanks, and happy digging! </p><p>Chris Crane, VP Product, Sysdig </p><p><em>You can find open source sysdig on <a href=https://github.com/draios/sysdig>github</a> and at <a href=http://sysdig.org/>sysdig.org</a>, and you can sign up for free trial of Sysdig Cloud at <a href=http://sysdig.com/>sysdig.com</a>. </em></p><p><em>To see a live demo and meet some of the folks behind the project join us this Thursday for a <a href=http://www.meetup.com/Bay-Area-Kubernetes-Meetup/events/226574438/>Kubernetes and Sysdig Meetup in San Francisco</a>.</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b84d35cb4a57e7bd8d88b045b7b67c9f>One million requests per second: Dependable and dynamic distributed systems at scale</h1><div class="td-byline mb-4"><time datetime=2015-11-11 class=text-muted>Wednesday, November 11, 2015</time></div><p>Recently, I’ve gotten in the habit of telling people that building a reliable service isn’t that hard. If you give me two Compute Engine virtual machines, a Cloud Load balancer, supervisord and nginx, I can create you a static web service that will serve a static web page, effectively forever.</p><p>The real challenge is building agile AND reliable services. In the new world of software development it's trivial to spin up enormous numbers of machines and push software to them. Developing a successful product must <em>also</em> include the ability to respond to changes in a predictable way, to handle upgrades elegantly and to minimize downtime for users. Missing on any one of these elements results in an <em>unsuccessful</em> product that's flaky and unreliable. I remember a time, not that long ago, when it was common for websites to be unavailable for an hour around midnight each day as a safety window for software upgrades. My bank still does this. It’s really not cool.</p><p>Fortunately, for developers, our infrastructure is evolving along with the requirements that we’re placing on it. Kubernetes has been designed from the ground up to make it easy to design, develop and deploy dependable, dynamic services that meet the demanding requirements of the cloud native world.</p><p>To demonstrate exactly what we mean by this, I've developed a simple demo of a Container Engine cluster serving 1 million HTTP requests per second. In all honesty, serving 1 million requests per second isn’t really that exciting. In fact, it’s really so very <a href=http://googlecloudplatform.blogspot.com/2013/11/compute-engine-load-balancing-hits-1-million-requests-per-second.html>2013</a>.</p><p><a href=https://4.bp.blogspot.com/-eACCKAzuQFQ/VkO1rwW1DRI/AAAAAAAAAko/zKu-19QCCBU/s1600/image01.gif><img src=https://4.bp.blogspot.com/-eACCKAzuQFQ/VkO1rwW1DRI/AAAAAAAAAko/zKu-19QCCBU/s640/image01.gif alt></a></p><p>What <em>is</em> exciting is that while successfully handling 1 million HTTP requests per second with uninterrupted availability, we have Kubernetes perform a zero-downtime rolling upgrade of the service to a new version of the software <em>while we're  <strong>still</strong> serving 1 million requests per second</em>.</p><p><a href=https://2.bp.blogspot.com/-_96_QwNRHLo/VkO1oDAyLLI/AAAAAAAAAkk/B_y5Uh5ngPU/s1600/image00.gif><img src=https://2.bp.blogspot.com/-_96_QwNRHLo/VkO1oDAyLLI/AAAAAAAAAkk/B_y5Uh5ngPU/s640/image00.gif alt></a></p><p>This is only possible due to a large number of performance tweaks and enhancements that have gone into the <a href=https://kubernetes.io/blog/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community>Kubernetes 1.1 release</a>. I’m incredibly proud of all of the features that our community has built into this release. Indeed in addition to making it possible to serve 1 million requests per second, we’ve also added an auto-scaler, so that you won’t even have to wake up in the middle of the night to scale your service in response to load or memory pressures.</p><p>If you want to try this out on your own cluster (or use the load test framework to test your own service) the code for the <a href=https://github.com/kubernetes/contrib/pull/226>demo is available on github</a>. And the <a href="https://www.youtube.com/watch?v=7TOWLerX0Ps">full video</a> is available.</p><p>I hope I’ve shown you how Kubernetes can enable developers of distributed systems to achieve both reliability and agility at scale, and as always, if you’re interested in learning more, head over to <a href=http://kubernetes.io/>kubernetes.io</a> or <a href=https://github.com/kubernetes/kubernetes>github</a> and connect with the community on our <a href=http://slack.kubernetes.io/>Slack</a> channel. </p><p>"https://www.youtube.com/embed/7TOWLerX0Ps"</p><ul><li>Brendan Burns, Senior Staff Software Engineer, Google, Inc.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-992cbdcc8b44baeab01f6bc0ef8b4eae>Kubernetes 1.1 Performance upgrades, improved tooling and a growing community</h1><div class="td-byline mb-4"><time datetime=2015-11-09 class=text-muted>Monday, November 09, 2015</time></div><p>Since the Kubernetes 1.0 release in July, we’ve seen tremendous adoption by companies building distributed systems to manage their container clusters. We’re also been humbled by the rapid growth of the community who help make Kubernetes better everyday. We have seen commercial offerings such as Tectonic by CoreOS and RedHat Atomic Host emerge to deliver deployment and support of Kubernetes. And a growing ecosystem has added Kubernetes support including tool vendors such as Sysdig and Project Calico.</p><p>With the help of hundreds of contributors, we’re proud to announce the availability of Kubernetes 1.1, which offers major performance upgrades, improved tooling, and new features that make applications even easier to build and deploy.</p><p>Some of the work we’d like to highlight includes:</p><ul><li><p><strong>Substantial performance improvements</strong> : We have architected Kubernetes from day one to handle Google-scale workloads, and our customers have put it through their paces. In Kubernetes 1.1, we have made further investments to ensure that you can run in extremely high-scale environments; later this week, we will be sharing examples of running thousand node clusters, and running over a million QPS against a single cluster. </p></li><li><p><strong>Significant improvement in network throughput</strong> : Running Google-scale workloads also requires Google-scale networking. In Kubernetes 1.1, we have included an option to use native IP tables offering an 80% reduction in tail latency, an almost complete elimination of CPU overhead and improvements in reliability and system architecture ensuring Kubernetes can handle high-scale throughput well into the future. </p></li><li><p><strong>Horizontal pod autoscaling (Beta)</strong>: Many workloads can go through spiky periods of utilization, resulting in uneven experiences for your users. Kubernetes now has support for horizontal pod autoscaling, meaning your pods can scale up and down based on CPU usage. Read more about <a href=http://kubernetes.io/v1.1/docs/user-guide/horizontal-pod-autoscaler.html>Horizontal pod autoscaling</a>. </p></li><li><p><strong>HTTP load balancer (Beta)</strong>: Kubernetes now has the built-in ability to route HTTP traffic based on the packets introspection. This means you can have ‘http://foo.com/bar’ go to one service, and ‘http://foo.com/meep’ go to a completely independent service. Read more about the <a href=http://kubernetes.io/v1.1/docs/user-guide/ingress.html>Ingress object</a>. </p></li><li><p><strong>Job objects (Beta)</strong>: We’ve also had frequent request for integrated batch jobs, such as processing a batch of images to create thumbnails or a particularly large data file that has been broken down into many chunks. <a href=https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/jobs.md#writing-a-job-spec>Job objects</a> introduces a new API object that runs a workload, restarts it if it fails, and keeps trying until it’s successfully completed. Read more about the<a href=http://kubernetes.io/v1.1/docs/user-guide/jobs.html>Job object</a>. </p></li><li><p><strong>New features to shorten the test cycle for developers</strong> : We continue to work on making developing for applications for Kubernetes quick and easy. Two new features that speeds developer’s workflows include the ability to run containers interactively, and improved schema validation to let you know if there are any issues with your configuration files before you deploy them. </p></li><li><p><strong>Rolling update improvements</strong> : Core to the DevOps movement is being able to release new updates without any affect on a running service. Rolling updates now ensure that updated pods are healthy before continuing the update. </p></li><li><p>And many more. For a complete list of updates, see the <a href=https://github.com/kubernetes/kubernetes/releases>1.1. release</a> notes on GitHub </p></li></ul><p>Today, we’re also proud to mark the inaugural Kubernetes conference, <a href=https://kubecon.io/>KubeCon</a>, where some 400 community members along with dozens of vendors are in attendance supporting the Kubernetes project.</p><p>We’d love to highlight just a few of the many partners making Kubernetes better:</p><blockquote><p>“We are betting our major product, Tectonic – which enables any company to deploy, manage and secure its containers anywhere – on Kubernetes because we believe it is the future of the data center. The release of Kubernetes 1.1 is another major milestone that will create more widespread adoption of distributed systems and containers, and puts us on a path that will inevitably lead to a whole new generation of products and services.” – Alex Polvi, CEO, CoreOS.</p></blockquote><blockquote><p>“Univa’s customers are looking for scalable, enterprise-caliber solutions to simplify managing container and non-container workloads in the enterprise. We selected Kubernetes as a foundational element of our new Navops suite which will help IT and DevOps rapidly integrate containerized workloads into their production systems and extend these workloads into cloud services.” – Gary Tyreman, CEO, Univa.</p></blockquote><blockquote><p>“The tremendous customer demand we’re seeing to run containers at scale with Kubernetes is a critical element driving growth in our professional services business at Redapt. As a trusted advisor, it’s great to have a tool like Kubernetes in our tool belt to help our customers achieve their objectives.” – Paul Welch, SR VP Cloud Solutions, Redapt</p></blockquote><blockquote></blockquote><p>As we mentioned above, we would love your help:</p><ul><li>Get involved with the Kubernetes project on <a href=https://github.com/kubernetes/kubernetes>GitHub</a> </li><li>Connect with the community on <a href=http://slack.kubernetes.io/>Slack</a></li><li>Follow us on Twitter <a href=https://twitter.com/kubernetesio>@Kubernetesio</a> for latest updates </li><li>Post questions (or answer questions) on Stackoverflow </li><li>Get started running, deploying, and using Kubernetes <a href=/docs/tutorials/kubernetes-basics/>guides</a>;</li></ul><p>But, most of all, just let us know how you are transforming your business using Kubernetes, and how we can help you do it even faster. Thank you for your support!</p><p> - David Aronchick, Senior Product Manager for Kubernetes and Google Container Engine</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9838f9decb115e0a07e334aa453f42f0>Kubernetes as Foundation for Cloud Native PaaS</h1><div class="td-byline mb-4"><time datetime=2015-11-03 class=text-muted>Tuesday, November 03, 2015</time></div><p>With Kubernetes continuing to gain momentum as a critical tool for building and scaling container based applications, we’ve been thrilled to see a growing number of platform as a service (PaaS) offerings adopt it as a foundation. PaaS developers have been drawn to Kubernetes by its rapid rate of maturation, the soundness of its core architectural concepts, and the strength of its contributor community. The <a href=https://kubernetes.io/blog/2015/07/the-growing-kubernetes-ecosystem>Kubernetes ecosystem</a> continues to grow, and these PaaS projects are great additions to it.</p><p><a href=https://1.bp.blogspot.com/-xX93tnoIlGo/Vjj2fSc_CDI/AAAAAAAAAi0/lvTkT9jyFog/s1600/k8%2Bipaas%2B1.png><img src=https://1.bp.blogspot.com/-xX93tnoIlGo/Vjj2fSc_CDI/AAAAAAAAAi0/lvTkT9jyFog/s400/k8%2Bipaas%2B1.png alt></a></p><blockquote><p>“<a href=http://deis.io/>Deis</a> is the leading Docker PaaS with over a million downloads, actively used by companies like Mozilla, The RealReal, ShopKeep and Coinbase. Deis provides software teams with a turn-key platform for running containers in production, featuring the ability to build and store Docker images, production-grade load balancing, a streamlined developer interface and an ops-ready suite of logging and monitoring infrastructure backed by world-class 24x7x365 support. After a community-led evaluation of alternative orchestrators, it was clear that Kubernetes represents a decade of experience running containers at scale inside Google. The Deis project is proud to be rebasing onto Kubernetes and is thrilled to join its vibrant community." - Gabriel Monroy, CTO of <a href=https://www.engineyard.com/>Engine Yard</a>, Inc.</p></blockquote><p><a href=https://1.bp.blogspot.com/-1XZFGRHGb34/Vjj2wUtA6pI/AAAAAAAAAi8/SD-qRhVIiIs/s1600/k8%2Bipaas%2B2.png><img src=https://1.bp.blogspot.com/-1XZFGRHGb34/Vjj2wUtA6pI/AAAAAAAAAi8/SD-qRhVIiIs/s400/k8%2Bipaas%2B2.png alt></a></p><p><a href=http://www.openshift.org/>OpenShift</a> by Red Hat helps organizations accelerate application delivery by enabling development and IT operations teams to be more agile, responsive and efficient. OpenShift Enterprise 3 is the first fully supported, enterprise-ready, web-scale container application platform that natively integrates the Docker container runtime and packaging format, Kubernetes container orchestration and management engine, on a foundation of Red Hat Enterprise Linux 7, all fully supported by Red Hat from the operating system to application runtimes.</p><blockquote><p>“Kubernetes provides OpenShift users with a powerful model for application orchestration, leveraging concepts like pods and services, to deploy (micro)services that inherently span multiple containers and application topologies that will require wiring together multiple services. Pods can be optionally mapped to storage, which means you can run both stateful and stateless services in OpenShift. Kubernetes also provides a powerful declarative management model to manage the lifecycle of application containers. Customers can then use Kubernetes’ integrated scheduler to deploy and manage containers across multiple hosts. As a leading contributor to both the Docker and Kubernetes open source projects, Red Hat is not just adopting these technologies but actively building them upstream in the community.”  - Joe Fernandes, Director of Product Management for Red Hat OpenShift.</p></blockquote><p><a href=https://2.bp.blogspot.com/-t3L1CANyhUs/Vjj28Zpf9WI/AAAAAAAAAjE/Ef-PLLmHGvU/s1600/k8%2Bipaas%2B3.png><img src=https://2.bp.blogspot.com/-t3L1CANyhUs/Vjj28Zpf9WI/AAAAAAAAAjE/Ef-PLLmHGvU/s400/k8%2Bipaas%2B3.png alt></a></p><p>Huawei, a leading global ICT technology solution provider, will offer container as a service (CaaS) built on Kubernetes in the public cloud for customers with Docker based applications. Huawei CaaS services will manage multiple clusters across data centers, and deploy, monitor and scale containers with high availability and high resource utilization for their customers. For example, one of Huawei’s current software products for their telecom customers utilizes tens of thousands of modules and hundreds of instances in virtual machines. By moving to a container based PaaS platform powered by Kubernetes, Huawei is migrating this product into a micro-service based, cloud native architecture. By decoupling the modules, they’re creating a high performance, scalable solution that runs hundreds, even thousands of containers in the system. Decoupling existing heavy modules could have been a painful exercise. However, using several key concepts introduced by Kubernetes, such as pods, services, labels, and proxies, Huawei has been able to re-architect their software with great ease.</p><p>Huawei has made Kubernetes the core runtime engine for container based applications/services, and they’ve been building other PaaS components or capabilities around Kubernetes, such as user access management, composite API, Portal and multiple cluster management. Additionally, as part of the migration to the new platform, they’re enhancing their PaaS solution in the areas of advanced scheduling algorithm, multi tenant support and enhanced container network communication to support customer needs.</p><blockquote><p>“Huawei chose Kubernetes as the foundation for our offering because we like the abstract concepts of services, pod and label for modeling and distributed applications. We developed an application model based on these concepts to model existing complex applications which works well for moving legacy applications into the cloud. In addition, Huawei intends for our PaaS platform to support many scenarios, and Kubernetes’ flexible architecture with its plug-in capability is key to our platform architecture.”- Ying Xiong, Chief Architect of PaaS at Huawei.</p></blockquote><p><a href=https://2.bp.blogspot.com/-Ys0Zn4IQzn0/Vjj3JIE0BVI/AAAAAAAAAjM/ktwltzVa1GE/s1600/k8%2Bipaas%2B4.png><img src=https://2.bp.blogspot.com/-Ys0Zn4IQzn0/Vjj3JIE0BVI/AAAAAAAAAjM/ktwltzVa1GE/s400/k8%2Bipaas%2B4.png alt></a></p><p><a href=https://gondor.io/>Gondor</a>is a PaaS with a focus on application hosting throughout the lifecycle, from development to testing to staging to production. It supports Python, Go, and Node.js applications as well as technologies such as Postgres, Redis and Elasticsearch. The Gondor team recently re-architected Gondor to incorporate Kubernetes, and discussed this in a <a href=https://gondor.io/blog/2015/07/21/rebuilding-gondor-kubernetes/>blog post.</a></p><blockquote><p>“There are two main reasons for our move to Kubernetes: One, by taking care of the lower layers in a truly scalable fashion, Kubernetes lets us focus on providing a great product at the application layer. Two, the portability of Kubernetes allows us to expand our PaaS offering to on-premises, private cloud and a multitude of alternative infrastructure providers.” - Brian Rosner, Chief Architect at Eldarion (the driving force behind Gondor)</p></blockquote><ul><li>Martin Buhr, Google Business Product Manager</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-70135ddec926f5b40edc548eb31767e5>Some things you didn’t know about kubectl</h1><div class="td-byline mb-4"><time datetime=2015-10-28 class=text-muted>Wednesday, October 28, 2015</time></div><p><a href=https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/kubectl-overview.md>kubectl</a> is the command line tool for interacting with Kubernetes clusters. Many people use it every day to deploy their container workloads into production clusters. But there’s more to kubectl than just <code>kubectl create -f or kubectl rolling-update</code>. kubectl is a veritable multi-tool of container orchestration and management. Below we describe some of the features of kubectl that you may not have seen.</p><p><strong>Important Note</strong> : Most of these features are part of the upcoming 1.1 release of Kubernetes. They are not present in the current stable 1.0.x release series.</p><h5 id=run-interactive-commands>Run interactive commands</h5><p><code>kubectl run</code> has been in kubectl since the 1.0 release, but recently we added the ability to run interactive containers in your cluster. That means that an interactive shell in your Kubernetes cluster is as close as:</p><pre><code>$&gt; kubectl run -i --tty busybox --image=busybox --restart=Never -- sh   
Waiting for pod default/busybox-tv9rm to be running, status is Pending, pod ready: false   
Waiting for pod default/busybox-tv9rm to be running, status is Running, pod ready: false   
$&gt; # ls 
bin dev etc home proc root sys tmp usr var 
$&gt; # exit  
</code></pre><p>The above <code>kubectl</code> command is equivalent to <code>docker run -i -t busybox sh.</code> Sadly we mistakenly used <code>-t</code> for template in kubectl 1.0, so we need to retain backwards compatibility with existing CLI user. But the existing use of <code>-t</code> is deprecated and we’ll eventually shorten <code>--tty</code> to <code>-t</code>.</p><p>In this example, <code>-i</code> indicates that you want an allocated <code>stdin</code> for your container and indicates that you want an interactive session, <code>--restart=Never</code> indicates that the container shouldn’t be restarted after you exit the terminal and <code>--tty</code> requests that you allocate a TTY for that session.</p><h5 id=view-your-pod-s-logs>View your Pod’s logs</h5><p>Sometimes you just want to watch what’s going on in your server. For this, <code>kubectl logs</code> is the subcommand to use. Adding the -f flag lets you live stream new logs to your terminal, just like tail -f.<br>$> kubectl logs -f redis-izl09</p><h5 id=attach-to-existing-containers>Attach to existing containers</h5><p>In addition to interactive execution of commands, you can now also attach to any running process. Like kubectl logs, you’ll get stderr and stdout data, but with attach, you’ll also be able to send stdin from your terminal to the program. Awesome for interactive debugging, or even just sending ctrl-c to a misbehaving application.</p><pre><code>      $&gt; kubectl attach redis -i
</code></pre><p>1:C 12 Oct 23:05:11.848 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf</p><pre><code>                _._                                                  
           _.-``__''-._                                             
      _.-`` `. `_. ''-._ Redis 3.0.3 (00000000/0) 64 bit
  .-`` .-```. ```\/ _.,_ ''-._                                   
 ( ' , .-` | `, ) Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379
 | `-._ `._ / _.-' | PID: 1
  `-._ `-._ `-./ _.-' _.-'                                   
 |`-._`-._ `-.__.-' _.-'_.-'|                                  
 | `-._`-._ _.-'_.-' | http://redis.io
`-._ `-._`-.__.-'_.-' _.-'                                   
 |`-._`-._ `-.__.-' _.-'_.-'|                                  
 | `-._`-._ _.-'_.-' |                                  
  `-._ `-._`-.__.-'_.-' _.-'                                   
      `-._ `-.__.-' _.-'                                       
          `-._ _.-'                                           
              `-.__.-'                                               

1:M 12 Oct 23:05:11.849 # Server started, Redis version 3.0.3
</code></pre><h5 id=forward-ports-from-pods-to-your-local-machine>Forward ports from Pods to your local machine</h5><p>Often times you want to be able to temporarily communicate with applications in your cluster without exposing them to the public internet for security reasons. To achieve this, the port-forward command allows you to securely forward a port on your local machine through the kubernetes API server to a Pod running in your cluster. For example:</p><p><code>$> kubectl port-forward redis-izl09 6379</code></p><p>Opens port 6379 on your local machine and forwards communication to that port to the Pod or Service in your cluster. For example, you can use the ‘telnet’ command to poke at a Redis service in your cluster:</p><pre><code>$&gt; telnet localhost 6379   
INCR foo   
:1   
INCR foo 
:2  
</code></pre><h3 id=execute-commands-inside-an-existing-container>Execute commands inside an existing container</h3><p>In addition to being able to attach to existing processes inside a container, the “exec” command allows you to spawn new processes inside existing containers. This can be useful for debugging, or examining your pods to see what’s going on inside without interrupting a running service. <code>kubectl exec</code> is different from <code>kubectl run</code>, because it runs a command inside of an <em>existing</em> container, rather than spawning a new container for execution.</p><pre><code>$&gt; kubectl exec redis-izl09 -- ls /
bin
boot
data
dev
entrypoint.sh
etc
home
</code></pre><h5 id=add-or-remove-labels>Add or remove Labels</h5><p>Sometimes you want to dynamically add or remove labels from a Pod, Service or Replication controller. Maybe you want to add an existing Pod to a Service, or you want to remove a Pod from a Service. No matter what you want, you can easily and dynamically add or remove labels using the <code>kubectl label</code> subcommand:</p><p><code>$> kubectl label pods redis-izl09 mylabel=awesome </code><br><code>pod "redis-izl09" labeled</code></p><h5 id=add-annotations-to-your-objects>Add annotations to your objects</h5><p>Just like labels, you can add or remove annotations from API objects using the kubectl annotate subcommand. Unlike labels, annotations are there to help describe your object, but aren’t used to identify pods via label queries (<a href=https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/annotations.md#annotations>more details on annotations</a>). For example, you might add an annotation of an icon for a GUI to use for displaying your pods.</p><p><code>$> kubectl annotate pods redis-izl09 icon-url=http://goo.gl/XXBTWq </code><br><code>pod "redis-izl09" annotated</code></p><h5 id=output-custom-format>Output custom format</h5><p>Sometimes, you want to customize the fields displayed when kubectl summarizes an object from your cluster. To do this, you can use the <code>custom-columns-file</code> format. <code>custom-columns-file</code> takes in a template file for rendering the output. Again, JSONPath expressions are used in the template to specify fields in the API object. For example, the following template first shows the number of restarts, and then the name of the object:</p><pre><code>$&gt; cat cols.tmpl   
RESTARTS                                   NAME   
.status.containerStatuses[0].restartCount .metadata.name  
</code></pre><p>If you pass this template to the <code>kubectl get pods</code> command you get a list of pods with the specified fields displayed.</p><pre><code> $&gt; kubectl get pods redis-izl09 -o=custom-columns-file --template=cols.tmpl                 RESTARTS           NAME   
 0                  redis-izl09   
 1                  redis-abl42  
</code></pre><h5 id=easily-manage-multiple-kubernetes-clusters>Easily manage multiple Kubernetes clusters</h5><p>If you’re running multiple Kubernetes clusters, you know it can be tricky to manage all of the credentials for the different clusters. Using the <code>kubectl config</code> subcommands, switching between different clusters is as easy as:</p><pre><code>        $&gt; kubectl config use-context
</code></pre><p>Not sure what clusters are available? You can view currently configured clusters with:</p><pre><code>        $&gt; kubectl config view
</code></pre><p>Phew, that outputs a lot of text. To restrict it down to only the things we’re interested in, we can use a JSONPath template:</p><pre><code>        $&gt; kubectl config view -o jsonpath=&quot;{.context[*].name}&quot;
</code></pre><p>Ahh, that’s better.</p><h5 id=conclusion>Conclusion</h5><p>So there you have it, nine new and exciting things you can do with your Kubernetes cluster and the kubectl command line. If you’re just getting started with Kubernetes, check out <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> or other ways to <a href=/docs/tutorials/kubernetes-basics/>get started with Kubernetes</a>.</p><ul><li>Brendan Burns, Google Software Engineer</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-644461d4337b4f197866d72505c5fcf5>Kubernetes Performance Measurements and Roadmap</h1><div class="td-byline mb-4"><time datetime=2015-09-10 class=text-muted>Thursday, September 10, 2015</time></div><p>No matter how flexible and reliable your container orchestration system is, ultimately, you have some work to be done, and you want it completed quickly. For big problems, a common answer is to just throw more machines at the problem. After all, more compute = faster, right?</p><p>Interestingly, adding more nodes is a little like the <a href=http://www.nasa.gov/mission_pages/station/expeditions/expedition30/tryanny.html>tyranny of the rocket equation</a> - in some systems, adding more machines can actually make your processing slower. However, unlike the rocket equation, we can do better. Kubernetes in v1.0 version supports clusters with up to 100 nodes. However, we have a goal to 10x the number of nodes we will support by the end of 2015. This blog post will cover where we are and how we intend to achieve the next level of performance.</p><h5 id=what-do-we-measure>What do we measure?</h5><p>The first question we need to answer is: “what does it mean that Kubernetes can manage an N-node cluster?” Users expect that it will handle all operations “reasonably quickly,” but we need a precise definition of that. We decided to define performance and scalability goals based on the following two metrics:</p><ol><li><p>1.<em>“API-responsiveness”</em>: 99% of all our API calls return in less than 1 second</p></li><li><p>2.<em>“Pod startup time”</em>: 99% of pods (with pre-pulled images) start within 5 seconds</p></li></ol><p>Note that for “pod startup time” we explicitly assume that all images necessary to run a pod are already pre-pulled on the machine where it will be running. In our experiments, there is a high degree of variability (network throughput, size of image, etc) between images, and these variations have little to do with Kubernetes’ overall performance.</p><p>The decision to choose those metrics was made based on our experience spinning up 2 billion containers a week at Google. We explicitly want to measure the latency of user-facing flows since that’s what customers will actually care about.</p><h5 id=how-do-we-measure>How do we measure?</h5><p>To monitor performance improvements and detect regressions we set up a continuous testing infrastructure. Every 2-3 hours we create a 100-node cluster from <a href=https://github.com/kubernetes/kubernetes>HEAD</a> and run our scalability tests on it. We use a GCE n1-standard-4 (4 cores, 15GB of RAM) machine as a master and n1-standard-1 (1 core, 3.75GB of RAM) machines for nodes.</p><p>In scalability tests, we explicitly focus only on the full-cluster case (full N-node cluster is a cluster with 30 * N pods running in it) which is the most demanding scenario from a performance point of view. To reproduce what a customer might actually do, we run through the following steps:</p><ul><li><p>Populate pods and replication controllers to fill the cluster</p></li><li><p>Generate some load (create/delete additional pods and/or replication controllers, scale the existing ones, etc.) and record performance metrics</p></li><li><p>Stop all running pods and replication controllers</p></li><li><p>Scrape the metrics and check whether they match our expectations</p></li></ul><p>It is worth emphasizing that the main parts of the test are done on full clusters (30 pods per node, 100 nodes) - starting a pod in an empty cluster, even if it has 100 nodes will be much faster.</p><p>To measure pod startup latency we are using very simple pods with just a single container running the “gcr.io/google_containers/pause:go” image, which starts and then sleeps forever. The container is guaranteed to be already pre-pulled on nodes (we use it as the so-called pod-infra-container).</p><h5 id=performance-data>Performance data</h5><p>The following table contains percentiles (50th, 90th and 99th) of pod startup time in 100-node clusters which are 10%, 25%, 50% and 100% full.</p><table><thead><tr><th></th><th>10%-full</th><th>25%-full</th><th>50%-full</th><th>100%-full</th></tr></thead><tbody><tr><td>50th percentile</td><td>.90s</td><td>1.08s</td><td>1.33s</td><td>1.94s</td></tr><tr><td>90th percentile</td><td>1.29s</td><td>1.49s</td><td>1.72s</td><td>2.50s</td></tr><tr><td>99th percentile</td><td>1.59s</td><td>1.86s</td><td>2.56s</td><td>4.32s</td></tr></tbody></table><p>As for api-responsiveness, the following graphs present 50th, 90th and 99th percentiles of latencies of API calls grouped by kind of operation and resource type. However, note that this also includes internal system API calls, not just those issued by users (in this case issued by the test itself).</p><p><img src="https://lh4.googleusercontent.com/NrKLoz2iB-TNdOxISL7OcqquCKL-MijDBCokf-u4ASAqgmo6zT7ZU24mXDvIwUUlRsFSsL3KF17dEAfUT41TSgNPvId5HN5ELQTXJSSBF0dp9EOccx4Y4WZ9fC9v9B_kCA=s1600" alt=get.png><img src="https://lh4.googleusercontent.com/53AtIdoGQ477Ju0FD4S76xbZs490JnmibhSZh67aq1-MU4Jw4B-7FBgzvFoJXHcAMeSU9r3bzJHpBFAfcSf7FIS3JGZ4TiAiHucyjH3ErrarKrwYNFopvxYSBo0qxP-U0w=s1600" alt=put.png></p><p><img src="https://lh4.googleusercontent.com/-wsLEXPfgtXNlu-pDfM4c0Qvr8lU7-G2w_nSgVeqg04D7RnhgSzg6Z5-mVmIYOzTWF7XaJ0zsDZBBlyZLqj4R1fkwWq-uaKJJI8xLAQ1gYWbh5qKXr5-rzkjm6CT3kBU=s1600" alt=delete.png><img src="https://lh6.googleusercontent.com/It8dH6iM2ZPypZ99KSUo_kJY4DnR2QD8yGJj26TiZ3U4owyf-WXoxrDfBAc1hcSn3i3LuxE3KGlUzQOaPgH6XVjSAU9Z2zMfZCKFAxEGtuCQiKlJPX4vH2JgQf3h1BXMRJQ=s1600" alt=post.png></p><p><img src="https://lh6.googleusercontent.com/6Gy-UKBZUoEwJ9iFytq-k_wrdvh6FsTJexSpn6nNnBwOvxv-Sp6PV7vmArCL22MUkz0tWH7MxhaIc-JE8YpEc0X4nDUMn-cKWF3ANHtgd2aJ5t3osoaezDe_xqjpi748Cbw=s1600" alt=list.png></p><p>Some resources only appear on certain graphs, based on what was running during that operation (e.g. no namespace was put at that time).</p><p>As you can see in the results, we are ahead of target for our 100-node cluster with pod startup time even in a fully-packed cluster occurring 14% faster in the 99th percentile than 5 seconds. It’s interesting to point out that LISTing pods is significantly slower than any other operation. This makes sense: in a full cluster there are 3000 pods and each of pod is roughly few kilobytes of data, meaning megabytes of data that need to processed for each LIST.</p><p>#####Work done and some future plans</p><p>The initial performance work to make 100-node clusters stable enough to run any tests on them involved a lot of small fixes and tuning, including increasing the limit for file descriptors in the apiserver and reusing tcp connections between different requests to etcd.</p><p>However, building a stable performance test was just step one to increasing the number of nodes our cluster supports by tenfold. As a result of this work, we have already taken on significant effort to remove future bottlenecks, including:</p><ul><li><p>Rewriting controllers to be watch-based: Previously they were relisting objects of a given type every few seconds, which generated a huge load on the apiserver.</p></li><li><p>Using code generators to produce conversions and deep-copy functions: Although the default implementation using Go reflections are very convenient, they proved to be extremely slow, as much as 10X in comparison to the generated code.</p></li><li><p>Adding a cache to apiserver to avoid deserialization of the same data read from etcd multiple times</p></li><li><p>Reducing frequency of updating statuses: Given the slow changing nature of statutes, it only makes sense to update pod status only on change and node status only every 10 seconds.</p></li><li><p>Implemented watch at the apiserver instead of redirecting the requests to etcd: We would prefer to avoid watching for the same data from etcd multiple times, since, in many cases, it was filtered out in apiserver anyway.</p></li></ul><p>Looking further out to our 1000-node cluster goal, proposed improvements include:</p><ul><li><p>Moving events out from etcd: They are more like system logs and are neither part of system state nor are crucial for Kubernetes to work correctly.</p></li><li><p>Using better json parsers: The default parser implemented in Go is very slow as it is based on reflection.</p></li><li><p>Rewriting the scheduler to make it more efficient and concurrent</p></li><li><p>Improving efficiency of communication between apiserver and Kubelets: In particular, we plan to reduce the size of data being sent on every update of node status.</p></li></ul><p>This is by no means an exhaustive list. We will be adding new elements (or removing existing ones) based on the observed bottlenecks while running the existing scalability tests and newly-created ones. If there are particular use cases or scenarios that you’d like to see us address, please join in!</p><ul><li>We have weekly meetings for our Kubernetes Scale Special Interest Group on Thursdays 11am PST where we discuss ongoing issues and plans for performance tracking and improvements.</li><li>If you have specific performance or scalability questions before then, please join our scalability special interest group on Slack: <a href=https://kubernetes.slack.com/messages/sig-scale>https://kubernetes.slack.com/messages/sig-scale</a></li><li>General questions? Feel free to join our Kubernetes community on Slack: <a href=https://kubernetes.slack.com/messages/kubernetes-users/>https://kubernetes.slack.com/messages/kubernetes-users/</a></li><li>Submit a pull request or file an issue! You can do this in our GitHub repository. Everyone is also enthusiastically encouraged to contribute with their own experiments (and their result) or PR contributions improving Kubernetes.
- Wojciech Tyczynski, Google Software Engineer</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1540b060dbf14e693016d6b73970786e>Using Kubernetes Namespaces to Manage Environments</h1><div class="td-byline mb-4"><time datetime=2015-08-28 class=text-muted>Friday, August 28, 2015</time></div><h5 id=one-of-the-advantages-that-kubernetes-provides-is-the-ability-to-manage-various-environments-easier-and-better-than-traditional-deployment-strategies-for-most-nontrivial-applications-you-have-test-staging-and-production-environments-you-can-spin-up-a-separate-cluster-of-resources-such-as-vms-with-the-same-configuration-in-staging-and-production-but-that-can-be-costly-and-managing-the-differences-between-the-environments-can-be-difficult>One of the advantages that Kubernetes provides is the ability to manage various environments easier and better than traditional deployment strategies. For most nontrivial applications, you have test, staging, and production environments. You can spin up a separate cluster of resources, such as VMs, with the same configuration in staging and production, but that can be costly and managing the differences between the environments can be difficult.</h5><h5 id=kubernetes-includes-a-cool-feature-called-namespaces-4-which-enable-you-to-manage-different-environments-within-the-same-cluster-for-example-you-can-have-different-test-and-staging-environments-in-the-same-cluster-of-machines-potentially-saving-resources-you-can-also-run-different-types-of-server-batch-or-other-jobs-in-the-same-cluster-without-worrying-about-them-affecting-each-other>Kubernetes includes a cool feature called [namespaces][4], which enable you to manage different environments within the same cluster. For example, you can have different test and staging environments in the same cluster of machines, potentially saving resources. You can also run different types of server, batch, or other jobs in the same cluster without worrying about them affecting each other.</h5><h3 id=the-default-namespace>The Default Namespace</h3><p>Specifying the namespace is optional in Kubernetes because by default Kubernetes uses the "default" namespace. If you've just created a cluster, you can check that the default namespace exists using this command:</p><pre><code>$ kubectl get namespaces
NAME          LABELS    STATUS
default                  Active
kube-system              Active
</code></pre><p>Here you can see that the default namespace exists and is active. The status of the namespace is used later when turning down and deleting the namespace.</p><h4 id=creating-a-new-namespace>Creating a New Namespace</h4><p>You create a namespace in the same way you would any other resource. Create a my-namespace.yaml file and add these contents:</p><pre><code>kind: Namespace  
apiVersion: v1  
metadata:  
 name: my-namespace  
 labels:  
   name: my-namespace  
</code></pre><p>Then you can run this command to create it:</p><pre><code>$ kubectl create -f my-namespace.yaml
</code></pre><h4 id=service-names>Service Names</h4><p>With namespaces you can have your apps point to static service endpoints that don't change based on the environment. For instance, your MySQL database service could be named mysql in production and staging even though it runs on the same infrastructure.</p><p>This works because each of the resources in the cluster will by default only "see" the other resources in the same namespace. This means that you can avoid naming collisions by creating pods, services, and replication controllers with the same names provided they are in separate namespaces. Within a namespace, short DNS names of services resolve to the IP of the service within that namespace. So for example, you might have an Elasticsearch service that can be accessed via the DNS name elasticsearch as long as the containers accessing it are located in the same namespace.</p><p>You can still access services in other namespaces by looking it up via the full DNS name which takes the form of SERVICE-NAME.NAMESPACE-NAME. So for example, elasticsearch.prod or elasticsearch.canary for the production and canary environments respectively.</p><h4 id=an-example>An Example</h4><p>Lets look at an example application. Let’s say you want to deploy your music store service MyTunes in Kubernetes. You can run the application production and staging environment as well as some one-off apps running in the same cluster. You can get a better idea of what’s going on by running some commands:</p><pre><code>~$ kubectl get namespaces  
NAME                    LABELS    STATUS  
default                     Active  
mytunes-prod                Active  
mytunes-staging             Active  
my-other-app                Active  
</code></pre><p>Here you can see a few namespaces running. Next let’s list the services in staging:</p><pre><code>~$ kubectl get services --namespace=mytunes-staging
NAME          LABELS                    SELECTOR        IP(S)             PORT(S)  
mytunes       name=mytunes,version=1    name=mytunes    10.43.250.14      80/TCP  
                                                        104.185.824.125     
mysql         name=mysql                name=mysql      10.43.250.63      3306/TCP  
</code></pre><p>Next check production:</p><pre><code>~$ kubectl get services --namespace=mytunes-prod  
NAME          LABELS                    SELECTOR        IP(S)             PORT(S)  
mytunes       name=mytunes,version=1    name=mytunes    10.43.241.145     80/TCP  
                                                        104.199.132.213     
mysql         name=mysql                name=mysql      10.43.245.77      3306/TCP  
</code></pre><p>Notice that the IP addresses are different depending on which namespace is used even though the names of the services themselves are the same. This capability makes configuring your app extremely easy—since you only have to point your app at the service name—and has the potential to allow you to configure your app exactly the same in your staging or test environments as you do in production.</p><h4 id=caveats>Caveats</h4><p>While you can run staging and production environments in the same cluster and save resources and money by doing so, you will need to be careful to set up resource limits so that your staging environment doesn't starve production for CPU, memory, or disk resources. Setting resource limits properly, and testing that they are working takes a lot of time and effort so unless you can measurably save money by running production in the same cluster as staging or test, you may not really want to do that.</p><p>Whether or not you run staging and production in the same cluster, namespaces are a great way to partition different apps within the same cluster. Namespaces will also serve as a level where you can apply resource limits so look for more resource management features at the namespace level in the future.</p><p>- Posted by Ian Lewis, Developer Advocate at Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a5d839dbca2e85bc033ad97da511f701>Weekly Kubernetes Community Hangout Notes - July 31 2015</h1><div class="td-byline mb-4"><time datetime=2015-08-04 class=text-muted>Tuesday, August 04, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Here are the notes from today's meeting:</p><ul><li><p>Private Registry Demo - Muhammed</p><ul><li><p>Run docker-registry as an RC/Pod/Service</p></li><li><p>Run a proxy on every node</p></li><li><p>Access as localhost:5000</p></li><li><p>Discussion:</p><ul><li><p>Should we back it by GCS or S3 when possible?</p></li><li><p>Run real registry backed by $object_store on each node</p></li><li><p>DNS instead of localhost?</p><ul><li><p>disassemble image strings?</p></li><li><p>more like DNS policy?</p></li></ul></li></ul></li></ul></li><li><p>Running Large Clusters - Joe</p><ul><li><p>Samsung keen to see large scale O(1000)</p><ul><li>Starting on AWS</li></ul></li><li><p>RH also interested - test plan needed</p></li><li><p>Plan for next week: discuss working-groups</p></li><li><p>If you are interested in joining conversation on cluster scalability send mail to [joe@0xBEDA.com][4]</p></li></ul></li><li><p>Resource API Proposal - Clayton</p><ul><li><p>New stuff wants more info on resources</p></li><li><p>Proposal for resources API - ask apiserver for info on pods</p></li><li><p>Send feedback to: #11951</p></li><li><p>Discussion on snapshot vs time-series vs aggregates</p></li></ul></li><li><p>Containerized kubelet - Clayton</p><ul><li><p>Open pull</p></li><li><p>Docker mount propagation - RH carries patches</p></li><li><p>Big issues around whole bootstrap of the system</p><ul><li>dual: boot-docker/system-docker</li></ul></li><li><p>Kube-in-docker is really nice, but maybe not critical</p><ul><li><p>Do the small stuff to make progress</p></li><li><p>Keep pressure on docker</p></li></ul></li></ul></li><li><p>Web UI (preilly)</p><ul><li><p>Where does web UI stand?</p><ul><li><p>OK to split it back out</p></li><li><p>Use it as a container image</p></li><li><p>Build image as part of kube release process</p></li><li><p>Vendor it back in? Maybe, maybe not.</p></li></ul></li><li><p>Will DNS be split out?</p><ul><li>Probably more tightly integrated, instead</li></ul></li><li><p>Other potential spin-outs:</p><ul><li><p>apiserver</p></li><li><p>clients</p></li></ul></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cc822ac3940885eb5a69dbb66cf6311b>The Growing Kubernetes Ecosystem</h1><div class="td-byline mb-4"><time datetime=2015-07-24 class=text-muted>Friday, July 24, 2015</time></div><p>Over the past year, we’ve seen fantastic momentum in the Kubernetes project, culminating with the release of <a href=https://tectonic.com/>Kubernetes v1</a> earlier this week. We’ve also witnessed the ecosystem around Kubernetes blossom, and wanted to draw attention to some of the cooler offerings we’ve seen.</p><p>| ----- |
|</p><p><img src=https://lh6.googleusercontent.com/Y6MY5k_Eq6CddNzfRrRo14kLuJwe1KYtJq_7KcIGy1bRf65KwoX1uAuCBwEL0P_FGSomZPQZ-hs7CG8Vze7qDKsISZrLEyRZkm5OSHngjjXfCItCiMXI3FtnD9iyDvYurd5sRXQ alt></p><p>|</p><p><a href=https://www.hds.com/corporate/press-analyst-center/press-releases/2015/gl150721.html>CloudBees</a> and the Jenkins community have created a Kubernetes plugin, allowing Jenkins slaves to be built as Docker images and run in Docker hosts managed by Kubernetes, either on the Google Cloud Platform or on a more local Kubernetes instance. These elastic slaves are then brought online as Jenkins schedules jobs for them and destroyed after their builds are complete, ensuring masters have steady access to clean workspaces and minimizing builds’ resource footprint.</p><p>|<br>|</p><p><img src=https://lh4.googleusercontent.com/iHZAfjvGPHYsIwUgevTTPN74fBU53Y1qdwq9hUsIixLWIbbv7P_02CQR6V5LPi4n4BCeg1LK3g5Iaizpkm5dXCmI7TdYKEaC7H2wLa9tzSkp8TyR93U1SilcGvpLDlzPLWhY664 alt></p><p>|</p><p><a href=https://www.kismatic.com/>CoreOS</a> has created launched Tectonic, an opinionated enterprise distribution of Kubernetes, CoreOS and Docker. Tectonic includes a management console for workflows and dashboards, an integrated registry to build and share containers, and additional tools to automate deployment and customize rolling updates. At KuberCon, CoreOS launched Tectonic Preview, giving users easy access to Kubernetes 1.0, 24x7 enterprise ready support, Kubernetes guides and Kubernetes training to help enterprises begin experiencing the power of Kubernetes, CoreOS and Docker.</p><p>|<br>|</p><p><img src=https://lh5.googleusercontent.com/kTu3RRmc1LH1vgdHQeCibALfJJCxE9JR5ZRE30xAn_bphO_uk-2n3RRolw3Yrb1uheyXMQRsH8ps7v3mrvhjkJo0f2ye7unVd1PT0trv8cE5VP1Pnq5P4oUx6m7DWKANZyyBnsg alt></p><p>|</p><p><a href=http://info.meteor.com/blog/meteor-and-a-galaxy-of-containers-with-kubernetes>Hitachi Data Systems</a> has announced that Kubernetes now joins the list of solutions validated to run on their enterprise Unified Computing Platform. With this announcement Hitachi has validated Kubernetes and VMware running side-by-side on the UCP platform, providing an enterprise solution for container-based applications and traditional virtualized workloads.</p><p>|<br>|</p><p><img src=https://lh5.googleusercontent.com/H1r-80pX8-ixDHCJDBKLWkNA1keMUvjv058e87-B80Wr8LSxP7SjSXc-5ru3MT4k18zYxl0L8aqJv3aylx8UYNGAXEmCCuHKwjZ4Z5tbG-LFCTiyRVdrlVUukHhi8QtsbuR1u3c alt></p><p>|</p><p><a href=https://mesosphere.com/training/kubernetes/>Kismatic</a> is providing enterprise support for pure play open source Kubernetes. They have announced open source and commercially supported Kubernetes plug-ins specifically built for production-grade enterprise environments. Any Kubernetes deployment can now benefit from modular role-based access controls (RBAC), Kerberos for bedrock authentication, LDAP/AD integration, rich auditing and platform-agnostic Linux distro packages.</p><p>|<br>|</p><p><img src=https://lh6.googleusercontent.com/7BkcAAf9SoEDyzjgGsNg_YVi8cRb1mdPHsc4FtK7JQkl2iVR_zIy9wkDPT7bls-z7FhgTIekAj1Z7q6Y_4oaZ2OLygkHxPmxZ3MNnkI4f8C78cjyk2gvt40Yk-m3_VSt8sIXz2Q alt></p><p>|</p><p><a href=https://www.mirantis.com/blog/kubernetes-docker-mirantis-openstack-6-1/>Meteor Development Group</a>, creators of Meteor, a JavaScript App Platform, are using Kubernetes to build <a href=https://www.mirantis.com/blog/kubernetes-docker-mirantis-openstack-6-1/>Galaxy</a> to run Meteor apps in production. Galaxy will scale from free test apps to production-suitable high-availability hosting.</p><p>|<br>|</p><p><img src=https://lh6.googleusercontent.com/Zi_nKEcB6uWZYXMOBStKPFLkHIXQn2FsnFP4ab2BFeBbUWv-d1oEBLQos-OpYpfwO3mao6xGusvX9O1JiyL4357XJBsmTXmcSnTnrBXCBOxJkB1uhOjntfAv8fN2YjZ6ITK53YU alt></p><p>|</p><p>Mesosphere has incorporated Kubernetes into its Data Center Operating System (DCOS) platform as a first class citizen. Using DCOS, enterprises can deploy Kubernetes across thousands of nodes, both bare-metal and virtualized machines that can run on-premise and in the cloud. Mesosphere also launched a beta of their <a href=http://www.opencontrail.org/opencontrail-kubernetes-integration/>Kubernetes Training Bootcamp</a> and will be offering more in the future.</p><p>|<br>|</p><p><img src=https://lh5.googleusercontent.com/F9dS-UFz8L50xoj8jCjgUvOo-r3pNLs4cEGRczHu5mD8YdMgnJctyzBuWQ0LmZeBB3cDHc1LB_4kHZDmjuP6KGr_n3W8Q0fGbBHxinRZggdMC0NDDWl-xDwy68GO6qotJr2JcOA alt></p><p>|</p><p><a href=http://pachyderm.io/>Mirantis</a> is enabling hybrid cloud applications across OpenStack and other clouds supporting Kubernetes. An OpenStack Murano app package supports full application lifecycle actions such as deploy, create cluster, create pod, add containers to pods, scale up and scale down.</p><p>|<br>|</p><p><img src=https://lh6.googleusercontent.com/dXhnvnlWtL9-oTd_irtLYTu8g78l9-LKj9PwjV5v4mpvGcPh4GQlHeQZpnIMJGwEyBxagut94Onagb0GsVJuVx10VVp-GHZ0vG_Z-jbxthLHhuzhQaBSFfA9pfoOI3cl6Rh7Hk4 alt></p><p>|</p><p><a href=http://www.platalytics.com/>OpenContrail</a> is creating a kubernetes-contrail plugin designed to stitch the cluster management capabilities of Kubernetes with the network service automation capabilities of OpenContrail. Given the event-driven abstractions of pods and services inherent in Kubernetes, it is a simple extension to address network service enforcement by leveraging OpenContrail’s Virtual Network policy approach and programmatic API’s.</p><p>|<br>|</p><p><img src=https://lh3.googleusercontent.com/0EQQc3sjVbw1cEYVeT0S5rT1iPLEMHteiKlSMDNqw8lNVOf4vG5qE6pVfvmZlRcg-NoOABC-mMcMSdD8ayrmpok0T91N15QqqmH378ydxK1843dcuJdtEsCnr1Y_RQQo-hWrBfI alt=logo.png></p><p>|</p><p><a href=https://github.com/metral/corekube>Pachyderm</a> is a containerized data analytics engine which provides the broad functionality of Hadoop with the ease of use of Docker. Users simply provide containers with their data analysis logic and Pachyderm will distribute that computation over the data. They have just released full deployment on Kubernetes for on premise deployments, and on Google Container Engine, eliminating all the operational overhead of running a cluster yourself.</p><p>|<br>|</p><p><img src=https://lh4.googleusercontent.com/qxQciTVBkyYDWeSgoxtg7InxQuuXsGSLBDfdxJB9Czo71BzQN5bUugLZhQKkERHqWAnkqHIY2VWi2J7g-pGn4V4AzPE0alBksedou78r0KMZm4QqYTN8QYHIMo4RtVmdw90azYw alt></p><p>|</p><p><a href=http://www.redhat.com/en/about/blog/welcoming-kubernetes-officially-enterprise-open-source-world>Platalytics, Inc</a>. and announced the release of one-touch deploy-anywhere feature for its Spark Application Platform. Based on Kubernetes, Docker, and CoreOS, it allows simple and automated deployment of Apache Hadoop, Spark, and Platalytics platform, with a single click, to all major public clouds, including Google, Amazon, Azure, DigitalOcean, and private on-premise clouds. It also enables hybrid cloud scenarios, where resources on public and private clouds can be mixed.</p><p>|<br>|</p><p><img src=https://lh3.googleusercontent.com/0EQQc3sjVbw1cEYVeT0S5rT1iPLEMHteiKlSMDNqw8lNVOf4vG5qE6pVfvmZlRcg-NoOABC-mMcMSdD8ayrmpok0T91N15QqqmH378ydxK1843dcuJdtEsCnr1Y_RQQo-hWrBfI alt></p><p>|</p><p><a href=https://github.com/metral/corekube>Rackspace</a> has created Corekube as a simple, quick way to deploy Kubernetes on OpenStack. By using a decoupled infrastructure that is coordinated by etcd, fleet and flannel, it enables users to try Kubernetes and CoreOS without all the fuss of setting things up by hand.</p><p>|<br>|</p><p><img src=https://lh4.googleusercontent.com/qxQciTVBkyYDWeSgoxtg7InxQuuXsGSLBDfdxJB9Czo71BzQN5bUugLZhQKkERHqWAnkqHIY2VWi2J7g-pGn4V4AzPE0alBksedou78r0KMZm4QqYTN8QYHIMo4RtVmdw90azYw alt></p><p>|</p><p><a href=http://www.redhat.com/en/about/blog/welcoming-kubernetes-officially-enterprise-open-source-world>Red Hat</a> is a long time proponent of Kubernetes, and a significant contributor to the project. In their own words, “From Red Hat Enterprise Linux 7 and Red Hat Enterprise Linux Atomic Host to OpenShift Enterprise 3 and the forthcoming Red Hat Atomic Enterprise Platform, we are well-suited to bring container innovations into the enterprise, leveraging Kubernetes as the common backbone for orchestration.”</p><p>|<br>|</p><p><img src=https://lh5.googleusercontent.com/8FfYhnwb__NUuoXEC-tNzuAuA6rFGz6IgQnVYh-fQ89i685-3t_2UjN291S-VZAAkyrPJ-MaAPMr36uV0PLWlv_GE1aE99shx_XzrEi4c8OKcEkiRs3z_tsB20w5ZiZ7UeZgzT8 alt></p><p>|</p><p><a href=http://www.redapt.com/kubernetes/%20%E2%80%8E>Redapt</a> has launching a variety of turnkey, on-premises Kubernetes solutions co-engineered with other partners in the Kubernetes partner ecosystem. These include appliances built to leverage the CoreOS/Tectonic, Mirantis OpenStack, and Mesosphere platforms for management and provisioning. Redapt also offers private, public, and multi-cloud solutions that help customers accelerate their Kubernetes deployments successfully into production.</p><p>|</p><p>| ----- |
|</p><p>|<br>|</p><p>We’ve also seen a community of services partners spring up to assist in adopting Kubernetes and containers:</p><p>| ----- |
|</p><p><img src=https://lh3.googleusercontent.com/dOHU9NjLGrG6UgGuNjvhuR5oDkrR5z1AZ0sM8BkLgaMuXY7pfDev8ukVbD1nrBeRj9LKryJcoGEvhZSo_dHIP8ahHIkAWqsT_QSOoiu7rfM9WX3lubCI4N1WKmE7yrRquaL7nAc alt="Screen Shot 2015-07-21 at 1.12.16 PM.png"></p><p>|</p><p><a href=http://biarca.io/building-distributed-multi-cloud-applications-using-kubernetes-and-containers/>Biarca</a> is using Kubernetes to ease application deployment and scale on demand across available hybrid and multi-cloud clusters through strategically managed policy. A video on their website illustrates how to use Kubernetes to deploy applications in a private cloud infrastructure based on OpenStack and use a public cloud like GCE to address bursting demand for applications.</p><p>|<br>|</p><p><img src=https://lh3.googleusercontent.com/Ac0FiR1FJ4tfp90zBVX7fr36BAVxUqRW7VIOFw12Rp6BzHRR0x_BwTfbaheXLYSYMuPZouf4huql04Uu9fVEn956b7BWIUcTzUgWuB5JYSFawwrP_AA6uzdOHZAQ2aROo1vhm1s alt></p><p>|</p><p><a href=http://www.cloudtp.com/container-adoption-services/>Cloud Technology Partners</a> has developed a Container Services Offering featuring Kubernetes to assist enterprises with container best practices, adoption and implementation. This offering helps organizations understand how containers deliver competitive edge.</p><p>|<br>|</p><p><img src=https://lh6.googleusercontent.com/tBtFRPzI6OAPKvaak9X3QWcrzGuBsrk1szFGi-Bq3EQweBo6nZ0Qmwxk9EwLZ9ItP9-1Zip4rxtwtFa0ILylO1CySuOa1qLcO2ab0yJCN1SCe-r_BNPX8hD5Qigxb7sqqXgx09A alt></p><p>|</p><p><a href=http://doit-intl.com/kubernetes>DoIT International</a> is offering a Kubernetes Bootcamp which consists of a series of hands-on exercises interleaved with mini-lectures covering hands on topics such as Container Basics, Using Docker, Kubernetes and Google Container Engine.</p><p>|<br>|</p><p><img src=https://lh3.googleusercontent.com/qO2YK7IxIVPpIsdN0Ry7B5zc_cdzfZb6DlgAJWpy-VJajL84m3u2nyo3-6QRZ_wFCY0-r4ryltiT4j1D_y_BeguxGXWap2YlSfdqyYAIbi2__p0uLXymtYkAu5VFVfA___eMbUY alt></p><p>|</p><p><a href=https://www.opencredo.com/2015/04/20/kubernetes/>OpenCredo</a> provides a practical, lab style container and scheduler course in addition to consulting and solution delivery. The three-day course allows development teams to quickly ramp up and make effective use of containers in real world scenarios, covering containers in general along with Docker and Kubernetes.</p><p>|<br>|</p><p><img src=https://lh5.googleusercontent.com/XgMDUbRt_UKn4v4D7roz4mpE4qqUYpLI2c9460vt65yXrLxhcrM3rmH9Xcg-C0RMylhRxTWIMFInHYLN1O9v9FZ1NoUVI6ynsmoAQUGMN1Nc27jhXzIRiRXwWzx_HOH5TtX3NaE alt></p><p>|</p><p><a href=http://www.pythian.com/google-kubernetes/>Pythian</a> focuses on helping clients design, implement, and manage systems that directly contribute to revenue and business success. They provide small, <a href=http://www.pythian.com/blog/lessons-learned-kubernetes/>dedicated teams of highly trained and experienced data experts</a> have the deep Kubernetes and container experience necessary to help companies solve Big Data problems with containers.</p><p>|</p><p>- Martin Buhr, Product Manager at Google</p></div><div class=td-content style=page-break-before:always><h1 id=pg-155f4585980e0a3cc7cc913d89e65e77>Weekly Kubernetes Community Hangout Notes - July 17 2015</h1><div class="td-byline mb-4"><time datetime=2015-07-23 class=text-muted>Thursday, July 23, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Here are the notes from today's meeting:</p><ul><li><p>Eric Paris: replacing salt with ansible (if we want)</p><ul><li><p>In contrib, there is a provisioning tool written in ansible</p></li><li><p>The goal in the rewrite was to eliminate as much of the cloud provider stuff as possible</p></li><li><p>The salt setup does a bunch of setup in scripts and then the environment is setup with salt</p><ul><li>This means that things like generating certs is done differently on GCE/AWS/Vagrant</li></ul></li><li><p>For ansible, everything must be done within ansible</p></li><li><p>Background on ansible</p><ul><li>Does not have clients</li><li>Provisioner ssh into the machine and runs scripts on the machine</li><li>You define what you want your cluster to look like, run the script, and it sets up everything at once</li><li>If you make one change in a config file, ansible re-runs everything (which isn’t always desirable)</li><li>Uses a jinja2 template</li></ul></li><li><p>Create machines with minimal software, then use ansible to get that machine into a runnable state</p><ul><li>Sets up all of the add-ons</li></ul></li><li><p>Eliminates the provisioner shell scripts</p></li><li><p>Full cluster setup currently takes about 6 minutes</p><ul><li>CentOS with some packages</li><li>Redeploy to the cluster takes 25 seconds</li></ul></li><li><p>Questions for Eric</p><ul><li><p>Where does the provider-specific configuration go?</p><ul><li>The only network setup that the ansible config does is flannel; you can turn it off</li></ul></li><li><p>What about init vs. systemd?</p><ul><li>Should be able to support in the code w/o any trouble (not yet implemented)</li></ul></li></ul></li><li><p>Discussion</p><ul><li><p>Why not push the setup work into containers or kubernetes config?</p><ul><li>To bootstrap a cluster drop a kubelet and a manifest</li></ul></li><li><p>Running a kubelet and configuring the network should be the only things required. We can cut a machine image that is preconfigured minus the data package (certs, etc)</p><ul><li>The ansible scripts install kubelet & docker if they aren’t already installed</li></ul></li><li><p>Each OS (RedHat, Debian, Ubuntu) could have a different image. We could view this as part of the build process instead of the install process.</p></li><li><p>There needs to be solution for bare metal as well.</p></li><li><p>In favor of the overall goal -- reducing the special configuration in the salt configuration</p></li><li><p>Everything except the kubelet should run inside a container (eventually the kubelet should as well)</p><ul><li>Running in a container doesn’t cut down on the complexity that we currently have</li><li>But it does more clearly define the interface about what the code expects</li></ul></li><li><p>These tools (Chef, Puppet, Ansible) conflate binary distribution with configuration</p><ul><li>Containers more clearly separate these problems</li></ul></li><li><p>The mesos deployment is not completely automated yet, but the mesos deployment is completely different: kubelets get put on top on an existing mesos cluster</p><ul><li>The bash scripts allow the mesos devs to see what each cloud provider is doing and re-use the relevant bits</li><li>There was a large reverse engineering curve, but the bash is at least readable as opposed to the salt</li></ul></li><li><p>Openstack uses a different deployment as well</p></li><li><p>We need a well documented list of steps (e.g. create certs) that are necessary to stand up a cluster</p><ul><li>This would allow us to compare across cloud providers</li><li>We should reduce the number of steps as much as possible</li><li>Ansible has 241 steps to launch a cluster</li></ul></li></ul></li></ul></li><li><p>1.0 Code freeze</p><ul><li><p>How are we getting out of code freeze?</p></li><li><p>This is a topic for next week, but the preview is that we will move slowly rather than totally opening the firehose</p><ul><li>We want to clear the backlog as fast as possible while maintaining stability both on HEAD and on the 1.0 branch</li><li>The backlog of almost 300 PRs but there are also various parallel feature branches that have been developed during the freeze</li></ul></li><li><p>Cutting a cherry pick release today (1.0.1) that fixes a few issues</p></li><li><p>Next week we will discuss the cadence for patch releases</p></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b1952d21d9f607dfdf6c5276208b640e>Strong, Simple SSL for Kubernetes Services</h1><div class="td-byline mb-4"><time datetime=2015-07-14 class=text-muted>Tuesday, July 14, 2015</time></div><p>Hi, I’m Evan Brown <a href=http://twitter.com/evandbrown>(@evandbrown</a>) and I work on the solutions architecture team for Google Cloud Platform. I recently wrote an <a href=https://cloud.google.com/solutions/automated-build-images-with-jenkins-kubernetes>article</a> and <a href=https://github.com/GoogleCloudPlatform/kube-jenkins-imager>tutorial</a> about using Jenkins on Kubernetes to automate the Docker and GCE image build process. Today I’m going to discuss how I used Kubernetes services and secrets to add SSL to the Jenkins web UI. After reading this, you’ll be able to add SSL termination (and HTTP->HTTPS redirects + basic auth) to your public HTTP Kubernetes services.</p><h3 id=in-the-beginning>In the beginning</h3><p>In the spirit of minimum viability, the first version of Jenkins-on-Kubernetes I built was very basic but functional:</p><ul><li>The Jenkins leader was just a single container in one pod, but it was managed by a replication controller, so if it failed it would automatically respawn.</li><li>The Jenkins leader exposes two ports - TCP 8080 for the web UI and TCP 50000 for build agents to register - and those ports are made available as a Kubernetes service with a public load balancer.</li></ul><p>Here’s a visual of that first version:</p><p><a href=https://1.bp.blogspot.com/-ccmpTmulrng/VaVxOs7gysI/AAAAAAAAAU8/bCEzgGGm-pE/s1600/0.png><img src=https://1.bp.blogspot.com/-ccmpTmulrng/VaVxOs7gysI/AAAAAAAAAU8/bCEzgGGm-pE/s400/0.png alt></a></p><p>This works, but I have a few problems with it. First, authentication isn’t configured in a default Jenkins installation. The leader is sitting on the public Internet, accessible to anyone, until you connect and configure authentication. And since there’s no encryption, configuring authentication is kind of a symbolic gesture. We need SSL, and we need it now!</p><h3 id=do-what-you-know>Do what you know</h3><p>For a few milliseconds I considered trying to get SSL working directly on Jenkins. I’d never done it before, and I caught myself wondering if it would be as straightforward as working with SSL on <a href=http://nginx.org/>Nginx</a>, something I do have experience with. I’m all for learning new things, but this seemed like a great place to not invent a new wheel: SSL on Nginx is straightforward and well documented (as are its reverse-proxy capabilities), and Kubernetes is all about building functionality by orchestrating and composing containers. Let’s use Nginx, and add a few bonus features that Nginx makes simple: HTTP->HTTPS redirection, and basic access authentication.</p><h3 id=ssl-termination-proxy-as-an-nginx-service>SSL termination proxy as an nginx service</h3><p>I started by putting together a <a href=https://github.com/GoogleCloudPlatform/nginx-ssl-proxy/blob/master/Dockerfile>Dockerfile</a> that inherited from the standard nginx image, copied a few Nginx config files, and added a custom entrypoint (start.sh). The entrypoint script checks an environment variable (ENABLE_SSL) and activates the correct Nginx config accordingly (meaning that unencrypted HTTP reverse proxy is possible, but that defeats the purpose). The script also configures basic access authentication if it’s enabled (the ENABLE_BASIC_AUTH env var).</p><p>Finally, start.sh evaluates the SERVICE_HOST_ENV_NAME and SERVICE_PORT_ENV_NAME env vars. These variables should be set to the names of the environment variables for the Kubernetes service you want to proxy to. In this example, the service for our Jenkins leader is cleverly named jenkins, which means pods in the cluster will see an environment variable named JENKINS_SERVICE_HOST and JENKINS_SERVICE_PORT_UI (the port that 8080 is mapped to on the Jenkins leader). SERVICE_HOST_ENV_NAME and SERVICE_PORT_ENV_NAME simply reference the correct service to use for a particular scenario, allowing the image to be used generically across deployments.</p><h3 id=defining-the-controller-and-service>Defining the Controller and Service</h3><p>LIke every other pod in this example, we’ll deploy Nginx with a replication controller, allowing us to scale out or in, and recover automatically from container failures. This excerpt from a<a href=https://github.com/GoogleCloudPlatform/kube-jenkins-imager/blob/master/ssl_proxy.yaml#L20-L48>complete descriptor in the sample app</a> shows some relevant bits of the pod spec:</p><pre><code>  spec:

    containers:

      -

        name: &quot;nginx-ssl-proxy&quot;

        image: &quot;gcr.io/cloud-solutions-images/nginx-ssl-proxy:latest&quot;

        env:

          -

            name: &quot;SERVICE\_HOST\_ENV\_NAME&quot;

            value: &quot;JENKINS\_SERVICE\_HOST&quot;

          -

            name: &quot;SERVICE\_PORT\_ENV\_NAME&quot;

            value: &quot;JENKINS\_SERVICE\_PORT\_UI&quot;

          -

            name: &quot;ENABLE\_SSL&quot;

            value: &quot;true&quot;

          -

            name: &quot;ENABLE\_BASIC\_AUTH&quot;

            value: &quot;true&quot;

        ports:

          -

            name: &quot;nginx-ssl-proxy-http&quot;

            containerPort: 80

          -

            name: &quot;nginx-ssl-proxy-https&quot;

            containerPort: 443
</code></pre><p>The pod will have a service exposing TCP 80 and 443 to a public load balancer. Here’s the service descriptor <a href=https://github.com/GoogleCloudPlatform/kube-jenkins-imager/blob/master/service_ssl_proxy.yaml>(also available in the sample app</a>):</p><pre><code>  kind: &quot;Service&quot;

  apiVersion: &quot;v1&quot;

  metadata:

    name: &quot;nginx-ssl-proxy&quot;

    labels:

      name: &quot;nginx&quot;

      role: &quot;ssl-proxy&quot;

  spec:

    ports:

      -

        name: &quot;https&quot;

        port: 443

        targetPort: &quot;nginx-ssl-proxy-https&quot;

        protocol: &quot;TCP&quot;

      -

        name: &quot;http&quot;

        port: 80

        targetPort: &quot;nginx-ssl-proxy-http&quot;

        protocol: &quot;TCP&quot;

    selector:

      name: &quot;nginx&quot;

      role: &quot;ssl-proxy&quot;

    type: &quot;LoadBalancer&quot;
</code></pre><p>And here’s an overview with the SSL termination proxy in place. Notice that Jenkins is no longer directly exposed to the public Internet:</p><p><a href=https://3.bp.blogspot.com/-0B1BEQo_fWc/VaVxVUBkf3I/AAAAAAAAAVE/5yCCnA29C88/s1600/0%2B%25281%2529.png><img src=https://3.bp.blogspot.com/-0B1BEQo_fWc/VaVxVUBkf3I/AAAAAAAAAVE/5yCCnA29C88/s400/0%2B%25281%2529.png alt></a></p><p>Now, how did the Nginx pods get ahold of the super-secret SSL key/cert and htpasswd file (for basic access auth)?</p><h3 id=keep-it-secret-keep-it-safe>Keep it secret, keep it safe</h3><p>Kubernetes has an <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/secrets.md>API and resource for Secrets</a>. Secrets “are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a pod definition or in a docker image.”</p><p>You can create secrets in your cluster in 3 simple steps:</p><ol><li></li></ol><p>Base64-encode your secret data (i.e., SSL key pair or htpasswd file)</p><pre><code>$ cat ssl.key | base64  
   LS0tLS1CRUdJTiBDRVJUS...
</code></pre><ol><li></li></ol><p>Create a json document describing your secret, and add the base64-encoded values:</p><pre><code>  apiVersion: &quot;v1&quot;

  kind: &quot;Secret&quot;

  metadata:

    name: &quot;ssl-proxy-secret&quot;

    namespace: &quot;default&quot;

  data:

    proxycert: &quot;LS0tLS1CRUd...&quot;

    proxykey: &quot;LS0tLS1CR...&quot;

    htpasswd: &quot;ZXZhb...&quot;
</code></pre><ol><li></li></ol><p>Create the secrets resource:</p><pre><code>$ kubectl create -f secrets.json
</code></pre><p>To access the secrets from a container, specify them as a volume mount in your pod spec. Here’s the relevant excerpt from the <a href=https://github.com/GoogleCloudPlatform/kube-jenkins-imager/blob/master/ssl_proxy.yaml###L41-L48>Nginx proxy template</a> we saw earlier:</p><pre><code>  spec:

    containers:

      -

        name: &quot;nginx-ssl-proxy&quot;

        image: &quot;gcr.io/cloud-solutions-images/nginx-ssl-proxy:latest&quot;

        env: [...]

        ports: ...[]

        volumeMounts:

          -

            name: &quot;secrets&quot;

            mountPath: &quot;/etc/secrets&quot;

            readOnly: true

    volumes:

      -

        name: &quot;secrets&quot;

        secret:

          secretName: &quot;ssl-proxy-secret&quot;
</code></pre><p>A volume of type secret that points to the ssl-proxy-secret secret resource is defined, and then mounted into /etc/secrets in the container. The secrets spec in the earlier example defined data.proxycert, data.proxykey, and data.htpasswd, so we would see those files appear (base64-decoded) in /etc/secrets/proxycert, /etc/secrets/proxykey, and /etc/secrets/htpasswd for the Nginx process to access.</p><p>All together now</p><p>I have “containers and Kubernetes are fun and cool!” moments all the time, like probably every day. I’m beginning to have “containers and Kubernetes are extremely useful and powerful and are adding value to what I do by helping me do important things with ease” more frequently. This SSL termination proxy with Nginx example is definitely one of the latter. I didn’t have to waste time learning a new way to use SSL. I was able to solve my problem using well-known tools, in a reusable way, and quickly (from idea to working took about 2 hours).</p><p>Check out the complete <a href=https://github.com/GoogleCloudPlatform/kube-jenkins-imager>Automated Image Builds with Jenkins, Packer, and Kubernetes</a> repo to see how the SSL termination proxy is used in a real cluster, or dig into the details of the proxy image in the <a href=https://github.com/GoogleCloudPlatform/nginx-ssl-proxy>nginx-ssl-proxy repo</a> (complete with a Dockerfile and Packer template so you can build the image yourself).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f2ff4be51ddb944d673294af1833fbf6>Weekly Kubernetes Community Hangout Notes - July 10 2015</h1><div class="td-byline mb-4"><time datetime=2015-07-13 class=text-muted>Monday, July 13, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Here are the notes from today's meeting:</p><ul><li>Eric Paris: replacing salt with ansible (if we want)<ul><li>In contrib, there is a provisioning tool written in ansible</li><li>The goal in the rewrite was to eliminate as much of the cloud provider stuff as possible</li><li>The salt setup does a bunch of setup in scripts and then the environment is setup with salt<ul><li>This means that things like generating certs is done differently on GCE/AWS/Vagrant</li></ul></li><li>For ansible, everything must be done within ansible</li><li>Background on ansible<ul><li>Does not have clients</li><li>Provisioner ssh into the machine and runs scripts on the machine</li><li>You define what you want your cluster to look like, run the script, and it sets up everything at once</li><li>If you make one change in a config file, ansible re-runs everything (which isn’t always desirable)</li><li>Uses a jinja2 template</li></ul></li><li>Create machines with minimal software, then use ansible to get that machine into a runnable state<ul><li>Sets up all of the add-ons</li></ul></li><li>Eliminates the provisioner shell scripts</li><li>Full cluster setup currently takes about 6 minutes<ul><li>CentOS with some packages</li><li>Redeploy to the cluster takes 25 seconds</li></ul></li><li>Questions for Eric<ul><li>Where does the provider-specific configuration go?<ul><li>The only network setup that the ansible config does is flannel; you can turn it off</li></ul></li><li>What about init vs. systemd?<ul><li>Should be able to support in the code w/o any trouble (not yet implemented)</li></ul></li></ul></li><li>Discussion<ul><li>Why not push the setup work into containers or kubernetes config?<ul><li>To bootstrap a cluster drop a kubelet and a manifest</li></ul></li><li>Running a kubelet and configuring the network should be the only things required. We can cut a machine image that is preconfigured minus the data package (certs, etc)<ul><li>The ansible scripts install kubelet & docker if they aren’t already installed</li></ul></li><li>Each OS (RedHat, Debian, Ubuntu) could have a different image. We could view this as part of the build process instead of the install process.</li><li>There needs to be solution for bare metal as well.</li><li>In favor of the overall goal -- reducing the special configuration in the salt configuration</li><li>Everything except the kubelet should run inside a container (eventually the kubelet should as well)<ul><li>Running in a container doesn’t cut down on the complexity that we currently have</li><li>But it does more clearly define the interface about what the code expects</li></ul></li><li>These tools (Chef, Puppet, Ansible) conflate binary distribution with configuration<ul><li>Containers more clearly separate these problems</li></ul></li><li>The mesos deployment is not completely automated yet, but the mesos deployment is completely different: kubelets get put on top on an existing mesos cluster<ul><li>The bash scripts allow the mesos devs to see what each cloud provider is doing and re-use the relevant bits</li><li>There was a large reverse engineering curve, but the bash is at least readable as opposed to the salt</li></ul></li><li>Openstack uses a different deployment as well</li><li>We need a well documented list of steps (e.g. create certs) that are necessary to stand up a cluster<ul><li>This would allow us to compare across cloud providers</li><li>We should reduce the number of steps as much as possible</li><li>Ansible has 241 steps to launch a cluster</li></ul></li></ul></li></ul></li><li>1.0 Code freeze<ul><li>How are we getting out of code freeze?</li><li>This is a topic for next week, but the preview is that we will move slowly rather than totally opening the firehose<ul><li>We want to clear the backlog as fast as possible while maintaining stability both on HEAD and on the 1.0 branch</li><li>The backlog of almost 300 PRs but there are also various parallel feature branches that have been developed during the freeze</li></ul></li><li>Cutting a cherry pick release today (1.0.1) that fixes a few issues</li><li>Next week we will discuss the cadence for patch releases</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c8d31fe28a6a2f2c0b717dd1b1f21835>Announcing the First Kubernetes Enterprise Training Course</h1><div class="td-byline mb-4"><time datetime=2015-07-08 class=text-muted>Wednesday, July 08, 2015</time></div><p>At Google we rely on Linux application containers to run our core infrastructure. Everything from Search to Gmail runs in containers.  In fact, we like containers so much that even our Google Compute Engine VMs run in containers!  Because containers are critical to our business, we have been working with the community on many of the basic container technologies (from cgroups to Docker’s LibContainer) and even decided to build the next generation of Google’s container scheduling technology, Kubernetes, in the open.</p><p>One year into the Kubernetes project, and on the eve of our planned V1 release at OSCON, we are pleased to announce the first-ever formal Kubernetes enterprise-focused training session organized by a key Kubernetes contributor, Mesosphere. The inaugural session will be taught by Zed Shaw and Michael Hausenblas from Mesosphere, and will take place on July 20 at OSCON in Portland. <a href=https://mesosphere.com/training/kubernetes/>Pre-registration</a> is free for early registrants, but space is limited so act soon!</p><p>This one-day course will cover the basics of building and deploying containerized applications using Kubernetes. It will walk attendees through the end-to-end process of creating a Kubernetes application architecture, building and configuring Docker images, and deploying them on a Kubernetes cluster. Users will also learn the fundamentals of deploying Kubernetes applications and services on our Google Container Engine and Mesosphere’s Datacenter Operating System.</p><p>The upcoming Kubernetes bootcamp will be a great way to learn how to apply Kubernetes to solve long-standing deployment and application management problems.  This is just the first of what we hope are many, and from a broad set of contributors.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1ca4bcef94d711edf520d75208ea5e0a>How did the Quake demo from DockerCon Work?</h1><div class="td-byline mb-4"><time datetime=2015-07-02 class=text-muted>Thursday, July 02, 2015</time></div><p>Shortly after its release in 2013, Docker became a very popular open source container management tool for Linux. Docker has a rich set of commands to control the execution of a container. Commands such as start, stop, restart, kill, pause, and unpause. However, what is still missing is the ability to Checkpoint and Restore (C/R) a container natively via Docker itself.</p><p>We’ve been actively working with upstream and community developers to add support in Docker for native C/R and hope that checkpoint and restore commands will be introduced in Docker 1.8. As of this writing, it’s possible to C/R a container externally because this functionality was recently merged in libcontainer.</p><p>External container C/R was demo’d at DockerCon 2015:</p><p><img src=https://lh4.googleusercontent.com/MectR1Mh-7XA0Q5cqiQPrtNxnBE-EDkKR6XJQfazYcIJg3mSbWTcV9EyRfhu6VbIP_sFdMVCXRH1l8YYIuG05SiuO_WqaXPvOf41j0CN8eD9djd6E4joS9Y5Aljlpi64NnffiRY alt="Screen Shot 2015-06-30 at 3.37.46 PM.png"></p><p>Container C/R offers many benefits including the following:</p><ul><li>Stop and restart the Docker daemon (say for an upgrade) without having to kill the running containers and restarting them from scratch, losing precious work they had done when they were stopped</li><li>Reboot the system without having to restart the containers from scratch. Same benefits as use case 1 above</li><li>Speed up the start time of slow-start applications</li><li>“Forensic debugging" of processes running in a container by examining their checkpoint images (open files, memory segments, etc.)</li><li>Migrate containers by restoring them on a different machine</li></ul><p>CRIU</p><p>Implementing C/R functionality from scratch is a major undertaking and a daunting task. Fortunately, there is a powerful open source tool written in C that has been used in production for checkpointing and restoring entire process trees in Linux. The tool is called CRIU which stands for Checkpoint Restore In Userspace (<a href=http://criu.org>http://criu.org</a>). CRIU works by:</p><ul><li>Freezing a running application.</li><li>Checkpointing the address space and state of the entire process tree to a collection of “image” files.</li><li>Restoring the process tree from checkpoint image files.</li><li>Resuming application from the point it was frozen.</li></ul><p>In April 2014, we decided to find out if CRIU could checkpoint and restore Docker containers to facilitate container migration.</p><h4 id=phase-1-external-c-r>Phase 1 - External C/R</h4><p>The first phase of this effort invoking CRIU directly to dump a process tree running inside a container and determining why the checkpoint or restore operation failed. There were quite a few issues that caused CRIU failure. The following three issues were among the more challenging ones.</p><h4 id=external-bind-mounts>External Bind Mounts</h4><p>Docker sets up /etc/{hostname,hosts,resolv.conf} as targets with source files outside the container's mount namespace.</p><p>The --ext-mount-map command line option was added to CRIU to specify the path of the external bind mounts. For example, assuming default Docker configuration, /etc/hostname in the container's mount namespace is bind mounted from the source at /var/lib/docker/containers/&lt;container-id>/hostname. When checkpointing, we tell CRIU to record /etc/hostname's "map" as, say, etc_hostname. When restoring, we tell CRIU that the file previously recorded as etc_hostname should be mapped from the external bind mount at /var/lib/docker/containers/&lt;container-id>/hostname.</p><p><img src=https://lh5.googleusercontent.com/STJqmZ-Z_b-rzLVj3vdFbI0687bnb7pgtiBzfZeELl3Ibp7I9EyV5SV55ykp2x0Bwx2M0ZQBYVCgWIAtsxBq9xPEpVTi29lhhYVlfYHx9VKBYge27VtCMS5j_B2JWHHXH0qAqy0 alt=ext_bind_mount.png></p><h4 id=aufs-pathnames>AUFS Pathnames</h4><p>Docker initially used AUFS as its preferred filesystem which is still in wide usage (the preferred filesystem is now OverlayFS).. Due to a bug, the AUFS symbolic link paths of /proc/&lt;pid>/map_files point inside AUFS branches instead of their pathnames relative to the container's root. This problem has been fixed in AUFS source code but hasn't made it to all the distros yet. CRIU would get confused seeing the same file in its physical location (in the branch) and its logical location (from the root of mount namespace).</p><p>The --root command line option that was used only during restore was generalized to understand the root of the mount namespace during checkpoint and automatically "fix" the exposed AUFS pathnames.</p><h4 id=cgroups>Cgroups</h4><p>After checkpointing, the Docker daemon removes the container’s cgroups subdirectories (because the container has “exited”). This causes restore to fail.</p><p>The --manage-cgroups command line option was added to CRIU to dump and restore the process's cgroups along with their properties.</p><p>The CRIU command lines are a simple container are shown below:</p><pre><code>$ docker run -d busybox:latest /bin/sh -c 'i=0; while true; do echo $i \&gt;\&gt; /foo; i=$(expr $i + 1); sleep 3; done'  

$ docker ps  
CONTAINER ID  IMAGE           COMMAND           CREATED        STATUS  
168aefb8881b  busybox:latest  &quot;/bin/sh -c 'i=0; 6 seconds ago  Up 4 seconds  

$ sudo criu dump -o dump.log -v4 -t 17810 \  
        -D /tmp/img/\&lt;container\_id\&gt; \  
        --root /var/lib/docker/aufs/mnt/\&lt;container\_id\&gt; \  
        --ext-mount-map /etc/resolv.conf:/etc/resolv.conf \  
        --ext-mount-map /etc/hosts:/etc/hosts \  
        --ext-mount-map /etc/hostname:/etc/hostname \  
        --ext-mount-map /.dockerinit:/.dockerinit \  
        --manage-cgroups \  
        --evasive-devices  

$ docker ps -a  
CONTAINER ID  IMAGE           COMMAND           CREATED        STATUS  
168aefb8881b  busybox:latest  &quot;/bin/sh -c 'i=0; 6 minutes ago  Exited (-1) 4 minutes ago  

$ sudo mount -t aufs -o br=\  
/var/lib/docker/aufs/diff/\&lt;container\_id\&gt;:\  
/var/lib/docker/aufs/diff/\&lt;container\_id\&gt;-init:\  
/var/lib/docker/aufs/diff/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721:\  
/var/lib/docker/aufs/diff/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16:\  
/var/lib/docker/aufs/diff/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229:\  
/var/lib/docker/aufs/diff/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158:\  
none /var/lib/docker/aufs/mnt/\&lt;container\_id\&gt;  

$ sudo criu restore -o restore.log -v4 -d  
        -D /tmp/img/\&lt;container\_id\&gt; \  
        --root /var/lib/docker/aufs/mnt/\&lt;container\_id\&gt; \  
        --ext-mount-map /etc/resolv.conf:/var/lib/docker/containers/\&lt;container\_id\&gt;/resolv.conf \  
        --ext-mount-map /etc/hosts:/var/lib/docker/containers/\&lt;container\_id\&gt;/hosts \  
        --ext-mount-map /etc/hostname:/var/lib/docker/containers/\&lt;container\_id\&gt;/hostname \  
        --ext-mount-map /.dockerinit:/var/lib/docker/init/dockerinit-1.0.0 \  
        --manage-cgroups \  
        --evasive-devices  

$ ps -ef | grep /bin/sh  
root     18580     1  0 12:38 ?        00:00:00 /bin/sh -c i=0; while true; do echo $i \&gt;\&gt; /foo; i=$(expr $i + 1); sleep 3; done  

$ docker ps -a  
CONTAINER ID  IMAGE           COMMAND           CREATED        STATUS  
168aefb8881b  busybox:latest  &quot;/bin/sh -c 'i=0; 7 minutes ago  Exited (-1) 5 minutes ago  

docker\_cr.sh
</code></pre><p>Since the command line arguments to CRIU were long, a helper script called docker_cr.sh was provided in the CRIU source tree to simplify the process. So, for the above container, one would simply C/R the container as follows (for details see <a href=http://criu.org/Docker>http://criu.org/Docker</a>):</p><pre><code>$ sudo docker\_cr.sh -c 4397   
dump successful  

$ sudo docker\_cr.sh -r 4397  
restore successful  
</code></pre><p>At the end of Phase 1, it was possible to externally checkpoint and restore a Docker 1.0 container using either VFS, AUFS, or UnionFS storage drivers with CRIU v1.3.</p><h4 id=phase-2-native-c-r>Phase 2 - Native C/R</h4><p>While external C/R served as a successful proof of concept for container C/R, it had the following limitations:</p><ol><li>State of a checkpointed container would show as "Exited".</li><li>Docker commands such as logs, kill, etc. will not work on a restored container.</li><li>The restored process tree will be a child of /etc/init instead of the Docker daemon.</li></ol><p>Therefore, the second phase of the effort concentrated on adding native checkpoint and restore commands to Docker.</p><h4 id=libcontainer-nsinit>libcontainer, nsinit</h4><p>Libcontainer is Docker’s native execution driver. It provides a set of APIs to create and manage containers. The first step of adding native support was the introduction of two methods, checkpoint() and restore(), to libcontainer and the corresponding checkpoint and restore subcommands to nsinit. Nsinit is a simple utility that is used to test and debug libcontainer.</p><h4 id=docker-checkpoint-docker-restore>docker checkpoint, docker restore</h4><p>With C/R support in libcontainer, the next step was adding checkpoint and restore subcommands to Docker itself. A big challenge in this step was to rebuild the “plumbing” between the container and the daemon. When the daemon initially starts a container, it sets up individual pipes between itself (parent) and the standard input, output, and error file descriptors of the container (child). This is how docker logs can show the output of a container.</p><p>When a container exits after being checkpointed, the pipes between it and the daemon are deleted. During container restore, it’s actually CRIU that is the parent. Therefore, setting up a pipe between the child (container) and an unrelated process (Docker daemon) required is not a bit of challenge.</p><p>To address this issue, the --inherit-fd command line option was added to CRIU. Using this option, the Docker daemon tells CRIU to let the restored container “inherit” certain file descriptors passed from the daemon to CRIU.</p><p>The first version of native C/R was demo'ed at the Linux Plumbers Conference (LPC) in October 2014 (<a href=http://linuxplumbersconf.org/2014/ocw/proposals/1899>http://linuxplumbersconf.org/2014/ocw/proposals/1899</a>).</p><p><img src=https://lh3.googleusercontent.com/8R9X9CJxoqZBj0n9IdYHnrp8dCZTNcdpueUgUIggALAV4mWBoRkm4k89BPB6ApCHyMnlailY6YwBUKDgtjlQm3rGU6KOWNK3s8Bmq46BP_qetl6z97l3k44GXPG8oE_az0142DU alt=external_cr.png></p><p>The LPC demo was done with a simple container that did not require network connectivity. Support for restoring network connections was done in early 2015 and demonstrated in this 2-minute <a href="https://www.youtube.com/watch?v=HFt9v6yqsXo">video clip</a>.</p><h4 id=current-status-of-container-c-r>Current Status of Container C/R</h4><p>In May 2015, the criu branch of libcontainer was merged into master. Using the newly-introduced lightweight <a href=https://blog.docker.com/2015/06/runc/>runC</a> container runtime, container migration was demo’ed at DockerCon15. In this
<a href="https://www.youtube.com/watch?v=?mL9AFkJJAq0"><img src=https://img.youtube.com/vi/7vZ9dRKRMyc/0.jpg alt=demo></a> (minute 23:00), a container running Quake was checkpointed and restored on a different machine, effectively implementing container migration.</p><p>At the time of this writing, there are two repos on GitHub that have native C/R support in Docker:</p><ul><li><a href=https://github.com/SaiedKazemi/docker/tree/cr>Docker 1.5</a> (old libcontainer, relatively stable)</li><li><a href=https://github.com/boucher/docker/tree/cr-combined>Docker 1.7</a> (newer, less stable)</li></ul><p>Work is underway to merge C/R functionality into Docker. You can use either of the above repositories to experiment with Docker C/R. If you are using OverlayFS or your container workload uses AIO, please note the following:</p><h4 id=overlayfs>OverlayFS</h4><p>When OverlayFS support was officially merged into the Linux kernel version 3.18, it became the preferred storage driver (instead of AUFS) . However, OverlayFS in 3.18 has the following issues:</p><ul><li>/proc/&lt;pid>/fdinfo/&lt;fd> contains mnt_id which isn’t in /proc/&lt;pid>/mountinfo</li><li>/proc/&lt;pid>/fd/&lt;fd> does not contain an absolute path to the opened file</li></ul><p>Both issues are fixed in this <a href=https://lkml.org/lkml/2015/3/20/372>patch</a> but the patch has not been merged upstream yet.</p><h4 id=aio>AIO</h4><p>If you are using a kernel older than 3.19 and your container uses AIO, you need the following kernel patches from 3.19:</p><ul><li><p><a href="https://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=bd9b51e7">torvalds: bd9b51e7</a> by Al Viro</p></li><li><p><a href="https://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=e4a0d3e72">torvalds: e4a0d3e72</a> by Pavel Emelyanov</p></li><li><p>Saied Kazemi, Software Engineer at Google</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6aa3a3141fa72e55eef75e87fa6e9c6b>Kubernetes 1.0 Launch Event at OSCON</h1><div class="td-byline mb-4"><time datetime=2015-07-02 class=text-muted>Thursday, July 02, 2015</time></div><p>In case you haven't heard, the Kubernetes project team & community have some awesome stuff lined up for our release event at OSCON in a few weeks.</p><p>If you haven't already registered for in person or live stream, please do it now! check out <a href=http://kuberneteslaunch.com/>kuberneteslaunch.com</a> for all the details. You can also find out there how to get a free expo pass for OSCON which you'll need to attend in person.</p><p>We'll have talks from Google executives Brian Stevens, VP of Cloud Product, and Eric Brewer, VP of Google Infrastructure. They will share their perspective on where Kubernetes is and where it's going that you won't want to miss.</p><p>Several of our community partners will be there including CoreOS, Redapt, Intel, Mesosphere, Mirantis, the OpenStack Foundation, CloudBees, Kismatic and Bitnami.</p><p>And real life users of Kubernetes will be there too. We've announced that zulily Principal Engineer Steve Reed is speaking, and we will let you know about others over the next few days. Let's just say it's a pretty cool list.</p><p>Check it out now - kuberneteslaunch.com</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9a4ed4202cfa82b6304415acbd82e5b7>The Distributed System ToolKit: Patterns for Composite Containers</h1><div class="td-byline mb-4"><time datetime=2015-06-29 class=text-muted>Monday, June 29, 2015</time></div><p>Having had the privilege of presenting some ideas from Kubernetes at DockerCon 2015, I thought I would make a blog post to share some of these ideas for those of you who couldn’t be there.</p><p>Over the past two years containers have become an increasingly popular way to package and deploy code. Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers.  By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers.</p><p>In many ways the switch from VMs to containers is like the switch from monolithic programs of the 1970s and early 80s to modular object-oriented programs of the late 1980s and onward. The abstraction layer provided by the container image has a great deal in common with the abstraction boundary of the class in object-oriented programming, and it allows the same opportunities to improve developer productivity and application quality.  Just like the right way to code is the separation of concerns into modular objects, the right way to package applications in containers is the separation of concerns into modular containers.  Fundamentally  this means breaking up not just the overall application, but also the pieces within any one server into multiple modular containers that are easy to parameterize and re-use. In this way, just like the standard libraries that are ubiquitous in modern languages, most application developers can compose together modular containers that are written by others, and build their applications more quickly and with higher quality components.</p><p>The benefits of thinking in terms of modular containers are enormous, in particular, modular containers provide the following:</p><ul><li>Speed application development, since containers can be re-used between teams and even larger communities</li><li>Codify expert knowledge, since everyone collaborates on a single containerized implementation that reflects best-practices rather than a myriad of different home-grown containers with roughly the same functionality</li><li>Enable agile teams, since the container boundary is a natural boundary and contract for team responsibilities</li><li>Provide separation of concerns and focus on specific functionality that reduces spaghetti dependencies and un-testable components</li></ul><p>Building an application from modular containers means thinking about symbiotic groups of containers that cooperate to provide a service, not one container per service.  In Kubernetes, the embodiment of this modular container service is a Pod.  A Pod is a group of containers that share resources like file systems, kernel namespaces and an IP address.  The Pod is the atomic unit of scheduling in a Kubernetes cluster, precisely because the symbiotic nature of the containers in the Pod require that they be co-scheduled onto the same machine, and the only way to reliably achieve this is by making container groups atomic scheduling units.</p><p>When you start thinking in terms of Pods, there are naturally some general patterns of modular application development that re-occur multiple times.  I’m confident that as we move forward in the development of Kubernetes more of these patterns will be identified, but here are three that we see commonly:</p><h2 id=example-1-sidecar-containers>Example #1: Sidecar containers</h2><p>Sidecar containers extend and enhance the "main" container, they take existing containers and make them better.  As an example, consider a container that runs the Nginx web server.  Add a different container that syncs the file system with a git repository, share the file system between the containers and you have built Git push-to-deploy.  But you’ve done it in a modular manner where the git synchronizer can be built by a different team, and can be reused across many different web servers (Apache, Python, Tomcat, etc).  Because of this modularity, you only have to write and test your git synchronizer once and reuse it across numerous apps. And if someone else writes it, you don’t even need to do that.</p><p><img src=/images/blog/2015-06-00-The-Distributed-System-Toolkit-Patterns/sidecar-containers.png alt="Sidecar Containers"></p><h2 id=example-2-ambassador-containers>Example #2: Ambassador containers</h2><p>Ambassador containers proxy a local connection to the world.  As an example, consider a Redis cluster with read-replicas and a single write master.  You can create a Pod that groups your main application with a Redis ambassador container.  The ambassador is a proxy is responsible for splitting reads and writes and sending them on to the appropriate servers.  Because these two containers share a network namespace, they share an IP address and your application can open a connection on “localhost” and find the proxy without any service discovery.  As far as your main application is concerned, it is simply connecting to a Redis server on localhost.  This is powerful, not just because of separation of concerns and the fact that different teams can easily own the components, but also because in the development environment, you can simply skip the proxy and connect directly to a Redis server that is running on localhost.</p><p><img src=/images/blog/2015-06-00-The-Distributed-System-Toolkit-Patterns/ambassador-containers.png alt="Ambassador Containers"></p><h2 id=example-3-adapter-containers>Example #3: Adapter containers</h2><p>Adapter containers standardize and normalize output.  Consider the task of monitoring N different applications.  Each application may be built with a different way of exporting monitoring data. (e.g. JMX, StatsD, application specific statistics) but every monitoring system expects a consistent and uniform data model for the monitoring data it collects.  By using the adapter pattern of composite containers, you can transform the heterogeneous monitoring data from different systems into a single unified representation by creating Pods that groups the application containers with adapters that know how to do the transformation.  Again because these Pods share namespaces and file systems, the coordination of these two containers is simple and straightforward.</p><p><img src=/images/blog/2015-06-00-The-Distributed-System-Toolkit-Patterns/adapter-containers.png alt="Adapter Containers"></p><p>In all of these cases, we've used the container boundary as an encapsulation/abstraction boundary that allows us to build modular, reusable components that we combine to build out applications.  This reuse enables us to more effectively share containers between different developers, reuse our code across multiple applications, and generally build more reliable, robust distributed systems more quickly.  I hope you’ve seen how Pods and composite container patterns can enable you to build robust distributed systems more quickly, and achieve container code re-use.  To try these patterns out yourself in your own applications. I encourage you to go check out open source Kubernetes or Google Container Engine.</p><ul><li>Brendan Burns, Software Engineer at Google</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d3c0039c2d00bc08b5dbc16a4f7f29e2>Slides: Cluster Management with Kubernetes, talk given at the University of Edinburgh</h1><div class="td-byline mb-4"><time datetime=2015-06-26 class=text-muted>Friday, June 26, 2015</time></div><p>On Friday 5 June 2015 I gave a talk called <a href="https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&loop=false&delayms=3000">Cluster Management with Kubernetes</a> to a general audience at the University of Edinburgh. The talk includes an example of a music store system with a Kibana front end UI and an Elasticsearch based back end which helps to make concrete concepts like pods, replication controllers and services.</p><p><a href="https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&loop=false&delayms=3000">Cluster Management with Kubernetes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b87887ddd92e224cf8adab9f25724a65>Cluster Level Logging with Kubernetes</h1><div class="td-byline mb-4"><time datetime=2015-06-11 class=text-muted>Thursday, June 11, 2015</time></div><p>A Kubernetes cluster will typically be humming along running many system and application pods. How does the system administrator collect, manage and query the logs of the system pods? How does a user query the logs of their application which is composed of many pods which may be restarted or automatically generated by the Kubernetes system? These questions are addressed by the Kubernetes cluster level logging services.</p><p>Cluster level logging for Kubernetes allows us to collect logs which persist beyond the lifetime of the pod’s container images or the lifetime of the pod or even cluster. In this article we assume that a Kubernetes cluster has been created with cluster level logging support for sending logs to <a href=https://cloud.google.com/logging/docs/>Google Cloud Logging</a>. This is an option when creating a <a href=https://cloud.google.com/container-engine/>Google Container Engine</a> (GKE) cluster, and is enabled by default for the open source <a href=https://cloud.google.com/compute/>Google Compute Engine</a> (GCE) Kubernetes distribution. After a cluster has been created you will have a collection of system <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/pods.md>pods</a> running that support monitoring, logging and DNS resolution for names of Kubernetes services:</p><pre><code>$ kubectl get pods
</code></pre><pre><code>NAME                                           READY     REASON    RESTARTS   AGE

fluentd-cloud-logging-kubernetes-minion-0f64   1/1       Running   0          32m

fluentd-cloud-logging-kubernetes-minion-27gf   1/1       Running   0          32m

fluentd-cloud-logging-kubernetes-minion-pk22   1/1       Running   0          31m

fluentd-cloud-logging-kubernetes-minion-20ej   1/1       Running   0          31m

kube-dns-v3-pk22                               3/3       Running   0          32m


monitoring-heapster-v1-20ej                    0/1       Running   9          32m
</code></pre><p>Here is the same information in a picture which shows how the pods might be placed on specific nodes.</p><p><a href=https://1.bp.blogspot.com/-FSXnrHLDMJs/Vxfzx2rsreI/AAAAAAAAAbk/PaDTpksKEZk4e8YQff5-JhGPoEpgyWaHgCLcB/s1600/cloud-logging.png><img src=https://1.bp.blogspot.com/-FSXnrHLDMJs/Vxfzx2rsreI/AAAAAAAAAbk/PaDTpksKEZk4e8YQff5-JhGPoEpgyWaHgCLcB/s400/cloud-logging.png alt></a></p><p>Here is a close up of what is running on each node.</p><p><a href=https://4.bp.blogspot.com/-T7kPtjq8O9A/Vxfz6k7XogI/AAAAAAAAAbo/-59dO6F58sERDOQGJ7872ex_KkEKFpArwCLcB/s1600/0f64.png><img src=https://4.bp.blogspot.com/-T7kPtjq8O9A/Vxfz6k7XogI/AAAAAAAAAbo/-59dO6F58sERDOQGJ7872ex_KkEKFpArwCLcB/s400/0f64.png alt></a></p><p><a href=https://3.bp.blogspot.com/-5VRLexsSJwA/Vxf0F0ccVDI/AAAAAAAAAbs/rh4KGFc95-cIdrTxAujYH2LMrCQ8vrdzQCLcB/s1600/27gf.png><img src=https://3.bp.blogspot.com/-5VRLexsSJwA/Vxf0F0ccVDI/AAAAAAAAAbs/rh4KGFc95-cIdrTxAujYH2LMrCQ8vrdzQCLcB/s400/27gf.png alt></a></p><p><a href=https://4.bp.blogspot.com/-UXOxauNy8FQ/Vxf0SaGujNI/AAAAAAAAAb0/Pnf6e_iiUfoKkooGyrF3Gmd8wh0vPrteQCLcB/s1600/pk22.png><img src=https://4.bp.blogspot.com/-UXOxauNy8FQ/Vxf0SaGujNI/AAAAAAAAAb0/Pnf6e_iiUfoKkooGyrF3Gmd8wh0vPrteQCLcB/s400/pk22.png alt></a></p><p><a href=https://2.bp.blogspot.com/-UgpwCx4BNwQ/Vxf0Wc8-HwI/AAAAAAAAAb4/g3D1bE74FQA2k9uwc9ZbZuB1N7MTU7swgCLcB/s1600/20ej.png><img src=https://2.bp.blogspot.com/-UgpwCx4BNwQ/Vxf0Wc8-HwI/AAAAAAAAAb4/g3D1bE74FQA2k9uwc9ZbZuB1N7MTU7swgCLcB/s400/20ej.png alt></a></p><p>The first diagram shows four nodes created on a GCE cluster with the name of each VM node on a purple background. The internal and public IPs of each node are shown on gray boxes and the pods running in each node are shown in green boxes. Each pod box shows the name of the pod and the namespace it runs in, the IP address of the pod and the images which are run as part of the pod’s execution. Here we see that every node is running a fluentd-cloud-logging pod which is collecting the log output of the containers running on the same node and sending them to Google Cloud Logging. A pod which provides a <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/dns.md>cluster DNS service</a> runs on one of the nodes and a pod which provides monitoring support runs on another node.</p><p>To help explain how cluster level logging works let’s start off with a synthetic log generator pod specification <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/examples/blog-logging/counter-pod.yaml>counter-pod.yaml</a>:</p><pre><code>  apiVersion : v1  
  kind : Pod  
  metadata :  
    name : counter  
  spec :  
    containers :  
   - name : count  
      image : ubuntu:14.04  
      args : [bash, -c,   
            'for ((i = 0; ; i++)); do echo &quot;$i: $(date)&quot;; sleep 1; done']  
</code></pre><p>This pod specification has one container which runs a bash script when the container is born. This script simply writes out the value of a counter and the date once per second and runs indefinitely. Let’s create the pod.</p><pre><code>
$ kubectl create -f counter-pod.yaml


pods/counter

</code></pre><p>We can observe the running pod:</p><pre><code>$ kubectl get pods
</code></pre><pre><code>NAME                                           READY     REASON    RESTARTS   AGE

counter                                        1/1       Running   0          5m

fluentd-cloud-logging-kubernetes-minion-0f64   1/1       Running   0          55m

fluentd-cloud-logging-kubernetes-minion-27gf   1/1       Running   0          55m

fluentd-cloud-logging-kubernetes-minion-pk22   1/1       Running   0          55m

fluentd-cloud-logging-kubernetes-minion-20ej   1/1       Running   0          55m

kube-dns-v3-pk22                               3/3       Running   0          55m

monitoring-heapster-v1-20ej                    0/1       Running   9          56m
</code></pre><p>This step may take a few minutes to download the ubuntu:14.04 image during which the pod status will be shown as Pending.</p><p>One of the nodes is now running the counter pod:</p><p><a href=https://4.bp.blogspot.com/-BI3zOVlrHwA/Vxf0KwcqtCI/AAAAAAAAAbw/vzv8X8vQrso9Iycx4qQHuOslE8So7smLgCLcB/s1600/27gf-counter.png><img src=https://4.bp.blogspot.com/-BI3zOVlrHwA/Vxf0KwcqtCI/AAAAAAAAAbw/vzv8X8vQrso9Iycx4qQHuOslE8So7smLgCLcB/s400/27gf-counter.png alt></a></p><p>When the pod status changes to Running we can use the kubectl logs command to view the output of this counter pod.</p><pre><code>$ kubectl logs counter

0: Tue Jun  2 21:37:31 UTC 2015

1: Tue Jun  2 21:37:32 UTC 2015

2: Tue Jun  2 21:37:33 UTC 2015

3: Tue Jun  2 21:37:34 UTC 2015

4: Tue Jun  2 21:37:35 UTC 2015

5: Tue Jun  2 21:37:36 UTC 2015

</code></pre><p>This command fetches the log text from the Docker log file for the image that is running in this container. We can connect to the running container and observe the running counter bash script.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl <span style=color:#a2f>exec</span> -i counter bash

ps aux

USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND

root         <span style=color:#666>1</span>  0.0  0.0  <span style=color:#666>17976</span>  <span style=color:#666>2888</span> ?        Ss   00:02   0:00 bash -c <span style=color:#a2f;font-weight:700>for</span> <span style=color:#666>((</span><span style=color:#b8860b>i</span> <span style=color:#666>=</span> 0; ; i++<span style=color:#666>))</span>; <span style=color:#a2f;font-weight:700>do</span> <span style=color:#a2f>echo</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$i</span><span style=color:#b44>: </span><span style=color:#a2f;font-weight:700>$(</span>date<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>; sleep 1; <span style=color:#a2f;font-weight:700>done</span>

root       <span style=color:#666>468</span>  0.0  0.0  <span style=color:#666>17968</span>  <span style=color:#666>2904</span> ?        Ss   00:05   0:00 bash

root       <span style=color:#666>479</span>  0.0  0.0   <span style=color:#666>4348</span>   <span style=color:#666>812</span> ?        S    00:05   0:00 sleep <span style=color:#666>1</span>

root       <span style=color:#666>480</span>  0.0  0.0  <span style=color:#666>15572</span>  <span style=color:#666>2212</span> ?        R    00:05   0:00 ps aux
</code></pre></div><p>What happens if for any reason the image in this pod is killed off and then restarted by Kubernetes? Will we still see the log lines from the previous invocation of the container followed by the log lines for the started container? Or will we lose the log lines from the original container’s execution and only see the log lines for the new container? Let’s find out. First let’s stop the currently running counter.</p><pre><code>$ kubectl stop pod counter

pods/counter


Now let’s restart the counter.


$ kubectl create -f counter-pod.yaml

pods/counter
</code></pre><p>Let’s wait for the container to restart and get the log lines again.</p><pre><code>$ kubectl logs counter

0: Tue Jun  2 21:51:40 UTC 2015

1: Tue Jun  2 21:51:41 UTC 2015

2: Tue Jun  2 21:51:42 UTC 2015

3: Tue Jun  2 21:51:43 UTC 2015

4: Tue Jun  2 21:51:44 UTC 2015

5: Tue Jun  2 21:51:45 UTC 2015

6: Tue Jun  2 21:51:46 UTC 2015

7: Tue Jun  2 21:51:47 UTC 2015

8: Tue Jun  2 21:51:48 UTC 2015

</code></pre><p>Oh no! We’ve lost the log lines from the first invocation of the container in this pod! Ideally, we want to preserve all the log lines from each invocation of each container in the pod. Furthermore, even if the pod is restarted we would still like to preserve all the log lines that were ever emitted by the containers in the pod. But don’t fear, this is the functionality provided by cluster level logging in Kubernetes. When a cluster is created, the standard output and standard error output of each container can be ingested using a <a href=http://www.fluentd.org/>Fluentd</a> agent running on each node into either <a href=https://cloud.google.com/logging/docs/>Google Cloud Logging</a> or into Elasticsearch and viewed with Kibana. This blog article focuses on Google Cloud Logging.</p><p>When a Kubernetes cluster is created with logging to Google Cloud Logging enabled, the system creates a pod called fluentd-cloud-logging on each node of the cluster to collect Docker container logs. These pods were shown at the start of this blog article in the response to the first get pods command.</p><p>This log collection pod has a specification which looks something like this <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/saltbase/salt/fluentd-gcp/fluentd-gcp.yaml>fluentd-gcp.yaml</a>:</p><pre><code>apiVersion: v1

kind: Pod

metadata:

  name: fluentd-cloud-logging

spec:

  containers:

  - name: fluentd-cloud-logging

    image: gcr.io/google\_containers/fluentd-gcp:1.6

    env:

    - name: FLUENTD\_ARGS

      value: -qq

    volumeMounts:

    - name: containers

      mountPath: /var/lib/docker/containers

  volumes:

  - name: containers

    hostPath:

      path: /var/lib/docker/containers
</code></pre><p>This pod specification maps the directory on the host containing the Docker log files, /var/lib/docker/containers, to a directory inside the container which has the same path. The pod runs one image, gcr.io/google_containers/fluentd-gcp:1.6, which is configured to collect the Docker log files from the logs directory and ingest them into Google Cloud Logging. One instance of this pod runs on each node of the cluster. Kubernetes will notice if this pod fails and automatically restart it.</p><p>We can click on the Logs item under the Monitoring section of the Google Developer Console and select the logs for the counter container, which will be called kubernetes.counter_default_count. This identifies the name of the pod (counter), the namespace (default) and the name of the container (count) for which the log collection occurred. Using this name we can select just the logs for our counter container from the drop down menu:</p><p><em>(image-counter-new-logs.png)</em></p><p>When we view the logs in the Developer Console we observe the logs for both invocations of the container.</p><p><em>(image-screenshot-2015-06-02)</em></p><p>Note the first container counted to 108 and then it was terminated. When the next container image restarted the counting process resumed from 0. Similarly if we deleted the pod and restarted it we would capture the logs for all instances of the containers in the pod whenever the pod was running.</p><p>Logs ingested into Google Cloud Logging may be exported to various other destinations including <a href=https://cloud.google.com/storage/>Google Cloud Storage</a> buckets and <a href=https://cloud.google.com/bigquery/>BigQuery</a>. Use the Exports tab in the Cloud Logging console to specify where logs should be streamed to (or follow this link to the <a href=https://pantheon.corp.google.com/project/_/logs/settings>settings tab</a>).</p><p>We could query the ingested logs from BigQuery using the SQL query which reports the counter log lines showing the newest lines first.</p><p>SELECT metadata.timestamp, structPayload.log FROM [mylogs.kubernetes_counter_default_count_20150611] ORDER BY metadata.timestamp DESC</p><p>Here is some sample output:</p><p><strong><em>(image-bigquery-log-new.png)</em></strong></p><p>We could also fetch the logs from Google Cloud Storage buckets to our desktop or laptop and then search them locally. The following command fetches logs for the counter pod running in a cluster which is itself in a GCE project called myproject. Only logs for the date 2015-06-11 are fetched.</p><pre><code>$ gsutil -m cp -r gs://myproject/kubernetes.counter\_default\_count/2015/06/11 .
</code></pre><p>Now we can run queries over the ingested logs. The example below uses the<a href=http://stedolan.github.io/jq/>jq</a> program to extract just the log lines.</p><pre><code>$ cat 21\:00\:00\_21\:59\:59\_S0.json | jq '.structPayload.log'

&quot;0: Thu Jun 11 21:39:38 UTC 2015\n&quot;

&quot;1: Thu Jun 11 21:39:39 UTC 2015\n&quot;

&quot;2: Thu Jun 11 21:39:40 UTC 2015\n&quot;

&quot;3: Thu Jun 11 21:39:41 UTC 2015\n&quot;

&quot;4: Thu Jun 11 21:39:42 UTC 2015\n&quot;

&quot;5: Thu Jun 11 21:39:43 UTC 2015\n&quot;

&quot;6: Thu Jun 11 21:39:44 UTC 2015\n&quot;

&quot;7: Thu Jun 11 21:39:45 UTC 2015\n&quot;
</code></pre><p>This article has touched briefly on the underlying mechanisms that support gathering cluster level logs on a Kubernetes deployment. The approach here only works for gathering the standard output and standard error output of the processes running in the pod’s containers. To gather other logs that are stored in files one can use a sidecar container to gather the required files as described at the page <a href=https://github.com/GoogleCloudPlatform/kubernetes/tree/master/contrib/logging/fluentd-sidecar-gcp>Collecting log files within containers with Fluentd and sending them to the Google Cloud Logging service</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0bd7fb297b618aacb101744a911d5f19>Weekly Kubernetes Community Hangout Notes - May 22 2015</h1><div class="td-byline mb-4"><time datetime=2015-06-02 class=text-muted>Tuesday, June 02, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Discussion / Topics</p><ul><li>Code Freeze</li><li>Upgrades of cluster</li><li>E2E test issues</li></ul><p>Code Freeze process starts EOD 22-May, including</p><ul><li>Code Slush -- draining PRs that are active. If there are issues for v1 to raise, please do so today.</li><li>Community PRs -- plan is to reopen in ~6 weeks.</li><li>Key areas for fixes in v1 -- docs, the experience.</li></ul><p>E2E issues and LGTM process</p><ul><li><p>Seen end-to-end tests go red.</p></li><li><p>Plan is to limit merging to on-call. Quinton to communicate.</p></li><li><p>Can we expose Jenkins runs to community? (Paul)</p><ul><li><p>Question/concern to work out is securing Jenkins. Short term conclusion: Will look at pushing Jenkins logs into GCS bucket. Lavalamp will follow up with Jeff Grafton.</p></li><li><p>Longer term solution may be a merge queue, where e2e runs for each merge (as opposed to multiple merges). This exists in OpenShift today.</p></li></ul></li></ul><p>Cluster Upgrades for Kubernetes as final v1 feature</p><ul><li><p>GCE will use Persistent Disk (PD) to mount new image.</p></li><li><p>OpenShift will follow a tradition update model, with "yum update".</p></li><li><p>A strawman approach is to have an analog of "kube-push" to update the master, in-place. Feedback in the meeting was</p><ul><li><p>Upgrading Docker daemon on the master will kill the master's pods. Agreed. May consider an 'upgrade' phase or explicit step.</p></li><li><p>How is this different than HA master upgrade? See HA case as a superset. The work to do an upgrade would be a prerequisite for HA master upgrade.</p></li></ul></li><li><p>Mesos scheduler implements a rolling node upgrade.</p></li></ul><p>Attention requested for v1 in the Hangout</p><ul><li><ul><li><p>Discussed that it's an eventually consistent design.*</p><ul><li>In the meeting, the outcome was: seeking a pattern for atomicity of update across multiple piece. Paul to ping Tim when ready to review.</li></ul></li></ul></li><li><p>Regression in e2e <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/8499>#8499</a> (Eric Paris)</p></li><li><p>Asking for review of direction, if not review. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8334>#8334</a> (Mark)</p></li><li><p>Handling graceful termination (e.g. sigterm to postgres) is not implemented. <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/2789>#2789</a> (Clayton)</p><ul><li><p>Need is to bump up grace period or finish plumbing. In API, client tools, missing is kubelet does use and we don't set the timeout (>0) value.</p></li><li><p>Brendan will look into this graceful term issue.</p></li></ul></li><li><p>Load balancer almost ready by JustinSB.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3b8bd1fa11dba03c4234640bf5bebd0d>Kubernetes on OpenStack</h1><div class="td-byline mb-4"><time datetime=2015-05-19 class=text-muted>Tuesday, May 19, 2015</time></div><p><a href=https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s1600/Untitled%2Bdrawing.jpg><img src=https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s400/Untitled%2Bdrawing.jpg alt></a></p><p>Today, the <a href=https://www.openstack.org/foundation/>OpenStack foundation</a> made it even easier for you deploy and manage clusters of Docker containers on OpenStack clouds by including Kubernetes in its <a href=http://apps.openstack.org/>Community App Catalog</a>.  At a keynote today at the OpenStack Summit in Vancouver, Mark Collier, COO of the OpenStack Foundation, and Craig Peters,  <a href=https://www.mirantis.com/>Mirantis</a> product line manager, demonstrated the Community App Catalog workflow by launching a Kubernetes cluster in a matter of seconds by leveraging the compute, storage, networking and identity systems already present in an OpenStack cloud.</p><p>The entries in the catalog include not just the ability to <a href="http://apps.openstack.org/#tab=murano-apps&asset=Kubernetes%20Cluster">start a Kubernetes cluster</a>, but also a range of applications deployed in Docker containers managed by Kubernetes. These applications include:</p><ul><li>Apache web server</li><li>Nginx web server</li><li>Crate - The Distributed Database for Docker</li><li>GlassFish - Java EE 7 Application Server</li><li>Tomcat - An open-source web server and servlet container</li><li>InfluxDB - An open-source, distributed, time series database</li><li>Grafana - Metrics dashboard for InfluxDB</li><li>Jenkins - An extensible open source continuous integration server</li><li>MariaDB database</li><li>MySql database</li><li>Redis - Key-value cache and store</li><li>PostgreSQL database</li><li>MongoDB NoSQL database</li><li>Zend Server - The Complete PHP Application Platform</li></ul><p>This list will grow, and is curated <a href=https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes>here</a>. You can examine (and contribute to) the YAML file that tells Murano how to install and start the Kubernetes cluster <a href=https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml>here</a>.</p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes>The Kubernetes open source project</a> has continued to see fantastic community adoption and increasing momentum, with over 11,000 commits and 7,648 stars on GitHub. With supporters ranging from Red Hat and Intel to CoreOS and Box.net, it has come to represent a range of customer interests ranging from enterprise IT to cutting edge startups. We encourage you to give it a try, give us your feedback, and get involved in our growing community.</p><ul><li>Martin Buhr, Product Manager, Kubernetes Open Source Project</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5e05e7f4d2c034dc92074e1f775a8b28>Docker and Kubernetes and AppC</h1><div class="td-byline mb-4"><time datetime=2015-05-18 class=text-muted>Monday, May 18, 2015</time></div><p>Recently we announced the intent in Kubernetes, our open source cluster manager, to support AppC and RKT, an alternative container format that has been driven by CoreOS with input from many companies (including Google).  This announcement has generated a surprising amount of buzz and has been construed as a move from Google to support Appc over Docker.  Many have taken it as signal that Google is moving away from supporting Docker.  I would like to take a moment to clarify Google’s position in this.</p><p>Google has consistently supported the Docker initiative and has invested heavily in Docker. In the early days of containers, we decided to de-emphasize our own open source offering (LMCTFY) and to instead focus on Docker.  As a result of that we have two engineers that are active maintainers of LibContainer, a critical piece of the Docker ecosystem and are working closely with Docker to add many additional features and capabilities.  Docker is currently the only supported runtime in GKE (Google Container Engine) our commercial containers product, and in GAE (Google App Engine), our Platform-as-a-Service product.  </p><p>While we may introduce AppC support at some point in the future to GKE based on our customers demand, we intend to continue to support the Docker project and product, and Docker the company indefinitely.  To date Docker is by far the most mature and widely used container offering in the market, with over 400 million downloads.  It has been production ready for almost a year and seen widespread use in industry, and also here inside Google.</p><p>Beyond the obvious traction Docker has in the market, we are heartened by many of Docker’s recent initiatives to open the project and support ‘batteries included, but swappable options across the stack and recognize that it offers a great developer experience for engineers new to the containers world.  We are encouraged, for example, by the separation of the Docker Machine and Swarm projects from the core runtime, and are glad to see support for Docker Machine emerging for Google Compute Engine.</p><p>Our intent with our announcement for AppC and RKT support was to establish Kubernetes (our open source project) as a neutral ground in the world of containers.  Customers should be able to pick their container runtime and format based solely on its technical merits, and we do see AppC as offering some legitimate potential merits as the technology matures.  Somehow this was misconstrued as an ‘a vs b’ selection which is simply untrue.  The world is almost always better for having choice, and it is perfectly natural that different tools should be available for different purposes.  </p><p>Stepping back a little, one must recognize that Docker has done remarkable work in democratizing container technologies and making them accessible to everyone.  We believe that Docker will continue to drive great experiences for developers looking to use containers and plan to support this technology and its burgeoning community indefinitely.  We, for one,  are looking forward to the upcoming Dockercon where Brendan Burns (a Kubernetes co-founder) will be talking about the role of Docker in modern distributed systems design.</p><p>-- Craig McLuckie</p><p>Google Group Product Manager, and Kubernetes Project Co-Founder</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c1a4261c381e56c7a5f0cb522da45fda>Weekly Kubernetes Community Hangout Notes - May 15 2015</h1><div class="td-byline mb-4"><time datetime=2015-05-18 class=text-muted>Monday, May 18, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><ul><li><a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/7018>v1 API</a> - what's in, what's out<ul><li>We're trying to fix critical issues we discover with v1beta3</li><li>Would like to make a number of minor cleanups that will be expensive to do later<ul><li>defaulting replication controller spec default to 1</li><li>deduplicating security context</li><li>change id field to name</li><li>rename host</li><li>inconsistent times</li><li>typo in container states terminated (termination vs. terminated)</li><li>flatten structure (requested by heavy API user)</li><li>pod templates - could be added after V1, field is not implemented, remove template ref field</li><li>in general remove any fields not implemented (can be added later)</li><li>if we want to change any of the identifier validation rules, should do it now</li><li>recently changed label validation rules to be more precise</li></ul></li><li>Bigger changes<ul><li>generalized label selectors</li><li>service - change the fields in a way that we can add features in a forward compatible manner if possible</li><li>public IPs - what to do from a security perspective</li><li>Support aci format - there is an image field - add properties to signify the image, or include it in a string</li><li>inconsistent on object use / cross reference - needs design discussion</li></ul></li><li>Things to do later<ul><li>volume source cleanup</li><li>multiple API prefixes</li><li>watch changes - watch client is not notified of progress</li></ul></li></ul></li><li>A few other proposals<ul><li>swagger spec fixes - ongoing</li><li>additional field selectors - additive, backward compatible</li><li>additional status - additive, backward compatible</li><li>elimination of phase - won't make it for v1</li></ul></li><li>Service discussion - Public IPs<ul><li>with public IPs as it exists we can't go to v1</li><li>Tim has been developing a mitigation if we can't get Justin's overhaul in (but hopefully we will)</li><li>Justin's fix will describe public IPs in a much better way</li><li>The general problem is it's too flexible and you can do things that are scary, the mitigation is to restrict public ip usage to specific use cases -- validated public IPs would be copied to status, which is what kube-proxy would use</li><li>public IPs used for -<ul><li>binding to nodes / node</li><li>request a specific load balancer IP (GCE only)</li><li>emulate multi-port services -- now we support multi-port services, so no longer necessary</li></ul></li><li>This is a large change, 70% code complete, Tim & Justin working together, parallel code review and updates, need to reconcile and test</li><li>Do we want to allow people to request host ports - is there any value in letting people ask for a public port? or should we assign you one?<ul><li>Tim: we should assign one</li></ul></li><li>discussion of what to do with status - if users set to empty then probably their intention</li><li>general answer to the pattern is binding</li><li>post v1: if we can make portal ip a non-user settable field, then we need to figure out the transition plan. need to have a fixed ip for dns.</li><li>we should be able to just randomly assign services a new port and everything should adjust, but this is not feasible for v1</li><li>next iteration of the proposal: PR is being iterated on, testing over the weekend, so PR hopefully ready early next week - gonna be a doozie!</li></ul></li><li>API transition<ul><li>actively removing all dependencies on v1beta1 and v1beta2, announced their going away</li><li>working on a script that will touch everything in the system and will force everything to flip to v1beta3</li><li>a release with both APIs supported and with this script can make sure clusters are moved over and we can move the API</li><li>Should be gone by 0.19</li><li>Help is welcome, especially for trivial things and will try to get as much done as possible in next few weeks</li><li>Release candidate targeting mid june</li><li>The new kubectl will not work for old APIs, will be a problem for GKE for clusters pinned to old version. Will be a problem for k8s users as well if they update kubectl</li><li>Since there's no way to upgrade a GKE cluster, users are going to have to tear down and upgrade their cluster</li><li>we're going to stop testing v1beta1 very soon, trying to streamline the testing paths in our CI pipelines</li></ul></li><li>Did we decide we are not going to do namespace autoprovisioning?<ul><li>Brian would like to turn it off - no objections</li><li>Documentation should include creating namepspaces</li><li>Would like to impose a default CPU for the default namespace</li><li>would cap the number of pods, would reduce the resource exhaustion issue</li><li>would eliminate need to explicitly cap the number of pods on a node due to IP exhaustion</li><li>could add resources as arguments to the porcelain commands</li><li>kubectl run is a simplified command, but it could include some common things (image, command, ports). but could add resources</li></ul></li><li>Kubernetes 1.0 Launch Event<ul><li>Save the * Blog posts, whitepapers, etc. welcome to be published</li><li>Event will be live streamed, mostly demos & customer talks, keynote</li><li>Big launch party in the evening</li><li>Kit to send more info in next couple weeks</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5ae4b125dfdf018d47858d52d3823ab4>Kubernetes Release: 0.17.0</h1><div class="td-byline mb-4"><time datetime=2015-05-15 class=text-muted>Friday, May 15, 2015</time></div><p>Release Notes:</p><ul><li><p>Cleanups</p><ul><li>Remove old salt configs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8065 title="Remove old salt configs">#8065</a> (roberthbailey)</li><li>Kubelet: minor cleanups <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8069 title="Kubelet: minor cleanups">#8069</a> (yujuhong)</li></ul></li><li><p>v1beta3</p><ul><li>update example/walkthrough to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7940 title="update example/walkthrough to v1beta3">#7940</a> (caesarxuchao)</li><li>update example/rethinkdb to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7946 title="update example/rethinkdb to v1beta3">#7946</a> (caesarxuchao)</li><li>verify the v1beta3 yaml files all work; merge the yaml files <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7917 title="verify the v1beta3 yaml files all work; merge the yaml files">#7917</a> (caesarxuchao)</li><li>update examples/cassandra to api v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7258 title="update examples/cassandra to api v1beta3">#7258</a> (caesarxuchao)</li><li>update service.json in persistent-volume example to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7899 title="update service.json in persistent-volume example to v1beta3">#7899</a> (caesarxuchao)</li><li>update mysql-wordpress example to use v1beta3 API <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7864 title="update mysql-wordpress example to use v1beta3 API">#7864</a> (caesarxuchao)</li><li>Update examples/meteor to use API v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7848 title="Update examples/meteor to use API v1beta3">#7848</a> (caesarxuchao)</li><li>update node-selector example to API v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7872 title="update node-selector example to API v1beta3">#7872</a> (caesarxuchao)</li><li>update logging-demo to use API v1beta3; modify the way to access Elasticsearch and Kibana services <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7824 title="update logging-demo to use API v1beta3; modify the way to access Elasticsearch and Kibana services">#7824</a> (caesarxuchao)</li><li>Convert the skydns rc to use v1beta3 and add a health check to it <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7619 title="Convert the skydns rc to use v1beta3 and add a health check to it">#7619</a> (a-robinson)</li><li>update the hazelcast example to API version v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7728 title="update the hazelcast example to API version v1beta3">#7728</a> (caesarxuchao)</li><li>Fix YAML parsing for v1beta3 objects in the kubelet for file/http <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7515 title="Fix YAML parsing for v1beta3 objects in the kubelet for file/http">#7515</a> (brendandburns)</li><li>Updated kubectl cluster-info to show v1beta3 addresses <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7502 title="Updated kubectl cluster-info to show v1beta3 addresses">#7502</a> (piosz)</li></ul></li><li><p>Kubelet</p><ul><li>kubelet: Fix racy kubelet tests. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7980 title="kubelet: Fix racy kubelet tests.">#7980</a> (yifan-gu)</li><li>kubelet/container: Move prober.ContainerCommandRunner to container. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8079 title="kubelet/container: Move prober.ContainerCommandRunner to container.">#8079</a> (yifan-gu)</li><li>Kubelet: set host field in the pending pod status <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6127 title="Kubelet: set host field in the pending pod status">#6127</a> (yujuhong)</li><li>Fix the kubelet node watch <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6442 title="Fix the kubelet node watch">#6442</a> (yujuhong)</li><li>Kubelet: recreate mirror pod if the static pod changes <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6607 title="Kubelet: recreate mirror pod if the static pod changes">#6607</a> (yujuhong)</li><li>Kubelet: record the timestamp correctly in the runtime cache <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7749 title="Kubelet: record the timestamp correctly in the runtime cache">#7749</a> (yujuhong)</li><li>Kubelet: wait until container runtime is up <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7729 title="Kubelet: wait until container runtime is up">#7729</a> (yujuhong)</li><li>Kubelet: replace DockerManager with the Runtime interface <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7674 title="Kubelet: replace DockerManager with the Runtime interface">#7674</a> (yujuhong)</li><li>Kubelet: filter out terminated pods in SyncPods <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7301 title="Kubelet: filter out terminated pods in SyncPods">#7301</a> (yujuhong)</li><li>Kubelet: parallelize cleaning up containers in unwanted pods <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7048 title="Kubelet: parallelize cleaning up containers in unwanted pods">#7048</a> (yujuhong)</li><li>kubelet: Add container runtime option for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7952 title="kubelet: Add container runtime option for rkt.">#7952</a> (yifan-gu)</li><li>kubelet/rkt: Remove build label. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7916 title="kubelet/rkt: Remove build label.">#7916</a> (yifan-gu)</li><li>kubelet/metrics: Move instrumented_docker.go to dockertools. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7327 title="kubelet/metrics: Move instrumented_docker.go to dockertools.">#7327</a> (yifan-gu)</li><li>kubelet/rkt: Add GetPods() for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7599 title="kubelet/rkt: Add GetPods() for rkt.">#7599</a> (yifan-gu)</li><li>kubelet/rkt: Add KillPod() and GetPodStatus() for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7605 title="kubelet/rkt: Add KillPod() and GetPodStatus() for rkt.">#7605</a> (yifan-gu)</li><li>pkg/kubelet: Fix logging. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/4755 title="pkg/kubelet: Fix logging.">#4755</a> (yifan-gu)</li><li>kubelet: Refactor RunInContainer/ExecInContainer/PortForward. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6491 title="kubelet: Refactor RunInContainer/ExecInContainer/PortForward.">#6491</a> (yifan-gu)</li><li>kubelet/DockerManager: Fix returning empty error from GetPodStatus(). <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6609 title="kubelet/DockerManager: Fix returning empty error from GetPodStatus().">#6609</a> (yifan-gu)</li><li>kubelet: Move pod infra container image setting to dockertools. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6634 title="kubelet: Move pod infra container image setting to dockertools.">#6634</a> (yifan-gu)</li><li>kubelet/fake_docker_client: Use self's PID instead of 42 in testing. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6653 title="kubelet/fake_docker_client: Use self's PID instead of 42 in testing.">#6653</a> (yifan-gu)</li><li>kubelet/dockertool: Move Getpods() to DockerManager. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6778 title="kubelet/dockertool: Move Getpods() to DockerManager.">#6778</a> (yifan-gu)</li><li>kubelet/dockertools: Add puller interfaces in the containerManager. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6776 title="kubelet/dockertools: Add puller interfaces in the containerManager.">#6776</a> (yifan-gu)</li><li>kubelet: Introduce PodInfraContainerChanged(). <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6608 title="kubelet: Introduce PodInfraContainerChanged().">#6608</a> (yifan-gu)</li><li>kubelet/container: Replace DockerCache with RuntimeCache. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6795 title="kubelet/container: Replace DockerCache with RuntimeCache.">#6795</a> (yifan-gu)</li><li>kubelet: Clean up computePodContainerChanges. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6844 title="kubelet: Clean up computePodContainerChanges.">#6844</a> (yifan-gu)</li><li>kubelet: Refactor prober. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7009 title="kubelet: Refactor prober.">#7009</a> (yifan-gu)</li><li>kubelet/container: Update the runtime interface. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7466 title="kubelet/container: Update the runtime interface.">#7466</a> (yifan-gu)</li><li>kubelet: Refactor isPodRunning() in runonce.go <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7477 title="kubelet: Refactor isPodRunning() in runonce.go">#7477</a> (yifan-gu)</li><li>kubelet/rkt: Add basic rkt runtime routines. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7465 title="kubelet/rkt: Add basic rkt runtime routines.">#7465</a> (yifan-gu)</li><li>kubelet/rkt: Add podInfo. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7555 title="kubelet/rkt: Add podInfo.">#7555</a> (yifan-gu)</li><li>kubelet/container: Add GetContainerLogs to runtime interface. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7488 title="kubelet/container: Add GetContainerLogs to runtime interface.">#7488</a> (yifan-gu)</li><li>kubelet/rkt: Add routines for converting kubelet pod to rkt pod. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7543 title="kubelet/rkt: Add routines for converting kubelet pod to rkt pod.">#7543</a> (yifan-gu)</li><li>kubelet/rkt: Add RunPod() for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7589 title="kubelet/rkt: Add RunPod() for rkt.">#7589</a> (yifan-gu)</li><li>kubelet/rkt: Add RunInContainer()/ExecInContainer()/PortForward(). <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7553 title="kubelet/rkt: Add RunInContainer()/ExecInContainer()/PortForward().">#7553</a> (yifan-gu)</li><li>kubelet/container: Move ShouldContainerBeRestarted() to runtime. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7613 title="kubelet/container: Move ShouldContainerBeRestarted() to runtime.">#7613</a> (yifan-gu)</li><li>kubelet/rkt: Add SyncPod() to rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7611 title="kubelet/rkt: Add SyncPod() to rkt.">#7611</a> (yifan-gu)</li><li>Kubelet: persist restart count of a container <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6794 title="Kubelet: persist restart count of a container">#6794</a> (yujuhong)</li><li>kubelet/container: Move pty*.go to container runtime package. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7951 title="kubelet/container: Move pty*.go to container runtime package.">#7951</a> (yifan-gu)</li><li>kubelet: Add container runtime option for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7900 title="kubelet: Add container runtime option for rkt.">#7900</a> (yifan-gu)</li><li>kubelet/rkt: Add docker prefix to image string. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7803 title="kubelet/rkt: Add docker prefix to image string.">#7803</a> (yifan-gu)</li><li>kubelet/rkt: Inject dependencies to rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7849 title="kubelet/rkt: Inject dependencies to rkt.">#7849</a> (yifan-gu)</li><li>kubelet/rkt: Remove dependencies on rkt.store <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7859 title="kubelet/rkt: Remove dependencies on rkt.store">#7859</a> (yifan-gu)</li><li>Kubelet talks securely to apiserver <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/2387 title="Kubelet talks securely to apiserver">#2387</a> (erictune)</li><li>Rename EnvVarSource.FieldPath -> FieldRef and add example <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7592 title="Rename EnvVarSource.FieldPath -> FieldRef and add example">#7592</a> (pmorie)</li><li>Add containerized option to kubelet binary <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7741 title="Add containerized option to kubelet binary">#7741</a> (pmorie)</li><li>Ease building kubelet image <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7948 title="Ease building kubelet image">#7948</a> (pmorie)</li><li>Remove unnecessary bind-mount from dockerized kubelet run <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7854 title="Remove unnecessary bind-mount from dockerized kubelet run">#7854</a> (pmorie)</li><li>Add ability to dockerize kubelet in local cluster <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7798 title="Add ability to dockerize kubelet in local cluster">#7798</a> (pmorie)</li><li>Create docker image for kubelet <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7797 title="Create docker image for kubelet">#7797</a> (pmorie)</li><li>Security context - types, kubelet, admission <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7343 title="Security context - types, kubelet, admission">#7343</a> (pweil-)</li><li>Kubelet: Add rkt as a runtime option <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7743 title="Kubelet: Add rkt as a runtime option">#7743</a> (vmarmol)</li><li>Fix kubelet's docker RunInContainer implementation <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7746 title="Fix kubelet's docker RunInContainer implementation ">#7746</a> (vishh)</li></ul></li><li><p>AWS</p><ul><li>AWS: Don't try to copy gce_keys in jenkins e2e job <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8018 title="AWS: Don't try to copy gce_keys in jenkins e2e job">#8018</a> (justinsb)</li><li>AWS: Copy some new properties from config-default => config.test <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7992 title="AWS: Copy some new properties from config-default => config.test">#7992</a> (justinsb)</li><li>AWS: make it possible to disable minion public ip assignment <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7928 title="AWS: make it possible to disable minion public ip assignment">#7928</a> (manolitto)</li><li>update AWS CloudFormation template and cloud-configs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7667 title="update AWS CloudFormation template and cloud-configs">#7667</a> (antoineco)</li><li>AWS: Fix variable naming that meant not all tokens were written <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7736 title="AWS: Fix variable naming that meant not all tokens were written">#7736</a> (justinsb)</li><li>AWS: Change apiserver to listen on 443 directly, not through nginx <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7678 title="AWS: Change apiserver to listen on 443 directly, not through nginx">#7678</a> (justinsb)</li><li>AWS: Improving getting existing VPC and subnet <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6606 title="AWS: Improving getting existing VPC and subnet">#6606</a> (gust1n)</li><li>AWS EBS volume support <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5138 title="AWS EBS volume support">#5138</a> (justinsb)</li></ul></li><li><p>Introduce an 'svc' segment for DNS search <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8089 title="Introduce an 'svc' segment for DNS search">#8089</a> (thockin)</p></li><li><p>Adds ability to define a prefix for etcd paths <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5707 title="Adds ability to define a prefix for etcd paths">#5707</a> (kbeecher)</p></li><li><p>Add kubectl log --previous support to view last terminated container log <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7973 title="Add kubectl log --previous support to view last terminated container log">#7973</a> (dchen1107)</p></li><li><p>Add a flag to disable legacy APIs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8083 title="Add a flag to disable legacy APIs">#8083</a> (brendandburns)</p></li><li><p>make the dockerkeyring handle mutiple matching credentials <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7971 title="make the dockerkeyring handle mutiple matching credentials">#7971</a> (deads2k)</p></li><li><p>Convert Fluentd to Cloud Logging pod specs to YAML <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8078 title="Convert Fluentd to Cloud Logging pod specs to YAML">#8078</a> (satnam6502)</p></li><li><p>Use etcd to allocate PortalIPs instead of in-mem <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7704 title="Use etcd to allocate PortalIPs instead of in-mem">#7704</a> (smarterclayton)</p></li><li><p>eliminate auth-path <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8064 title="eliminate auth-path">#8064</a> (deads2k)</p></li><li><p>Record failure reasons for image pulling <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7981 title="Record failure reasons for image pulling">#7981</a> (yujuhong)</p></li><li><p>Rate limit replica creation <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7869 title="Rate limit replica creation">#7869</a> (bprashanth)</p></li><li><p>Upgrade to Kibana 4 for cluster logging <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7995 title="Upgrade to Kibana 4 for cluster logging">#7995</a> (satnam6502)</p></li><li><p>Added name to kube-dns service <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8049 title="Added name to kube-dns service">#8049</a> (piosz)</p></li><li><p>Fix validation by moving it into the resource builder. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7919 title="Fix validation by moving it into the resource builder.">#7919</a> (brendandburns)</p></li><li><p>Add cache with multiple shards to decrease lock contention <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8050 title="Add cache with multiple shards to decrease lock contention">#8050</a> (fgrzadkowski)</p></li><li><p>Delete status from displayable resources <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8039 title="Delete status from displayable resources">#8039</a> (nak3)</p></li><li><p>Refactor volume interfaces to receive pod instead of ObjectReference <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8044 title="Refactor volume interfaces to receive pod instead of ObjectReference">#8044</a> (pmorie)</p></li><li><p>fix kube-down for provider gke <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7565 title="fix kube-down for provider gke">#7565</a> (jlowdermilk)</p></li><li><p>Service port names are required for multi-port <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7786 title="Service port names are required for multi-port">#7786</a> (thockin)</p></li><li><p>Increase disk size for kubernetes master. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8051 title="Increase disk size for kubernetes master.">#8051</a> (fgrzadkowski)</p></li><li><p>expose: Load input object for increased safety <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7774 title="expose: Load input object for increased safety">#7774</a> (kargakis)</p></li><li><p>Improments to conversion methods generator <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7896 title="Improments to conversion methods generator">#7896</a> (wojtek-t)</p></li><li><p>Added displaying external IPs to kubectl cluster-info <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7557 title="Added displaying external IPs to kubectl cluster-info">#7557</a> (piosz)</p></li><li><p>Add missing Errorf formatting directives <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8037 title="Add missing Errorf formatting directives">#8037</a> (shawnps)</p></li><li><p>Add startup code to apiserver to migrate etcd keys <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7567 title="WIP: Add startup code to apiserver to migrate etcd keys">#7567</a> (kbeecher)</p></li><li><p>Use error type from docker go-client instead of string <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8021 title="Use error type from docker go-client instead of string">#8021</a> (ddysher)</p></li><li><p>Accurately get hardware cpu count in Vagrantfile. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/8024 title="Accurately get hardware cpu count in Vagrantfile.">#8024</a> (BenTheElder)</p></li><li><p>Stop setting a GKE specific version of the kubeconfig file <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7921 title="Stop setting a GKE specific version of the kubeconfig file">#7921</a> (roberthbailey)</p></li><li><p>Make the API server deal with HEAD requests via the service proxy <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7950 title="Make the API server deal with HEAD requests via the service proxy">#7950</a> (satnam6502)</p></li><li><p>GlusterFS Critical Bug Resolved - Removing warning in README <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7983 title="GlusterFS Critical Bug Resolved - Removing warning in README">#7983</a> (wattsteve)</p></li><li><p>Don't use the first token <code>uname -n</code> as the hostname <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7967 title="Don't use the first token `uname -n` as the hostname">#7967</a> (yujuhong)</p></li><li><p>Call kube-down in test-teardown for vagrant. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7982 title="Call kube-down in test-teardown for vagrant.">#7982</a> (BenTheElder)</p></li><li><p>defaults_tests: verify defaults when converting to an API object <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6235 title="defaults_tests: verify defaults when converting to an API object">#6235</a> (yujuhong)</p></li><li><p>Use the full hostname for mirror pod name. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7910 title="Use the full hostname for mirror pod name.">#7910</a> (yujuhong)</p></li><li><p>Removes RunPod in the Runtime interface <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7657 title="Removes RunPod in the Runtime interface">#7657</a> (yujuhong)</p></li><li><p>Clean up dockertools/manager.go and add more unit tests <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7533 title="Clean up dockertools/manager.go and add more unit tests">#7533</a> (yujuhong)</p></li><li><p>Adapt pod killing and cleanup for generic container runtime <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7525 title="Adapt pod killing and cleanup for generic container runtime">#7525</a> (yujuhong)</p></li><li><p>Fix pod filtering in replication controller <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7198 title="Fix pod filtering in replication controller">#7198</a> (yujuhong)</p></li><li><p>Print container statuses in <code>kubectl get pods</code> <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7116 title="Print container statuses in `kubectl get pods`">#7116</a> (yujuhong)</p></li><li><p>Prioritize deleting the non-running pods when reducing replicas <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6992 title="Prioritize deleting the non-running pods when reducing replicas">#6992</a> (yujuhong)</p></li><li><p>Fix locking issue in pod manager <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6872 title="Fix locking issue in pod manager">#6872</a> (yujuhong)</p></li><li><p>Limit the number of concurrent tests in integration.go <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6655 title="Limit the number of concurrent tests in integration.go">#6655</a> (yujuhong)</p></li><li><p>Fix typos in different config comments <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7931 title="Fix typos in different config comments">#7931</a> (pmorie)</p></li><li><p>Update cAdvisor dependency. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7929 title="Update cAdvisor dependency.">#7929</a> (vmarmol)</p></li><li><p>Ubuntu-distro: deprecate & merge ubuntu single node work to ubuntu cluster node stuff<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5498 title="Ubuntu-distro: deprecate & merge ubuntu single node work to ubuntu cluster node stuff">#5498</a> (resouer)</p></li><li><p>Add control variables to Jenkins E2E script <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7935 title="Add control variables to Jenkins E2E script">#7935</a> (saad-ali)</p></li><li><p>Check node status as part of validate-cluster.sh. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7932 title="Check node status as part of validate-cluster.sh.">#7932</a> (fabioy)</p></li><li><p>Add old endpoint cleanup function <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7821 title="Add old endpoint cleanup function">#7821</a> (lavalamp)</p></li><li><p>Support recovery from in the middle of a rename. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7620 title="Support recovery from in the middle of a rename.">#7620</a> (brendandburns)</p></li><li><p>Update Exec and Portforward client to use pod subresource <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7715 title="Update Exec and Portforward client to use pod subresource">#7715</a> (csrwng)</p></li><li><p>Added NFS to PV structs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7564 title="Added NFS to PV structs">#7564</a> (markturansky)</p></li><li><p>Fix environment variable error in Vagrant docs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7904 title="Fix environment variable error in Vagrant docs">#7904</a> (posita)</p></li><li><p>Adds a simple release-note builder that scrapes the GitHub API for recent PRs <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7616 title="Adds a simple release-note builder that scrapes the GitHub API for recent PRs">#7616</a>(brendandburns)</p></li><li><p>Scheduler ignores nodes that are in a bad state <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7668 title="Scheduler ignores nodes that are in a bad state">#7668</a> (bprashanth)</p></li><li><p>Set GOMAXPROCS for etcd <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7863 title="Set GOMAXPROCS for etcd">#7863</a> (fgrzadkowski)</p></li><li><p>Auto-generated conversion methods calling one another <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7556 title="Auto-generated conversion methods calling one another">#7556</a> (wojtek-t)</p></li><li><p>Bring up a kuberenetes cluster using coreos image as worker nodes <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7445 title="Bring up a kuberenetes cluster using coreos image as worker nodes">#7445</a> (dchen1107)</p></li><li><p>Godep: Add godep for rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7410 title="Godep: Add godep for rkt.">#7410</a> (yifan-gu)</p></li><li><p>Add volumeGetter to rkt. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7870 title="Add volumeGetter to rkt.">#7870</a> (yifan-gu)</p></li><li><p>Update cAdvisor dependency. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7897 title="Update cAdvisor dependency.">#7897</a> (vmarmol)</p></li><li><p>DNS: expose 53/TCP <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7822 title="DNS: expose 53/TCP">#7822</a> (thockin)</p></li><li><p>Set NodeReady=False when docker is dead <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7763 title="Set NodeReady=False when docker is dead">#7763</a> (wojtek-t)</p></li><li><p>Ignore latency metrics for events <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7857 title="Ignore latency metrics for events">#7857</a> (fgrzadkowski)</p></li><li><p>SecurityContext admission clean up <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7792 title="SecurityContext admission clean up">#7792</a> (pweil-)</p></li><li><p>Support manually-created and generated conversion functions <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7832 title="Support manually-created and generated conversion functions">#7832</a> (wojtek-t)</p></li><li><p>Add latency metrics for etcd operations <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7833 title="Add latency metrics for etcd operations">#7833</a> (fgrzadkowski)</p></li><li><p>Update errors_test.go <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7885 title="Update errors_test.go">#7885</a> (hurf)</p></li><li><p>Change signature of container runtime PullImage to allow pull w/ secret <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7861 title="Change signature of container runtime PullImage to allow pull w/ secret">#7861</a> (pmorie)</p></li><li><p>Fix bug in Service documentation: incorrect location of "selector" in JSON [#7873]<a href=bkeroackdsc>146</a></p></li><li><p>Fix controller-manager manifest for providers that don't specify CLUSTER_IP_RANGE[#7876][147] (cjcullen)</p></li><li><p>Fix controller unittests [#7867][148] (bprashanth)</p></li><li><p>Enable GCM and GCL instead of InfluxDB on GCE [#7751][149] (saad-ali)</p></li><li><p>Remove restriction that cluster-cidr be a class-b [#7862][150] (cjcullen)</p></li><li><p>Fix OpenShift example [#7591][151] (derekwaynecarr)</p></li><li><p>API Server - pass path name in context of create request for subresource [#7718][152] (csrwng)</p></li><li><p>Rolling Updates: Add support for --rollback. [#7575][153] (brendandburns)</p></li><li><p>Update to container-vm-v20150505 (Also updates GCE to Docker 1.6) [#7820][154] (zmerlynn)</p></li><li><p>Fix metric label [#7830][155] (rhcarvalho)</p></li><li><p>Fix v1beta1 typos in v1beta2 conversions [#7838][156] (pmorie)</p></li><li><p>skydns: use the etcd-2.x native syntax, enable IANA attributed ports. [#7764]<a href=AntonioMeireles>157</a></p></li><li><p>Added port 6443 to kube-proxy default IP address for api-server [#7794][158] (markllama)</p></li><li><p>Added client header info for authentication doc. [#7834][159] (ashcrow)</p></li><li><p>Clean up safe_format_and_mount spam in the startup logs [#7827][160] (zmerlynn)</p></li><li><p>Set allocate_node_cidrs to be blank by default. [#7829][161] (roberthbailey)</p></li><li><p>Fix sync problems in [#5246][162] [#7799][163] (cjcullen)</p></li><li><p>Fix event doc link [#7823][164] (saad-ali)</p></li><li><p>Cobra update and bash completions fix [#7776][165] (eparis)</p></li><li><p>Fix kube2sky flakes. Fix tools.GetEtcdVersion to work with etcd > 2.0.7 [#7675][166] (cjcullen)</p></li><li><p>Change kube2sky to use token-system-dns secret, point at https endpoint ... [#7154]<a href=cjcullen>167</a></p></li><li><p>replica: serialize created-by reference [#7468][168] (simon3z)</p></li><li><p>Inject mounter into volume plugins [#7702][169] (pmorie)</p></li><li><p>bringing CoreOS cloud-configs up-to-date (against 0.15.x and latest OS' alpha) [#6973]<a href=AntonioMeireles>170</a></p></li><li><p>Update kubeconfig-file doc. [#7787][171] (jlowdermilk)</p></li><li><p>Throw an API error when deleting namespace in termination [#7780][172] (derekwaynecarr)</p></li><li><p>Fix command field PodExecOptions [#7773][173] (csrwng)</p></li><li><p>Start ImageManager housekeeping in Run(). [#7785][174] (vmarmol)</p></li><li><p>fix DeepCopy to properly support runtime.EmbeddedObject [#7769][175] (deads2k)</p></li><li><p>fix master service endpoint system for multiple masters [#7273][176] (lavalamp)</p></li><li><p>Add genbashcomp to KUBE_TEST_TARGETS [#7757][177] (nak3)</p></li><li><p>Change the cloud provider TCPLoadBalancerExists function to GetTCPLoadBalancer...[#7669][178] (a-robinson)</p></li><li><p>Add containerized option to kubelet binary [#7772][179] (pmorie)</p></li><li><p>Fix swagger spec [#7779][180] (pmorie)</p></li><li><p>FIX: Issue [#7750][181] - Hyperkube docker image needs certificates to connect to cloud-providers[#7755][182] (viklas)</p></li><li><p>Add build labels to rkt [#7752][183] (vmarmol)</p></li><li><p>Check license boilerplate for python files [#7672][184] (eparis)</p></li><li><p>Reliable updates in rollingupdate [#7705][185] (bprashanth)</p></li><li><p>Don't exit abruptly if there aren't yet any minions right after the cluster is created. [#7650]<a href=roberthbailey>186</a></p></li><li><p>Make changes suggested in [#7675][166] [#7742][187] (cjcullen)</p></li><li><p>A guide to set up kubernetes multiple nodes cluster with flannel on fedora [#7357]<a href=aveshagarwal>188</a></p></li><li><p>Setup generators in factory [#7760][189] (kargakis)</p></li><li><p>Reduce usage of time.After [#7737][190] (lavalamp)</p></li><li><p>Remove node status from "componentstatuses" call. [#7735][191] (fabioy)</p></li><li><p>React to failure by growing the remaining clusters [#7614][192] (tamsky)</p></li><li><p>Fix typo in runtime_cache.go [#7725][193] (pmorie)</p></li><li><p>Update non-GCE Salt distros to 1.6.0, fallback to ContainerVM Docker version on GCE[#7740][194] (zmerlynn)</p></li><li><p>Skip SaltStack install if it's already installed [#7744][195] (zmerlynn)</p></li><li><p>Expose pod name as a label on containers. [#7712][196] (rjnagal)</p></li><li><p>Log which SSH key is used in e2e SSH test [#7732][197] (mbforbes)</p></li><li><p>Add a central simple getting started guide with kubernetes guide. [#7649][198] (brendandburns)</p></li><li><p>Explicitly state the lack of support for 'Requests' for the purposes of scheduling [#7443]<a href=vishh>199</a></p></li><li><p>Select IPv4-only from host interfaces [#7721][200] (smarterclayton)</p></li><li><p>Metrics tests can't run on Mac [#7723][201] (smarterclayton)</p></li><li><p>Add step to API changes doc for swagger regen [#7727][202] (pmorie)</p></li><li><p>Add NsenterMounter mount implementation [#7703][203] (pmorie)</p></li><li><p>add StringSet.HasAny [#7509][204] (deads2k)</p></li><li><p>Add an integration test that checks for the metrics we expect to be exported from the master [#6941][205] (a-robinson)</p></li><li><p>Minor bash update found by shellcheck.net [#7722][206] (eparis)</p></li><li><p>Add --hostport to run-container. [#7536][207] (rjnagal)</p></li><li><p>Have rkt implement the container Runtime interface [#7659][208] (vmarmol)</p></li><li><p>Change the order the different versions of API are registered [#7629][209] (caesarxuchao)</p></li><li><p>expose: Create objects in a generic way [#7699][210] (kargakis)</p></li><li><p>Requeue rc if a single get/put retry on status.Replicas fails [#7643][211] (bprashanth)</p></li><li><p>logs for master components [#7316][212] (ArtfulCoder)</p></li><li><p>cloudproviders: add ovirt getting started guide [#7522][213] (simon3z)</p></li><li><p>Make rkt-install a oneshot. [#7671][214] (vmarmol)</p></li><li><p>Provide container_runtime flag to Kubelet in CoreOS. [#7665][215] (vmarmol)</p></li><li><p>Boilerplate speedup [#7654][216] (eparis)</p></li><li><p>Log host for failed pod in Density test [#7700][217] (wojtek-t)</p></li><li><p>Removes spurious quotation mark [#7655][218] (alindeman)</p></li><li><p>Add kubectl_label to custom functions in bash completion [#7694][219] (nak3)</p></li><li><p>Enable profiling in kube-controller [#7696][220] (wojtek-t)</p></li><li><p>Set vagrant test cluster default NUM_MINIONS=2 [#7690][221] (BenTheElder)</p></li><li><p>Add metrics to measure cache hit ratio [#7695][222] (fgrzadkowski)</p></li><li><p>Change IP to IP(S) in service columns for kubectl get [#7662][223] (jlowdermilk)</p></li><li><p>annotate required flags for bash_completions [#7076][224] (eparis)</p></li><li><p>(minor) Add pgrep debugging to etcd error [#7685][225] (jayunit100)</p></li><li><p>Fixed nil pointer issue in describe when volume is unbound [#7676][226] (markturansky)</p></li><li><p>Removed unnecessary closing bracket [#7691][227] (piosz)</p></li><li><p>Added TerminationGracePeriod field to PodSpec and grace-period flag to kubectl stop[#7432][228] (piosz)</p></li><li><p>Fix boilerplate in test/e2e/scale.go [#7689][229] (wojtek-t)</p></li><li><p>Update expiration timeout based on observed latencies [#7628][230] (bprashanth)</p></li><li><p>Output generated conversion functions/names [#7644][231] (liggitt)</p></li><li><p>Moved the Scale tests into a scale file. [#7645][232] [#7646][233] (rrati)</p></li><li><p>Truncate GCE load balancer names to 63 chars [#7609][234] (brendandburns)</p></li><li><p>Add SyncPod() and remove Kill/Run InContainer(). [#7603][235] (vmarmol)</p></li><li><p>Merge release 0.16 to master [#7663][236] (brendandburns)</p></li><li><p>Update license boilerplate for examples/rethinkdb [#7637][237] (eparis)</p></li><li><p>First part of improved rolling update, allow dynamic next replication controller generation.[#7268][238] (brendandburns)</p></li><li><p>Add license boilerplate to examples/phabricator [#7638][239] (eparis)</p></li><li><p>Use generic copyright holder name in license boilerplate [#7597][240] (eparis)</p></li><li><p>Retry incrementing quota if there is a conflict [#7633][241] (derekwaynecarr)</p></li><li><p>Remove GetContainers from Runtime interface [#7568][242] (yujuhong)</p></li><li><p>Add image-related methods to DockerManager [#7578][243] (yujuhong)</p></li><li><p>Remove more docker references in kubelet [#7586][244] (yujuhong)</p></li><li><p>Add KillContainerInPod in DockerManager [#7601][245] (yujuhong)</p></li><li><p>Kubelet: Add container runtime option. [#7652][246] (vmarmol)</p></li><li><p>bump heapster to v0.11.0 and grafana to v0.7.0 [#7626][247] (idosh)</p></li><li><p>Build github.com/onsi/ginkgo/ginkgo as a part of the release [#7593][248] (ixdy)</p></li><li><p>Do not automatically decode runtime.RawExtension [#7490][249] (smarterclayton)</p></li><li><p>Update changelog. [#7500][250] (brendandburns)</p></li><li><p>Add SyncPod() to DockerManager and use it in Kubelet [#7610][251] (vmarmol)</p></li><li><p>Build: Push .md5 and .sha1 files for every file we push to GCS [#7602][252] (zmerlynn)</p></li><li><p>Fix rolling update --image [#7540][253] (bprashanth)</p></li><li><p>Update license boilerplate for docs/man/md2man-all.sh [#7636][254] (eparis)</p></li><li><p>Include shell license boilerplate in examples/k8petstore [#7632][255] (eparis)</p></li><li><p>Add --cgroup_parent flag to Kubelet to set the parent cgroup for pods [#7277][256] (guenter)</p></li><li><p>change the current dir to the config dir [#7209][257] (you-n-g)</p></li><li><p>Set Weave To 0.9.0 And Update Etcd Configuration For Azure [#7158][258] (idosh)</p></li><li><p>Augment describe to search for matching things if it doesn't match the original resource.[#7467][259] (brendandburns)</p></li><li><p>Add a simple cache for objects stored in etcd. [#7559][260] (fgrzadkowski)</p></li><li><p>Rkt gc [#7549][261] (yifan-gu)</p></li><li><p>Rkt pull [#7550][262] (yifan-gu)</p></li><li><p>Implement Mount interface using mount(8) and umount(8) [#6400][263] (ddysher)</p></li><li><p>Trim Fleuntd tag for Cloud Logging [#7588][264] (satnam6502)</p></li><li><p>GCE CoreOS cluster - set master name based on variable [#7569][265] (bakins)</p></li><li><p>Capitalization of KubeProxyVersion wrong in JSON [#7535][266] (smarterclayton)</p></li><li><p>Make nodes report their external IP rather than the master's. [#7530][267] (mbforbes)</p></li><li><p>Trim cluster log tags to pod name and container name [#7539][268] (satnam6502)</p></li><li><p>Handle conversion of boolean query parameters with a value of "false" [#7541][269] (csrwng)</p></li><li><p>Add image-related methods to Runtime interface. [#7532][270] (vmarmol)</p></li><li><p>Test whether auto-generated conversions weren't manually edited [#7560][271] (wojtek-t)</p></li><li><p>Mention :latest behavior for image version tag [#7484][272] (colemickens)</p></li><li><p>readinessProbe calls livenessProbe.Exec.Command which cause "invalid memory address or nil pointer dereference". [#7487][273] (njuicsgz)</p></li><li><p>Add RuntimeHooks to abstract Kubelet logic [#7520][274] (vmarmol)</p></li><li><p>Expose URL() on Request to allow building URLs [#7546][275] (smarterclayton)</p></li><li><p>Add a simple cache for objects stored in etcd [#7288][276] (fgrzadkowski)</p></li><li><p>Prepare for chaining autogenerated conversion methods [#7431][277] (wojtek-t)</p></li><li><p>Increase maxIdleConnection limit when creating etcd client in apiserver. [#7353][278] (wojtek-t)</p></li><li><p>Improvements to generator of conversion methods. [#7354][279] (wojtek-t)</p></li><li><p>Code to automatically generate conversion methods [#7107][280] (wojtek-t)</p></li><li><p>Support recovery for anonymous roll outs [#7407][281] (brendandburns)</p></li><li><p>Bump kube2sky to 1.2. Point it at https endpoint (3rd try). [#7527][282] (cjcullen)</p></li><li><p>cluster/gce/coreos: Add metadata-service in node.yaml [#7526][283] (yifan-gu)</p></li><li><p>Move ComputePodChanges to the Docker runtime [#7480][284] (vmarmol)</p></li><li><p>Cobra rebase [#7510][285] (eparis)</p></li><li><p>Adding system oom events from kubelet [#6718][286] (vishh)</p></li><li><p>Move Prober to its own subpackage [#7479][287] (vmarmol)</p></li><li><p>Fix parallel-e2e.sh to work on my macbook (bash v3.2) [#7513][288] (cjcullen)</p></li><li><p>Move network plugin TearDown to DockerManager [#7449][289] (vmarmol)</p></li><li><p>Fixes [#7498][290] - CoreOS Getting Started Guide had invalid cloud config [#7499][291] (elsonrodriguez)</p></li><li><p>Fix invalid character '"' after object key:value pair [#7504][292] (resouer)</p></li><li><p>Fixed kubelet deleting data from volumes on stop ([#7317][293]). [#7503][294] (jsafrane)</p></li><li><p>Fixing hooks/description to catch API fields without description tags [#7482][295] (nikhiljindal)</p></li><li><p>cadvisor is obsoleted so kubelet service does not require it. [#7457][296] (aveshagarwal)</p></li><li><p>Set the default namespace for events to be "default" [#7408][297] (vishh)</p></li><li><p>Fix typo in namespace conversion [#7446][298] (liggitt)</p></li><li><p>Convert Secret registry to use update/create strategy, allow filtering by Type [#7419][299] (liggitt)</p></li><li><p>Use pod namespace when looking for its GlusterFS endpoints. [#7102][300] (jsafrane)</p></li><li><p>Fixed name of kube-proxy path in deployment scripts. [#7427][301] (jsafrane)</p></li></ul><p>To download, please visit <a href=https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.17.0>https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.17.0</a></p><p>Simple theme. Powered by [Blogger][385].</p><p>[ ![][327] ][386]</p><p>[146]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7873>https://github.com/GoogleCloudPlatform/kubernetes/pull/7873</a> "Fix bug in Service documentation: incorrect location of "selector" in JSON"
[147]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7876>https://github.com/GoogleCloudPlatform/kubernetes/pull/7876</a> "Fix controller-manager manifest for providers that don't specify CLUSTER_IP_RANGE"
[148]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7867>https://github.com/GoogleCloudPlatform/kubernetes/pull/7867</a> "Fix controller unittests"
[149]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7751>https://github.com/GoogleCloudPlatform/kubernetes/pull/7751</a> "Enable GCM and GCL instead of InfluxDB on GCE"
[150]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7862>https://github.com/GoogleCloudPlatform/kubernetes/pull/7862</a> "Remove restriction that cluster-cidr be a class-b"
[151]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7591>https://github.com/GoogleCloudPlatform/kubernetes/pull/7591</a> "Fix OpenShift example"
[152]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7718>https://github.com/GoogleCloudPlatform/kubernetes/pull/7718</a> "API Server - pass path name in context of create request for subresource"
[153]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7575>https://github.com/GoogleCloudPlatform/kubernetes/pull/7575</a> "Rolling Updates: Add support for --rollback."
[154]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7820>https://github.com/GoogleCloudPlatform/kubernetes/pull/7820</a> "Update to container-vm-v20150505 (Also updates GCE to Docker 1.6)"
[155]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7830>https://github.com/GoogleCloudPlatform/kubernetes/pull/7830</a> "Fix metric label"
[156]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7838>https://github.com/GoogleCloudPlatform/kubernetes/pull/7838</a> "Fix v1beta1 typos in v1beta2 conversions"
[157]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7764>https://github.com/GoogleCloudPlatform/kubernetes/pull/7764</a> "skydns: use the etcd-2.x native syntax, enable IANA attributed ports."
[158]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7794>https://github.com/GoogleCloudPlatform/kubernetes/pull/7794</a> "Added port 6443 to kube-proxy default IP address for api-server"
[159]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7834>https://github.com/GoogleCloudPlatform/kubernetes/pull/7834</a> "Added client header info for authentication doc."
[160]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7827>https://github.com/GoogleCloudPlatform/kubernetes/pull/7827</a> "Clean up safe_format_and_mount spam in the startup logs"
[161]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7829>https://github.com/GoogleCloudPlatform/kubernetes/pull/7829</a> "Set allocate_node_cidrs to be blank by default."
[162]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5246>https://github.com/GoogleCloudPlatform/kubernetes/pull/5246</a> "Make nodecontroller configure nodes' pod IP ranges"
[163]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7799>https://github.com/GoogleCloudPlatform/kubernetes/pull/7799</a> "Fix sync problems in #5246"
[164]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7823>https://github.com/GoogleCloudPlatform/kubernetes/pull/7823</a> "Fix event doc link"
[165]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7776>https://github.com/GoogleCloudPlatform/kubernetes/pull/7776</a> "Cobra update and bash completions fix"
[166]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7675>https://github.com/GoogleCloudPlatform/kubernetes/pull/7675</a> "Fix kube2sky flakes. Fix tools.GetEtcdVersion to work with etcd > 2.0.7"
[167]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7154>https://github.com/GoogleCloudPlatform/kubernetes/pull/7154</a> "Change kube2sky to use token-system-dns secret, point at https endpoint ..."
[168]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7468>https://github.com/GoogleCloudPlatform/kubernetes/pull/7468</a> "replica: serialize created-by reference"
[169]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7702>https://github.com/GoogleCloudPlatform/kubernetes/pull/7702</a> "Inject mounter into volume plugins"
[170]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6973>https://github.com/GoogleCloudPlatform/kubernetes/pull/6973</a> "bringing CoreOS cloud-configs up-to-date (against 0.15.x and latest OS' alpha) "
[171]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7787>https://github.com/GoogleCloudPlatform/kubernetes/pull/7787</a> "Update kubeconfig-file doc."
[172]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7780>https://github.com/GoogleCloudPlatform/kubernetes/pull/7780</a> "Throw an API error when deleting namespace in termination"
[173]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7773>https://github.com/GoogleCloudPlatform/kubernetes/pull/7773</a> "Fix command field PodExecOptions"
[174]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7785>https://github.com/GoogleCloudPlatform/kubernetes/pull/7785</a> "Start ImageManager housekeeping in Run()."
[175]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7769>https://github.com/GoogleCloudPlatform/kubernetes/pull/7769</a> "fix DeepCopy to properly support runtime.EmbeddedObject"
[176]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7273>https://github.com/GoogleCloudPlatform/kubernetes/pull/7273</a> "fix master service endpoint system for multiple masters"
[177]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7757>https://github.com/GoogleCloudPlatform/kubernetes/pull/7757</a> "Add genbashcomp to KUBE_TEST_TARGETS"
[178]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7669>https://github.com/GoogleCloudPlatform/kubernetes/pull/7669</a> "Change the cloud provider TCPLoadBalancerExists function to GetTCPLoadBalancer..."
[179]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7772>https://github.com/GoogleCloudPlatform/kubernetes/pull/7772</a> "Add containerized option to kubelet binary"
[180]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7779>https://github.com/GoogleCloudPlatform/kubernetes/pull/7779</a> "Fix swagger spec"
[181]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/7750>https://github.com/GoogleCloudPlatform/kubernetes/issues/7750</a> "Hyperkube image requires root certificates to work with cloud-providers (at least AWS)"
[182]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7755>https://github.com/GoogleCloudPlatform/kubernetes/pull/7755</a> "FIX: Issue #7750 - Hyperkube docker image needs certificates to connect to cloud-providers"
[183]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7752>https://github.com/GoogleCloudPlatform/kubernetes/pull/7752</a> "Add build labels to rkt"
[184]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7672>https://github.com/GoogleCloudPlatform/kubernetes/pull/7672</a> "Check license boilerplate for python files"
[185]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7705>https://github.com/GoogleCloudPlatform/kubernetes/pull/7705</a> "Reliable updates in rollingupdate"
[186]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7650>https://github.com/GoogleCloudPlatform/kubernetes/pull/7650</a> "Don't exit abruptly if there aren't yet any minions right after the cluster is created."
[187]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7742>https://github.com/GoogleCloudPlatform/kubernetes/pull/7742</a> "Make changes suggested in #7675"
[188]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7357>https://github.com/GoogleCloudPlatform/kubernetes/pull/7357</a> "A guide to set up kubernetes multiple nodes cluster with flannel on fedora"
[189]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7760>https://github.com/GoogleCloudPlatform/kubernetes/pull/7760</a> "Setup generators in factory"
[190]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7737>https://github.com/GoogleCloudPlatform/kubernetes/pull/7737</a> "Reduce usage of time.After"
[191]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7735>https://github.com/GoogleCloudPlatform/kubernetes/pull/7735</a> "Remove node status from "componentstatuses" call."
[192]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7614>https://github.com/GoogleCloudPlatform/kubernetes/pull/7614</a> "React to failure by growing the remaining clusters"
[193]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7725>https://github.com/GoogleCloudPlatform/kubernetes/pull/7725</a> "Fix typo in runtime_cache.go"
[194]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7740>https://github.com/GoogleCloudPlatform/kubernetes/pull/7740</a> "Update non-GCE Salt distros to 1.6.0, fallback to ContainerVM Docker version on GCE"
[195]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7744>https://github.com/GoogleCloudPlatform/kubernetes/pull/7744</a> "Skip SaltStack install if it's already installed"
[196]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7712>https://github.com/GoogleCloudPlatform/kubernetes/pull/7712</a> "Expose pod name as a label on containers."
[197]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7732>https://github.com/GoogleCloudPlatform/kubernetes/pull/7732</a> "Log which SSH key is used in e2e SSH test"
[198]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7649>https://github.com/GoogleCloudPlatform/kubernetes/pull/7649</a> "Add a central simple getting started guide with kubernetes guide."
[199]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7443>https://github.com/GoogleCloudPlatform/kubernetes/pull/7443</a> "Explicitly state the lack of support for 'Requests' for the purposes of scheduling"
[200]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7721>https://github.com/GoogleCloudPlatform/kubernetes/pull/7721</a> "Select IPv4-only from host interfaces"
[201]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7723>https://github.com/GoogleCloudPlatform/kubernetes/pull/7723</a> "Metrics tests can't run on Mac"
[202]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7727>https://github.com/GoogleCloudPlatform/kubernetes/pull/7727</a> "Add step to API changes doc for swagger regen"
[203]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7703>https://github.com/GoogleCloudPlatform/kubernetes/pull/7703</a> "Add NsenterMounter mount implementation"
[204]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7509>https://github.com/GoogleCloudPlatform/kubernetes/pull/7509</a> "add StringSet.HasAny"
[205]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6941>https://github.com/GoogleCloudPlatform/kubernetes/pull/6941</a> "Add an integration test that checks for the metrics we expect to be exported from the master"
[206]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7722>https://github.com/GoogleCloudPlatform/kubernetes/pull/7722</a> "Minor bash update found by shellcheck.net"
[207]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7536>https://github.com/GoogleCloudPlatform/kubernetes/pull/7536</a> "Add --hostport to run-container."
[208]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7659>https://github.com/GoogleCloudPlatform/kubernetes/pull/7659</a> "Have rkt implement the container Runtime interface"
[209]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7629>https://github.com/GoogleCloudPlatform/kubernetes/pull/7629</a> "Change the order the different versions of API are registered "
[210]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7699>https://github.com/GoogleCloudPlatform/kubernetes/pull/7699</a> "expose: Create objects in a generic way"
[211]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7643>https://github.com/GoogleCloudPlatform/kubernetes/pull/7643</a> "Requeue rc if a single get/put retry on status.Replicas fails"
[212]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7316>https://github.com/GoogleCloudPlatform/kubernetes/pull/7316</a> "logs for master components"
[213]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7522>https://github.com/GoogleCloudPlatform/kubernetes/pull/7522</a> "cloudproviders: add ovirt getting started guide"
[214]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7671>https://github.com/GoogleCloudPlatform/kubernetes/pull/7671</a> "Make rkt-install a oneshot."
[215]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7665>https://github.com/GoogleCloudPlatform/kubernetes/pull/7665</a> "Provide container_runtime flag to Kubelet in CoreOS."
[216]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7654>https://github.com/GoogleCloudPlatform/kubernetes/pull/7654</a> "Boilerplate speedup"
[217]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7700>https://github.com/GoogleCloudPlatform/kubernetes/pull/7700</a> "Log host for failed pod in Density test"
[218]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7655>https://github.com/GoogleCloudPlatform/kubernetes/pull/7655</a> "Removes spurious quotation mark"
[219]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7694>https://github.com/GoogleCloudPlatform/kubernetes/pull/7694</a> "Add kubectl_label to custom functions in bash completion"
[220]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7696>https://github.com/GoogleCloudPlatform/kubernetes/pull/7696</a> "Enable profiling in kube-controller"
[221]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7690>https://github.com/GoogleCloudPlatform/kubernetes/pull/7690</a> "Set vagrant test cluster default NUM_MINIONS=2"
[222]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7695>https://github.com/GoogleCloudPlatform/kubernetes/pull/7695</a> "Add metrics to measure cache hit ratio"
[223]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7662>https://github.com/GoogleCloudPlatform/kubernetes/pull/7662</a> "Change IP to IP(S) in service columns for kubectl get"
[224]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7076>https://github.com/GoogleCloudPlatform/kubernetes/pull/7076</a> "annotate required flags for bash_completions"
[225]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7685>https://github.com/GoogleCloudPlatform/kubernetes/pull/7685</a> "(minor) Add pgrep debugging to etcd error"
[226]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7676>https://github.com/GoogleCloudPlatform/kubernetes/pull/7676</a> "Fixed nil pointer issue in describe when volume is unbound"
[227]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7691>https://github.com/GoogleCloudPlatform/kubernetes/pull/7691</a> "Removed unnecessary closing bracket"
[228]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7432>https://github.com/GoogleCloudPlatform/kubernetes/pull/7432</a> "Added TerminationGracePeriod field to PodSpec and grace-period flag to kubectl stop"
[229]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7689>https://github.com/GoogleCloudPlatform/kubernetes/pull/7689</a> "Fix boilerplate in test/e2e/scale.go"
[230]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7628>https://github.com/GoogleCloudPlatform/kubernetes/pull/7628</a> "Update expiration timeout based on observed latencies"
[231]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7644>https://github.com/GoogleCloudPlatform/kubernetes/pull/7644</a> "Output generated conversion functions/names"
[232]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/7645>https://github.com/GoogleCloudPlatform/kubernetes/issues/7645</a> "Move the scale tests into a separate file"
[233]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7646>https://github.com/GoogleCloudPlatform/kubernetes/pull/7646</a> "Moved the Scale tests into a scale file. #7645"
[234]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7609>https://github.com/GoogleCloudPlatform/kubernetes/pull/7609</a> "Truncate GCE load balancer names to 63 chars"
[235]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7603>https://github.com/GoogleCloudPlatform/kubernetes/pull/7603</a> "Add SyncPod() and remove Kill/Run InContainer()."
[236]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7663>https://github.com/GoogleCloudPlatform/kubernetes/pull/7663</a> "Merge release 0.16 to master"
[237]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7637>https://github.com/GoogleCloudPlatform/kubernetes/pull/7637</a> "Update license boilerplate for examples/rethinkdb"
[238]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7268>https://github.com/GoogleCloudPlatform/kubernetes/pull/7268</a> "First part of improved rolling update, allow dynamic next replication controller generation."
[239]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7638>https://github.com/GoogleCloudPlatform/kubernetes/pull/7638</a> "Add license boilerplate to examples/phabricator"
[240]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7597>https://github.com/GoogleCloudPlatform/kubernetes/pull/7597</a> "Use generic copyright holder name in license boilerplate"
[241]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7633>https://github.com/GoogleCloudPlatform/kubernetes/pull/7633</a> "Retry incrementing quota if there is a conflict"
[242]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7568>https://github.com/GoogleCloudPlatform/kubernetes/pull/7568</a> "Remove GetContainers from Runtime interface"
[243]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7578>https://github.com/GoogleCloudPlatform/kubernetes/pull/7578</a> "Add image-related methods to DockerManager"
[244]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7586>https://github.com/GoogleCloudPlatform/kubernetes/pull/7586</a> "Remove more docker references in kubelet"
[245]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7601>https://github.com/GoogleCloudPlatform/kubernetes/pull/7601</a> "Add KillContainerInPod in DockerManager"
[246]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7652>https://github.com/GoogleCloudPlatform/kubernetes/pull/7652</a> "Kubelet: Add container runtime option."
[247]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7626>https://github.com/GoogleCloudPlatform/kubernetes/pull/7626</a> "bump heapster to v0.11.0 and grafana to v0.7.0"
[248]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7593>https://github.com/GoogleCloudPlatform/kubernetes/pull/7593</a> "Build github.com/onsi/ginkgo/ginkgo as a part of the release"
[249]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7490>https://github.com/GoogleCloudPlatform/kubernetes/pull/7490</a> "Do not automatically decode runtime.RawExtension"
[250]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7500>https://github.com/GoogleCloudPlatform/kubernetes/pull/7500</a> "Update changelog."
[251]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7610>https://github.com/GoogleCloudPlatform/kubernetes/pull/7610</a> "Add SyncPod() to DockerManager and use it in Kubelet"
[252]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7602>https://github.com/GoogleCloudPlatform/kubernetes/pull/7602</a> "Build: Push .md5 and .sha1 files for every file we push to GCS"
[253]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7540>https://github.com/GoogleCloudPlatform/kubernetes/pull/7540</a> "Fix rolling update --image "
[254]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7636>https://github.com/GoogleCloudPlatform/kubernetes/pull/7636</a> "Update license boilerplate for docs/man/md2man-all.sh"
[255]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7632>https://github.com/GoogleCloudPlatform/kubernetes/pull/7632</a> "Include shell license boilerplate in examples/k8petstore"
[256]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7277>https://github.com/GoogleCloudPlatform/kubernetes/pull/7277</a> "Add --cgroup_parent flag to Kubelet to set the parent cgroup for pods"
[257]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7209>https://github.com/GoogleCloudPlatform/kubernetes/pull/7209</a> "change the current dir to the config dir"
[258]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7158>https://github.com/GoogleCloudPlatform/kubernetes/pull/7158</a> "Set Weave To 0.9.0 And Update Etcd Configuration For Azure"
[259]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7467>https://github.com/GoogleCloudPlatform/kubernetes/pull/7467</a> "Augment describe to search for matching things if it doesn't match the original resource."
[260]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7559>https://github.com/GoogleCloudPlatform/kubernetes/pull/7559</a> "Add a simple cache for objects stored in etcd."
[261]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7549>https://github.com/GoogleCloudPlatform/kubernetes/pull/7549</a> "Rkt gc"
[262]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7550>https://github.com/GoogleCloudPlatform/kubernetes/pull/7550</a> "Rkt pull"
[263]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6400>https://github.com/GoogleCloudPlatform/kubernetes/pull/6400</a> "Implement Mount interface using mount(8) and umount(8)"
[264]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7588>https://github.com/GoogleCloudPlatform/kubernetes/pull/7588</a> "Trim Fleuntd tag for Cloud Logging"
[265]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7569>https://github.com/GoogleCloudPlatform/kubernetes/pull/7569</a> "GCE CoreOS cluster - set master name based on variable"
[266]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7535>https://github.com/GoogleCloudPlatform/kubernetes/pull/7535</a> "Capitalization of KubeProxyVersion wrong in JSON"
[267]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7530>https://github.com/GoogleCloudPlatform/kubernetes/pull/7530</a> "Make nodes report their external IP rather than the master's."
[268]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7539>https://github.com/GoogleCloudPlatform/kubernetes/pull/7539</a> "Trim cluster log tags to pod name and container name"
[269]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7541>https://github.com/GoogleCloudPlatform/kubernetes/pull/7541</a> "Handle conversion of boolean query parameters with a value of "false""
[270]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7532>https://github.com/GoogleCloudPlatform/kubernetes/pull/7532</a> "Add image-related methods to Runtime interface."
[271]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7560>https://github.com/GoogleCloudPlatform/kubernetes/pull/7560</a> "Test whether auto-generated conversions weren't manually edited"
[272]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7484>https://github.com/GoogleCloudPlatform/kubernetes/pull/7484</a> "Mention :latest behavior for image version tag"
[273]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7487>https://github.com/GoogleCloudPlatform/kubernetes/pull/7487</a> "readinessProbe calls livenessProbe.Exec.Command which cause "invalid memory address or nil pointer dereference"."
[274]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7520>https://github.com/GoogleCloudPlatform/kubernetes/pull/7520</a> "Add RuntimeHooks to abstract Kubelet logic"
[275]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7546>https://github.com/GoogleCloudPlatform/kubernetes/pull/7546</a> "Expose URL() on Request to allow building URLs"
[276]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7288>https://github.com/GoogleCloudPlatform/kubernetes/pull/7288</a> "Add a simple cache for objects stored in etcd"
[277]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7431>https://github.com/GoogleCloudPlatform/kubernetes/pull/7431</a> "Prepare for chaining autogenerated conversion methods "
[278]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7353>https://github.com/GoogleCloudPlatform/kubernetes/pull/7353</a> "Increase maxIdleConnection limit when creating etcd client in apiserver."
[279]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7354>https://github.com/GoogleCloudPlatform/kubernetes/pull/7354</a> "Improvements to generator of conversion methods."
[280]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7107>https://github.com/GoogleCloudPlatform/kubernetes/pull/7107</a> "Code to automatically generate conversion methods"
[281]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7407>https://github.com/GoogleCloudPlatform/kubernetes/pull/7407</a> "Support recovery for anonymous roll outs"
[282]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7527>https://github.com/GoogleCloudPlatform/kubernetes/pull/7527</a> "Bump kube2sky to 1.2. Point it at https endpoint (3rd try)."
[283]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7526>https://github.com/GoogleCloudPlatform/kubernetes/pull/7526</a> "cluster/gce/coreos: Add metadata-service in node.yaml"
[284]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7480>https://github.com/GoogleCloudPlatform/kubernetes/pull/7480</a> "Move ComputePodChanges to the Docker runtime"
[285]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7510>https://github.com/GoogleCloudPlatform/kubernetes/pull/7510</a> "Cobra rebase"
[286]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6718>https://github.com/GoogleCloudPlatform/kubernetes/pull/6718</a> "Adding system oom events from kubelet"
[287]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7479>https://github.com/GoogleCloudPlatform/kubernetes/pull/7479</a> "Move Prober to its own subpackage"
[288]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7513>https://github.com/GoogleCloudPlatform/kubernetes/pull/7513</a> "Fix parallel-e2e.sh to work on my macbook (bash v3.2)"
[289]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7449>https://github.com/GoogleCloudPlatform/kubernetes/pull/7449</a> "Move network plugin TearDown to DockerManager"
[290]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/7498>https://github.com/GoogleCloudPlatform/kubernetes/issues/7498</a> "CoreOS Getting Started Guide not working"
[291]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7499>https://github.com/GoogleCloudPlatform/kubernetes/pull/7499</a> "Fixes #7498 - CoreOS Getting Started Guide had invalid cloud config"
[292]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7504>https://github.com/GoogleCloudPlatform/kubernetes/pull/7504</a> "Fix invalid character '"' after object key:value pair"
[293]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/7317>https://github.com/GoogleCloudPlatform/kubernetes/issues/7317</a> "GlusterFS Volume Plugin deletes the contents of the mounted volume upon Pod deletion"
[294]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7503>https://github.com/GoogleCloudPlatform/kubernetes/pull/7503</a> "Fixed kubelet deleting data from volumes on stop (#7317)."
[295]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7482>https://github.com/GoogleCloudPlatform/kubernetes/pull/7482</a> "Fixing hooks/description to catch API fields without description tags"
[296]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7457>https://github.com/GoogleCloudPlatform/kubernetes/pull/7457</a> "cadvisor is obsoleted so kubelet service does not require it."
[297]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7408>https://github.com/GoogleCloudPlatform/kubernetes/pull/7408</a> "Set the default namespace for events to be "default""
[298]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7446>https://github.com/GoogleCloudPlatform/kubernetes/pull/7446</a> "Fix typo in namespace conversion"
[299]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7419>https://github.com/GoogleCloudPlatform/kubernetes/pull/7419</a> "Convert Secret registry to use update/create strategy, allow filtering by Type"
[300]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7102>https://github.com/GoogleCloudPlatform/kubernetes/pull/7102</a> "Use pod namespace when looking for its GlusterFS endpoints."
[301]: <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7427>https://github.com/GoogleCloudPlatform/kubernetes/pull/7427</a> "Fixed name of kube-proxy path in deployment scripts."
[</p></div><div class=td-content style=page-break-before:always><h1 id=pg-52ce46eb666293177a169315be15b141>Resource Usage Monitoring in Kubernetes</h1><div class="td-byline mb-4"><time datetime=2015-05-12 class=text-muted>Tuesday, May 12, 2015</time></div><p>Understanding how an application behaves when deployed is crucial to scaling the application and providing a reliable service. In a Kubernetes cluster, application performance can be examined at many different levels: containers, <a href=/docs/user-guide/pods>pods</a>, <a href=/docs/user-guide/services>services</a>, and whole clusters. As part of Kubernetes we want to provide users with detailed resource usage information about their running applications at all these levels. This will give users deep insights into how their applications are performing and where possible application bottlenecks may be found. In comes <a href=https://github.com/kubernetes/heapster>Heapster</a>, a project meant to provide a base monitoring platform on Kubernetes.</p><p><strong>Overview</strong></p><p>Heapster is a cluster-wide aggregator of monitoring and event data. It currently supports Kubernetes natively and works on all Kubernetes setups. Heapster runs as a pod in the cluster, similar to how any Kubernetes application would run. The Heapster pod discovers all nodes in the cluster and queries usage information from the nodes’ <a href=https://github.com/kubernetes/kubernetes/blob/master/DESIGN.md#kubelet>Kubelets</a>, the on-machine Kubernetes agent. The Kubelet itself fetches the data from <a href=https://github.com/google/cadvisor>cAdvisor</a>. Heapster groups the information by pod along with the relevant labels. This data is then pushed to a configurable backend for storage and visualization. Currently supported backends include <a href=http://influxdb.com/>InfluxDB</a> (with <a href=http://grafana.org/>Grafana</a> for visualization), <a href=https://cloud.google.com/monitoring/>Google Cloud Monitoring</a> and many others described in more details here. The overall architecture of the service can be seen below:</p><p><a href=https://2.bp.blogspot.com/-6Bu15356Zqk/V4mGINP8eOI/AAAAAAAAAmk/-RwvkJUt4rY2cmjqYFBmRo25FQQPRb27ACEw/s1600/monitoring-architecture.png><img src=https://2.bp.blogspot.com/-6Bu15356Zqk/V4mGINP8eOI/AAAAAAAAAmk/-RwvkJUt4rY2cmjqYFBmRo25FQQPRb27ACEw/s640/monitoring-architecture.png alt></a></p><p>Let’s look at some of the other components in more detail.</p><p><strong>cAdvisor</strong></p><p>cAdvisor is an open source container resource usage and performance analysis agent. It is purpose built for containers and supports Docker containers natively. In Kubernetes, cadvisor is integrated into the Kubelet binary. cAdvisor auto-discovers all containers in the machine and collects CPU, memory, filesystem, and network usage statistics. cAdvisor also provides the overall machine usage by analyzing the ‘root’? container on the machine.</p><p>On most Kubernetes clusters, cAdvisor exposes a simple UI for on-machine containers on port 4194. Here is a snapshot of part of cAdvisor’s UI that shows the overall machine usage:</p><p><a href=https://3.bp.blogspot.com/-V5KAfomW7Cg/V4mGH6OTKSI/AAAAAAAAAmo/EZHcG0afrs0606eTDMCryT6j6SoNzu3PgCEw/s1600/cadvisor.png><img src=https://3.bp.blogspot.com/-V5KAfomW7Cg/V4mGH6OTKSI/AAAAAAAAAmo/EZHcG0afrs0606eTDMCryT6j6SoNzu3PgCEw/s400/cadvisor.png alt></a></p><p><strong>Kubelet</strong></p><p>The Kubelet acts as a bridge between the Kubernetes master and the nodes. It manages the pods and containers running on a machine. Kubelet translates each pod into its constituent containers and fetches individual container usage statistics from cAdvisor. It then exposes the aggregated pod resource usage statistics via a REST API.</p><p><strong>STORAGE BACKENDS</strong></p><p><strong>InfluxDB and Grafana</strong></p><p>A Grafana setup with InfluxDB is a very popular combination for monitoring in the open source world. InfluxDB exposes an easy to use API to write and fetch time series data. Heapster is setup to use this storage backend by default on most Kubernetes clusters. A detailed setup guide can be found <a href=https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md>here</a>. InfluxDB and Grafana run in Pods. The pod exposes itself as a Kubernetes service which is how Heapster discovers it.</p><p>The Grafana container serves Grafana’s UI which provides an easy to configure dashboard interface. The default dashboard for Kubernetes contains an example dashboard that monitors resource usage of the cluster and the pods inside of it. This dashboard can easily be customized and expanded. Take a look at the storage schema for InfluxDB <a href=https://github.com/kubernetes/heapster/blob/master/docs/storage-schema.md#metrics>here</a>.</p><p>Here is a video showing how to monitor a Kubernetes cluster using heapster, InfluxDB and Grafana:</p><p><a href=https://www.youtube.com/watch?SZgqjMrxo3g><img src=https://img.youtube.com/vi/SZgqjMrxo3g/0.jpg alt></a></p><p>Here is a snapshot of the default Kubernetes Grafana dashboard that shows the CPU and Memory usage of the entire cluster, individual pods and containers:</p><p><a href=https://1.bp.blogspot.com/-lHMeU_4UnAk/V4mGHyrWkBI/AAAAAAAAAms/SvnncgJ7ieAduBqQzpI86oaboIkAKEpEQCEw/s1600/influx.png><img src=https://1.bp.blogspot.com/-lHMeU_4UnAk/V4mGHyrWkBI/AAAAAAAAAms/SvnncgJ7ieAduBqQzpI86oaboIkAKEpEQCEw/s640/influx.png alt></a></p><p><strong>Google Cloud Monitoring</strong></p><p>Google Cloud Monitoring is a hosted monitoring service that allows you to visualize and alert on important metrics in your application. Heapster can be setup to automatically push all collected metrics to Google Cloud Monitoring. These metrics are then available in the <a href=https://app.google.stackdriver.com/>Cloud Monitoring Console</a>. This storage backend is the easiest to setup and maintain. The monitoring console allows you to easily create and customize dashboards using the exported data.</p><p>Here is a video showing how to setup and run a Google Cloud Monitoring backed Heapster:
"https://youtube.com/embed/xSMNR2fcoLs"
Here is a snapshot of the a Google Cloud Monitoring dashboard showing cluster-wide resource usage.</p><p><a href=https://2.bp.blogspot.com/-F2j3kYn3IoA/V4mGH3M-0gI/AAAAAAAAAmg/aoml93zPeKsKbTX1tN5sTtRRTw7dAKsxwCEw/s1600/gcm.png><img src=https://2.bp.blogspot.com/-F2j3kYn3IoA/V4mGH3M-0gI/AAAAAAAAAmg/aoml93zPeKsKbTX1tN5sTtRRTw7dAKsxwCEw/s640/gcm.png alt></a></p><p><strong>Try it out!</strong></p><p>Now that you’ve learned a bit about Heapster, feel free to try it out on your own clusters! The <a href=https://github.com/kubernetes/heapster>Heapster repository</a> is available on GitHub. It contains detailed instructions to setup Heapster and its storage backends. Heapster runs by default on most Kubernetes clusters, so you may already have it! Feedback is always welcome. Please let us know if you run into any issues via the troubleshooting channels.</p><p><em>-- Vishnu Kannan and Victor Marmol, Google Software Engineers</em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-47a6b0051c9c158177447b53745a04dd>Kubernetes Release: 0.16.0</h1><div class="td-byline mb-4"><time datetime=2015-05-11 class=text-muted>Monday, May 11, 2015</time></div><p>Release Notes:</p><ul><li>Bring up a kuberenetes cluster using coreos image as worker nodes <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7445>#7445</a> (dchen1107)</li><li>Cloning v1beta3 as v1 and exposing it in the apiserver <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7454>#7454</a> (nikhiljindal)</li><li>API Conventions for Late-initializers <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7366>#7366</a> (erictune)</li><li>Upgrade Elasticsearch to 1.5.2 for cluster logging <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7455>#7455</a> (satnam6502)</li><li>Make delete actually stop resources by default. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7210>#7210</a> (brendandburns)</li><li>Change kube2sky to use token-system-dns secret, point at https endpoint ... <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7154>#7154</a>(cjcullen)</li><li>Updated CoreOS bare metal docs for 0.15.0 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7364>#7364</a> (hvolkmer)</li><li>Print named ports in 'describe service' <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7424>#7424</a> (thockin)</li><li>AWS</li><li>Return public & private addresses in GetNodeAddresses <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7040>#7040</a> (justinsb)</li><li>Improving getting existing VPC and subnet <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6606>#6606</a> (gust1n)</li><li>Set hostname_override for minions, back to fully-qualified name <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7182>#7182</a> (justinsb)</li><li>Conversion to v1beta3</li><li>Convert node level logging agents to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7274>#7274</a> (satnam6502)</li><li>Removing more references to v1beta1 from pkg/ <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7128>#7128</a> (nikhiljindal)</li><li>update examples/cassandra to api v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7258>#7258</a> (caesarxuchao)</li><li>Convert Elasticsearch logging to v1beta3 and de-salt <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7246>#7246</a> (satnam6502)</li><li>Update examples/storm for v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7231>#7231</a> (bcbroussard)</li><li>Update examples/spark for v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7230>#7230</a> (bcbroussard)</li><li>Update Kibana RC and service to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7240>#7240</a> (satnam6502)</li><li>Updating the guestbook example to v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7194>#7194</a> (nikhiljindal)</li><li>Update Phabricator to v1beta3 example <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7232>#7232</a> (bcbroussard)</li><li>Update Kibana pod to speak to Elasticsearch using v1beta3 <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7206>#7206</a> (satnam6502)</li><li>Validate Node IPs; clean up validation code <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7180>#7180</a> (ddysher)</li><li>Add PortForward to runtime API. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7391>#7391</a> (vmarmol)</li><li>kube-proxy uses token to access port 443 of apiserver <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7303>#7303</a> (erictune)</li><li>Move the logging-related directories to where I think they belong <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7014>#7014</a> (a-robinson)</li><li>Make client service requests use the default timeout now that external load balancers are created asynchronously <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6870>#6870</a> (a-robinson)</li><li>Fix bug in kube-proxy of not updating iptables rules if a service's public IPs change <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6123>#6123</a>(a-robinson)</li><li>PersistentVolumeClaimBinder <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6105>#6105</a> (markturansky)</li><li>Fixed validation message when trying to submit incorrect secret <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7356>#7356</a> (soltysh)</li><li>First step to supporting multiple k8s clusters <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6006>#6006</a> (justinsb)</li><li>Parity for namespace handling in secrets E2E <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7361>#7361</a> (pmorie)</li><li>Add cleanup policy to RollingUpdater <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6996>#6996</a> (ironcladlou)</li><li>Use narrowly scoped interfaces for client access <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6871>#6871</a> (ironcladlou)</li><li>Warning about Critical bug in the GlusterFS Volume Plugin <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7319>#7319</a> (wattsteve)</li><li>Rolling update</li><li>First part of improved rolling update, allow dynamic next replication controller generation. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7268>#7268</a> (brendandburns)</li><li>Further implementation of rolling-update, add rename <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7279>#7279</a> (brendandburns)</li><li>Added basic apiserver authz tests. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7293>#7293</a> (ashcrow)</li><li>Retry pod update on version conflict error in e2e test. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7297>#7297</a> (quinton-hoole)</li><li>Add "kubectl validate" command to do a cluster health check. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6597>#6597</a> (fabioy)</li><li>coreos/azure: Weave version bump, various other enhancements <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7224>#7224</a> (errordeveloper)</li><li>Azure: Wait for salt completion on cluster initialization <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6576>#6576</a> (jeffmendoza)</li><li>Tighten label parsing <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6674>#6674</a> (kargakis)</li><li>fix watch of single object <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7263>#7263</a> (lavalamp)</li><li>Upgrade go-dockerclient dependency to support CgroupParent <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7247>#7247</a> (guenter)</li><li>Make secret volume plugin idempotent <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7166>#7166</a> (pmorie)</li><li>Salt reconfiguration to get rid of nginx on GCE <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6618>#6618</a> (roberthbailey)</li><li>Revert "Change kube2sky to use token-system-dns secret, point at https e... <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7207>#7207</a> (fabioy)</li><li>Pod templates as their own type <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5012>#5012</a> (smarterclayton)</li><li>iscsi Test: Add explicit check for attach and detach calls. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7110>#7110</a> (swagiaal)</li><li>Added field selector for listing pods <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7067>#7067</a> (ravigadde)</li><li>Record an event on node schedulable changes <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7138>#7138</a> (pravisankar)</li><li>Resolve <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/6812>#6812</a>, limit length of load balancer names <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7145>#7145</a> (caesarxuchao)</li><li>Convert error strings to proper validation errors. <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7131>#7131</a> (rjnagal)</li><li>ResourceQuota add object count support for secret and volume claims <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6593>#6593</a>(derekwaynecarr)</li><li>Use Pod.Spec.Host instead of Pod.Status.HostIP for pod subresources <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6985>#6985</a> (csrwng)</li><li>Prioritize deleting the non-running pods when reducing replicas <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6992>#6992</a> (yujuhong)</li><li>Kubernetes UI with Dashboard component <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/7056>#7056</a> (preillyme)</li></ul><p>To download, please visit <a href=https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.16.0>https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.16.0</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-801e598fff1bb0a31a4322ac49bf5bf1>Weekly Kubernetes Community Hangout Notes - May 1 2015</h1><div class="td-byline mb-4"><time datetime=2015-05-11 class=text-muted>Monday, May 11, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><ul><li><p>Simple rolling update - Brendan</p><ul><li><p>Rolling update = nice example of why RCs and Pods are good.</p></li><li><p>...pause… (Brendan needs demo recovery tips from Kelsey)</p></li><li><p>Rolling update has recovery: Cancel update and restart, update continues from where it stopped.</p></li><li><p>New controller gets name of old controller, so appearance is pure update.</p></li><li><p>Can also name versions in update (won't do rename at the end).</p></li></ul></li><li><p>Rocket demo - CoreOS folks</p><ul><li><p>2 major differences between rocket & docker: Rocket is daemonless & pod-centric.</p></li><li><p>Rocket has AppContainer format as native, but also supports docker image format.</p></li><li><p>Can run AppContainer and docker containers in same pod.</p></li><li><p>Changes are close to merged.</p></li></ul></li><li><p>demo service accounts and secrets being added to pods - Jordan</p><ul><li><p>Problem: It's hard to get a token to talk to the API.</p></li><li><p>New API object: "ServiceAccount"</p></li><li><p>ServiceAccount is namespaced, controller makes sure that at least 1 default service account exists in a namespace.</p></li><li><p>Typed secret "ServiceAccountToken", controller makes sure there is at least 1 default token.</p></li><li><p>DEMO</p></li><li><pre><code>* Can create new service account with ServiceAccountToken. Controller will create token for it.
</code></pre></li><li><p>Can create a pod with service account, pods will have service account secret mounted at /var/run/secrets/kubernetes.io/…</p></li></ul></li><li><p>Kubelet running in a container - Paul</p><ul><li>Kubelet successfully ran pod w/ mounted secret.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-05680064b0bb19d48e280af9d6152529>AppC Support for Kubernetes through RKT</h1><div class="td-byline mb-4"><time datetime=2015-05-04 class=text-muted>Monday, May 04, 2015</time></div><p>We very recently accepted a pull request to the Kubernetes project to add appc support for the Kubernetes community.  Appc is a new open container specification that was initiated by CoreOS, and is supported through CoreOS rkt container runtime.</p><p>This is an important step forward for the Kubernetes project and for the broader containers community.  It adds flexibility and choice to the container-verse and brings the promise of  compelling new security and performance capabilities to the Kubernetes developer.</p><p>Container based runtimes (like Docker or rkt) when paired with smart orchestration technologies (like Kubernetes and/or Apache Mesos) are a legitimate disruption to the way that developers build and run their applications.  While the supporting technologies are relatively nascent, they do offer the promise of some very powerful new ways to assemble, deploy, update, debug and extend solutions.  I believe that the world has not yet felt the full potential of containers and the next few years are going to be particularly exciting!  With that in mind it makes sense for several projects to emerge with different properties and different purposes. It also makes sense to be able to plug together different pieces (whether it be the container runtime or the orchestrator) based on the specific needs of a given application.</p><p>Docker has done an amazing job of democratizing container technologies and making them accessible to the outside world, and we expect Kubernetes to support Docker indefinitely. CoreOS has also started to do interesting work with rkt to create an elegant, clean, simple and open platform that offers some really interesting properties.  It looks poised deliver a secure and performant operating environment for containers.  The Kubernetes team has been working with the appc team at CoreOS for a while and in many ways they built rkt with Kubernetes in mind as a simple pluggable runtime component.  </p><p>The really nice thing is that with Kubernetes you can now pick the container runtime that works best for you based on your workloads’ needs, change runtimes without having the replace your cluster environment, or even mix together applications where different parts are running in different container runtimes in the same cluster.  Additional choices can’t help but ultimately benefit the end developer.</p><p>-- Craig McLuckie<br>Google Product Manager and Kubernetes co-founder</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c21afabad8ee96c94af485cfbcc582a5>Weekly Kubernetes Community Hangout Notes - April 24 2015</h1><div class="td-byline mb-4"><time datetime=2015-04-30 class=text-muted>Thursday, April 30, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Agenda:</p><ul><li>Flocker and Kubernetes integration demo</li></ul><p>Notes:</p><ul><li>flocker and kubernetes integration demo</li><li><ul><li><p>Flocker Q/A</p><ul><li><p>Does the file still exists on node1 after migration?</p></li><li><p>Brendan: Any plan this to make it a volume? So we don't need powerstrip?</p><ul><li><p>Luke: Need to figure out interest to decide if we want to make it a first-class persistent disk provider in kube.</p></li><li><p>Brendan: Removing need for powerstrip would make it simple to use. Totally go for it.</p></li><li><p>Tim: Should take no more than 45 minutes to add it to kubernetes:)</p></li></ul></li><li><p>Derek: Contrast this with persistent volumes and claims?</p><ul><li><p>Luke: Not much difference, except for the novel ZFS based backend. Makes workloads really portable.</p></li><li><p>Tim: very different than network-based volumes. Its interesting that it is the only offering that allows upgrading media.</p></li><li><p>Brendan: claims, how does it look for replicated claims? eg Cassandra wants to have replicated data underneath. It would be efficient to scale up and down. Create storage on the fly based on load dynamically. Its step beyond taking snapshots - programmatically creating replicas with preallocation.</p></li><li><p>Tim: helps with auto-provisioning.</p></li></ul></li><li><p>Brian: Does flocker requires any other component?</p><ul><li><p>Kai: Flocker control service co-located with the master. (dia on blog post). Powerstrip + Powerstrip Flocker. Very interested in mpersisting state in etcd. It keeps metadata about each volume.</p></li><li><p>Brendan: In future, flocker can be a plugin and we'll take care of persistence. Post v1.0.</p></li><li><p>Brian: Interested in adding generic plugin for services like flocker.</p></li><li><p>Luke: Zfs can become really valuable when scaling to lot of containers on a single node.</p></li></ul></li><li><p>Alex: Can flocker service can be run as a pod?</p><ul><li><p>Kai: Yes, only requirement is the flocker control service should be able to talk to zfs agent. zfs agent needs to be installed on the host and zfs binaries need to be accessible.</p></li><li><p>Brendan: In theory, all zfs bits can be put it into a container with devices.</p></li><li><p>Luke: Yes, still working through cross-container mounting issue.</p></li><li><p>Tim: pmorie is working through it to make kubelet work in a container. Possible re-use.</p></li></ul></li><li><p>Kai: Cinder support is coming. Few days away.</p></li></ul></li></ul></li><li>Bob: What's the process of pushing kube to GKE? Need more visibility for confidence.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cb033081886f937e8367cbdc7c9aa2f6>Borg: The Predecessor to Kubernetes</h1><div class="td-byline mb-4"><time datetime=2015-04-23 class=text-muted>Thursday, April 23, 2015</time></div><p>Google has been running containerized workloads in production for more than a decade. Whether it's service jobs like web front-ends and stateful servers, infrastructure systems like <a href=http://research.google.com/archive/bigtable.html>Bigtable</a> and <a href=http://research.google.com/archive/spanner.html>Spanner</a>, or batch frameworks like <a href=http://research.google.com/archive/mapreduce.html>MapReduce</a> and <a href=http://research.google.com/pubs/pub41378.html>Millwheel</a>, virtually everything at Google runs as a container. Today, we took the wraps off of Borg, Google’s long-rumored internal container-oriented cluster-management system, publishing details at the academic computer systems conference <a href=http://eurosys2015.labri.fr/>Eurosys</a>. You can find the paper <a href=https://research.google.com/pubs/pub43438.html>here</a>.</p><p>Kubernetes traces its lineage directly from Borg. Many of the developers at Google working on Kubernetes were formerly developers on the Borg project. We've incorporated the best ideas from Borg in Kubernetes, and have tried to address some pain points that users identified with Borg over the years.</p><p>To give you a flavor, here are four Kubernetes features that came from our experiences with Borg:</p><ol><li><a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/pods.md>Pods</a>. A pod is the unit of scheduling in Kubernetes. It is a resource envelope in which one or more containers run. Containers that are part of the same pod are guaranteed to be scheduled together onto the same machine, and can share state via local volumes.</li></ol><p>Borg has a similar abstraction, called an alloc (short for “resource allocation”). Popular uses of allocs in Borg include running a web server that generates logs alongside a lightweight log collection process that ships the log to a cluster filesystem (not unlike fluentd or logstash); running a web server that serves data from a disk directory that is populated by a process that reads data from a cluster filesystem and prepares/stages it for the web server (not unlike a Content Management System); and running user-defined processing functions alongside a storage shard. Pods not only support these use cases, but they also provide an environment similar to running multiple processes in a single VM -- Kubernetes users can deploy multiple co-located, cooperating processes in a pod without having to give up the simplicity of a one-application-per-container deployment model.</p><ol start=2><li><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md>Services</a>. Although Borg’s primary role is to manage the lifecycles of tasks and machines, the applications that run on Borg benefit from many other cluster services, including naming and load balancing. Kubernetes supports naming and load balancing using the service abstraction: a service has a name and maps to a dynamic set of pods defined by a label selector (see next section). Any container in the cluster can connect to the service using the service name. Under the covers, Kubernetes automatically load-balances connections to the service among the pods that match the label selector, and keeps track of where the pods are running as they get rescheduled over time due to failures.</p></li><li><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/labels.md>Labels</a>. A container in Borg is usually one replica in a collection of identical or nearly identical containers that correspond to one tier of an Internet service (e.g. the front-ends for Google Maps) or to the workers of a batch job (e.g. a MapReduce). The collection is called a Job, and each replica is called a Task. While the Job is a very useful abstraction, it can be limiting. For example, users often want to manage their entire service (composed of many Jobs) as a single entity, or to uniformly manage several related instances of their service, for example separate canary and stable release tracks. At the other end of the spectrum, users frequently want to reason about and control subsets of tasks within a Job -- the most common example is during rolling updates, when different subsets of the Job need to have different configurations.</p></li></ol><p>Kubernetes supports more flexible collections than Borg by organizing pods using labels, which are arbitrary key/value pairs that users attach to pods (and in fact to any object in the system). Users can create groupings equivalent to Borg Jobs by using a “job:&lt;jobname>” label on their pods, but they can also use additional labels to tag the service name, service instance (production, staging, test), and in general, any subset of their pods. A label query (called a “label selector”) is used to select which set of pods an operation should be applied to. Taken together, labels and <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/replication-controller.md>replication controllers</a> allow for very flexible update semantics, as well as for operations that span the equivalent of Borg Jobs.</p><ol start=4><li>IP-per-Pod. In Borg, all tasks on a machine use the IP address of that host, and thus share the host’s port space. While this means Borg can use a vanilla network, it imposes a number of burdens on infrastructure and application developers: Borg must schedule ports as a resource; tasks must pre-declare how many ports they need, and take as start-up arguments which ports to use; the Borglet (node agent) must enforce port isolation; and the naming and RPC systems must handle ports as well as IP addresses.</li></ol><p>Thanks to the advent of software-defined overlay networks such as <a href=https://coreos.com/blog/introducing-rudder/>flannel</a> or those built into <a href=https://cloud.google.com/compute/docs/networking>public clouds</a>, Kubernetes is able to give every pod and service its own IP address. This removes the infrastructure complexity of managing ports, and allows developers to choose any ports they want rather than requiring their software to adapt to the ones chosen by the infrastructure. The latter point is crucial for making it easy to run off-the-shelf open-source applications on Kubernetes--pods can be treated much like VMs or physical hosts, with access to the full port space, oblivious to the fact that they may be sharing the same physical machine with other pods.</p><p>With the growing popularity of container-based microservice architectures, the lessons Google has learned from running such systems internally have become of increasing interest to the external DevOps community. By revealing some of the inner workings of our cluster manager Borg, and building our next-generation cluster manager as both an open-source project (Kubernetes) and a publicly available hosted service (<a href=http://cloud.google.com/container-engine>Google Container Engine</a>), we hope these lessons can benefit the broader community outside of Google and advance the state-of-the-art in container scheduling and cluster management.  </p></div><div class=td-content style=page-break-before:always><h1 id=pg-cd6be1fea08ace15bbf056a7d703bd90>Kubernetes and the Mesosphere DCOS</h1><div class="td-byline mb-4"><time datetime=2015-04-22 class=text-muted>Wednesday, April 22, 2015</time></div><h1 id=kubernetes-and-the-mesosphere-dcos>Kubernetes and the Mesosphere DCOS</h1><p>Today Mesosphere announced the addition of Kubernetes as a standard part of their <a href=https://mesosphere.com/product/>DCOS</a> offering. This is a great step forwards in bringing cloud native application management to the world, and should lay to rest many questions we hear about 'Kubernetes or Mesos, which one should I use?'. Now you can have your cake and eat it too: use both. Today's announcement extends the reach of Kubernetes to a new class of users, and add some exciting new capabilities for everyone.</p><p>By way of background, Kubernetes is a cluster management framework that was started by Google nine months ago, inspired by the internal system known as Borg. You can learn a little more about Borg by checking out this <a href=http://research.google.com/pubs/pub43438.html>paper</a>. At the heart of it Kubernetes offers what has been dubbed 'cloud native' application management. To us, there are three things that together make something 'cloud native':</p><ul><li><strong>Container oriented deployments</strong> Package up your application components with all their dependencies and deploy them using technologies like Docker or Rocket. Containers radically simplify the deployment process, making rollouts repeatable and predictable.</li><li><strong>Dynamically managed</strong> Rely on modern control systems to make moment-to-moment decisions around the health management and scheduling of applications to radically improve reliability and efficiency. There are some things that just machines do better than people, and actively running applications is one of those things.</li><li><strong>Micro-services oriented</strong> Tease applications apart into small semi-autonomous services that can be consumed easily so that the resulting systems are easier to understand, extend and adapt.</li></ul><p>Kubernetes was designed from the start to make these capabilities available to everyone, and built by the same engineers that built the system internally known as Borg. For many users the promise of 'Google style app management' is interesting, but they want to run these new classes of applications on the same set of physical resources as their existing workloads like Hadoop, Spark, Kafka, etc. Now they will have access to commercially supported offering that brings the two worlds together.</p><p>Mesosphere, one of the earliest supporters of the Kubernetes project, has been working closely with the core Kubernetes team to create a natural experience for users looking to get the best of both worlds, adding Kubernetes to every Mesos deployment they instantiate, whether it be in the public cloud, private cloud, or in a hybrid deployment model. This is well aligned with the overall Kubernetes vision of creating ubiquitous management framework that runs anywhere a container can. It will be interesting to see how you blend together the old world and the new on a commercially supported, versatile platform.</p><p>Craig McLuckie</p><p>Product Manager, Google and Kubernetes co-founder</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a383598a2a3cf35ecf5f846ce0d2772>Weekly Kubernetes Community Hangout Notes - April 17 2015</h1><div class="td-byline mb-4"><time datetime=2015-04-17 class=text-muted>Friday, April 17, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Agenda</p><ul><li>Mesos Integration</li><li>High Availability (HA)</li><li>Adding performance and profiling details to e2e to track regressions</li><li>Versioned clients</li></ul><p>Notes</p><ul><li><p>Mesos integration</p><ul><li><p>Mesos integration proposal:</p></li><li><p>No blockers to integration.</p></li><li><p>Documentation needs to be updated.</p></li></ul></li><li><p>HA</p><ul><li><p>Proposal should land today.</p></li><li><p>Etcd cluster.</p></li><li><p>Load-balance apiserver.</p></li><li><p>Cold standby for controller manager and other master components.</p></li></ul></li><li><p>Adding performance and profiling details to e2e to track regression</p><ul><li><p>Want red light for performance regression</p></li><li><p>Need a public DB to post the data</p><ul><li>See</li></ul></li><li><p>Justin working on multi-platform e2e dashboard</p></li></ul></li><li><p>Versioned clients</p><ul><li></li></ul><ul><li></li></ul><ul><li><p>Client library currently uses internal API objects.</p></li><li><p>Nobody reported that frequent changes to types.go have been painful, but we are worried about it.</p></li><li><p>Structured types are useful in the client. Versioned structs would be ok.</p></li><li><p>If start with json/yaml (kubectl), shouldn’t convert to structured types. Use swagger.</p></li></ul></li><li><p>Security context</p><ul><li></li></ul><ul><li><p>Administrators can restrict who can run privileged containers or require specific unix uids</p></li><li><p>Kubelet will be able to get pull credentials from apiserver</p></li><li><p>Policy proposal coming in the next week or so</p></li></ul></li><li><p>Discussing upstreaming of users, etc. into Kubernetes, at least as optional</p></li><li><p>1.0 Roadmap</p><ul><li><p>Focus is performance, stability, cluster upgrades</p></li><li><p>TJ has been making some edits to <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/roadmap.md>roadmap.md</a> but hasn’t sent out a PR yet</p></li></ul></li><li><p>Kubernetes UI</p><ul><li><p>Dependencies broken out into third-party</p></li><li><p>@lavalamp is reviewer</p></li></ul></li></ul><p>[*[3:27 PM]: 2015-04-17T15:27:00-07:00</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5f615a3f6d44366138310ebc9b83a28e>Introducing Kubernetes API Version v1beta3</h1><div class="td-byline mb-4"><time datetime=2015-04-16 class=text-muted>Thursday, April 16, 2015</time></div><p>We've been hard at work on cleaning up the API over the past several months (see <a href=https://github.com/GoogleCloudPlatform/kubernetes/issues/1519>https://github.com/GoogleCloudPlatform/kubernetes/issues/1519</a> for details). The result is v1beta3, which is considered to be the release candidate for the v1 API.</p><p>We would like you to move to this new API version as soon as possible. v1beta1 and v1beta2 are deprecated, and will be removed by the end of June, shortly after we introduce the v1 API.</p><p>As of the latest release, v0.15.0, v1beta3 is the primary, default API. We have changed the default kubectl and client API versions as well as the default storage version (which means objects persisted in etcd will be converted from v1beta1 to v1beta3 as they are rewritten). </p><p>You can take a look at v1beta3 examples such as:</p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/guestbook/v1beta3>https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/guestbook/v1beta3</a></p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/walkthrough/v1beta3>https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/walkthrough/v1beta3</a></p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/update-demo/v1beta3>https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/update-demo/v1beta3</a></p><p>To aid the transition, we've also created a conversion <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/cluster_management.md#switching-your-config-files-to-a-new-api-version>tool</a> and put together a list of important <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/api.md#v1beta3-conversion-tips>different API changes</a>.</p><ul><li>The resource <code>id</code> is now called <code>name</code>.</li><li><code>name</code>, <code>labels</code>, <code>annotations</code>, and other metadata are now nested in a map called <code>metadata</code></li><li><code>desiredState</code> is now called <code>spec</code>, and <code>currentState</code> is now called <code>status</code></li><li><code>/minions</code> has been moved to <code>/nodes</code>, and the resource has kind <code>Node</code></li><li>The namespace is required (for all namespaced resources) and has moved from a URL parameter to the path:<code>/api/v1beta3/namespaces/{namespace}/{resource_collection}/{resource_name}</code></li><li>The names of all resource collections are now lower cased - instead of <code>replicationControllers</code>, use<code>replicationcontrollers</code>.</li><li>To watch for changes to a resource, open an HTTP or Websocket connection to the collection URL and provide the<code>?watch=true</code> URL parameter along with the desired <code>resourceVersion</code> parameter to watch from.</li><li>The container <code>entrypoint</code> has been renamed to <code>command</code>, and <code>command</code> has been renamed to <code>args</code>.</li><li>Container, volume, and node resources are expressed as nested maps (e.g., <code>resources{cpu:1}</code>) rather than as individual fields, and resource values support <a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/resources.md#resource-quantities>scaling suffixes</a> rather than fixed scales (e.g., milli-cores).</li><li>Restart policy is represented simply as a string (e.g., "Always") rather than as a nested map ("always{}").</li><li>The volume <code>source</code> is inlined into <code>volume</code> rather than nested.</li><li>Host volumes have been changed to hostDir to hostPath  to better reflect that they can be files or directories</li></ul><p>And the most recently generated Swagger specification of the API is here:</p><p><a href=http://kubernetes.io/third_party/swagger-ui/#!/v1beta3>http://kubernetes.io/third_party/swagger-ui/#!/v1beta3</a></p><p>More details about our approach to API versioning and the transition can be found here:</p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/api.md>https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/api.md</a></p><p>Another change we discovered is that with the change to the default API version in kubectl, commands that use "-o template" will break unless you specify "--api-version=v1beta1" or update to v1beta3 syntax. An example of such a change can be seen here:</p><p><a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6377/files>https://github.com/GoogleCloudPlatform/kubernetes/pull/6377/files</a></p><p>If you use "-o template", I recommend always explicitly specifying the API version rather than relying upon the default. We may add this setting to kubeconfig in the future.</p><p>Let us know if you have any questions. As always, we're available on IRC (#google-containers) and github issues.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-79141a7a1c693c72f0adcbe84b41d683>Kubernetes Release: 0.15.0</h1><div class="td-byline mb-4"><time datetime=2015-04-16 class=text-muted>Thursday, April 16, 2015</time></div><p>Release Notes:</p><ul><li>Enables v1beta3 API and sets it to the default API version (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6098 title="Enabling v1beta3 api version by default in master">#6098</a>)</li><li>Added multi-port Services (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6182 title="Implement multi-port Services">#6182</a>)<ul><li>New Getting Started Guides</li><li>Multi-node local startup guide (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6505 title="Docker multi-node">#6505</a>)</li><li>Mesos on Google Cloud Platform (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5442 title="Getting started guide for Mesos on Google Cloud Platform">#5442</a>)</li><li>Ansible Setup instructions (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6237 title="example ansible setup repo">#6237</a>)</li></ul></li><li>Added a controller framework (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5270 title="Controller framework">#5270</a>, <a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5473 title="Add DeltaFIFO (a controller framework piece)">#5473</a>)</li><li>The Kubelet now listens on a secure HTTPS port (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6380 title="Configure the kubelet to use HTTPS (take 2)">#6380</a>)</li><li>Made kubectl errors more user-friendly (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6338 title="Return a typed error for config validation, and make errors simple">#6338</a>)</li><li>The apiserver now supports client cert authentication (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6190 title="Add client cert authentication">#6190</a>)</li><li>The apiserver now limits the number of concurrent requests it processes (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6207 title="Add a limit to the number of in-flight requests that a server processes.">#6207</a>)</li><li>Added rate limiting to pod deleting (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6355 title="Added rate limiting to pod deleting">#6355</a>)</li><li>Implement Balanced Resource Allocation algorithm as a PriorityFunction in scheduler package (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6150 title="Implement Balanced Resource Allocation (BRA) algorithm as a PriorityFunction in scheduler package.">#6150</a>)</li><li>Enabled log collection from master (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6396 title="Enable log collection from master.">#6396</a>)</li><li>Added an api endpoint to pull logs from Pods (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6497 title="Pod log subresource">#6497</a>)</li><li>Added latency metrics to scheduler (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6368 title="Add basic latency metrics to scheduler.">#6368</a>)</li><li>Added latency metrics to REST client (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6409 title="Add latency metrics to REST client">#6409</a>)</li><li>etcd now runs in a pod on the master (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6221 title="Run etcd 2.0.5 in a pod">#6221</a>)</li><li>nginx now runs in a container on the master (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6334 title="Add an nginx docker image for use on the master.">#6334</a>)</li><li>Began creating Docker images for master components (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6326 title="Create Docker images for master components ">#6326</a>)</li><li>Updated GCE provider to work with gcloud 0.9.54 (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6270 title="Updates for gcloud 0.9.54">#6270</a>)</li><li>Updated AWS provider to fix Region vs Zone semantics (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6011 title="Fix AWS region vs zone">#6011</a>)</li><li>Record event when image GC fails (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6091 title="Record event when image GC fails.">#6091</a>)</li><li>Add a QPS limiter to the kubernetes client (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6203 title="Add a QPS limiter to the kubernetes client.">#6203</a>)</li><li>Decrease the time it takes to run make release (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6196 title="Parallelize architectures in both the building and packaging phases of `make release`">#6196</a>)</li><li>New volume support<ul><li>Added iscsi volume plugin (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5506 title="add iscsi volume plugin">#5506</a>)</li><li>Added glusterfs volume plugin (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6174 title="implement glusterfs volume plugin">#6174</a>)</li><li>AWS EBS volume support (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/5138 title="AWS EBS volume support">#5138</a>)</li></ul></li><li>Updated to heapster version to v0.10.0 (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6331 title="Update heapster version to v0.10.0">#6331</a>)</li><li>Updated to etcd 2.0.9 (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6544 title="Build etcd image (version 2.0.9), and upgrade kubernetes cluster to the new version">#6544</a>)</li><li>Updated to Kibana to v1.2 (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6426 title="Update Kibana to v1.2 which paramaterizes location of Elasticsearch">#6426</a>)</li><li>Bug Fixes<ul><li>Kube-proxy now updates iptables rules if a service's public IPs change (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6123 title="Fix bug in kube-proxy of not updating iptables rules if a service's public IPs change">#6123</a>)</li><li>Retry kube-addons creation if the initial creation fails (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6200 title="Retry kube-addons creation if kube-addons creation fails.">#6200</a>)</li><li>Make kube-proxy more resiliant to running out of file descriptors (<a href=https://github.com/GoogleCloudPlatform/kubernetes/pull/6727 title="pkg/proxy: panic if run out of fd">#6727</a>)</li></ul></li></ul><p>To download, please visit <a href=https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.15.0>https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.15.0</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-87053e12af520116753ef6507a31f8f7>Weekly Kubernetes Community Hangout Notes - April 10 2015</h1><div class="td-byline mb-4"><time datetime=2015-04-11 class=text-muted>Saturday, April 11, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Agenda:</p><ul><li>kubectl tooling, rolling update, deployments, imperative commands.</li><li>Downward API / env. substitution, and maybe preconditions/dependencies.</li></ul><p><strong>Notes from meeting:</strong></p><p>1. kubectl improvements</p><ul><li><p>make it simpler to use, finish rolling update, higher-level deployment concepts.</p></li><li><p>rolling update</p><ul><li><p>today</p><ul><li><p>can replace one rc by another rc specified by a file.</p></li><li><p>no explicit support for rollback, can sort of do it by doing rolling update to old version.</p></li><li><p>we keep annotations on rcs to keep track of desired # instances; won't work for rollback case b/c not symmetric.</p></li></ul></li><li><p>need immutable image ids; currently no uuid that corresponds to image,version so if someone pushes on top you'll re-pull that; in API server we should translate images into uuids (as close to edge as possible).</p></li><li><p>would be nice to auto-gen new rc instead of having user update it (e.g. when change image tag for container, etc.; currently need to change rc name and label value; could automate generating new rc).</p></li><li><p>treating rcs as pets vs. cattle.</p></li><li><p>"roll me from v1 to v2" (or v2 to v1) - good enough for most people. don't care about record of what happened in the past.</p></li><li><p>we're providing the module ansible can call to make something happen.</p></li><li><p>how do you keep track of multiple templates; today we use multiple RCs.</p></li><li><p>if we had a deployment controller; deployment config spawns pos that runs rolling update; trigger is level-based update of image repository.</p></li><li><p>alternative short-term proposal: create new rc as clone of old one, futz with counts so new one is old one and vv, bring prev-named one (pet) down to zero and bring it back up with new template (this is very similar to how Borg does job updates).</p><ul><li>is it worthwhile if we want to have the deployments anyway? Yes b/c we have lots of concepts already; need to simplify.</li></ul></li><li><p>deployment controller keeps track of multiple templates which is what you need for rolling updates and canaries.</p></li><li><p>only reason for new thing is to move the process into the server instead of the client?</p></li><li><p>may not need to make it an API object; should provide experience where it's not an API object and is just something client side.</p></li><li><p>need an experience now so need to do it in client because object won't land before 1.0.</p></li><li><p>having simplified experience for people who only want to enageg w/ RCs.</p></li><li><p>how does rollback work: ctrl-c, rollout v2 v1. rollback pattern can be in person's head. 2 kinds of rollback: i'm at steady state and want to go back, and i've got canary deployment and hit ctrl-c how do i get rid of the canary deployment (e.g. new is failing). ctrl-c might not work. delete canary controller and its pods. wish there was a command to also delete pods (there is -- kbectl stop). argument for not reusing name: when you move fwd you can stop the new thing and you're ok, vs. if you replace the old one and you've created a copy if you hit ctrl-c you don't have anything you can stop. but you could wait to flip the name until the end, use naming convention so can figure out what is going on, etc.</p></li><li><p>two different experiences: (1) i'm using version control, have version history of last week rollout this week, rolling update with two files -> create v2, ??? v1, don't have a pet - moved into world of version control where have cumulative history and; (1) imperative kubectl v1 v2 where sys takes care of details, that's where we use the snapshot pattern.</p></li></ul></li><li><p>other imperative commands</p><ul><li><p>run-container (or just run): spec command on command line which makes it more similar to docker run; but not multi-container pods.</p></li><li><p>--forever vs. not (one shot exec via simple command).</p></li><li><p>would like it go interactive - run -it and runs in cluster but you have interactive terminal to your process.</p></li><li><p>how do command line args work. could say --image multiple times. will cobra support? in openshift we have clever syntax for grouping arguments together. doesn't work for real structured parameters.</p></li><li><p>alternative: create pod; add container add container ...; run pod -- build and don't run object until 'run pod'.</p><ul><li><p>-- to separate container args.</p></li><li><p>create a pod, mutate it before you run it - like initializer pattern.</p></li></ul></li></ul></li><li><p>kind discovery</p><ul><li><p>if we have run and sometimes it creates an rc and sometimes it doesn't, how does user know what to delete if they want to delete whatever they created with run.</p></li><li><p>bburns has proposal for don't specify kind if you do command like stop, delete; let kubectl figure it out.</p></li><li><p>alternative: allow you to define alias from name to set of resource types, eg. delete all which would follow that alias (all could mean everything in some namespace, or unscoped, etc.) - someone explicitly added something to a set vs. accidentally showed up like nodes.</p></li><li><p>would like to see extended to allow tools to specify their own aliases (not just users); e.g. resize can say i can handle RCs, delete can say I can handle everything, et.c so we can automatically do these things w/o users have to specify stuff. but right mechanism.</p></li><li><p>resourcebuilder has concept of doing that kind of expansion depending on how we fit in targeted commands. for instance if you want to add a volume to pods and rcs, you need something to go find the pod template and change it. there's the search part of it (delete nginx -> you have to figure out what object they are referring to) and then command can say i got a pod i know what to do with a pod.</p></li><li><p>alternative heuristic: what if default target of all commands was deployments. kubectl run -> deployment. too much work, easier to clean up existing CLI. leave door open for that. macro objects OK but a lot more work to make that work. eventually will want index to make these efficient. could rely more on swagger to tell us types.</p></li></ul></li></ul><p>2. paul/downward api: env substitution</p><ul><li>create ad-hoc env var like strings, e.g. k8s_pod_name that would get sub'd by system in objects.</li><li>allow people to create env vars that refer to fields of k8s objects w/o query api from inside their container; in some cases enables query api from their container (e.g. pass obj names, namespaces); e.g. sidecar containers need this for pulling things from api server.</li><li>another proposal similar: instead of env var like names, have JSON-path-like syntax for referring to object field names; e.g. $.<a href=http://metadata.name/>metadata.name</a> to refer to name of current object, maybe have some syntax for referring to related objects like node that a pod is on. advantage of JSON path-like syntax is that it's less ad hoc. disadvantage is that you can only refer to things that are fields of objects.</li><li>for both, if you populate env vars then you have drawback that fields only set when container is created. but least degree of coupling -- off the shelf containers, containers don't need to know how to talk to k8s API. keeps the k8s concepts in the control plane.</li><li>we were converging on JSON path like approach. but need prototype or at least deeper proposal to demo.</li><li>paul: one variant is for env vars in addition to value field have different sources which is where you would plug in e.g. syntax you use to describe a field of an object; another source would be a source that described info about the host. have partial prototype. clean separation between what's in image vs. control plane. could use source idea for volume plugin.</li><li>use case: provide info for sidecar container to contact API server.</li><li>use case: pass down unique identifiers or things like using UID as unique identifier.</li><li>clayton: for rocket or gce metadata service being available for every pod for more sophisticated things; most containers want to find endpoint of service.</li></ul><p>3. preconditions/dependencies</p><ul><li>when you create pods that talk to services, the service env vars only get populated if you create the objs in the right order. if you use dns it's less of a problem but some apps are fragile. may crash if svc they depend on is not there, may take a long time to restart. proposal to have preconds that block starting pods until objs they depend on exist.</li><li>infer automatically if we ask people to declare which env vars they wanted, or have dep mech at pod or rc or obj level to say this obj doesn't become active until this other thing exists.</li><li>can use event hook? only app owner knows their dependency or when service is ready to serve.</li><li>one proposal is to use pre-start hook. another is precondition probe - pre-start hook could do a probe. does anything respond when i hit this svc address or ip, then probe fails. could be implemented in pre-start hook. more useful than post-start. is part of rkt spec. has stages 0, 1, 2. hard to do in docker today, easy in rocket.</li><li>pre-start hook in container: how will affect readiness probe since the container might have a lock until some arbitrary condition is met if you implement with prestart hook. there has to be some compensation on when kubelet runs readiness/liveness probes if you have a hook. Systemd has timeouts around the stages of process lifecycle.</li><li>if we go to black box model of container pre-start makes sense; if container spec becomes more descriptive of process model like systemd, then does kubelet need to know more about process model to do the right thing.</li><li>ideally msg from inside the container to say i've done all of my pre-start actions. sdnotify for systemd does this. you tell systemd that you're done, it will communicate to other deps that you're alive.</li><li>but... someone could just implement preconds inside their container. makes it easier to adapt an app w/o having to change their image. alternative is just have a pattern how they do it themselves but we don't do it for them.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d1dfdc30e1999f3dfec7c332480b37e4>Faster than a speeding Latte</h1><div class="td-byline mb-4"><time datetime=2015-04-06 class=text-muted>Monday, April 06, 2015</time></div><p>Check out Brendan Burns racing Kubernetes.
<a href="https://www.youtube.com/watch?v=?7vZ9dRKRMyc"><img src=https://img.youtube.com/vi/7vZ9dRKRMyc/0.jpg alt="Check out Brendan Burns racing Kubernetes"></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1f81358fabb3f1915bb19a2c37312d39>Weekly Kubernetes Community Hangout Notes - April 3 2015</h1><div class="td-byline mb-4"><time datetime=2015-04-04 class=text-muted>Saturday, April 04, 2015</time></div><h1 id=kubernetes-weekly-kubernetes-community-hangout-notes>Kubernetes: Weekly Kubernetes Community Hangout Notes</h1><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Agenda:</p><ul><li>Quinton - Cluster federation</li><li>Satnam - Performance benchmarking update</li></ul><p><em>Notes from meeting:</em></p><ol><li>Quinton - Cluster federation</li></ol><ul><li>Ideas floating around after meetup in SF</li><li><ul><li>Please read and comment</li></ul></li><li>Not 1.0, but put a doc together to show roadmap</li><li>Can be built outside of Kubernetes</li><li>API to control things across multiple clusters, include some logic</li></ul><ol><li><p>Auth(n)(z)</p></li><li><p>Scheduling Policies</p></li><li><p>…</p></li></ol><ul><li>Different reasons for cluster federation</li></ul><ol><li><p>Zone (un) availability : Resilient to zone failures</p></li><li><p>Hybrid cloud: some in cloud, some on prem. for various reasons</p></li><li><p>Avoid cloud provider lock-in. For various reasons</p></li><li><p>"Cloudbursting" - automatic overflow into the cloud</p></li></ol><ul><li>Hard problems</li></ul><ol><li><p>Location affinity. How close do pods need to be?</p><ol><li><p>Workload coupling</p></li><li><p>Absolute location (e.g. eu data needs to be in eu)</p></li></ol></li><li><p>Cross cluster service discovery</p><ol><li>How does service/DNS work across clusters</li></ol></li><li><p>Cross cluster workload migration</p><ol><li>How do you move an application piece by piece across clusters?</li></ol></li><li><p>Cross cluster scheduling</p><ol><li><p>How do know enough about clusters to know where to schedule</p></li><li><p>Possibly use a cost function to achieve affinities with minimal complexity</p></li><li><p>Can also use cost to determine where to schedule (under used clusters are cheaper than over-used clusters)</p></li></ol></li></ol><ul><li>Implicit requirements</li></ul><ol><li><p>Cross cluster integration shouldn't create cross-cluster failure modes</p><ol><li>Independently usable in a disaster situation where Ubernetes dies.</li></ol></li><li><p>Unified visibility</p><ol><li>Want to have unified monitoring, alerting, logging, introspection, ux, etc.</li></ol></li><li><p>Unified quota and identity management</p><ol><li>Want to have user database and auth(n)/(z) in a single place</li></ol></li></ol><ul><li>Important to note, most causes of software failure are not the infrastructure</li></ul><ol><li><p>Botched software upgrades</p></li><li><p>Botched config upgrades</p></li><li><p>Botched key distribution</p></li><li><p>Overload</p></li><li><p>Failed external dependencies</p></li></ol><ul><li>Discussion:</li></ul><ol><li><p>Where do you draw the "ubernetes" line</p><ol><li>Likely at the availability zone, but could be at the rack, or the region</li></ol></li><li><p>Important to not pigeon hole and prevent other users</p></li><li><p>Satnam - Soak Test</p></li></ol><ul><li>Want to measure things that run for a long time to make sure that the cluster is stable over time. Performance doesn't degrade, no memory leaks, etc.</li><li>github.com/GoogleCloudPlatform/kubernetes/test/soak/…</li><li>Single binary, puts lots of pods on each node, and queries each pod to make sure that it is running.</li><li>Pods are being created much, much more quickly (even in the past week) to make things go more quickly.</li><li>Once the pods are up running, we hit the pods via the proxy. Decision to hit the proxy was deliberate so that we test the kubernetes apiserver.</li><li>Code is already checked in.</li><li>Pin pods to each node, exercise every pod, make sure that you get a response for each node.</li><li>Single binary, run forever.</li><li>Brian - v1beta3 is enabled by default, v1beta1 and v1beta2 deprecated, turned off in June. Should still work with upgrading existing clusters, etc.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dbae61099ed11bff986f221ad7cf6c18>Participate in a Kubernetes User Experience Study</h1><div class="td-byline mb-4"><time datetime=2015-03-31 class=text-muted>Tuesday, March 31, 2015</time></div><p>We need your help in shaping the future of Kubernetes and Google Container Engine, and we'd love to have you participate in a remote UX research study to help us learn about your experiences!  If you're interested in participating, we invite you to take <a href=http://goo.gl/AXFFMs>this brief survey</a> to see if you qualify. If you’re selected to participate, we’ll follow up with you directly.</p><ul><li>Length: 60 minute interview</li><li>Date: April 7th-15th</li><li>Location: Remote</li><li>Your gift: $100 Perks gift code*</li><li>Study format: Interview with our researcher</li></ul><p>Interested in participating? Take <a href=http://goo.gl/AXFFMs>this brief survey</a>.</p><p>* Perks gift codes can be redeemed for gift certificates from VISA and used at a number of online retailers (<a href=http://www.google.com/forms/perks/index1.html>http://www.google.com/forms/perks/index1.html</a>). Gift codes are only for participants who successfully complete the study session. You’ll be emailed the gift code after you complete the study session.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6fac4a6542db8c1435843173a812efe8>Weekly Kubernetes Community Hangout Notes - March 27 2015</h1><div class="td-byline mb-4"><time datetime=2015-03-28 class=text-muted>Saturday, March 28, 2015</time></div><p>Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.</p><p>Agenda:</p><p>- Andy - demo remote execution and port forwarding</p><p>- Quinton - Cluster federation - Postponed</p><p>- Clayton - UI code sharing and collaboration around Kubernetes</p><p>Notes from meeting:</p><p>1. Andy from RedHat:</p><ul><li><p>Demo remote execution</p><ul><li><p>kubectl exec -p $POD -- $CMD</p></li><li><p>Makes a connection to the master as proxy, figures out which node the pod is on, proxies connection to kubelet, which does the interesting bit. via nsenter.</p></li><li><p>Multiplexed streaming over HTTP using SPDY</p></li><li><p>Also interactive mode:</p></li><li><p>Assumes first container. Can use -c $CONTAINER to pick a particular one.</p></li><li><p>If have gdb pre-installed in container, then can interactively attach it to running process</p><ul><li>backtrace, symbol tbles, print, etc. Most things you can do with gdb.</li></ul></li><li><p>Can also with careful flag crafting run rsync over this or set up sshd inside container.</p></li><li><p>Some feedback via chat:</p></li></ul></li><li><p>Andy also demoed port forwarding</p></li><li><p>nsenter vs. docker exec</p><ul><li><p>want to inject a binary under control of the host, similar to pre-start hooks</p></li><li><p>socat, nsenter, whatever the pre-start hook needs</p></li></ul></li><li><p>would be nice to blog post on this</p></li><li><p>version of nginx in wheezy is too old to support needed master-proxy functionality</p></li></ul><p>2. Clayton: where are we wrt a community organization for e.g. kubernetes UI components?</p><ul><li>google-containers-ui IRC channel, mailing list.</li><li>Tim: google-containers prefix is historical, should just do "kubernetes-ui"</li><li>also want to put design resources in, and bower expects its own repo.</li><li>General agreement</li></ul><p>3. Brian Grant:</p><ul><li>Testing v1beta3, getting that ready to go in.</li><li>Paul working on changes to commandline stuff.</li><li>Early to mid next week, try to enable v1beta3 by default?</li><li>For any other changes, file issue and CC thockin.</li></ul><p>4. General consensus that 30 minutes is better than 60</p><ul><li>Shouldn't artificially try to extend just to fill time.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-64e10f0095f0c1f67ad7b47541c7860c>Kubernetes Gathering Videos</h1><div class="td-byline mb-4"><time datetime=2015-03-23 class=text-muted>Monday, March 23, 2015</time></div><p>If you missed the Kubernetes Gathering in SF last month, fear not! Here are the videos from the evening presentations organized into a playlist on YouTube</p><p><a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP2FBVvSLHpJE8_6hRHW8Kxe"><img src=https://img.youtube.com/vi/q8lGZCKktYo/0.jpg alt="Kubernetes Gathering"></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-38fe5117270c19dea659675ee7fa1a8a>Welcome to the Kubernetes Blog!</h1><div class="td-byline mb-4"><time datetime=2015-03-20 class=text-muted>Friday, March 20, 2015</time></div><p>Welcome to the new Kubernetes Blog. Follow this blog to learn about the Kubernetes Open Source project. We plan to post release notes, how-to articles, events, and maybe even some off topic fun here from time to time.</p><p>If you are using Kubernetes or contributing to the project and would like to do a guest post, <a href=mailto:kitm@google.com>please let me know</a>.</p><p>To start things off, here's a roundup of recent Kubernetes posts from other sites:</p><ul><li><a href=http://googlecloudplatform.blogspot.com/2015/03/scaling-MySQL-in-the-cloud-with-Vitess-and-Kubernetes.html>Scaling MySQL in the cloud with Vitess and Kubernetes</a></li><li><a href=http://googlecloudplatform.blogspot.com/2015/02/container-clusters-on-vms.html>Container Clusters on VMs</a></li><li><a href=http://googlecloudplatform.blogspot.com/2015/01/everything-you-wanted-to-know-about-Kubernetes-but-were-afraid-to-ask.html>Everything you wanted to know about Kubernetes but were afraid to ask</a></li><li><a href=http://googlecloudplatform.blogspot.com/2015/01/what-makes-a-container-cluster.html>What makes a container cluster?</a></li><li><a href=https://www.mirantis.com/blog/integrating-openstack-and-kubernetes-with-murano/>Integrating OpenStack and Kubernetes with Murano</a></li><li><a href=http://googlecloudplatform.blogspot.com/2015/01/in-coming-weeks-we-will-be-publishing.html>An introduction to containers, Kubernetes, and the trajectory of modern cloud computing</a></li><li><a href=http://www.centurylinklabs.com/what-is-kubernetes-and-how-to-use-it/>What is Kubernetes and how to use it?</a></li><li><a href=https://blog.openshift.com/v3-docker-kubernetes-interview/>OpenShift V3, Docker and Kubernetes Strategy</a></li><li><a href=https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes>An Introduction to Kubernetes</a></li></ul><p>Happy cloud computing!</p><ul><li>Kit Merker - Product Manager, Google Cloud Platform</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>