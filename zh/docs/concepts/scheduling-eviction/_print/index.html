<!doctype html><html lang=zh class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=pt href=https://kubernetes.io/pt/docs/concepts/scheduling-eviction/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh/docs/concepts/scheduling-eviction/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>调度和驱逐 (Scheduling and Eviction) | Kubernetes</title><meta property="og:title" content="调度和驱逐 (Scheduling and Eviction)"><meta property="og:description" content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="调度和驱逐 (Scheduling and Eviction)"><meta itemprop=description content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><meta name=twitter:card content="summary"><meta name=twitter:title content="调度和驱逐 (Scheduling and Eviction)"><meta name=twitter:description content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><meta property="og:description" content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><meta name=twitter:description content="在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。"><meta property="og:url" content="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/"><meta property="og:title" content="调度和驱逐 (Scheduling and Eviction)"><meta name=twitter:title content="调度和驱逐 (Scheduling and Eviction)"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh/docs/>文档</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/blog/>Kubernetes 博客</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/training/>培训</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/partners/>合作伙伴</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/community/>社区</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/case-studies/>案例分析</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/zh/docs/concepts/scheduling-eviction/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 Chinese</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/scheduling-eviction/>English</a>
<a class=dropdown-item href=/ko/docs/concepts/scheduling-eviction/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/concepts/scheduling-eviction/>日本語 Japanese</a>
<a class=dropdown-item href=/pt/docs/concepts/scheduling-eviction/>Português</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh/docs/concepts/scheduling-eviction/>返回本页常规视图</a>.</p></div><h1 class=title>调度和驱逐 (Scheduling and Eviction)</h1><div class=lead>在Kubernetes中，调度 (Scheduling) 指的是确保 Pods 匹配到合适的节点，以便 kubelet 能够运行它们。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。</div><ul><li>1: <a href=#pg-da22fe2278df236f71efbe672f392677>Pod 开销</a></li><li>2: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>污点和容忍度</a></li><li>3: <a href=#pg-598f36d691ab197f9d995784574b0a12>Kubernetes 调度器</a></li><li>4: <a href=#pg-21169f516071aea5d16734a4c27789a5>将 Pod 分配给节点</a></li><li>5: <a href=#pg-961126cd43559012893979e568396a49>扩展资源的资源装箱</a></li><li>6: <a href=#pg-45b5702b685c32798723d679c86aad02>驱逐策略</a></li><li>7: <a href=#pg-602208c95fe7b1f1170310ce993f5814>调度框架</a></li><li>8: <a href=#pg-d9574a30fcbc631b0d2a57850e161e89>调度器性能调优</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-da22fe2278df236f71efbe672f392677>1 - Pod 开销</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些资源是运行 Pod 内容器所需资源的附加资源。
<em>POD 开销</em> 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。</p><h2 id=pod-开销>Pod 开销</h2><p>在 Kubernetes 中，Pod 的开销是根据与 Pod 的 <a href=/zh/docs/concepts/containers/runtime-class/>RuntimeClass</a> 相关联的开销在
<a href=/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>准入</a> 时设置的。</p><p>如果启用了 Pod Overhead，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。
类似地，kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。</p><h2 id=set-up>启用 Pod 开销</h2><p>您需要确保在集群中启用了 <code>PodOverhead</code> <a href=/zh/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>
（在 1.18 默认是开启的），以及一个用于定义 <code>overhead</code> 字段的 <code>RuntimeClass</code>。</p><h2 id=使用示例>使用示例</h2><p>要使用 PodOverhead 特性，需要一个定义 <code>overhead</code> 字段的 RuntimeClass。
作为例子，可以在虚拟机和寄宿操作系统中通过一个虚拟化容器运行时来定义
RuntimeClass 如下，其中每个 Pod 大约使用 120MiB:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>通过指定 <code>kata-fc</code> RuntimeClass 处理程序创建的工作负载会将内存和 cpu 开销计入资源配额计算、节点调度以及 Pod cgroup 分级。</p><p>假设我们运行下面给出的工作负载示例 test-pod:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></code></pre></div><p>在准入阶段 RuntimeClass <a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a> 更新工作负载的 PodSpec 以包含
RuntimeClass 中定义的 <code>overhead</code>. 如果 PodSpec 中该字段已定义，该 Pod 将会被拒绝。
在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod, 包含了一个 <code>overhead</code>.</p><p>在 RuntimeClass 准入控制器之后，可以检验一下已更新的 PodSpec:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</code></pre></div><p>输出：</p><pre><code>map[cpu:250m memory:120Mi]
</code></pre><p>如果定义了 ResourceQuata, 则容器请求的总量以及 <code>overhead</code> 字段都将计算在内。</p><p>当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的 <code>overhead</code> 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。</p><p>一旦 Pod 调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个 <a class=glossary-tooltip title="一组具有可选资源隔离、审计和限制的 Linux 进程。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-cgroup" target=_blank aria-label=cgroup>cgroup</a>. 底层容器运行时将在这个 pod 中创建容器。</p><p>如果该资源对每一个容器都定义了一个限制（定义了受限的 Guaranteed QoS 或者 Bustrable QoS），kubelet 会为与该资源（CPU 的 cpu.cfs_quota_us 以及内存的 memory.limit_in_bytes）
相关的 pod cgroup 设定一个上限。该上限基于容器限制总量与 PodSpec 中定义的 <code>overhead</code> 之和。</p><p>对于 CPU, 如果 Pod 的 QoS 是 Guaranteed 或者 Burstable, kubelet 会基于容器请求总量与 PodSpec 中定义的 <code>overhead</code> 之和设置 <code>cpu.shares</code>.</p><p>请看这个例子，验证工作负载的容器请求：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</code></pre></div><p>容器请求总计 2000m CPU 和 200MiB 内存：</p><pre><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>对照从节点观察到的情况来检查一下：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl describe node | grep test-pod -B2
</code></pre></div><p>该输出显示请求了 2250m CPU 以及 320MiB 内存，包含了 PodOverhead 在内：</p><pre><code>  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=验证-pod-cgroup-限制>验证 Pod cgroup 限制</h2><p>在工作负载所运行的节点上检查 Pod 的内存 cgroups. 在接下来的例子中，将在该节点上使用具备 CRI 兼容的容器运行时命令行工具 <a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>.
这是一个展示 PodOverhead 行为的进阶示例，用户并不需要直接在该节点上检查 cgroups.</p><p>首先在特定的节点上确定该 Pod 的标识符：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 调度的节点上执行如下命令：</span>
<span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div><p>可以依此判断该 Pod 的 cgroup 路径：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 调度的节点上执行如下命令：</span>
sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</code></pre></div><p>执行结果的 cgroup 路径中包含了该 Pod 的 <code>pause</code> 容器。Pod 级别的 cgroup 即上面的一个目录。</p><pre><code>        &quot;cgroupsPath&quot;: &quot;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&quot;
</code></pre><p>在这个例子中，该 pod 的 cgroup 路径是 <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>。验证内存的 Pod 级别 cgroup 设置：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 在该 Pod 调度的节点上执行这个命令。</span>
<span style=color:#080;font-style:italic># 另外，修改 cgroup 的名称以匹配为该 pod 分配的 cgroup。</span>
 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</code></pre></div><p>和预期的一样是 320 MiB</p><pre><code>335544320
</code></pre><h3 id=可观察性>可观察性</h3><p>在 <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> 中可以通过 <code>kube_pod_overhead</code> 指标来协助确定何时使用 PodOverhead 以及协助观察以一个既定开销运行的工作负载的稳定性。
该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。在此之前，用户需要从源代码构建 kube-state-metrics.</p><h2 id=接下来>接下来</h2><ul><li><a href=/zh/docs/concepts/containers/runtime-class/>RuntimeClass</a></li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead 设计</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>2 - 污点和容忍度</h1><p>节点亲和性（详见<a href=/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>这里</a>）
是 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 的一种属性，它使 Pod
被吸引到一类特定的<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>。
这可能出于一种偏好，也可能是硬性要求。
Taint（污点）则相反，它使节点能够排斥一类特定的 Pod。</p><p>容忍度（Tolerations）是应用于 Pod 上的，允许（但并不要求）Pod
调度到带有与之匹配的污点的节点上。</p><p>污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。
每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，是不会被该节点接受的。</p><h2 id=概念>概念</h2><p>您可以使用命令 <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a> 给节点增加一个污点。比如，</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</code></pre></div><p>给节点 <code>node1</code> 增加一个污点，它的键名是 <code>key1</code>，键值是 <code>value1</code>，效果是 <code>NoSchedule</code>。
这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 <code>node1</code> 这个节点。</p><p>若要移除上述命令所添加的污点，你可以执行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</code></pre></div><p>您可以在 PodSpec 中定义 Pod 的容忍度。
下面两个容忍度均与上面例子中使用 <code>kubectl taint</code> 命令创建的污点相匹配，
因此如果一个 Pod 拥有其中的任何一个容忍度都能够被分配到 <code>node1</code> ：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>这里是一个使用了容忍度的 Pod：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-toleration-yaml')" title="Copy pods/pod-with-toleration.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-toleration-yaml><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div></div></div><p><code>operator</code> 的默认值是 <code>Equal</code>。</p><p>一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：</p><ul><li>如果 <code>operator</code> 是 <code>Exists</code> （此时容忍度不能指定 <code>value</code>），或者</li><li>如果 <code>operator</code> 是 <code>Equal</code> ，则它们的 <code>value</code> 应该相等</li></ul><blockquote class="note callout"><div><strong>说明：</strong><p>存在两种特殊情况：</p><p>如果一个容忍度的 <code>key</code> 为空且 operator 为 <code>Exists</code>，
表示这个容忍度与任意的 key 、value 和 effect 都匹配，即这个容忍度能容忍任意 taint。</p><p>如果 <code>effect</code> 为空，则可以与所有键名 <code>key1</code> 的效果相匹配。</p></div></blockquote><p>上述例子中 <code>effect</code> 使用的值为 <code>NoSchedule</code>，您也可以使用另外一个值 <code>PreferNoSchedule</code>。
这是“优化”或“软”版本的 <code>NoSchedule</code> —— 系统会 <em>尽量</em> 避免将 Pod 调度到存在其不能容忍污点的节点上，
但这不是强制的。<code>effect</code> 的值还可以设置为 <code>NoExecute</code>，下文会详细描述这个值。</p><p>您可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。
Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历，
过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了
Pod 是否会被分配到该节点，特别是以下情况：</p><ul><li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoSchedule</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点。</li><li>如果未被过滤的污点中不存在 effect 值为 <code>NoSchedule</code> 的污点，
但是存在 effect 值为 <code>PreferNoSchedule</code> 的污点，
则 Kubernetes 会 <em>尝试</em> 不将 Pod 分配到该节点。</li><li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoExecute</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点（如果 Pod 还未在节点上运行），
或者将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。</li></ul><p>例如，假设您给一个节点添加了如下污点</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</code></pre></div><p>假定有一个 Pod，它有两个容忍度：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>在这种情况下，上述 Pod 不会被分配到上述节点，因为其没有容忍度和第三个污点相匹配。
但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行，
那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。</p><p>通常情况下，如果给一个节点添加了一个 effect 值为 <code>NoExecute</code> 的污点，
则任何不能忍受这个污点的 Pod 都会马上被驱逐，
任何可以忍受这个污点的 Pod 都不会被驱逐。
但是，如果 Pod 存在一个 effect 值为 <code>NoExecute</code> 的容忍度指定了可选属性
<code>tolerationSeconds</code> 的值，则表示在给节点添加了上述污点之后，
Pod 还能继续在节点上运行的时间。例如，</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></code></pre></div><p>这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点，
那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。
如果在此之前上述污点被删除了，则 Pod 不会被驱逐。</p><h2 id=使用例子>使用例子</h2><p>通过污点和容忍度，可以灵活地让 Pod <em>避开</em> 某些节点或者将 Pod 从某些节点驱逐。下面是几个使用例子：</p><ul><li><strong>专用节点</strong>：如果您想将某些节点专门分配给特定的一组用户使用，您可以给这些节点添加一个污点（即，
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>），
然后给这组用户的 Pod 添加一个相对应的 toleration（通过编写一个自定义的
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>，很容易就能做到）。
拥有上述容忍度的 Pod 就能够被分配到上述专用节点，同时也能够被分配到集群中的其它节点。
如果您希望这些 Pod 只能被分配到上述专用节点，那么您还需要给这些专用节点另外添加一个和上述
污点类似的 label （例如：<code>dedicated=groupName</code>），同时 还要在上述准入控制器中给 Pod
增加节点亲和性要求上述 Pod 只能被分配到添加了 <code>dedicated=groupName</code> 标签的节点上。</li></ul><ul><li><strong>配备了特殊硬件的节点</strong>：在部分节点配备了特殊硬件（比如 GPU）的集群中，
我们希望不需要这类硬件的 Pod 不要被分配到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。
要达到这个目的，可以先给配备了特殊硬件的节点添加 taint
（例如 <code>kubectl taint nodes nodename special=true:NoSchedule</code> 或
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>)，
然后给使用了这类特殊硬件的 Pod 添加一个相匹配的 toleration。
和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>。
比如，我们推荐使用<a href=/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources>扩展资源</a>
来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称，
然后运行一个 <a href=/zh/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>
准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod
会被调度到这些节点。但当你创建一个使用了扩展资源的 Pod 时，
<code>ExtendedResourceToleration</code> 准入控制器会自动给 Pod 加上正确的容忍度，
这样 Pod 就会被自动调度到这些配置了特殊硬件件的节点上。
这样就能够确保这些配置了特殊硬件的节点专门用于运行需要使用这些硬件的 Pod，
并且您无需手动给这些 Pod 添加容忍度。</li></ul><ul><li><strong>基于污点的驱逐</strong>: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为，接下来的章节会描述这个特性。</li></ul><h2 id=taint-based-evictions>基于污点的驱逐</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>前文提到过污点的 effect 值 <code>NoExecute</code>会影响已经在节点上运行的 Pod</p><ul><li>如果 Pod 不能忍受 effect 值为 <code>NoExecute</code> 的污点，那么 Pod 将马上被驱逐</li><li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，但是在容忍度定义中没有指定
<code>tolerationSeconds</code>，则 Pod 还会一直在这个节点上运行。</li><li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，而且指定了 <code>tolerationSeconds</code>，
则 Pod 还能在这个节点上继续运行这个指定的时间长度。</li></ul><p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p><ul><li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 "<code>False</code>"。</li><li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 "<code>Unknown</code>"。</li><li><code>node.kubernetes.io/out-of-disk</code>：节点磁盘耗尽。</li><li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li><li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li><li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 "外部" 云平台驱动，
它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager
的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li></ul><p>在节点被驱逐时，节点控制器或者 kubelet 会添加带有 <code>NoExecute</code> 效应的相关污点。
如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。</p><blockquote class="note callout"><div><strong>说明：</strong> 为了保证由于节点问题引起的 Pod 驱逐
<a href=/zh/docs/concepts/architecture/nodes/>速率限制</a>行为正常，
系统实际上会以限定速率的方式添加污点。在像主控节点与工作节点间通信中断等场景下，
这样做可以避免 Pod 被大量驱逐。</div></blockquote><p>使用这个功能特性，结合 <code>tolerationSeconds</code>，Pod 就可以指定当节点出现一个
或全部上述问题时还将在这个节点上运行多长的时间。</p><p>比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停留在当前节点上运行一段较长的时间，
愿意等待网络恢复以避免被驱逐。在这种情况下，Pod 的容忍度可能是下面这样的：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></code></pre></div><blockquote class="note callout"><div><strong>说明：</strong><p>Kubernetes 会自动给 Pod 添加一个 key 为 <code>node.kubernetes.io/not-ready</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/not-ready</code> 的容忍度。</p><p>同样，Kubernetes 会给 Pod 添加一个 key 为 <code>node.kubernetes.io/unreachable</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/unreachable</code> 的容忍度。</p></div></blockquote><p>这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod
默认能够继续停留在当前节点运行 5 分钟。</p><p><a href=/zh/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> 中的 Pod 被创建时，
针对以下污点自动添加的 <code>NoExecute</code> 的容忍度将不会指定 <code>tolerationSeconds</code>：</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。</p><h2 id=基于节点状态添加污点>基于节点状态添加污点</h2><p>Node 生命周期控制器会自动创建与 Node 条件相对应的带有 <code>NoSchedule</code> 效应的污点。
同样，调度器不检查节点条件，而是检查节点污点。这确保了节点条件不会影响调度到节点上的内容。
用户可以通过添加适当的 Pod 容忍度来选择忽略某些 Node 的问题(表示为 Node 的调度条件)。</p><p>DaemonSet 控制器自动为所有守护进程添加如下 <code>NoSchedule</code> 容忍度以防 DaemonSet 崩溃：</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/out-of-disk</code> (<em>只适合关键 Pod</em>)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 或更高版本)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>只适合主机网络配置</em>)</li></ul><p>添加上述容忍度确保了向后兼容，您也可以选择自由向 DaemonSet 添加容忍度。</p><h2 id=接下来>接下来</h2><ul><li>阅读<a href=/zh/docs/tasks/administer-cluster/out-of-resource/>资源耗尽的处理</a>，以及如何配置其行为</li><li>阅读 <a href=/zh/docs/concepts/configuration/pod-priority-preemption/>Pod 优先级</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-598f36d691ab197f9d995784574b0a12>3 - Kubernetes 调度器</h1><p>在 Kubernetes 中，<em>调度</em> 是指将 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 放置到合适的
<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> 上，然后对应 Node 上的
<a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> 才能够运行这些 pod。</p><h2 id=scheduling>调度概览</h2><p>调度器通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。
调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。
调度器会依据下文的调度原则来做出调度选择。</p><p>如果你想要理解 Pod 为什么会被调度到特定的 Node 上，或者你想要尝试实现
一个自定义的调度器，这篇文章将帮助你了解调度。</p><h2 id=kube-scheduler>kube-scheduler</h2><p><a href=/zh/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
是 Kubernetes 集群的默认调度器，并且是集群
<a class=glossary-tooltip title="控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。" data-toggle=tooltip data-placement=top href="/zh/docs/reference/glossary/?all=true#term-control-plane" target=_blank aria-label=控制面>控制面</a> 的一部分。
如果你真的希望或者有这方面的需求，kube-scheduler 在设计上是允许
你自己写一个调度组件并替换原有的 kube-scheduler。</p><p>对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的
Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且
Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前，
根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。</p><p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度节点</em>。
如果没有任何一个 Node 能满足 Pod 的资源请求，那么这个 Pod 将一直停留在
未调度状态直到调度器能够找到合适的 Node。</p><p>调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，
选出其中得分最高的 Node 来运行 Pod。之后，调度器将这个调度决定通知给
kube-apiserver，这个过程叫做 <em>绑定</em>。</p><p>在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、
亲和以及反亲和要求、数据局域性、负载间的干扰等等。</p><h2 id=kube-scheduler-implementation>kube-scheduler 调度流程</h2><p>kube-scheduler 给一个 pod 做调度选择包含两个步骤：</p><ol><li>过滤</li><li>打分</li></ol><p>过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。
例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。
在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下，
这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。</p><p>在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。
根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。</p><p>最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。
如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。</p><p>支持以下两种方式配置调度器的过滤和打分行为：</p><ol><li><a href=/zh/docs/reference/scheduling/policies>调度策略</a> 允许你配置过滤的 <em>断言(Predicates)</em>
和打分的 <em>优先级(Priorities)</em> 。</li><li><a href=/zh/docs/reference/scheduling/config/#profiles>调度配置</a> 允许你配置实现不同调度阶段的插件，
包括：<code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code> 等等。
你也可以配置 kube-scheduler 运行不同的配置文件。</li></ol><h2 id=接下来>接下来</h2><ul><li>阅读关于 <a href=/zh/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>调度器性能调优</a></li><li>阅读关于 <a href=/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/>Pod 拓扑分布约束</a></li><li>阅读关于 kube-scheduler 的 <a href=/zh/docs/reference/command-line-tools-reference/kube-scheduler/>参考文档</a></li><li>了解关于 <a href=/zh/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>配置多个调度器</a> 的方式</li><li>了解关于 <a href=/zh/docs/tasks/administer-cluster/topology-manager/>拓扑结构管理策略</a></li><li>了解关于 <a href=/zh/docs/concepts/scheduling-eviction/pod-overhead/>Pod 额外开销</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21169f516071aea5d16734a4c27789a5>4 - 将 Pod 分配给节点</h1><p>你可以约束一个 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> 只能在特定的
<a class=glossary-tooltip title="Kubernetes 中的工作机器称作节点。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a> 上运行，或者优先运行在特定的节点上。
有几种方法可以实现这点，推荐的方法都是用
<a href=/zh/docs/concepts/overview/working-with-objects/labels/>标签选择算符</a>来进行选择。
通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上，
而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制
Pod 停靠的节点，例如，确保 Pod 最终落在连接了 SSD 的机器上，或者将来自两个不同的服务
且有大量通信的 Pods 被放置在同一个可用区。</p><h2 id=nodeselector>nodeSelector</h2><p><code>nodeSelector</code> 是节点选择约束的最简单推荐形式。<code>nodeSelector</code> 是 PodSpec 的一个字段。
它包含键值对的映射。为了使 pod 可以在某个节点上运行，该节点的标签中
必须包含这里的每个键值对（它也可以具有其他标签）。
最常见的用法的是一对键值对。</p><p>让我们来看一个使用 <code>nodeSelector</code> 的例子。</p><h3 id=步骤零-先决条件>步骤零：先决条件</h3><p>本示例假设你已基本了解 Kubernetes 的 Pod 并且已经<a href=/zh/docs/setup/>建立一个 Kubernetes 集群</a>。</p><h3 id=attach-labels-to-node>步骤一：添加标签到节点</h3><p>执行 <code>kubectl get nodes</code> 命令获取集群的节点名称。
选择一个你要增加标签的节点，然后执行
<code>kubectl label nodes &lt;node-name> &lt;label-key>=&lt;label-value></code>
命令将标签添加到你所选择的节点上。
例如，如果你的节点名称为 'kubernetes-foo-node-1.c.a-robinson.internal'
并且想要的标签是 'disktype=ssd'，则可以执行
<code>kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd</code> 命令。</p><p>你可以通过重新运行 <code>kubectl get nodes --show-labels</code>，
查看节点当前具有了所指定的标签来验证它是否有效。
你也可以使用 <code>kubectl describe node "nodename"</code> 命令查看指定节点的标签完整列表。</p><h3 id=步骤二-添加-nodeselector-字段到-pod-配置中>步骤二：添加 nodeSelector 字段到 Pod 配置中</h3><p>选择任何一个你想运行的 Pod 的配置文件，并且在其中添加一个 nodeSelector 部分。
例如，如果下面是我的 pod 配置：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></code></pre></div><p>然后像下面这样添加 nodeSelector：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/pods/pod-nginx.yaml download=pods/pod-nginx.yaml><code>pods/pod-nginx.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-nginx-yaml')" title="Copy pods/pod-nginx.yaml to clipboard"></img></div><div class=includecode id=pods-pod-nginx-yaml><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>disktype</span>:<span style=color:#bbb> </span>ssd<span style=color:#bbb>
</span></code></pre></div></div></div><p>当你之后运行 <code>kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml</code> 命令，
Pod 将会调度到将标签添加到的节点上。
你可以通过运行 <code>kubectl get pods -o wide</code> 并查看分配给 pod 的 “NODE” 来验证其是否有效。</p><h2 id=built-in-node-labels>插曲：内置的节点标签</h2><p>除了你<a href=#attach-labels-to-node>添加</a>的标签外，节点还预先填充了一组标准标签。
这些标签有：</p><ul><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname><code>kubernetes.io/hostname</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone><code>failure-domain.beta.kubernetes.io/zone</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesioregion><code>failure-domain.beta.kubernetes.io/region</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone><code>topology.kubernetes.io/zone</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone><code>topology.kubernetes.io/region</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#beta-kubernetes-io-instance-type><code>beta.kubernetes.io/instance-type</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type><code>node.kubernetes.io/instance-type</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-os><code>kubernetes.io/os</code></a></li><li><a href=/zh/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-arch><code>kubernetes.io/arch</code></a></li></ul><blockquote class="note callout"><div><strong>说明：</strong><p>这些标签的值是特定于云供应商的，因此不能保证可靠。
例如，<code>kubernetes.io/hostname</code> 的值在某些环境中可能与节点名称相同，
但在其他环境中可能是一个不同的值。</div></blockquote><h2 id=node-isolation-restriction>节点隔离/限制</h2><p>向 Node 对象添加标签可以将 pod 定位到特定的节点或节点组。
这可以用来确保指定的 Pod 只能运行在具有一定隔离性，安全性或监管属性的节点上。
当为此目的使用标签时，强烈建议选择节点上的 kubelet 进程无法修改的标签键。
这可以防止受感染的节点使用其 kubelet 凭据在自己的 Node 对象上设置这些标签，
并影响调度器将工作负载调度到受感染的节点。</p><p><code>NodeRestriction</code> 准入插件防止 kubelet 使用 <code>node-restriction.kubernetes.io/</code>
前缀设置或修改标签。要使用该标签前缀进行节点隔离：</p><ol><li>检查是否在使用 Kubernetes v1.11+，以便 NodeRestriction 功能可用。</li><li>确保你在使用<a href=/zh/docs/reference/access-authn-authz/node/>节点授权</a>并且已经_启用_
<a href=/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction 准入插件</a>。</li><li>将 <code>node-restriction.kubernetes.io/</code> 前缀下的标签添加到 Node 对象，
然后在节点选择器中使用这些标签。
例如，<code>example.com.node-restriction.kubernetes.io/fips=true</code> 或
<code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>。</li></ol><h2 id=亲和性与反亲和性>亲和性与反亲和性</h2><p><code>nodeSelector</code> 提供了一种非常简单的方法来将 Pod 约束到具有特定标签的节点上。
亲和性/反亲和性功能极大地扩展了你可以表达约束的类型。关键的增强点包括：</p><ol><li>语言更具表现力（不仅仅是“对完全匹配规则的 AND”）</li><li>你可以发现规则是“软需求”/“偏好”，而不是硬性要求，因此，
如果调度器无法满足该要求，仍然调度该 Pod</li><li>你可以使用节点上（或其他拓扑域中）的 Pod 的标签来约束，而不是使用
节点本身的标签，来允许哪些 pod 可以或者不可以被放置在一起。</li></ol><p>亲和性功能包含两种类型的亲和性，即“节点亲和性”和“Pod 间亲和性/反亲和性”。
节点亲和性就像现有的 <code>nodeSelector</code>（但具有上面列出的前两个好处），然而
Pod 间亲和性/反亲和性约束 Pod 标签而不是节点标签（在上面列出的第三项中描述，
除了具有上面列出的第一和第二属性）。</p><h3 id=node-affinity>节点亲和性</h3><p>节点亲和性概念上类似于 <code>nodeSelector</code>，它使你可以根据节点上的标签来约束
Pod 可以调度到哪些节点。</p><p>目前有两种类型的节点亲和性，分别为 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 和
<code>preferredDuringSchedulingIgnoredDuringExecution</code>。
你可以视它们为“硬需求”和“软需求”，意思是，前者指定了将 Pod 调度到一个节点上
<em>必须</em>满足的规则（就像 <code>nodeSelector</code> 但使用更具表现力的语法），
后者指定调度器将尝试执行但不能保证的<em>偏好</em>。
名称的“IgnoredDuringExecution”部分意味着，类似于 <code>nodeSelector</code> 的工作原理，
如果节点的标签在运行时发生变更，从而不再满足 Pod 上的亲和性规则，那么 Pod
将仍然继续在该节点上运行。
将来我们计划提供 <code>requiredDuringSchedulingRequiredDuringExecution</code>，
它将类似于 <code>requiredDuringSchedulingIgnoredDuringExecution</code>，
除了它会将 pod 从不再满足 pod 的节点亲和性要求的节点上驱逐。</p><p>因此，<code>requiredDuringSchedulingIgnoredDuringExecution</code> 的示例将是
“仅将 Pod 运行在具有 Intel CPU 的节点上”，而
<code>preferredDuringSchedulingIgnoredDuringExecution</code> 的示例为
“尝试将这组 Pod 运行在 XYZ 故障区域，如果这不可能的话，则允许一些
Pod 在其他地方运行”。</p><p>节点亲和性通过 PodSpec 的 <code>affinity</code> 字段下的 <code>nodeAffinity</code> 字段进行指定。</p><p>下面是一个使用节点亲和性的 Pod 的实例：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/pods/pod-with-node-affinity.yaml download=pods/pod-with-node-affinity.yaml><code>pods/pod-with-node-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-node-affinity-yaml')" title="Copy pods/pod-with-node-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-node-affinity-yaml><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/e2e-az-name<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- e2e-az1<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- e2e-az2<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>another-node-label-key<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- another-node-label-value<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:2.0</code></pre></div></div></div><p>此节点亲和性规则表示，Pod 只能放置在具有标签键 <code>kubernetes.io/e2e-az-name</code>
且标签值为 <code>e2e-az1</code> 或 <code>e2e-az2</code> 的节点上。
另外，在满足这些标准的节点中，具有标签键为 <code>another-node-label-key</code>
且标签值为 <code>another-node-label-value</code> 的节点应该优先使用。</p><p>你可以在上面的例子中看到 <code>In</code> 操作符的使用。新的节点亲和性语法支持下面的操作符：
<code>In</code>，<code>NotIn</code>，<code>Exists</code>，<code>DoesNotExist</code>，<code>Gt</code>，<code>Lt</code>。
你可以使用 <code>NotIn</code> 和 <code>DoesNotExist</code> 来实现节点反亲和性行为，或者使用
<a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>节点污点</a>
将 Pod 从特定节点中驱逐。</p><p>如果你同时指定了 <code>nodeSelector</code> 和 <code>nodeAffinity</code>，<em>两者</em>必须都要满足，
才能将 Pod 调度到候选节点上。</p><p>如果你指定了多个与 <code>nodeAffinity</code> 类型关联的 <code>nodeSelectorTerms</code>，则
<strong>如果其中一个</strong> <code>nodeSelectorTerms</code> 满足的话，pod将可以调度到节点上。</p><p>如果你指定了多个与 <code>nodeSelectorTerms</code> 关联的 <code>matchExpressions</code>，则
<strong>只有当所有</strong> <code>matchExpressions</code> 满足的话，Pod 才会可以调度到节点上。</p><p>如果你修改或删除了 pod 所调度到的节点的标签，Pod 不会被删除。
换句话说，亲和性选择只在 Pod 调度期间有效。</p><p><code>preferredDuringSchedulingIgnoredDuringExecution</code> 中的 <code>weight</code> 字段值的
范围是 1-100。
对于每个符合所有调度要求（资源请求、RequiredDuringScheduling 亲和性表达式等）
的节点，调度器将遍历该字段的元素来计算总和，并且如果节点匹配对应的
MatchExpressions，则添加“权重”到总和。
然后将这个评分与该节点的其他优先级函数的评分进行组合。
总分最高的节点是最优选的。</p><h4 id=node-affinity-per-scheduling-profile>逐个调度方案中设置节点亲和性</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>在配置多个<a href=/zh/docs/reference/scheduling/config/#multiple-profiles>调度方案</a>时，
你可以将某个方案与节点亲和性关联起来，如果某个调度方案仅适用于某组
特殊的节点时，这样做是很有用的。
要实现这点，可以在<a href=/zh/docs/reference/scheduling/config/>调度器配置</a>中为
<a href=/zh/docs/reference/scheduling/config/#scheduling-plugins><code>NodeAffinity</code> 插件</a>
添加 <code>addedAffinity</code> 参数。
例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>foo-scheduler<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeAffinity<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>addedAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>scheduler-profile<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                  </span>- foo<span style=color:#bbb>
</span></code></pre></div><p>这里的 <code>addedAffinity</code> 除遵从 Pod 规约中设置的节点亲和性之外，还
适用于将 <code>.spec.schedulerName</code> 设置为 <code>foo-scheduler</code>。</p><blockquote class="note callout"><div><strong>说明：</strong> DaemonSet 控制器<a href=/zh/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler>为 DaemonSet 创建 Pods</a>，
但该控制器不理会调度方案。因此，建议你保留一个调度方案，例如
<code>default-scheduler</code>，不要在其中设置 <code>addedAffinity</code>。
这样，DaemonSet 的 Pod 模板将会使用此调度器名称。
否则，DaemonSet 控制器所创建的某些 Pods 可能持续处于不可调度状态。</div></blockquote><h3 id=inter-pod-affinity-and-anti-affinity>pod 间亲和性与反亲和性</h3><p>Pod 间亲和性与反亲和性使你可以 <em>基于已经在节点上运行的 Pod 的标签</em> 来约束
Pod 可以调度到的节点，而不是基于节点上的标签。
规则的格式为“如果 X 节点上已经运行了一个或多个 满足规则 Y 的 Pod，
则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 节点”。
Y 表示一个具有可选的关联命令空间列表的 LabelSelector；
与节点不同，因为 Pod 是命名空间限定的（因此 Pod 上的标签也是命名空间限定的），
因此作用于 Pod 标签的标签选择算符必须指定选择算符应用在哪个命名空间。
从概念上讲，X 是一个拓扑域，如节点、机架、云供应商可用区、云供应商地理区域等。
你可以使用 <code>topologyKey</code> 来表示它，<code>topologyKey</code> 是节点标签的键以便系统
用来表示这样的拓扑域。
请参阅上面<a href=#built-in-node-labels>插曲：内置的节点标签</a>部分中列出的标签键。</p><blockquote class="note callout"><div><strong>说明：</strong><p>Pod 间亲和性与反亲和性需要大量的处理，这可能会显著减慢大规模集群中的调度。
我们不建议在超过数百个节点的集群中使用它们。</div></blockquote><blockquote class="note callout"><div><strong>说明：</strong><p>Pod 反亲和性需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配
<code>topologyKey</code>。如果某些或所有节点缺少指定的 <code>topologyKey</code> 标签，可能会导致意外行为。</div></blockquote><p>与节点亲和性一样，当前有两种类型的 Pod 亲和性与反亲和性，即
<code>requiredDuringSchedulingIgnoredDuringExecution</code> 和
<code>preferredDuringSchedulingIgnoredDuringExecution</code>，分别表示“硬性”与“软性”要求。
请参阅前面节点亲和性部分中的描述。
<code>requiredDuringSchedulingIgnoredDuringExecution</code> 亲和性的一个示例是
“将服务 A 和服务 B 的 Pod 放置在同一区域，因为它们之间进行大量交流”，而
<code>preferredDuringSchedulingIgnoredDuringExecution</code> 反亲和性的示例将是
“将此服务的 pod 跨区域分布”（硬性要求是说不通的，因为你可能拥有的
Pod 数多于区域数）。</p><p>Pod 间亲和性通过 PodSpec 中 <code>affinity</code> 字段下的 <code>podAffinity</code> 字段进行指定。
而 Pod 间反亲和性通过 PodSpec 中 <code>affinity</code> 字段下的 <code>podAntiAffinity</code> 字段进行指定。</p><h3 id=pod-使用-pod-亲和性-的示例>Pod 使用 pod 亲和性 的示例：</h3><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/pods/pod-with-pod-affinity.yaml download=pods/pod-with-pod-affinity.yaml><code>pods/pod-with-pod-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick="copyCode('pods-pod-with-pod-affinity-yaml')" title="Copy pods/pod-with-pod-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-pod-affinity-yaml><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- S1<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinityTerm</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- S2<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:2.0<span style=color:#bbb>
</span></code></pre></div></div></div><p>在这个 Pod 的亲和性配置定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则。
在此示例中，<code>podAffinity</code> 配置为 <code>requiredDuringSchedulingIgnoredDuringExecution</code>，
然而 <code>podAntiAffinity</code> 配置为 <code>preferredDuringSchedulingIgnoredDuringExecution</code>。
Pod 亲和性规则表示，仅当节点和至少一个已运行且有键为“security”且值为“S1”的标签
的 Pod 处于同一区域时，才可以将该 Pod 调度到节点上。
（更确切的说，如果节点 N 具有带有键 <code>topology.kubernetes.io/zone</code> 和某个值 V 的标签，
则 Pod 有资格在节点 N 上运行，以便集群中至少有一个节点具有键
<code>topology.kubernetes.io/zone</code> 和值为 V 的节点正在运行具有键“security”和值
“S1”的标签的 pod。）
Pod 反亲和性规则表示，如果节点处于 Pod 所在的同一可用区且具有键“security”和值“S2”的标签，
则该 pod 不应将其调度到该节点上。
（如果 <code>topologyKey</code> 为 <code>topology.kubernetes.io/zone</code>，则意味着当节点和具有键
“security”和值“S2”的标签的 Pod 处于相同的区域，Pod 不能被调度到该节点上。）
查阅<a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md>设计文档</a>
以获取 Pod 亲和性与反亲和性的更多样例，包括
<code>requiredDuringSchedulingIgnoredDuringExecution</code>
和 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 两种配置。</p><p>Pod 亲和性与反亲和性的合法操作符有 <code>In</code>，<code>NotIn</code>，<code>Exists</code>，<code>DoesNotExist</code>。</p><p>原则上，<code>topologyKey</code> 可以是任何合法的标签键。
然而，出于性能和安全原因，topologyKey 受到一些限制：</p><ol><li>对于亲和性与 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 要求的
Pod 反亲和性，<code>topologyKey</code> 不允许为空。</li><li>对于 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 要求的 Pod 反亲和性，
准入控制器 <code>LimitPodHardAntiAffinityTopology</code> 被引入来限制 <code>topologyKey</code>
不为 <code>kubernetes.io/hostname</code>。
如果你想使它可用于自定义拓扑结构，你必须修改准入控制器或者禁用它。</li><li>对于 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 要求的 Pod 反亲和性，
空的 <code>topologyKey</code> 被解释为“所有拓扑结构”（这里的“所有拓扑结构”限制为
<code>kubernetes.io/hostname</code>，<code>topology.kubernetes.io/zone</code> 和
<code>topology.kubernetes.io/region</code> 的组合）。</li><li>除上述情况外，<code>topologyKey</code> 可以是任何合法的标签键。</li></ol><p>除了 <code>labelSelector</code> 和 <code>topologyKey</code>，你也可以指定表示命名空间的
<code>namespaces</code> 队列，<code>labelSelector</code> 也应该匹配它
（这个与 <code>labelSelector</code> 和 <code>topologyKey</code> 的定义位于相同的级别）。
如果忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的命名空间。</p><p>所有与 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 亲和性与反亲和性
关联的 <code>matchExpressions</code> 必须满足，才能将 pod 调度到节点上。</p><h4 id=更实际的用例>更实际的用例</h4><p>Pod 间亲和性与反亲和性在与更高级别的集合（例如 ReplicaSets、StatefulSets、
Deployments 等）一起使用时，它们可能更加有用。
可以轻松配置一组应位于相同定义拓扑（例如，节点）中的工作负载。</p><h5 id=始终放置在相同节点上>始终放置在相同节点上</h5><p>在三节点集群中，一个 web 应用程序具有内存缓存，例如 redis。
我们希望 web 服务器尽可能与缓存放置在同一位置。</p><p>下面是一个简单 redis Deployment 的 YAML 代码段，它有三个副本和选择器标签 <code>app=store</code>。
Deployment 配置了 <code>PodAntiAffinity</code>，用来确保调度器不会将副本调度到单个节点上。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-cache<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-server<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis:3.2-alpine<span style=color:#bbb>
</span></code></pre></div><p>下面 webserver Deployment 的 YAML 代码段中配置了 <code>podAntiAffinity</code> 和 <code>podAffinity</code>。
这将通知调度器将它的所有副本与具有 <code>app=store</code> 选择器标签的 Pod 放置在一起。
这还确保每个 web 服务器副本不会调度到单个节点上。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- web-store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-app<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.16-alpine<span style=color:#bbb>
</span></code></pre></div><p>如果我们创建了上面的两个 Deployment，我们的三节点集群将如下表所示。</p><table><thead><tr><th style=text-align:center>node-1</th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center><em>webserver-1</em></td><td style=text-align:center><em>webserver-2</em></td><td style=text-align:center><em>webserver-3</em></td></tr><tr><td style=text-align:center><em>cache-1</em></td><td style=text-align:center><em>cache-2</em></td><td style=text-align:center><em>cache-3</em></td></tr></tbody></table><p>如你所见，<code>web-server</code> 的三个副本都按照预期那样自动放置在同一位置。</p><pre><code>kubectl get pods -o wide
</code></pre><p>输出类似于如下内容：</p><pre><code>NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
redis-cache-1450370735-6dzlj   1/1       Running   0          8m        10.192.4.2   kube-node-3
redis-cache-1450370735-j2j96   1/1       Running   0          8m        10.192.2.2   kube-node-1
redis-cache-1450370735-z73mh   1/1       Running   0          8m        10.192.3.1   kube-node-2
web-server-1287567482-5d4dz    1/1       Running   0          7m        10.192.2.3   kube-node-1
web-server-1287567482-6f7v5    1/1       Running   0          7m        10.192.4.3   kube-node-3
web-server-1287567482-s330j    1/1       Running   0          7m        10.192.3.2   kube-node-2
</code></pre><h5 id=永远不放置在相同节点>永远不放置在相同节点</h5><p>上面的例子使用 <code>PodAntiAffinity</code> 规则和 <code>topologyKey: "kubernetes.io/hostname"</code>
来部署 redis 集群以便在同一主机上没有两个实例。
参阅 <a href=/zh/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure>ZooKeeper 教程</a>，
以获取配置反亲和性来达到高可用性的 StatefulSet 的样例（使用了相同的技巧）。</p><h2 id=nodename>nodeName</h2><p><code>nodeName</code> 是节点选择约束的最简单方法，但是由于其自身限制，通常不使用它。
<code>nodeName</code> 是 PodSpec 的一个字段。
如果它不为空，调度器将忽略 Pod，并且给定节点上运行的 kubelet 进程尝试执行该 Pod。
因此，如果 <code>nodeName</code> 在 PodSpec 中指定了，则它优先于上面的节点选择方法。</p><p>使用 <code>nodeName</code> 来选择节点的一些限制：</p><ul><li>如果指定的节点不存在，</li><li>如果指定的节点没有资源来容纳 Pod，Pod 将会调度失败并且其原因将显示为，
比如 OutOfmemory 或 OutOfcpu。</li><li>云环境中的节点名称并非总是可预测或稳定的。</li></ul><p>下面的是使用 <code>nodeName</code> 字段的 Pod 配置文件的例子：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></code></pre></div><p>上面的 pod 将运行在 kube-01 节点上。</p><h2 id=接下来>接下来</h2><p><a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>污点</a>
允许节点<em>排斥</em>一组 Pod。</p><p><a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md>节点亲和性</a>与
<a href=https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md>pod 间亲和性/反亲和性</a>
的设计文档包含这些功能的其他背景信息。</p><p>一旦 Pod 分配给 节点，kubelet 应用将运行该 pod 并且分配节点本地资源。
<a href=/zh/docs/tasks/administer-cluster/topology-manager/>拓扑管理器</a>
可以参与到节点级别的资源分配决定中。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-961126cd43559012893979e568396a49>5 - 扩展资源的资源装箱</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.16 [alpha]</code></div><p>使用 <code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数，可以将 kube-scheduler
配置为支持包含扩展资源在内的资源装箱操作。
优先级函数可用于根据自定义需求微调 kube-scheduler 。</p><h2 id=使用-requestedtocapacityratioresourceallocation-启用装箱>使用 RequestedToCapacityRatioResourceAllocation 启用装箱</h2><p>在 Kubernetes 1.15 之前，Kube-scheduler 通常允许根据对主要资源（如 CPU 和内存）
的请求数量和可用容量 之比率对节点评分。
Kubernetes 1.16 在优先级函数中添加了一个新参数，该参数允许用户指定资源以及每类资源的权重，
以便根据请求数量与可用容量之比率为节点评分。
这就使得用户可以通过使用适当的参数来对扩展资源执行装箱操作，从而提高了大型集群中稀缺资源的利用率。
<code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数的行为可以通过名为
<code>requestedToCapacityRatioArguments</code> 的配置选项进行控制。
该标志由两个参数 <code>shape</code> 和 <code>resources</code> 组成。
<code>shape</code> 允许用户根据 <code>utilization</code> 和 <code>score</code> 值将函数调整为最少请求
（least requested）或
最多请求（most requested）计算。
<code>resources</code> 包含由 <code>name</code> 和 <code>weight</code> 组成，<code>name</code> 指定评分时要考虑的资源，
<code>weight</code> 指定每种资源的权重。</p><p>以下是一个配置示例，该配置将 <code>requestedToCapacityRatioArguments</code> 设置为对扩展资源
<code>intel.com/foo</code> 和 <code>intel.com/bar</code> 的装箱行为</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
  <span>...</span>
  <span style=color:green;font-weight:700>&#34;priorities&#34;</span>: [
     <span>...</span>
    {
      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;RequestedToCapacityRatioPriority&#34;</span>,
      <span style=color:green;font-weight:700>&#34;weight&#34;</span>: <span style=color:#666>2</span>,
      <span style=color:green;font-weight:700>&#34;argument&#34;</span>: {
        <span style=color:green;font-weight:700>&#34;requestedToCapacityRatioArguments&#34;</span>: {
          <span style=color:green;font-weight:700>&#34;shape&#34;</span>: [
            {<span style=color:green;font-weight:700>&#34;utilization&#34;</span>: <span style=color:#666>0</span>, <span style=color:green;font-weight:700>&#34;score&#34;</span>: <span style=color:#666>0</span>},
            {<span style=color:green;font-weight:700>&#34;utilization&#34;</span>: <span style=color:#666>100</span>, <span style=color:green;font-weight:700>&#34;score&#34;</span>: <span style=color:#666>10</span>}
          ],
          <span style=color:green;font-weight:700>&#34;resources&#34;</span>: [
            {<span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;intel.com/foo&#34;</span>, <span style=color:green;font-weight:700>&#34;weight&#34;</span>: <span style=color:#666>3</span>},
            {<span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;intel.com/bar&#34;</span>, <span style=color:green;font-weight:700>&#34;weight&#34;</span>: <span style=color:#666>5</span>}
          ]
        }
      }
    }
  ],
}
</code></pre></div><p><strong>默认情况下此功能处于被禁用状态</strong></p><h3 id=调整-requestedtocapacityratioresourceallocation-优先级函数>调整 RequestedToCapacityRatioResourceAllocation 优先级函数</h3><p><code>shape</code> 用于指定 <code>RequestedToCapacityRatioPriority</code> 函数的行为。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb> </span>{<span style=color:green;font-weight:700>&#34;utilization&#34;: 0, &#34;score&#34;: </span><span style=color:#666>0</span>},<span style=color:#bbb>
</span><span style=color:#bbb> </span>{<span style=color:green;font-weight:700>&#34;utilization&#34;: 100, &#34;score&#34;: </span><span style=color:#666>10</span>}<span style=color:#bbb>
</span></code></pre></div><p>上面的参数在 <code>utilization</code> 为 0% 时给节点评分为 0，在 <code>utilization</code> 为
100% 时给节点评分为 10，因此启用了装箱行为。
要启用最少请求（least requested）模式，必须按如下方式反转得分值。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb> </span>{<span style=color:green;font-weight:700>&#34;utilization&#34;: 0, &#34;score&#34;: </span><span style=color:#666>10</span>},<span style=color:#bbb>
</span><span style=color:#bbb> </span>{<span style=color:green;font-weight:700>&#34;utilization&#34;: 100, &#34;score&#34;: </span><span style=color:#666>0</span>}<span style=color:#bbb>
</span></code></pre></div><p><code>resources</code> 是一个可选参数，默认情况下设置为：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>&#34;resources&#34;: </span>[<span style=color:#bbb>
</span><span style=color:#bbb>    </span>{<span style=color:green;font-weight:700>&#34;name&#34;: &#34;CPU&#34;, &#34;weight&#34;: </span><span style=color:#666>1</span>},<span style=color:#bbb>
</span><span style=color:#bbb>    </span>{<span style=color:green;font-weight:700>&#34;name&#34;: &#34;Memory&#34;, &#34;weight&#34;: </span><span style=color:#666>1</span>}<span style=color:#bbb>
</span><span style=color:#bbb></span>]<span style=color:#bbb>
</span></code></pre></div><p>它可以用来添加扩展资源，如下所示：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>&#34;resources&#34;: </span>[<span style=color:#bbb>
</span><span style=color:#bbb>    </span>{<span style=color:green;font-weight:700>&#34;name&#34;: &#34;intel.com/foo&#34;, &#34;weight&#34;: </span><span style=color:#666>5</span>},<span style=color:#bbb>
</span><span style=color:#bbb>    </span>{<span style=color:green;font-weight:700>&#34;name&#34;: &#34;CPU&#34;, &#34;weight&#34;: </span><span style=color:#666>3</span>},<span style=color:#bbb>
</span><span style=color:#bbb>    </span>{<span style=color:green;font-weight:700>&#34;name&#34;: &#34;Memory&#34;, &#34;weight&#34;: </span><span style=color:#666>1</span>}<span style=color:#bbb>
</span><span style=color:#bbb></span>]<span style=color:#bbb>
</span></code></pre></div><p>weight 参数是可选的，如果未指定，则设置为 1。
同时，weight 不能设置为负值。</p><h3 id=requestedtocapacityratioresourceallocation-优先级函数如何对节点评分>RequestedToCapacityRatioResourceAllocation 优先级函数如何对节点评分</h3><p>本节适用于希望了解此功能的内部细节的人员。
以下是如何针对给定的一组值来计算节点得分的示例。</p><pre><code>请求的资源

intel.com/foo: 2
Memory: 256MB
CPU: 2

资源权重

intel.com/foo: 5
Memory: 1
CPU: 3

FunctionShapePoint {{0, 0}, {100, 10}}

节点 Node 1 配置

可用：
  intel.com/foo : 4
  Memory : 1 GB
  CPU: 8

已用：
  intel.com/foo: 1
  Memory: 256MB
  CPU: 1

节点得分：

intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75
               = rawScoringFunction(75)
               = 7

Memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50
               = rawScoringFunction(50)
               = 5

CPU            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5
               = rawScoringFunction(37.5)
               = 3

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5


节点 Node 2 配置

可用：
  intel.com/foo: 8
  Memory: 1GB
  CPU: 8

已用：
  intel.com/foo: 2
  Memory: 512MB
  CPU: 6

节点得分：

intel.com/foo  = resourceScoringFunction((2+2),8)
               = (100 - ((8-4)*100/8)
               = (100 - 50)
               = 50
               = rawScoringFunction(50)
               = 5

Memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

CPU            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-45b5702b685c32798723d679c86aad02>6 - 驱逐策略</h1><p>本页提供 Kubernetes 驱逐策略的概览。</p><h2 id=eviction-policy>驱逐策略</h2><p><a class=glossary-tooltip title="一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。" data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> 主动监测和防止
计算资源的全面短缺。在资源短缺时，<code>kubelet</code> 可以主动地结束一个或多个 Pod
以回收短缺的资源。
当 <code>kubelet</code> 结束一个 Pod 时，它将终止 Pod 中的所有容器，而 Pod 的 <code>Phase</code>
将变为 <code>Failed</code>。
如果被驱逐的 Pod 由 Deployment 管理，这个 Deployment 会创建另一个 Pod 给
Kubernetes 来调度。</p><h2 id=接下来>接下来</h2><ul><li>阅读<a href=/zh/docs/tasks/administer-cluster/out-of-resource/>配置资源不足的处理</a>，
进一步了解驱逐信号和阈值。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-602208c95fe7b1f1170310ce993f5814>7 - 调度框架</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.15 [alpha]</code></div><p>调度框架是 Kubernetes Scheduler 的一种可插入架构，可以简化调度器的自定义。
它向现有的调度器增加了一组新的“插件” API。插件被编译到调度器程序中。
这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。
请参考<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md>调度框架的设计提案</a>
获取框架设计的更多技术信息。</p><h1 id=框架工作流程>框架工作流程</h1><p>调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。
这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。</p><p>每次调度一个 Pod 的尝试都分为两个阶段，即 <strong>调度周期</strong> 和 <strong>绑定周期</strong>。</p><h2 id=调度周期和绑定周期>调度周期和绑定周期</h2><p>调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。
调度周期和绑定周期一起被称为“调度上下文”。</p><p>调度周期是串行运行的，而绑定周期可能是同时运行的。</p><p>如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。
Pod 将返回队列并重试。</p><h2 id=扩展点>扩展点</h2><p>下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。
在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。</p><p>一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。</p><figure><img src=/images/docs/scheduling-framework-extensions.png><figcaption><h4>调度框架扩展点</h4></figcaption></figure><h3 id=queue-sort>队列排序</h3><p>队列排序插件用于对调度队列中的 Pod 进行排序。
队列排序插件本质上提供 <code>less(Pod1, Pod2)</code> 函数。
一次只能启动一个队列插件。</p><h3 id=pre-filter>前置过滤</h3><p>前置过滤插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。
如果 PreFilter 插件返回错误，则调度周期将终止。</p><h3 id=过滤>过滤</h3><p>过滤插件用于过滤出不能运行该 Pod 的节点。对于每个节点，
调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，
则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。</p><h3 id=post-filter>后置过滤</h3><p>这些插件在筛选阶段后调用，但仅在该 Pod 没有可行的节点时调用。
插件按其配置的顺序调用。如果任何后过滤器插件标记节点为“可调度”，
则其余的插件不会调用。典型的后筛选实现是抢占，试图通过抢占其他 Pod
的资源使该 Pod 可以调度。</p><h3 id=pre-score>前置评分</h3><p>前置评分插件用于执行 “前置评分” 工作，即生成一个可共享状态供评分插件使用。
如果 PreScore 插件返回错误，则调度周期将终止。</p><h3 id=scoring>评分</h3><p>评分插件用于对通过过滤阶段的节点进行排名。调度器将为每个节点调用每个评分插件。
将有一个定义明确的整数范围，代表最小和最大分数。
在<a href=#normalize-scoring>标准化评分</a>阶段之后，调度器将根据配置的插件权重
合并所有插件的节点分数。</p><h3 id=normalize-scoring>标准化评分</h3><p>标准化评分插件用于在调度器计算节点的排名之前修改分数。
在此扩展点注册的插件将使用同一插件的<a href=#scoring>评分</a> 结果被调用。
每个插件在每个调度周期调用一次。</p><p>例如，假设一个 <code>BlinkingLightScorer</code> 插件基于具有的闪烁指示灯数量来对节点进行排名。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>ScoreNode</span>(_ <span style=color:#666>*</span>v1.pod, n <span style=color:#666>*</span>v1.Node) (<span style=color:#0b0;font-weight:700>int</span>, <span style=color:#0b0;font-weight:700>error</span>) {
   <span style=color:#a2f;font-weight:700>return</span> <span style=color:#00a000>getBlinkingLightCount</span>(n)
}
</code></pre></div><p>然而，最大的闪烁灯个数值可能比 <code>NodeScoreMax</code> 小。要解决这个问题，
<code>BlinkingLightScorer</code> 插件还应该注册该扩展点。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>NormalizeScores</span>(scores <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>int</span>) {
   highest <span style=color:#666>:=</span> <span style=color:#666>0</span>
   <span style=color:#a2f;font-weight:700>for</span> _, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
      highest = <span style=color:#00a000>max</span>(highest, score)
   }
   <span style=color:#a2f;font-weight:700>for</span> node, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
      scores[node] = score<span style=color:#666>*</span>NodeScoreMax<span style=color:#666>/</span>highest
   }
}
</code></pre></div><p>如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。</p><blockquote class="note callout"><div><strong>说明：</strong> 希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。</div></blockquote><h3 id=reserve>Reserve</h3><p>Reserve 是一个信息性的扩展点。
管理运行时状态的插件（也成为“有状态插件”）应该使用此扩展点，以便
调度器在节点给指定 Pod 预留了资源时能够通知该插件。
这是在调度器真正将 Pod 绑定到节点之前发生的，并且它存在是为了防止
在调度器等待绑定成功时发生竞争情况。</p><p>这个是调度周期的最后一步。
一旦 Pod 处于保留状态，它将在绑定周期结束时触发<a href=#unreserve>不保留</a> 插件
（失败时）或 <a href=#post-bind>绑定后</a> 插件（成功时）。</p><h3 id=permit>Permit</h3><p><em>Permit</em> 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。
一个允许插件可以做以下三件事之一：</p><ol><li><strong>批准</strong><br>一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。</li></ol><ol><li><strong>拒绝</strong><br>如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。
这将触发<a href=#unreserve>Unreserve</a> 插件。</li></ol><ol><li><strong>等待</strong>（带有超时）<br>如果一个 Permit 插件返回 “等待” 结果，则 Pod 将保持在一个内部的 “等待中”
的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到
<a href=#frameworkhandle>批准</a>。如果超时发生，<strong>等待</strong> 变成 <strong>拒绝</strong>，并且 Pod
将返回调度队列，从而触发 <a href=#unreserve>Unreserve</a> 插件。</li></ol><blockquote class="note callout"><div><strong>说明：</strong> 尽管任何插件可以访问 “等待中” 状态的 Pod 列表并批准它们
(查看 <a href=#frameworkhandle><code>FrameworkHandle</code></a>)。
我们希望只有允许插件可以批准处于 “等待中” 状态的预留 Pod 的绑定。
一旦 Pod 被批准了，它将发送到<a href=#pre-bind>预绑定</a> 阶段。</div></blockquote><h3 id=pre-bind>预绑定</h3><p>预绑定插件用于执行 Pod 绑定前所需的任何工作。
例如，一个预绑定插件可能需要提供网络卷并且在允许 Pod 运行在该节点之前
将其挂载到目标节点上。</p><p>如果任何 PreBind 插件返回错误，则 Pod 将被<a href=#unreserve>拒绝</a> 并且
退回到调度队列中。</p><h3 id=bind>Bind</h3><p>Bind 插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，Bind 插件才会被调用。
各绑定插件按照配置顺序被调用。绑定插件可以选择是否处理指定的 Pod。
如果绑定插件选择处理 Pod，<strong>剩余的绑定插件将被跳过</strong>。</p><h3 id=post-bind>绑定后</h3><p>这是个信息性的扩展点。
绑定后插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。</p><h3 id=unreserve>Unreserve</h3><p>这是个信息性的扩展点。
如果 Pod 被保留，然后在后面的阶段中被拒绝，则 Unreserve 插件将被通知。
Unreserve 插件应该清楚保留 Pod 的相关状态。</p><p>使用此扩展点的插件通常也使用<a href=#reserve>Reserve</a>。</p><h2 id=插件-api>插件 API</h2><p>插件 API 分为两个步骤。首先，插件必须完成注册并配置，然后才能使用扩展点接口。
扩展点接口具有以下形式。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#a2f;font-weight:700>type</span> Plugin <span style=color:#a2f;font-weight:700>interface</span> {
   <span style=color:#00a000>Name</span>() <span style=color:#0b0;font-weight:700>string</span>
}

<span style=color:#a2f;font-weight:700>type</span> QueueSortPlugin <span style=color:#a2f;font-weight:700>interface</span> {
   Plugin
   <span style=color:#00a000>Less</span>(<span style=color:#666>*</span>v1.pod, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>bool</span>
}

<span style=color:#a2f;font-weight:700>type</span> PreFilterPlugin <span style=color:#a2f;font-weight:700>interface</span> {
   Plugin
   <span style=color:#00a000>PreFilter</span>(context.Context, <span style=color:#666>*</span>framework.CycleState, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>error</span>
}

<span style=color:#080;font-style:italic>// ...
</span></code></pre></div><h1 id=插件配置>插件配置</h1><p>你可以在调度器配置中启用或禁用插件。
如果你在使用 Kubernetes v1.18 或更高版本，大部分调度
<a href=/zh/docs/reference/scheduling/config/#scheduling-plugins>插件</a>
都在使用中且默认启用。</p><p>除了默认的插件，你还可以实现自己的调度插件并且将它们与默认插件一起配置。
你可以访问<a href=https://github.com/kubernetes-sigs/scheduler-plugins>scheduler-plugins</a>
了解更多信息。</p><p>如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为
一个调度器配置文件，然后定义不同的配置文件来满足各类工作负载。
了解更多关于<a href=/zh/docs/reference/scheduling/config/#multiple-profiles>多配置文件</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d9574a30fcbc631b0d2a57850e161e89>8 - 调度器性能调优</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.14 [beta]</code></div><p>作为 kubernetes 集群的默认调度器，
<a href=/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler>kube-scheduler</a>
主要负责将 Pod 调度到集群的 Node 上。</p><p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度</em> Node。
调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 打分，
之后选出其中得分最高的 Node 来运行 Pod。
最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 <em>绑定（Binding）</em>。</p><p>这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。</p><p>在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）
和精度（调度器很少做出糟糕的放置决策）。</p><p>你可以通过设置 kube-scheduler 的 <code>percentageOfNodesToScore</code> 来配置这个调优设置。
这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。</p><h3 id=设置阈值>设置阈值</h3><p><code>percentageOfNodesToScore</code> 选项接受从 0 到 100 之间的整数值。
0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。
如果你设置 <code>percentageOfNodesToScore</code> 的值超过了 100，
kube-scheduler 的表现等价于设置值为 100。</p><p>要修改这个值，编辑 kube-scheduler 的配置文件
（通常是 <code>/etc/kubernetes/config/kube-scheduler.yaml</code>），
然后重启调度器。</p><p>修改完成后，你可以执行</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get pods -n kube-system | grep kube-scheduler
</code></pre></div><p>来检查该 kube-scheduler 组件是否健康。</p><h2 id=percentage-of-nodes-to-score>节点打分阈值</h2><p>要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。
在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。</p><p>你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。
kube-scheduler 会将它转换为节点数的整数值。在调度期间，如果
kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，
kube-scheduler 将停止继续查找可调度节点并继续进行
<a href=/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation>打分阶段</a>。</p><p><a href=#how-the-scheduler-iterates-over-nodes>调度器如何遍历节点</a> 详细介绍了这个过程。</p><h3 id=默认阈值>默认阈值</h3><p>如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-节点集群
下取 50%，在 5000-节点的集群下取 10%。这个自动设置的参数的最低值是 5%。</p><p>这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。</p><p>如果你想让调度器对集群内所有节点进行打分，则将 <code>percentageOfNodesToScore</code> 设置为 100。</p><h2 id=示例>示例</h2><p>下面就是一个将 <code>percentageOfNodesToScore</code> 参数设置为 50% 的例子。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>algorithmSource</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>provider</span>:<span style=color:#bbb> </span>DefaultProvider<span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>percentageOfNodesToScore</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></code></pre></div><h3 id=调节-percentageofnodestoscore-参数>调节 percentageOfNodesToScore 参数</h3><p><code>percentageOfNodesToScore</code> 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。
另外，还有一个 50 个 Node 的最小值是硬编码在程序中。</p><blockquote class="note callout"><div><strong>说明：</strong><p>当集群中的可调度节点少于 50 个时，调度器仍然会去检查所有的 Node，
因为可调度节点太少，不足以停止调度器最初的过滤选择。</p><p>同理，在小规模集群中，如果你将 <code>percentageOfNodesToScore</code> 设置为
一个较低的值，则没有或者只有很小的效果。</p><p>如果集群只有几百个节点或者更少，请保持这个配置的默认值。
改变基本不会对调度器的性能有明显的提升。</p></div></blockquote><p>值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，
很多节点都没有进入到打分阶段。这样就会造成一种后果，
一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段。</p><p>由于这个原因，这个参数不应该被设置成一个很低的值。
通常的做法是不会将这个参数的值设置的低于 10。
很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用。
换句话说，只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时，
才使用很低的参数设置。</p><h3 id=how-the-scheduler-iterates-over-nodes>调度器做调度选择的时候如何覆盖所有的 Node</h3><p>如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。</p><p>在将 Pod 调度到节点上时，为了让集群中所有节点都有公平的机会去运行这些 Pod，
调度器将会以轮询的方式覆盖全部的 Node。
你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，
依次向后直到可调度节点的数量达到 <code>percentageOfNodesToScore</code> 参数的要求。
在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，
将会来作为这次调度筛选 Node 开始的位置。</p><p>如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，
来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：</p><pre><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>调度器将会按照如下的顺序去评估 Node 的可调度性：</p><pre><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>在评估完所有 Node 后，将会返回到 Node 1，从头开始。</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh/docs/home/>主页</a>
<a class=text-white href=/zh/blog/>博客</a>
<a class=text-white href=/zh/training/>培训</a>
<a class=text-white href=/zh/partners/>合作伙伴</a>
<a class=text-white href=/zh/community/>社区</a>
<a class=text-white href=/zh/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>