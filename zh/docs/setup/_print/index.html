<!doctype html><html lang=zh class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/setup/><link rel=alternate hreflang=pt href=https://kubernetes.io/pt/docs/setup/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/setup/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/setup/><link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/setup/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh/docs/setup/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>入门 | Kubernetes</title><meta property="og:title" content="入门"><meta property="og:description" content="生产级别的容器编排系统"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh/docs/setup/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="入门"><meta itemprop=description content="生产级别的容器编排系统"><meta name=twitter:card content="summary"><meta name=twitter:title content="入门"><meta name=twitter:description content="生产级别的容器编排系统"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="本节介绍了设置和运行 Kubernetes 环境的不同选项。
不同的 Kubernetes 解决方案满足不同的要求：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以在本地机器、云、本地数据中心上部署 Kubernetes 集群，或选择一个托管的 Kubernetes 集群。还可以跨各种云提供商或裸机环境创建自定义解决方案。
更简单地说，可以在学习和生产环境中创建一个 Kubernetes 集群。
学习环境 如果正打算学习 Kubernetes，请使用基于 Docker 的解决方案：Docker 是 Kubernetes 社区支持或生态系统中用来在本地计算机上设置 Kubernetes 集群的一种工具。
Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes.   Community Ecosystem     Minikube Docker Desktop   kind (Kubernetes IN Docker) Minishift    MicroK8s   -->     Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes."><meta property="og:description" content="本节介绍了设置和运行 Kubernetes 环境的不同选项。
不同的 Kubernetes 解决方案满足不同的要求：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以在本地机器、云、本地数据中心上部署 Kubernetes 集群，或选择一个托管的 Kubernetes 集群。还可以跨各种云提供商或裸机环境创建自定义解决方案。
更简单地说，可以在学习和生产环境中创建一个 Kubernetes 集群。
学习环境 如果正打算学习 Kubernetes，请使用基于 Docker 的解决方案：Docker 是 Kubernetes 社区支持或生态系统中用来在本地计算机上设置 Kubernetes 集群的一种工具。
Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes.   Community Ecosystem     Minikube Docker Desktop   kind (Kubernetes IN Docker) Minishift    MicroK8s   -->     Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes."><meta name=twitter:description content="本节介绍了设置和运行 Kubernetes 环境的不同选项。
不同的 Kubernetes 解决方案满足不同的要求：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以在本地机器、云、本地数据中心上部署 Kubernetes 集群，或选择一个托管的 Kubernetes 集群。还可以跨各种云提供商或裸机环境创建自定义解决方案。
更简单地说，可以在学习和生产环境中创建一个 Kubernetes 集群。
学习环境 如果正打算学习 Kubernetes，请使用基于 Docker 的解决方案：Docker 是 Kubernetes 社区支持或生态系统中用来在本地计算机上设置 Kubernetes 集群的一种工具。
Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes.   Community Ecosystem     Minikube Docker Desktop   kind (Kubernetes IN Docker) Minishift    MicroK8s   -->     Local machine solutions table that lists the tools supported by the community and the ecosystem to deploy Kubernetes."><meta property="og:url" content="https://kubernetes.io/zh/docs/setup/"><meta property="og:title" content="入门"><meta name=twitter:title content="入门"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh/docs/>文档</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/blog/>Kubernetes 博客</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/training/>培训</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/partners/>合作伙伴</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/community/>社区</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/case-studies/>案例分析</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/zh/docs/setup/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh/docs/setup/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh/docs/setup/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh/docs/setup/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/zh/docs/setup/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 Chinese</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/>English</a>
<a class=dropdown-item href=/ko/docs/setup/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/setup/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/setup/>Français</a>
<a class=dropdown-item href=/de/docs/setup/>Deutsch</a>
<a class=dropdown-item href=/pt/docs/setup/>Português</a>
<a class=dropdown-item href=/es/docs/setup/>Español</a>
<a class=dropdown-item href=/id/docs/setup/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/setup/>Русский</a>
<a class=dropdown-item href=/pl/docs/setup/>Polski</a>
<a class=dropdown-item href=/uk/docs/setup/>Українська</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh/docs/setup/>返回本页常规视图</a>.</p></div><h1 class=title>入门</h1><ul><li>1: <a href=#pg-d33663ac044e1981b406949f9124cc04>Kubernetes 发行说明和版本偏差</a></li><ul><li>1.1: <a href=#pg-a49e4163901749c7ce817c73e249f1bc>v1.18 发布说明</a></li><li>1.2: <a href=#pg-85b7e96ac42e5e28ec570ad43f0ef5cd>Kubernetes 版本及版本偏差支持策略</a></li></ul><li>2: <a href=#pg-0b597086a9d1382f86abadcfeab657d6>学习环境</a></li><ul></ul><li>3: <a href=#pg-4e14853fdaa3bd273f31a60112b9b5ac>生产环境</a></li><ul><li>3.1: <a href=#pg-a77d3feb6e6d9978f32fa14622642e9a>容器运行时</a></li><li>3.2: <a href=#pg-d2f55eefe7222b7c637875af9c3ec199>Turnkey 云解决方案</a></li><li>3.3: <a href=#pg-00e1646f68aeb89f9722cf6f6cfcad94>使用部署工具安装 Kubernetes</a></li><ul><li>3.3.1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>使用 kubeadm 引导集群</a></li><ul><li>3.3.1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>安装 kubeadm</a></li><li>3.3.1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>对 kubeadm 进行故障排查</a></li><li>3.3.1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>使用 kubeadm 创建集群</a></li><li>3.3.1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>使用 kubeadm 定制控制平面配置</a></li><li>3.3.1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>高可用拓扑选项</a></li><li>3.3.1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>利用 kubeadm 创建高可用集群</a></li><li>3.3.1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>使用 kubeadm 创建一个高可用 etcd 集群</a></li><li>3.3.1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>使用 kubeadm 配置集群中的每个 kubelet</a></li><li>3.3.1.9: <a href=#pg-ed857e09999827b013ee9062dc9c59bb>配置您的 kubernetes 集群以自托管控制平台</a></li></ul><li>3.3.2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>使用 Kops 安装 Kubernetes</a></li><li>3.3.3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>使用 Kubespray 安装 Kubernetes</a></li></ul><li>3.4: <a href=#pg-acce7e24090fea04715a7a516ba3e69b>Windows Kubernetes</a></li><ul><li>3.4.1: <a href=#pg-a307d413f1f7430fced233023087e2a1>Kubernetes 对 Windows 的支持</a></li><li>3.4.2: <a href=#pg-3a51e66c5de55f9093a8dc55742006d3>Kubernetes 中调度 Windows 容器的指南</a></li></ul></ul><li>4: <a href=#pg-84b6491601d6a2b3da4cd5a105c866ba>最佳实践</a></li><ul><li>4.1: <a href=#pg-970615c97499e3651fd3a98e0387cefc>运行于多区环境</a></li><li>4.2: <a href=#pg-c797ee17120176c685455db89ae091a9>创建大型集群</a></li><li>4.3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>校验节点设置</a></li><li>4.4: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI 证书和要求</a></li></ul></ul><div class=content><p>本节介绍了设置和运行 Kubernetes 环境的不同选项。</p><p>不同的 Kubernetes 解决方案满足不同的要求：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。</p><p>可以在本地机器、云、本地数据中心上部署 Kubernetes 集群，或选择一个托管的 Kubernetes 集群。还可以跨各种云提供商或裸机环境创建自定义解决方案。</p><p>更简单地说，可以在学习和生产环境中创建一个 Kubernetes 集群。</p><h2 id=学习环境>学习环境</h2><p>如果正打算学习 Kubernetes，请使用基于 Docker 的解决方案：Docker 是 Kubernetes 社区支持或生态系统中用来在本地计算机上设置 Kubernetes 集群的一种工具。</p><h2 id=生产环境>生产环境</h2><p>在评估生产环境的解决方案时，请考虑要管理自己 Kubernetes 集群（<em>抽象层面</em>）的哪些方面或将其转移给提供商。</p><p><a href=https://kubernetes.io/zh/partners/#kcsp>Kubernetes 合作伙伴</a> 包括一个
<a href=https://github.com/cncf/k8s-conformance/#certified-kubernetes>已认证的 Kubernetes</a> 提供商列表。</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d33663ac044e1981b406949f9124cc04>1 - Kubernetes 发行说明和版本偏差</h1></div><div class=td-content><h1 id=pg-a49e4163901749c7ce817c73e249f1bc>1.1 - v1.18 发布说明</h1><h1 id=v1-18-0>v1.18.0</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0>Downloads for v1.18.0</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>cd5b86a3947a4f2cea6d857743ab2009be127d782b6f2eb4d37d88918a5e433ad2c7ba34221c34089ba5ba13701f58b657f0711401e51c86f4007cb78744dee7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>fb42cf133355ef18f67c8c4bb555aa1f284906c06e21fa41646e086d34ece774e9d547773f201799c0c703ce48d4d0e62c6ba5b2a4d081e12a339a423e111e52</code></td></tr></tbody></table><h3 id=client-binaries>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>26df342ef65745df12fa52931358e7f744111b6fe1e0bddb8c3c6598faf73af997c00c8f9c509efcd7cd7e82a0341a718c08fbd96044bfb58e80d997a6ebd3c2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>803a0fed122ef6b85f7a120b5485723eaade765b7bc8306d0c0da03bd3df15d800699d15ea2270bb7797fa9ce6a81da90e730dc793ea4ed8c0149b63d26eca30</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>110844511b70f9f3ebb92c15105e6680a05a562cd83f79ce2d2e25c2dd70f0dbd91cae34433f61364ae1ce4bd573b635f2f632d52de8f72b54acdbc95a15e3f0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>594ca3eadc7974ec4d9e4168453e36ca434812167ef8359086cd64d048df525b7bd46424e7cc9c41e65c72bda3117326ba1662d1c9d739567f10f5684fd85bee</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>d3627b763606557a6c9a5766c34198ec00b3a3cd72a55bc2cb47731060d31c4af93543fb53f53791062bb5ace2f15cbaa8592ac29009641e41bd656b0983a079</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>ba9056eff1452cbdaef699efbf88f74f5309b3f7808d372ebf6918442d0c9fea1653c00b9db3b7626399a460eef9b1fa9e29b827b7784f34561cbc380554e2ea</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>f80fb3769358cb20820ff1a1ce9994de5ed194aabe6c73fb8b8048bffc394d1b926de82c204f0e565d53ffe7562faa87778e97a3ccaaaf770034a992015e3a86</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>a9b658108b6803d60fa3cd4e76d9e58bf75201017164fe54054b7ccadbb68c4ad7ba7800746940bc518d90475e6c0a96965a26fa50882f4f0e56df404f4ae586</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>18adffab5d1be146906fd8531f4eae7153576aac235150ce2da05aee5ae161f6bd527e8dec34ae6131396cd4b3771e0d54ce770c065244ad3175a1afa63c89e1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>162396256429cef07154f817de2a6b67635c770311f414e38b1e2db25961443f05d7b8eb1f8da46dec8e31c5d1d2cd45f0c95dad1bc0e12a0a7278a62a0b9a6b</code></td></tr></tbody></table><h3 id=server-binaries>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>a92f8d201973d5dfa44a398e95fcf6a7b4feeb1ef879ab3fee1c54370e21f59f725f27a9c09ace8c42c96ac202e297fd458e486c489e05f127a5cade53b8d7c4</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>62fbff3256bc0a83f70244b09149a8d7870d19c2c4b6dee8ca2714fc7388da340876a0f540d2ae9bbd8b81fdedaf4b692c72d2840674db632ba2431d1df1a37d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>842910a7013f61a60d670079716b207705750d55a9e4f1f93696d19d39e191644488170ac94d8740f8e3aa3f7f28f61a4347f69d7e93d149c69ac0efcf3688fe</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>95c5b952ac1c4127a5c3b519b664972ee1fb5e8e902551ce71c04e26ad44b39da727909e025614ac1158c258dc60f504b9a354c5ab7583c2ad769717b30b3836</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>a46522d2119a0fd58074564c1fa95dd8a929a79006b82ba3c4245611da8d2db9fd785c482e1b61a9aa361c5c9a6d73387b0e15e6a7a3d84fffb3f65db3b9deeb</code></td></tr></tbody></table><h3 id=node-binaries>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>f714f80feecb0756410f27efb4cf4a1b5232be0444fbecec9f25cb85a7ccccdcb5be588cddee935294f460046c0726b90f7acc52b20eeb0c46a7200cf10e351a</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>806000b5f6d723e24e2f12d19d1b9b3d16c74b855f51c7063284adf1fcc57a96554a3384f8c05a952c6f6b929a05ed12b69151b1e620c958f74c9600f3db0fcb</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>c207e9ab60587d135897b5366af79efe9d2833f33401e469b2a4e0d74ecd2cf6bb7d1e5bc18d80737acbe37555707f63dd581ccc6304091c1d98dafdd30130b7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>a542ed5ed02722af44ef12d1602f363fcd4e93cf704da2ea5d99446382485679626835a40ae2ba47a4a26dce87089516faa54479a1cfdee2229e8e35aa1c17d7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>651e0db73ee67869b2ae93cb0574168e4bd7918290fc5662a6b12b708fa628282e3f64be2b816690f5a2d0f4ff8078570f8187e65dee499a876580a7a63d1d19</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>d726ed904f9f7fe7e8831df621dc9094b87e767410a129aa675ee08417b662ddec314e165f29ecb777110fbfec0dc2893962b6c71950897ba72baaa7eb6371ed</code></td></tr></tbody></table><h2 id=changelog-since-v1-17-0>Changelog since v1.17.0</h2><p>A complete changelog for the release notes is now hosted in a customizable
format at <a href="https://relnotes.k8s.io/?releaseVersions=1.18.0">https://relnotes.k8s.io</a>. Check it out and please give us your
feedback!</p><h2 id=what-s-new-major-themes>What’s New (Major Themes)</h2><h3 id=kubernetes-topology-manager-moves-to-beta-align-up>Kubernetes Topology Manager Moves to Beta - Align Up!</h3><p>A beta feature of Kubernetes in release 1.18, the <a href=https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md>Topology Manager feature</a> enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.</p><h3 id=serverside-apply-beta-2>Serverside Apply - Beta 2</h3><p>Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.</p><h3 id=extending-ingress-with-and-replacing-a-deprecated-annotation-with-ingressclass>Extending Ingress with and replacing a deprecated annotation with IngressClass</h3><p>In Kubernetes 1.18, there are two significant additions to Ingress: A new <code>pathType</code> field and a new <code>IngressClass</code> resource. The <code>pathType</code> field allows specifying how paths should be matched. In addition to the default <code>ImplementationSpecific</code> type, there are new <code>Exact</code> and <code>Prefix</code> path types.</p><p>The <code>IngressClass</code> resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new <code>ingressClassName</code> field on Ingresses. This new resource and field replace the deprecated <code>kubernetes.io/ingress.class</code> annotation.</p><h3 id=sig-cli-introduces-kubectl-debug>SIG CLI introduces kubectl debug</h3><p>SIG CLI was debating the need for a debug utility for quite some time already. With the development of <a href=/zh/docs/concepts/workloads/pods/ephemeral-containers/>ephemeral containers</a>, it became more obvious how we can support developers with tooling built on top of <code>kubectl exec</code>. The addition of the <code>kubectl debug</code> <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md>command</a> (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.</p><h3 id=introducing-windows-csi-support-alpha-for-kubernetes>Introducing Windows CSI support alpha for Kubernetes</h3><p>With the release of Kubernetes 1.18, an alpha version of CSI Proxy for Windows is getting released. CSI proxy enables non-privileged (pre-approved) containers to perform privileged storage operations on Windows. CSI drivers can now be supported in Windows by leveraging CSI proxy.
SIG Storage made a lot of progress in the 1.18 release.
In particular, the following storage features are moving to GA in Kubernetes 1.18:</p><ul><li>Raw Block Support: Allow volumes to be surfaced as block devices inside containers instead of just mounted filesystems.</li><li>Volume Cloning: Duplicate a PersistentVolumeClaim and underlying storage volume using the Kubernetes API via CSI.</li><li>CSIDriver Kubernetes API Object: Simplifies CSI driver discovery and allows CSI Drivers to customize Kubernetes behavior.</li></ul><p>SIG Storage is also introducing the following new storage features as alpha in Kubernetes 1.18:</p><ul><li>Windows CSI Support: Enabling containerized CSI node plugins in Windows via new <a href=https://github.com/kubernetes-csi/csi-proxy>CSIProxy</a></li><li>Recursive Volume Ownership OnRootMismatch Option: Add a new “OnRootMismatch” policy that can help shorten the mount time for volumes that require ownership change and have many directories and files.</li></ul><h3 id=other-notable-announcements>Other notable announcements</h3><p>SIG Network is moving IPv6 to Beta in Kubernetes 1.18, after incrementing significantly the test coverage with new CI jobs.</p><p>NodeLocal DNSCache is an add-on that runs a dnsCache pod as a daemonset to improve clusterDNS performance and reliability. The feature has been in Alpha since 1.13 release. The SIG Network is announcing the GA graduation of Node Local DNSCache <a href=https://github.com/kubernetes/enhancements/pull/1351>#1351</a></p><h2 id=known-issues>Known Issues</h2><p>No Known Issues Reported</p><h2 id=urgent-upgrade-notes>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade>(No, really, you MUST read this before you upgrade)</h3><h4 id=kube-apiserver>kube-apiserver:</h4><ul><li>in an <code>--encryption-provider-config</code> config file, an explicit <code>cacheSize: 0</code> parameter previously silently defaulted to caching 1000 keys. In Kubernetes 1.18, this now returns a config validation error. To disable caching, you can specify a negative cacheSize value in Kubernetes 1.18+.</li><li>consumers of the 'certificatesigningrequests/approval' API must now have permission to 'approve' CSRs for the specific signer requested by the CSR. More information on the new signerName field and the required authorization can be found at <a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests#authorization>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests#authorization</a> (<a href=https://github.com/kubernetes/kubernetes/pull/88246>#88246</a>, <a href=https://github.com/munnerz>@munnerz</a>) [SIG API Machinery, Apps, Auth, CLI, Node and Testing]</li><li>The following features are unconditionally enabled and the corresponding <code>--feature-gates</code> flags have been removed: <code>PodPriority</code>, <code>TaintNodesByCondition</code>, <code>ResourceQuotaScopeSelectors</code> and <code>ScheduleDaemonSetPods</code> (<a href=https://github.com/kubernetes/kubernetes/pull/86210>#86210</a>, <a href=https://github.com/draveness>@draveness</a>) [SIG Apps and Scheduling]</li></ul><h4 id=kubelet>kubelet:</h4><ul><li><code>--enable-cadvisor-endpoints</code> is now disabled by default. If you need access to the cAdvisor v1 Json API please enable it explicitly in the kubelet command line. Please note that this flag was deprecated in 1.15 and will be removed in 1.19. (<a href=https://github.com/kubernetes/kubernetes/pull/87440>#87440</a>, <a href=https://github.com/dims>@dims</a>) [SIG Instrumentation, Node and Testing]</li><li>Promote CSIMigrationOpenStack to Beta (off by default since it requires installation of the OpenStack Cinder CSI Driver. The in-tree AWS OpenStack Cinder driver "kubernetes.io/cinder" was deprecated in 1.16 and will be removed in 1.20. Users should enable CSIMigration + CSIMigrationOpenStack features and install the OpenStack Cinder CSI Driver (<a href=https://github.com/kubernetes-sigs/cloud-provider-openstack>https://github.com/kubernetes-sigs/cloud-provider-openstack</a>) to avoid disruption to existing Pod and PVC objects at that time. Users should start using the OpenStack Cinder CSI Driver directly for any new volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/85637>#85637</a>, <a href=https://github.com/dims>@dims</a>) [SIG Cloud Provider]</li></ul><h4 id=kubectl>kubectl:</h4><ul><li><code>kubectl</code> and k8s.io/client-go no longer default to a server address of <code>http://localhost:8080</code>. If you own one of these legacy clusters, you are <em>strongly</em> encouraged to secure your server. If you cannot secure your server, you can set the <code>$KUBERNETES_MASTER</code> environment variable to <code>http://localhost:8080</code> to continue defaulting the server address. <code>kubectl</code> users can also set the server address using the <code>--server</code> flag, or in a kubeconfig file specified via <code>--kubeconfig</code> or <code>$KUBECONFIG</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/86173>#86173</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG API Machinery, CLI and Testing]</li><li><code>kubectl run</code> has removed the previously deprecated generators, along with flags unrelated to creating pods. <code>kubectl run</code> now only creates pods. See specific <code>kubectl create</code> subcommands to create objects other than pods.
(<a href=https://github.com/kubernetes/kubernetes/pull/87077>#87077</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG Architecture, CLI and Testing]</li><li>The deprecated command <code>kubectl rolling-update</code> has been removed (<a href=https://github.com/kubernetes/kubernetes/pull/88057>#88057</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG Architecture, CLI and Testing]</li></ul><h4 id=client-go>client-go:</h4><ul><li><p>Signatures on methods in generated clientsets, dynamic, metadata, and scale clients have been modified to accept <code>context.Context</code> as a first argument. Signatures of Create, Update, and Patch methods have been updated to accept CreateOptions, UpdateOptions and PatchOptions respectively. Signatures of Delete and DeleteCollection methods now accept DeleteOptions by value instead of by reference. Generated clientsets with the previous interface have been added in new "deprecated" packages to allow incremental migration to the new APIs. The deprecated packages will be removed in the 1.21 release. A tool is available at <a href=http://sigs.k8s.io/clientgofix>http://sigs.k8s.io/clientgofix</a> to rewrite method invocations to the new signatures.</p></li><li><p>The following deprecated metrics are removed, please convert to the corresponding metrics:</p><ul><li>The following replacement metrics are available from v1.14.0:<ul><li><code>rest_client_request_latency_seconds</code> -> <code>rest_client_request_duration_seconds</code></li><li><code>scheduler_scheduling_latency_seconds</code> -> <code>scheduler_scheduling_duration_seconds</code></li><li><code>docker_operations</code> -> <code>docker_operations_total</code></li><li><code>docker_operations_latency_microseconds</code> -> <code>docker_operations_duration_seconds</code></li><li><code>docker_operations_errors</code> -> <code>docker_operations_errors_total</code></li><li><code>docker_operations_timeout</code> -> <code>docker_operations_timeout_total</code></li><li><code>network_plugin_operations_latency_microseconds</code> -> <code>network_plugin_operations_duration_seconds</code></li><li><code>kubelet_pod_worker_latency_microseconds</code> -> <code>kubelet_pod_worker_duration_seconds</code></li><li><code>kubelet_pod_start_latency_microseconds</code> -> <code>kubelet_pod_start_duration_seconds</code></li><li><code>kubelet_cgroup_manager_latency_microseconds</code> -> <code>kubelet_cgroup_manager_duration_seconds</code></li><li><code>kubelet_pod_worker_start_latency_microseconds</code> -> <code>kubelet_pod_worker_start_duration_seconds</code></li><li><code>kubelet_pleg_relist_latency_microseconds</code> -> <code>kubelet_pleg_relist_duration_seconds</code></li><li><code>kubelet_pleg_relist_interval_microseconds</code> -> <code>kubelet_pleg_relist_interval_seconds</code></li><li><code>kubelet_eviction_stats_age_microseconds</code> -> <code>kubelet_eviction_stats_age_seconds</code></li><li><code>kubelet_runtime_operations</code> -> <code>kubelet_runtime_operations_total</code></li><li><code>kubelet_runtime_operations_latency_microseconds</code> -> <code>kubelet_runtime_operations_duration_seconds</code></li><li><code>kubelet_runtime_operations_errors</code> -> <code>kubelet_runtime_operations_errors_total</code></li><li><code>kubelet_device_plugin_registration_count</code> -> <code>kubelet_device_plugin_registration_total</code></li><li><code>kubelet_device_plugin_alloc_latency_microseconds</code> -> <code>kubelet_device_plugin_alloc_duration_seconds</code></li><li><code>scheduler_e2e_scheduling_latency_microseconds</code> -> <code>scheduler_e2e_scheduling_duration_seconds</code></li><li><code>scheduler_scheduling_algorithm_latency_microseconds</code> -> <code>scheduler_scheduling_algorithm_duration_seconds</code></li><li><code>scheduler_scheduling_algorithm_predicate_evaluation</code> -> <code>scheduler_scheduling_algorithm_predicate_evaluation_seconds</code></li><li><code>scheduler_scheduling_algorithm_priority_evaluation</code> -> <code>scheduler_scheduling_algorithm_priority_evaluation_seconds</code></li><li><code>scheduler_scheduling_algorithm_preemption_evaluation</code> -> <code>scheduler_scheduling_algorithm_preemption_evaluation_seconds</code></li><li><code>scheduler_binding_latency_microseconds</code> -> <code>scheduler_binding_duration_seconds</code></li><li><code>kubeproxy_sync_proxy_rules_latency_microseconds</code> -> <code>kubeproxy_sync_proxy_rules_duration_seconds</code></li><li><code>apiserver_request_latencies</code> -> <code>apiserver_request_duration_seconds</code></li><li><code>apiserver_dropped_requests</code> -> <code>apiserver_dropped_requests_total</code></li><li><code>etcd_request_latencies_summary</code> -> <code>etcd_request_duration_seconds</code></li><li><code>apiserver_storage_transformation_latencies_microseconds</code> -> <code>apiserver_storage_transformation_duration_seconds</code></li><li><code>apiserver_storage_data_key_generation_latencies_microseconds</code> -> <code>apiserver_storage_data_key_generation_duration_seconds</code></li><li><code>apiserver_request_count</code> -> <code>apiserver_request_total</code></li><li><code>apiserver_request_latencies_summary</code></li></ul></li><li>The following replacement metrics are available from v1.15.0:<ul><li><code>apiserver_storage_transformation_failures_total</code> -> <code>apiserver_storage_transformation_operations_total</code> (<a href=https://github.com/kubernetes/kubernetes/pull/76496>#76496</a>, <a href=https://github.com/danielqsj>@danielqsj</a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation, Network, Node and Scheduling]</li></ul></li></ul></li></ul><h2 id=changes-by-kind>Changes by Kind</h2><h3 id=deprecation>Deprecation</h3><h4 id=kube-apiserver-1>kube-apiserver:</h4><ul><li>the following deprecated APIs can no longer be served:<ul><li>All resources under <code>apps/v1beta1</code> and <code>apps/v1beta2</code> - use <code>apps/v1</code> instead</li><li><code>daemonsets</code>, <code>deployments</code>, <code>replicasets</code> resources under <code>extensions/v1beta1</code> - use <code>apps/v1</code> instead</li><li><code>networkpolicies</code> resources under <code>extensions/v1beta1</code> - use <code>networking.k8s.io/v1</code> instead</li><li><code>podsecuritypolicies</code> resources under <code>extensions/v1beta1</code> - use <code>policy/v1beta1</code> instead (<a href=https://github.com/kubernetes/kubernetes/pull/85903>#85903</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps, Cluster Lifecycle, Instrumentation and Testing]</li></ul></li></ul><h4 id=kube-controller-manager>kube-controller-manager:</h4><ul><li>Azure service annotation service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset has been deprecated. Its support would be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/88462>#88462</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li></ul><h4 id=kubelet-1>kubelet:</h4><ul><li>The StreamingProxyRedirects feature and <code>--redirect-container-streaming</code> flag are deprecated, and will be removed in a future release. The default behavior (proxy streaming requests through the kubelet) will be the only supported option. If you are setting <code>--redirect-container-streaming=true</code>, then you must migrate off this configuration. The flag will no longer be able to be enabled starting in v1.20. If you are not setting the flag, no action is necessary. (<a href=https://github.com/kubernetes/kubernetes/pull/88290>#88290</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG API Machinery and Node]</li><li>resource metrics endpoint <code>/metrics/resource/v1alpha1</code> as well as all metrics under this endpoint have been deprecated. Please convert to the following metrics emitted by endpoint <code>/metrics/resource</code>:
- scrape_error --> scrape_error
- node_cpu_usage_seconds_total --> node_cpu_usage_seconds
- node_memory_working_set_bytes --> node_memory_working_set_bytes
- container_cpu_usage_seconds_total --> container_cpu_usage_seconds
- container_memory_working_set_bytes --> container_memory_working_set_bytes
- scrape_error --> scrape_error
(<a href=https://github.com/kubernetes/kubernetes/pull/86282>#86282</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG Node]</li><li>In a future release, kubelet will no longer create the CSI NodePublishVolume target directory, in accordance with the CSI specification. CSI drivers may need to be updated accordingly to properly create and process the target path. (<a href=https://github.com/kubernetes/kubernetes/issues/75535>#75535</a>) [SIG Storage]</li></ul><h4 id=kube-proxy>kube-proxy:</h4><ul><li><code>--healthz-port</code> and <code>--metrics-port</code> flags are deprecated, please use <code>--healthz-bind-address</code> and <code>--metrics-bind-address</code> instead (<a href=https://github.com/kubernetes/kubernetes/pull/88512>#88512</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Network]</li><li>a new <code>EndpointSliceProxying</code> feature gate has been added to control the use of EndpointSlices in kube-proxy. The EndpointSlice feature gate that used to control this behavior no longer affects kube-proxy. This feature has been disabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/86137>#86137</a>, <a href=https://github.com/robscott>@robscott</a>)</li></ul><h4 id=kubeadm>kubeadm:</h4><ul><li>command line option "kubelet-version" for <code>kubeadm upgrade node</code> has been deprecated and will be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/87942>#87942</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>deprecate the usage of the experimental flag '--use-api' under the 'kubeadm alpha certs renew' command. (<a href=https://github.com/kubernetes/kubernetes/pull/88827>#88827</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>kube-dns is deprecated and will not be supported in a future version (<a href=https://github.com/kubernetes/kubernetes/pull/86574>#86574</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>the <code>ClusterStatus</code> struct present in the kubeadm-config ConfigMap is deprecated and will be removed in a future version. It is going to be maintained by kubeadm until it gets removed. The same information can be found on <code>etcd</code> and <code>kube-apiserver</code> pod annotations, <code>kubeadm.kubernetes.io/etcd.advertise-client-urls</code> and <code>kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint</code> respectively. (<a href=https://github.com/kubernetes/kubernetes/pull/87656>#87656</a>, <a href=https://github.com/ereslibre>@ereslibre</a>) [SIG Cluster Lifecycle]</li></ul><h4 id=kubectl-1>kubectl:</h4><ul><li>the boolean and unset values for the --dry-run flag are deprecated and a value --dry-run=server|client|none will be required in a future version. (<a href=https://github.com/kubernetes/kubernetes/pull/87580>#87580</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI]</li><li><code>kubectl apply --server-dry-run</code> is deprecated and replaced with --dry-run=server (<a href=https://github.com/kubernetes/kubernetes/pull/87580>#87580</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI]</li></ul><h4 id=add-ons>add-ons:</h4><ul><li>Remove cluster-monitoring addon (<a href=https://github.com/kubernetes/kubernetes/pull/85512>#85512</a>, <a href=https://github.com/serathius>@serathius</a>) [SIG Cluster Lifecycle, Instrumentation, Scalability and Testing]</li></ul><h4 id=kube-scheduler>kube-scheduler:</h4><ul><li>The <code>scheduling_duration_seconds</code> summary metric is deprecated (<a href=https://github.com/kubernetes/kubernetes/pull/86586>#86586</a>, <a href=https://github.com/xiaoanyunfei>@xiaoanyunfei</a>) [SIG Scheduling]</li><li>The <code>scheduling_algorithm_predicate_evaluation_seconds</code> and
<code>scheduling_algorithm_priority_evaluation_seconds</code> metrics are deprecated, replaced by <code>framework_extension_point_duration_seconds[extension_point="Filter"]</code> and <code>framework_extension_point_duration_seconds[extension_point="Score"]</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/86584>#86584</a>, <a href=https://github.com/xiaoanyunfei>@xiaoanyunfei</a>) [SIG Scheduling]</li><li><code>AlwaysCheckAllPredicates</code> is deprecated in scheduler Policy API. (<a href=https://github.com/kubernetes/kubernetes/pull/86369>#86369</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li></ul><h4 id=other-deprecations>Other deprecations:</h4><ul><li>The k8s.io/node-api component is no longer updated. Instead, use the RuntimeClass types located within k8s.io/api, and the generated clients located within k8s.io/client-go (<a href=https://github.com/kubernetes/kubernetes/pull/87503>#87503</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node and Release]</li><li>Removed the 'client' label from apiserver_request_total. (<a href=https://github.com/kubernetes/kubernetes/pull/87669>#87669</a>, <a href=https://github.com/logicalhan>@logicalhan</a>) [SIG API Machinery and Instrumentation]</li></ul><h3 id=api-change>API Change</h3><h4 id=new-api-types-versions>New API types/versions:</h4><ul><li>A new IngressClass resource has been added to enable better Ingress configuration. (<a href=https://github.com/kubernetes/kubernetes/pull/88509>#88509</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps, CLI, Network, Node and Testing]</li><li>The CSIDriver API has graduated to storage.k8s.io/v1, and is now available for use. (<a href=https://github.com/kubernetes/kubernetes/pull/84814>#84814</a>, <a href=https://github.com/huffmanca>@huffmanca</a>) [SIG Storage]</li></ul><h4 id=new-api-fields>New API fields:</h4><ul><li>autoscaling/v2beta2 HorizontalPodAutoscaler added a <code>spec.behavior</code> field that allows scale behavior to be configured. Behaviors are specified separately for scaling up and down. In each direction a stabilization window can be specified as well as a list of policies and how to select amongst them. Policies can limit the absolute number of pods added or removed, or the percentage of pods added or removed. (<a href=https://github.com/kubernetes/kubernetes/pull/74525>#74525</a>, <a href=https://github.com/gliush>@gliush</a>) [SIG API Machinery, Apps, Autoscaling and CLI]</li><li>Ingress:<ul><li><code>spec.ingressClassName</code> replaces the deprecated <code>kubernetes.io/ingress.class</code> annotation, and allows associating an Ingress object with a particular controller.</li><li>path definitions added a <code>pathType</code> field to allow indicating how the specified path should be matched against incoming requests. Valid values are <code>Exact</code>, <code>Prefix</code>, and <code>ImplementationSpecific</code> (<a href=https://github.com/kubernetes/kubernetes/pull/88587>#88587</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Apps, Cluster Lifecycle and Network]</li></ul></li><li>The alpha feature <code>AnyVolumeDataSource</code> enables PersistentVolumeClaim objects to use the spec.dataSource field to reference a custom type as a data source (<a href=https://github.com/kubernetes/kubernetes/pull/88636>#88636</a>, <a href=https://github.com/bswartz>@bswartz</a>) [SIG Apps and Storage]</li><li>The alpha feature <code>ConfigurableFSGroupPolicy</code> enables v1 Pods to specify a spec.securityContext.fsGroupChangePolicy policy to control how file permissions are applied to volumes mounted into the pod. (<a href=https://github.com/kubernetes/kubernetes/pull/88488>#88488</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Storage]</li><li>The alpha feature <code>ServiceAppProtocol</code> enables setting an <code>appProtocol</code> field in ServicePort and EndpointPort definitions. (<a href=https://github.com/kubernetes/kubernetes/pull/88503>#88503</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</li><li>The alpha feature <code>ImmutableEphemeralVolumes</code> enables an <code>immutable</code> field in both Secret and ConfigMap objects to mark their contents as immutable. (<a href=https://github.com/kubernetes/kubernetes/pull/86377>#86377</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG Apps, CLI and Testing]</li></ul><h4 id=other-api-changes>Other API changes:</h4><ul><li>The beta feature <code>ServerSideApply</code> enables tracking and managing changed fields for all new objects, which means there will be <code>managedFields</code> in <code>metadata</code> with the list of managers and their owned fields.</li><li>The alpha feature <code>ServiceAccountIssuerDiscovery</code> enables publishing OIDC discovery information and service account token verification keys at <code>/.well-known/openid-configuration</code> and <code>/openid/v1/jwks</code> endpoints by API servers configured to issue service account tokens. (<a href=https://github.com/kubernetes/kubernetes/pull/80724>#80724</a>, <a href=https://github.com/cceckman>@cceckman</a>) [SIG API Machinery, Auth, Cluster Lifecycle and Testing]</li><li>CustomResourceDefinition schemas that use <code>x-kubernetes-list-map-keys</code> to specify properties that uniquely identify list items must make those properties required or have a default value, to ensure those properties are present for all list items. See <a href=https://kubernetes.io/docs/reference/using-api/api-concepts/&amp;#35;merge-strategy>https://kubernetes.io/docs/reference/using-api/api-concepts/&amp;#35;merge-strategy</a> for details. (<a href=https://github.com/kubernetes/kubernetes/pull/88076>#88076</a>, <a href=https://github.com/eloyekunle>@eloyekunle</a>) [SIG API Machinery and Testing]</li><li>CustomResourceDefinition schemas that use <code>x-kubernetes-list-type: map</code> or <code>x-kubernetes-list-type: set</code> now enable validation that the list items in the corresponding custom resources are unique. (<a href=https://github.com/kubernetes/kubernetes/pull/84920>#84920</a>, <a href=https://github.com/sttts>@sttts</a>) [SIG API Machinery]</li></ul><h4 id=configuration-file-changes>Configuration file changes:</h4><h4 id=kube-apiserver-2>kube-apiserver:</h4><ul><li>The <code>--egress-selector-config-file</code> configuration file now accepts an apiserver.k8s.io/v1beta1 EgressSelectorConfiguration configuration object, and has been updated to allow specifying HTTP or GRPC connections to the network proxy (<a href=https://github.com/kubernetes/kubernetes/pull/87179>#87179</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG API Machinery, Cloud Provider and Cluster Lifecycle]</li></ul><h4 id=kube-scheduler-1>kube-scheduler:</h4><ul><li>A kubescheduler.config.k8s.io/v1alpha2 configuration file version is now accepted, with support for multiple scheduling profiles (<a href=https://github.com/kubernetes/kubernetes/pull/87628>#87628</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]<ul><li>HardPodAffinityWeight moved from a top level ComponentConfig parameter to a PluginConfig parameter of InterPodAffinity Plugin in <code>kubescheduler.config.k8s.io/v1alpha2</code> (<a href=https://github.com/kubernetes/kubernetes/pull/88002>#88002</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling and Testing]</li><li>Kube-scheduler can run more than one scheduling profile. Given a pod, the profile is selected by using its <code>.spec.schedulerName</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/88285>#88285</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Apps, Scheduling and Testing]</li><li>Scheduler Extenders can now be configured in the v1alpha2 component config (<a href=https://github.com/kubernetes/kubernetes/pull/88768>#88768</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Release, Scheduling and Testing]</li><li>The PostFilter of scheduler framework is renamed to PreScore in kubescheduler.config.k8s.io/v1alpha2. (<a href=https://github.com/kubernetes/kubernetes/pull/87751>#87751</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG Scheduling and Testing]</li></ul></li></ul><h4 id=kube-proxy-1>kube-proxy:</h4><ul><li>Added kube-proxy flags <code>--ipvs-tcp-timeout</code>, <code>--ipvs-tcpfin-timeout</code>, <code>--ipvs-udp-timeout</code> to configure IPVS connection timeouts. (<a href=https://github.com/kubernetes/kubernetes/pull/85517>#85517</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Cluster Lifecycle and Network]</li><li>Added optional <code>--detect-local-mode</code> flag to kube-proxy. Valid values are "ClusterCIDR" (default matching previous behavior) and "NodeCIDR" (<a href=https://github.com/kubernetes/kubernetes/pull/87748>#87748</a>, <a href=https://github.com/satyasm>@satyasm</a>) [SIG Cluster Lifecycle, Network and Scheduling]</li><li>Kube-controller-manager and kube-scheduler expose profiling by default to match the kube-apiserver. Use <code>--enable-profiling=false</code> to disable. (<a href=https://github.com/kubernetes/kubernetes/pull/88663>#88663</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Cloud Provider and Scheduling]</li><li>Kubelet pod resources API now provides the information about active pods only. (<a href=https://github.com/kubernetes/kubernetes/pull/79409>#79409</a>, <a href=https://github.com/takmatsu>@takmatsu</a>) [SIG Node]</li><li>New flag <code>--endpointslice-updates-batch-period</code> in kube-controller-manager can be used to reduce the number of endpointslice updates generated by pod changes. (<a href=https://github.com/kubernetes/kubernetes/pull/88745>#88745</a>, <a href=https://github.com/mborsz>@mborsz</a>) [SIG API Machinery, Apps and Network]</li><li>New flag <code>--show-hidden-metrics-for-version</code> in kube-proxy, kubelet, kube-controller-manager, and kube-scheduler can be used to show all hidden metrics that are deprecated in the previous minor release. (<a href=https://github.com/kubernetes/kubernetes/pull/85279>#85279</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG Cluster Lifecycle and Network]</li></ul><h4 id=features-graduated-to-beta>Features graduated to beta:</h4><ul><li>StartupProbe (<a href=https://github.com/kubernetes/kubernetes/pull/83437>#83437</a>, <a href=https://github.com/matthyx>@matthyx</a>) [SIG Node, Scalability and Testing]</li></ul><h4 id=features-graduated-to-ga>Features graduated to GA:</h4><ul><li>VolumePVCDataSource (<a href=https://github.com/kubernetes/kubernetes/pull/88686>#88686</a>, <a href=https://github.com/j-griffith>@j-griffith</a>) [SIG Storage]</li><li>TaintBasedEvictions (<a href=https://github.com/kubernetes/kubernetes/pull/87487>#87487</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG API Machinery, Apps, Node, Scheduling and Testing]</li><li>BlockVolume and CSIBlockVolume (<a href=https://github.com/kubernetes/kubernetes/pull/88673>#88673</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Storage]</li><li>Windows RunAsUserName (<a href=https://github.com/kubernetes/kubernetes/pull/87790>#87790</a>, <a href=https://github.com/marosset>@marosset</a>) [SIG Apps and Windows]</li><li>The following feature gates are removed, because the associated features were unconditionally enabled in previous releases: CustomResourceValidation, CustomResourceSubresources, CustomResourceWebhookConversion, CustomResourcePublishOpenAPI, CustomResourceDefaulting (<a href=https://github.com/kubernetes/kubernetes/pull/87475>#87475</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</li></ul><h3 id=feature>Feature</h3><ul><li>API request throttling (due to a high rate of requests) is now reported in client-go logs at log level 2. The messages are of the form:<code>Throttling request took 1.50705208s, request: GET:&lt;URL></code> The presence of these messages may indicate to the administrator the need to tune the cluster accordingly. (<a href=https://github.com/kubernetes/kubernetes/pull/87740>#87740</a>, <a href=https://github.com/jennybuckley>@jennybuckley</a>) [SIG API Machinery]</li><li>Add support for mount options to the FC volume plugin (<a href=https://github.com/kubernetes/kubernetes/pull/87499>#87499</a>, <a href=https://github.com/ejweber>@ejweber</a>) [SIG Storage]</li><li>Added a config-mode flag in azure auth module to enable getting AAD token without spn: prefix in audience claim. When it's not specified, the default behavior doesn't change. (<a href=https://github.com/kubernetes/kubernetes/pull/87630>#87630</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG API Machinery, Auth, CLI and Cloud Provider]</li><li>Allow for configuration of CoreDNS replica count (<a href=https://github.com/kubernetes/kubernetes/pull/85837>#85837</a>, <a href=https://github.com/pickledrick>@pickledrick</a>) [SIG Cluster Lifecycle]</li><li>Allow user to specify resource using --filename flag when invoking kubectl exec (<a href=https://github.com/kubernetes/kubernetes/pull/88460>#88460</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI and Testing]</li><li>Apiserver added a new flag --goaway-chance which is the fraction of requests that will be closed gracefully(GOAWAY) to prevent HTTP/2 clients from getting stuck on a single apiserver. (<a href=https://github.com/kubernetes/kubernetes/pull/88567>#88567</a>, <a href=https://github.com/answer1991>@answer1991</a>) [SIG API Machinery]</li><li>Azure Cloud Provider now supports using Azure network resources (Virtual Network, Load Balancer, Public IP, Route Table, Network Security Group, etc.) in different AAD Tenant and Subscription than those for the Kubernetes cluster. To use the feature, please reference <a href=https://github.com/kubernetes-sigs/cloud-provider-azure/blob/master/docs/cloud-provider-config.md&amp;#35;host-network-resources-in-different-aad-tenant-and-subscription>https://github.com/kubernetes-sigs/cloud-provider-azure/blob/master/docs/cloud-provider-config.md&amp;#35;host-network-resources-in-different-aad-tenant-and-subscription</a>. (<a href=https://github.com/kubernetes/kubernetes/pull/88384>#88384</a>, <a href=https://github.com/bowen5>@bowen5</a>) [SIG Cloud Provider]</li><li>Azure VMSS/VMSSVM clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/86740>#86740</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Azure cloud provider cache TTL is configurable, list of the azure cloud provider is as following:<ul><li>"availabilitySetNodesCacheTTLInSeconds"</li><li>"vmssCacheTTLInSeconds"</li><li>"vmssVirtualMachinesCacheTTLInSeconds"</li><li>"vmCacheTTLInSeconds"</li><li>"loadBalancerCacheTTLInSeconds"</li><li>"nsgCacheTTLInSeconds"</li><li>"routeTableCacheTTLInSeconds"
(<a href=https://github.com/kubernetes/kubernetes/pull/86266>#86266</a>, <a href=https://github.com/zqingqing1>@zqingqing1</a>) [SIG Cloud Provider]</li></ul></li><li>Azure global rate limit is switched to per-client. A set of new rate limit configure options are introduced, including routeRateLimit, SubnetsRateLimit, InterfaceRateLimit, RouteTableRateLimit, LoadBalancerRateLimit, PublicIPAddressRateLimit, SecurityGroupRateLimit, VirtualMachineRateLimit, StorageAccountRateLimit, DiskRateLimit, SnapshotRateLimit, VirtualMachineScaleSetRateLimit and VirtualMachineSizeRateLimit. The original rate limit options would be default values for those new client's rate limiter. (<a href=https://github.com/kubernetes/kubernetes/pull/86515>#86515</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Azure network and VM clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/87122>#87122</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Azure storage clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/87306>#87306</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Azure: add support for single stack IPv6 (<a href=https://github.com/kubernetes/kubernetes/pull/88448>#88448</a>, <a href=https://github.com/aramase>@aramase</a>) [SIG Cloud Provider]</li><li>DefaultConstraints can be specified for PodTopologySpread Plugin in the scheduler’s ComponentConfig (<a href=https://github.com/kubernetes/kubernetes/pull/88671>#88671</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>DisableAvailabilitySetNodes is added to avoid VM list for VMSS clusters. It should only be used when vmType is "vmss" and all the nodes (including control plane nodes) are VMSS virtual machines. (<a href=https://github.com/kubernetes/kubernetes/pull/87685>#87685</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Elasticsearch supports automatically setting the advertise address (<a href=https://github.com/kubernetes/kubernetes/pull/85944>#85944</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle and Instrumentation]</li><li>EndpointSlices will now be enabled by default. A new <code>EndpointSliceProxying</code> feature gate determines if kube-proxy will use EndpointSlices, this is disabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/86137>#86137</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Network]</li><li>Kube-proxy: Added dual-stack IPv4/IPv6 support to the iptables proxier. (<a href=https://github.com/kubernetes/kubernetes/pull/82462>#82462</a>, <a href=https://github.com/vllry>@vllry</a>) [SIG Network]</li><li>Kubeadm now supports automatic calculations of dual-stack node cidr masks to kube-controller-manager. (<a href=https://github.com/kubernetes/kubernetes/pull/85609>#85609</a>, <a href=https://github.com/Arvinderpal>@Arvinderpal</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: add a upgrade health check that deploys a Job (<a href=https://github.com/kubernetes/kubernetes/pull/81319>#81319</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: add the experimental feature gate PublicKeysECDSA that can be used to create a
cluster with ECDSA certificates from "kubeadm init". Renewal of existing ECDSA certificates is also supported using "kubeadm alpha certs renew", but not switching between the RSA and ECDSA algorithms on the fly or during upgrades. (<a href=https://github.com/kubernetes/kubernetes/pull/86953>#86953</a>, <a href=https://github.com/rojkov>@rojkov</a>) [SIG API Machinery, Auth and Cluster Lifecycle]</li><li>Kubeadm: implemented structured output of 'kubeadm config images list' command in JSON, YAML, Go template and JsonPath formats (<a href=https://github.com/kubernetes/kubernetes/pull/86810>#86810</a>, <a href=https://github.com/bart0sh>@bart0sh</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: on kubeconfig certificate renewal, keep the embedded CA in sync with the one on disk (<a href=https://github.com/kubernetes/kubernetes/pull/88052>#88052</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: reject a node joining the cluster if a node with the same name already exists (<a href=https://github.com/kubernetes/kubernetes/pull/81056>#81056</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: support Windows specific kubelet flags in kubeadm-flags.env (<a href=https://github.com/kubernetes/kubernetes/pull/88287>#88287</a>, <a href=https://github.com/gab-satchi>@gab-satchi</a>) [SIG Cluster Lifecycle and Windows]</li><li>Kubeadm: support automatic retry after failing to pull image (<a href=https://github.com/kubernetes/kubernetes/pull/86899>#86899</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: upgrade supports fallback to the nearest known etcd version if an unknown k8s version is passed (<a href=https://github.com/kubernetes/kubernetes/pull/88373>#88373</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubectl/drain: add disable-eviction option.Force drain to use delete, even if eviction is supported. This will bypass checking PodDisruptionBudgets, and should be used with caution. (<a href=https://github.com/kubernetes/kubernetes/pull/85571>#85571</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG CLI]</li><li>Kubectl/drain: add skip-wait-for-delete-timeout option. If a pod’s <code>DeletionTimestamp</code> is older than N seconds, skip waiting for the pod. Seconds must be greater than 0 to skip. (<a href=https://github.com/kubernetes/kubernetes/pull/85577>#85577</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG CLI]</li><li>Option <code>preConfiguredBackendPoolLoadBalancerTypes</code> is added to azure cloud provider for the pre-configured load balancers, possible values: <code>""</code>, <code>"internal"</code>, <code>"external"</code>,<code>"all"</code> (<a href=https://github.com/kubernetes/kubernetes/pull/86338>#86338</a>, <a href=https://github.com/gossion>@gossion</a>) [SIG Cloud Provider]</li><li>PodTopologySpread plugin now excludes terminatingPods when making scheduling decisions. (<a href=https://github.com/kubernetes/kubernetes/pull/87845>#87845</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li><li>Provider/azure: Network security groups can now be in a separate resource group. (<a href=https://github.com/kubernetes/kubernetes/pull/87035>#87035</a>, <a href=https://github.com/CecileRobertMichon>@CecileRobertMichon</a>) [SIG Cloud Provider]</li><li>SafeSysctlWhitelist: add net.ipv4.ping_group_range (<a href=https://github.com/kubernetes/kubernetes/pull/85463>#85463</a>, <a href=https://github.com/AkihiroSuda>@AkihiroSuda</a>) [SIG Auth]</li><li>Scheduler framework permit plugins now run at the end of the scheduling cycle, after reserve plugins. Waiting on permit will remain in the beginning of the binding cycle. (<a href=https://github.com/kubernetes/kubernetes/pull/88199>#88199</a>, <a href=https://github.com/mateuszlitwin>@mateuszlitwin</a>) [SIG Scheduling]</li><li>Scheduler: Add DefaultBinder plugin (<a href=https://github.com/kubernetes/kubernetes/pull/87430>#87430</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling and Testing]</li><li>Skip default spreading scoring plugin for pods that define TopologySpreadConstraints (<a href=https://github.com/kubernetes/kubernetes/pull/87566>#87566</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG Scheduling]</li><li>The kubectl --dry-run flag now accepts the values 'client', 'server', and 'none', to support client-side and server-side dry-run strategies. The boolean and unset values for the --dry-run flag are deprecated and a value will be required in a future version. (<a href=https://github.com/kubernetes/kubernetes/pull/87580>#87580</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI]</li><li>Support server-side dry-run in kubectl with --dry-run=server for commands including apply, patch, create, run, annotate, label, set, autoscale, drain, rollout undo, and expose. (<a href=https://github.com/kubernetes/kubernetes/pull/87714>#87714</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG API Machinery, CLI and Testing]</li><li>Add --dry-run=server|client to kubectl delete, taint, replace (<a href=https://github.com/kubernetes/kubernetes/pull/88292>#88292</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI and Testing]</li><li>The feature PodTopologySpread (feature gate <code>EvenPodsSpread</code>) has been enabled by default in 1.18. (<a href=https://github.com/kubernetes/kubernetes/pull/88105>#88105</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling and Testing]</li><li>The kubelet and the default docker runtime now support running ephemeral containers in the Linux process namespace of a target container. Other container runtimes must implement support for this feature before it will be available for that runtime. (<a href=https://github.com/kubernetes/kubernetes/pull/84731>#84731</a>, <a href=https://github.com/verb>@verb</a>) [SIG Node]</li><li>The underlying format of the <code>CPUManager</code> state file has changed. Upgrades should be seamless, but any third-party tools that rely on reading the previous format need to be updated. (<a href=https://github.com/kubernetes/kubernetes/pull/84462>#84462</a>, <a href=https://github.com/klueska>@klueska</a>) [SIG Node and Testing]</li><li>Update CNI version to v0.8.5 (<a href=https://github.com/kubernetes/kubernetes/pull/78819>#78819</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, Cluster Lifecycle, Network, Release and Testing]</li><li>Webhooks have alpha support for network proxy (<a href=https://github.com/kubernetes/kubernetes/pull/85870>#85870</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG API Machinery, Auth and Testing]</li><li>When client certificate files are provided, reload files for new connections, and close connections when a certificate changes. (<a href=https://github.com/kubernetes/kubernetes/pull/79083>#79083</a>, <a href=https://github.com/jackkleeman>@jackkleeman</a>) [SIG API Machinery, Auth, Node and Testing]</li><li>When deleting objects using kubectl with the --force flag, you are no longer required to also specify --grace-period=0. (<a href=https://github.com/kubernetes/kubernetes/pull/87776>#87776</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li>Windows nodes on GCE can use virtual TPM-based authentication to the control plane. (<a href=https://github.com/kubernetes/kubernetes/pull/85466>#85466</a>, <a href=https://github.com/pjh>@pjh</a>) [SIG Cluster Lifecycle]</li><li>You can now pass "--node-ip ::" to kubelet to indicate that it should autodetect an IPv6 address to use as the node's primary address. (<a href=https://github.com/kubernetes/kubernetes/pull/85850>#85850</a>, <a href=https://github.com/danwinship>@danwinship</a>) [SIG Cloud Provider, Network and Node]</li><li><code>kubectl</code> now contains a <code>kubectl alpha debug</code> command. This command allows attaching an ephemeral container to a running pod for the purposes of debugging. (<a href=https://github.com/kubernetes/kubernetes/pull/88004>#88004</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI]</li><li>TLS Server Name overrides can now be specified in a kubeconfig file and via --tls-server-name in kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/88769>#88769</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Auth and CLI]</li></ul><h4 id=metrics>Metrics:</h4><ul><li>Add <code>rest_client_rate_limiter_duration_seconds</code> metric to component-base to track client side rate limiter latency in seconds. Broken down by verb and URL. (<a href=https://github.com/kubernetes/kubernetes/pull/88134>#88134</a>, <a href=https://github.com/jennybuckley>@jennybuckley</a>) [SIG API Machinery, Cluster Lifecycle and Instrumentation]</li><li>Added two client certificate metrics for exec auth:<ul><li><code>rest_client_certificate_expiration_seconds</code> a gauge reporting the lifetime of the current client certificate. Reports the time of expiry in seconds since January 1, 1970 UTC.</li><li><code>rest_client_certificate_rotation_age</code> a histogram reporting the age of a just rotated client certificate in seconds. (<a href=https://github.com/kubernetes/kubernetes/pull/84382>#84382</a>, <a href=https://github.com/sambdavidson>@sambdavidson</a>) [SIG API Machinery, Auth, Cluster Lifecycle and Instrumentation]</li></ul></li><li>Controller manager serve workqueue metrics (<a href=https://github.com/kubernetes/kubernetes/pull/87967>#87967</a>, <a href=https://github.com/zhan849>@zhan849</a>) [SIG API Machinery]</li><li>Following metrics have been turned off:<ul><li>kubelet_pod_worker_latency_microseconds</li><li>kubelet_pod_start_latency_microseconds</li><li>kubelet_cgroup_manager_latency_microseconds</li><li>kubelet_pod_worker_start_latency_microseconds</li><li>kubelet_pleg_relist_latency_microseconds</li><li>kubelet_pleg_relist_interval_microseconds</li><li>kubelet_eviction_stats_age_microseconds</li><li>kubelet_runtime_operations</li><li>kubelet_runtime_operations_latency_microseconds</li><li>kubelet_runtime_operations_errors</li><li>kubelet_device_plugin_registration_count</li><li>kubelet_device_plugin_alloc_latency_microseconds</li><li>kubelet_docker_operations</li><li>kubelet_docker_operations_latency_microseconds</li><li>kubelet_docker_operations_errors</li><li>kubelet_docker_operations_timeout</li><li>network_plugin_operations_latency_microseconds (<a href=https://github.com/kubernetes/kubernetes/pull/83841>#83841</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG Network and Node]</li></ul></li><li>Kube-apiserver metrics will now include request counts, latencies, and response sizes for /healthz, /livez, and /readyz requests. (<a href=https://github.com/kubernetes/kubernetes/pull/83598>#83598</a>, <a href=https://github.com/jktomer>@jktomer</a>) [SIG API Machinery]</li><li>Kubelet now exports a <code>server_expiration_renew_failure</code> and <code>client_expiration_renew_failure</code> metric counter if the certificate rotations cannot be performed. (<a href=https://github.com/kubernetes/kubernetes/pull/84614>#84614</a>, <a href=https://github.com/rphillips>@rphillips</a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Node and Release]</li><li>Kubelet: the metric process_start_time_seconds be marked as with the ALPHA stability level. (<a href=https://github.com/kubernetes/kubernetes/pull/85446>#85446</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li><li>New metric <code>kubelet_pleg_last_seen_seconds</code> to aid diagnosis of PLEG not healthy issues. (<a href=https://github.com/kubernetes/kubernetes/pull/86251>#86251</a>, <a href=https://github.com/bboreham>@bboreham</a>) [SIG Node]</li></ul><h3 id=other-bug-cleanup-or-flake>Other (Bug, Cleanup or Flake)</h3><ul><li>Fixed a regression with clients prior to 1.15 not being able to update podIP in pod status, or podCIDR in node spec, against >= 1.16 API servers (<a href=https://github.com/kubernetes/kubernetes/pull/88505>#88505</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Apps and Network]</li><li>Fixed "kubectl describe statefulsets.apps" printing garbage for rolling update partition (<a href=https://github.com/kubernetes/kubernetes/pull/85846>#85846</a>, <a href=https://github.com/phil9909>@phil9909</a>) [SIG CLI]</li><li>Add a event to PV when filesystem on PV does not match actual filesystem on disk (<a href=https://github.com/kubernetes/kubernetes/pull/86982>#86982</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Storage]</li><li>Add azure disk WriteAccelerator support (<a href=https://github.com/kubernetes/kubernetes/pull/87945>#87945</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Add delays between goroutines for vm instance update (<a href=https://github.com/kubernetes/kubernetes/pull/88094>#88094</a>, <a href=https://github.com/aramase>@aramase</a>) [SIG Cloud Provider]</li><li>Add init containers log to cluster dump info. (<a href=https://github.com/kubernetes/kubernetes/pull/88324>#88324</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Addons: elasticsearch discovery supports IPv6 (<a href=https://github.com/kubernetes/kubernetes/pull/85543>#85543</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle and Instrumentation]</li><li>Adds "volume.beta.kubernetes.io/migrated-to" annotation to PV's and PVC's when they are migrated to signal external provisioners to pick up those objects for Provisioning and Deleting. (<a href=https://github.com/kubernetes/kubernetes/pull/87098>#87098</a>, <a href=https://github.com/davidz627>@davidz627</a>) [SIG Storage]</li><li>All api-server log request lines in a more greppable format. (<a href=https://github.com/kubernetes/kubernetes/pull/87203>#87203</a>, <a href=https://github.com/lavalamp>@lavalamp</a>) [SIG API Machinery]</li><li>Azure VMSS LoadBalancerBackendAddressPools updating has been improved with sequential-sync + concurrent-async requests. (<a href=https://github.com/kubernetes/kubernetes/pull/88699>#88699</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Azure cloud provider now obtains AAD token who audience claim will not have spn: prefix (<a href=https://github.com/kubernetes/kubernetes/pull/87590>#87590</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG Cloud Provider]</li><li>AzureFile and CephFS use the new Mount library that prevents logging of sensitive mount options. (<a href=https://github.com/kubernetes/kubernetes/pull/88684>#88684</a>, <a href=https://github.com/saad-ali>@saad-ali</a>) [SIG Storage]</li><li>Bind dns-horizontal containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83364>#83364</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Cluster Lifecycle and Windows]</li><li>Bind kube-dns containers to linux nodes to avoid Windows scheduling (<a href=https://github.com/kubernetes/kubernetes/pull/83358>#83358</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Cluster Lifecycle and Windows]</li><li>Bind metadata-agent containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83363>#83363</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Cluster Lifecycle, Instrumentation and Windows]</li><li>Bind metrics-server containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83362>#83362</a>, <a href=https://github.com/wawa0210>@wawa0210</a>) [SIG Cluster Lifecycle, Instrumentation and Windows]</li><li>Bug fixes: Make sure we include latest packages node #351 (@caseydavenport) (<a href=https://github.com/kubernetes/kubernetes/pull/84163>#84163</a>, <a href=https://github.com/david-tigera>@david-tigera</a>) [SIG Cluster Lifecycle]</li><li>CPU limits are now respected for Windows containers. If a node is over-provisioned, no weighting is used, only limits are respected. (<a href=https://github.com/kubernetes/kubernetes/pull/86101>#86101</a>, <a href=https://github.com/PatrickLang>@PatrickLang</a>) [SIG Node, Testing and Windows]</li><li>Changed core_pattern on COS nodes to be an absolute path. (<a href=https://github.com/kubernetes/kubernetes/pull/86329>#86329</a>, <a href=https://github.com/mml>@mml</a>) [SIG Cluster Lifecycle and Node]</li><li>Client-go certificate manager rotation gained the ability to preserve optional intermediate chains accompanying issued certificates (<a href=https://github.com/kubernetes/kubernetes/pull/88744>#88744</a>, <a href=https://github.com/jackkleeman>@jackkleeman</a>) [SIG API Machinery and Auth]</li><li>Cloud provider config CloudProviderBackoffMode has been removed since it won't be used anymore. (<a href=https://github.com/kubernetes/kubernetes/pull/88463>#88463</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Conformance image now depends on stretch-slim instead of debian-hyperkube-base as that image is being deprecated and removed. (<a href=https://github.com/kubernetes/kubernetes/pull/88702>#88702</a>, <a href=https://github.com/dims>@dims</a>) [SIG Cluster Lifecycle, Release and Testing]</li><li>Deprecate --generator flag from kubectl create commands (<a href=https://github.com/kubernetes/kubernetes/pull/88655>#88655</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</li><li>During initialization phase (preflight), kubeadm now verifies the presence of the conntrack executable (<a href=https://github.com/kubernetes/kubernetes/pull/85857>#85857</a>, <a href=https://github.com/hnanni>@hnanni</a>) [SIG Cluster Lifecycle]</li><li>EndpointSlice should not contain endpoints for terminating pods (<a href=https://github.com/kubernetes/kubernetes/pull/89056>#89056</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps and Network]</li><li>Evictions due to pods breaching their ephemeral storage limits are now recorded by the <code>kubelet_evictions</code> metric and can be alerted on. (<a href=https://github.com/kubernetes/kubernetes/pull/87906>#87906</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node]</li><li>Filter published OpenAPI schema by making nullable, required fields non-required in order to avoid kubectl to wrongly reject null values. (<a href=https://github.com/kubernetes/kubernetes/pull/85722>#85722</a>, <a href=https://github.com/sttts>@sttts</a>) [SIG API Machinery]</li><li>Fix /readyz to return error immediately after a shutdown is initiated, before the --shutdown-delay-duration has elapsed. (<a href=https://github.com/kubernetes/kubernetes/pull/88911>#88911</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery]</li><li>Fix API Server potential memory leak issue in processing watch request. (<a href=https://github.com/kubernetes/kubernetes/pull/85410>#85410</a>, <a href=https://github.com/answer1991>@answer1991</a>) [SIG API Machinery]</li><li>Fix EndpointSlice controller race condition and ensure that it handles external changes to EndpointSlices. (<a href=https://github.com/kubernetes/kubernetes/pull/85703>#85703</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</li><li>Fix IPv6 addresses lost issue in pure ipv6 vsphere environment (<a href=https://github.com/kubernetes/kubernetes/pull/86001>#86001</a>, <a href=https://github.com/hubv>@hubv</a>) [SIG Cloud Provider]</li><li>Fix LoadBalancer rule checking so that no unexpected LoadBalancer updates are made (<a href=https://github.com/kubernetes/kubernetes/pull/85990>#85990</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fix a bug in kube-proxy that caused it to crash when using load balancers with a different IP family (<a href=https://github.com/kubernetes/kubernetes/pull/87117>#87117</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li><li>Fix a bug in port-forward: named port not working with service (<a href=https://github.com/kubernetes/kubernetes/pull/85511>#85511</a>, <a href=https://github.com/oke-py>@oke-py</a>) [SIG CLI]</li><li>Fix a bug in the dual-stack IPVS proxier where stale IPv6 endpoints were not being cleaned up (<a href=https://github.com/kubernetes/kubernetes/pull/87695>#87695</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Network]</li><li>Fix a bug that orphan revision cannot be adopted and statefulset cannot be synced (<a href=https://github.com/kubernetes/kubernetes/pull/86801>#86801</a>, <a href=https://github.com/likakuli>@likakuli</a>) [SIG Apps]</li><li>Fix a bug where ExternalTrafficPolicy is not applied to service ExternalIPs. (<a href=https://github.com/kubernetes/kubernetes/pull/88786>#88786</a>, <a href=https://github.com/freehan>@freehan</a>) [SIG Network]</li><li>Fix a bug where kubenet fails to parse the tc output. (<a href=https://github.com/kubernetes/kubernetes/pull/83572>#83572</a>, <a href=https://github.com/chendotjs>@chendotjs</a>) [SIG Network]</li><li>Fix a regression in kubenet that prevent pods to obtain ip addresses (<a href=https://github.com/kubernetes/kubernetes/pull/85993>#85993</a>, <a href=https://github.com/chendotjs>@chendotjs</a>) [SIG Network and Node]</li><li>Fix azure file AuthorizationFailure (<a href=https://github.com/kubernetes/kubernetes/pull/85475>#85475</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix bug where EndpointSlice controller would attempt to modify shared objects. (<a href=https://github.com/kubernetes/kubernetes/pull/85368>#85368</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps and Network]</li><li>Fix handling of aws-load-balancer-security-groups annotation. Security-Groups assigned with this annotation are no longer modified by kubernetes which is the expected behaviour of most users. Also no unnecessary Security-Groups are created anymore if this annotation is used. (<a href=https://github.com/kubernetes/kubernetes/pull/83446>#83446</a>, <a href=https://github.com/Elias481>@Elias481</a>) [SIG Cloud Provider]</li><li>Fix invalid VMSS updates due to incorrect cache (<a href=https://github.com/kubernetes/kubernetes/pull/89002>#89002</a>, <a href=https://github.com/ArchangelSDY>@ArchangelSDY</a>) [SIG Cloud Provider]</li><li>Fix isCurrentInstance for Windows by removing the dependency of hostname. (<a href=https://github.com/kubernetes/kubernetes/pull/89138>#89138</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fix issue #85805 about a resource not found in azure cloud provider when LoadBalancer specified in another resource group. (<a href=https://github.com/kubernetes/kubernetes/pull/86502>#86502</a>, <a href=https://github.com/levimm>@levimm</a>) [SIG Cloud Provider]</li><li>Fix kubectl annotate error when local=true is set (<a href=https://github.com/kubernetes/kubernetes/pull/86952>#86952</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Fix kubectl create deployment image name (<a href=https://github.com/kubernetes/kubernetes/pull/86636>#86636</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Fix <code>kubectl drain ignore</code> daemonsets and others. (<a href=https://github.com/kubernetes/kubernetes/pull/87361>#87361</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Fix missing "apiVersion" for "involvedObject" in Events for Nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/87537>#87537</a>, <a href=https://github.com/uthark>@uthark</a>) [SIG Apps and Node]</li><li>Fix nil pointer dereference in azure cloud provider (<a href=https://github.com/kubernetes/kubernetes/pull/85975>#85975</a>, <a href=https://github.com/ldx>@ldx</a>) [SIG Cloud Provider]</li><li>Fix regression in statefulset conversion which prevents applying a statefulset multiple times. (<a href=https://github.com/kubernetes/kubernetes/pull/87706>#87706</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Apps and Testing]</li><li>Fix route conflicted operations when updating multiple routes together (<a href=https://github.com/kubernetes/kubernetes/pull/88209>#88209</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fix that prevents repeated fetching of PVC/PV objects by kubelet when processing of pod volumes fails. While this prevents hammering API server in these error scenarios, it means that some errors in processing volume(s) for a pod could now take up to 2-3 minutes before retry. (<a href=https://github.com/kubernetes/kubernetes/pull/88141>#88141</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node and Storage]</li><li>Fix the bug PIP's DNS is deleted if no DNS label service annotation isn't set. (<a href=https://github.com/kubernetes/kubernetes/pull/87246>#87246</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Fix control plane hosts rolling upgrade causing thundering herd of LISTs on etcd leading to control plane unavailability. (<a href=https://github.com/kubernetes/kubernetes/pull/86430>#86430</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery, Node and Testing]</li><li>Fix: add azure disk migration support for CSINode (<a href=https://github.com/kubernetes/kubernetes/pull/88014>#88014</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix: add non-retriable errors in azure clients (<a href=https://github.com/kubernetes/kubernetes/pull/87941>#87941</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix: add remediation in azure disk attach/detach (<a href=https://github.com/kubernetes/kubernetes/pull/88444>#88444</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix: azure data disk should use same key as os disk by default (<a href=https://github.com/kubernetes/kubernetes/pull/86351>#86351</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix: azure disk could not mounted on Standard_DC4s/DC2s instances (<a href=https://github.com/kubernetes/kubernetes/pull/86612>#86612</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix: azure file mount timeout issue (<a href=https://github.com/kubernetes/kubernetes/pull/88610>#88610</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix: check disk status before disk azure disk (<a href=https://github.com/kubernetes/kubernetes/pull/88360>#88360</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix: corrupted mount point in csi driver (<a href=https://github.com/kubernetes/kubernetes/pull/88569>#88569</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</li><li>Fix: get azure disk lun timeout issue (<a href=https://github.com/kubernetes/kubernetes/pull/88158>#88158</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix: update azure disk max count (<a href=https://github.com/kubernetes/kubernetes/pull/88201>#88201</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fixed "requested device X but found Y" attach error on AWS. (<a href=https://github.com/kubernetes/kubernetes/pull/85675>#85675</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Cloud Provider and Storage]</li><li>Fixed NetworkPolicy validation that <code>Except</code> values are accepted when they are outside the CIDR range. (<a href=https://github.com/kubernetes/kubernetes/pull/86578>#86578</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Network]</li><li>Fixed a bug in the TopologyManager. Previously, the TopologyManager would only guarantee alignment if container creation was serialized in some way. Alignment is now guaranteed under all scenarios of container creation. (<a href=https://github.com/kubernetes/kubernetes/pull/87759>#87759</a>, <a href=https://github.com/klueska>@klueska</a>) [SIG Node]</li><li>Fixed a bug which could prevent a provider ID from ever being set for node if an error occurred determining the provider ID when the node was added. (<a href=https://github.com/kubernetes/kubernetes/pull/87043>#87043</a>, <a href=https://github.com/zjs>@zjs</a>) [SIG Apps and Cloud Provider]</li><li>Fixed a data race in the kubelet image manager that can cause static pod workers to silently stop working. (<a href=https://github.com/kubernetes/kubernetes/pull/88915>#88915</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG Node]</li><li>Fixed a panic in the kubelet cleaning up pod volumes (<a href=https://github.com/kubernetes/kubernetes/pull/86277>#86277</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Storage]</li><li>Fixed a regression where the kubelet would fail to update the ready status of pods. (<a href=https://github.com/kubernetes/kubernetes/pull/84951>#84951</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node]</li><li>Fixed an issue that could cause the kubelet to incorrectly run concurrent pod reconciliation loops and crash. (<a href=https://github.com/kubernetes/kubernetes/pull/89055>#89055</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node]</li><li>Fixed block CSI volume cleanup after timeouts. (<a href=https://github.com/kubernetes/kubernetes/pull/88660>#88660</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Storage]</li><li>Fixed cleaning of CSI raw block volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/87978>#87978</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Storage]</li><li>Fixed AWS Cloud Provider attempting to delete LoadBalancer security group it didn’t provision, and fixed AWS Cloud Provider creating a default LoadBalancer security group even if annotation <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code> is present because the intended behavior of aws-load-balancer-security-groups is to replace all security groups assigned to the load balancer. (<a href=https://github.com/kubernetes/kubernetes/pull/84265>#84265</a>, <a href=https://github.com/bhagwat070919>@bhagwat070919</a>) [SIG Cloud Provider]</li><li>Fixed two scheduler metrics (pending_pods and schedule_attempts_total) not being recorded (<a href=https://github.com/kubernetes/kubernetes/pull/87692>#87692</a>, <a href=https://github.com/everpeace>@everpeace</a>) [SIG Scheduling]</li><li>Fixes an issue with kubelet-reported pod status on deleted/recreated pods. (<a href=https://github.com/kubernetes/kubernetes/pull/86320>#86320</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node]</li><li>Fixes conversion error in multi-version custom resources that could cause metadata.generation to increment on no-op patches or updates of a custom resource. (<a href=https://github.com/kubernetes/kubernetes/pull/88995>#88995</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</li><li>Fixes issue where AAD token obtained by kubectl is incompatible with on-behalf-of flow and oidc. The audience claim before this fix has "spn:" prefix. After this fix, "spn:" prefix is omitted. (<a href=https://github.com/kubernetes/kubernetes/pull/86412>#86412</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG API Machinery, Auth and Cloud Provider]</li><li>Fixes an issue where you can't attach more than 15 GCE Persistent Disks to c2, n2, m1, m2 machine types. (<a href=https://github.com/kubernetes/kubernetes/pull/88602>#88602</a>, <a href=https://github.com/yuga711>@yuga711</a>) [SIG Storage]</li><li>Fixes kube-proxy when EndpointSlice feature gate is enabled on Windows. (<a href=https://github.com/kubernetes/kubernetes/pull/86016>#86016</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Auth and Network]</li><li>Fixes kubelet crash in client certificate rotation cases (<a href=https://github.com/kubernetes/kubernetes/pull/88079>#88079</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Auth and Node]</li><li>Fixes service account token admission error in clusters that do not run the service account token controller (<a href=https://github.com/kubernetes/kubernetes/pull/87029>#87029</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Auth]</li><li>Fixes v1.17.0 regression in --service-cluster-ip-range handling with IPv4 ranges larger than 65536 IP addresses (<a href=https://github.com/kubernetes/kubernetes/pull/86534>#86534</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Network]</li><li>Fixes wrong validation result of NetworkPolicy PolicyTypes (<a href=https://github.com/kubernetes/kubernetes/pull/85747>#85747</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Network]</li><li>For subprotocol negotiation, both client and server protocol is required now. (<a href=https://github.com/kubernetes/kubernetes/pull/86646>#86646</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG API Machinery and Node]</li><li>For volumes that allow attaches across multiple nodes, attach and detach operations across different nodes are now executed in parallel. (<a href=https://github.com/kubernetes/kubernetes/pull/88678>#88678</a>, <a href=https://github.com/verult>@verult</a>) [SIG Storage]</li><li>Garbage collector now can correctly orphan ControllerRevisions when StatefulSets are deleted with orphan propagation policy. (<a href=https://github.com/kubernetes/kubernetes/pull/84984>#84984</a>, <a href=https://github.com/cofyc>@cofyc</a>) [SIG Apps]</li><li><code>Get-kube.sh</code> uses the gcloud's current local GCP service account for auth when the provider is GCE or GKE instead of the metadata server default (<a href=https://github.com/kubernetes/kubernetes/pull/88383>#88383</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG Cluster Lifecycle]</li><li>Golang/x/net has been updated to bring in fixes for CVE-2020-9283 (<a href=https://github.com/kubernetes/kubernetes/pull/88381>#88381</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle and Instrumentation]</li><li>If a serving certificate’s param specifies a name that is an IP for an SNI certificate, it will have priority for replying to server connections. (<a href=https://github.com/kubernetes/kubernetes/pull/85308>#85308</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery]</li><li>Improved yaml parsing performance (<a href=https://github.com/kubernetes/kubernetes/pull/85458>#85458</a>, <a href=https://github.com/cjcullen>@cjcullen</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Node]</li><li>Improves performance of the node authorizer (<a href=https://github.com/kubernetes/kubernetes/pull/87696>#87696</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Auth]</li><li>In GKE alpha clusters it will be possible to use the service annotation <code>cloud.google.com/network-tier: Standard</code> (<a href=https://github.com/kubernetes/kubernetes/pull/88487>#88487</a>, <a href=https://github.com/zioproto>@zioproto</a>) [SIG Cloud Provider]</li><li>Includes FSType when describing CSI persistent volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/85293>#85293</a>, <a href=https://github.com/huffmanca>@huffmanca</a>) [SIG CLI and Storage]</li><li>Iptables/userspace proxy: improve performance by getting local addresses only once per sync loop, instead of for every external IP (<a href=https://github.com/kubernetes/kubernetes/pull/85617>#85617</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Network]</li><li>Kube-aggregator: always sets unavailableGauge metric to reflect the current state of a service. (<a href=https://github.com/kubernetes/kubernetes/pull/87778>#87778</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery]</li><li>Kube-apiserver: fixed a conflict error encountered attempting to delete a pod with gracePeriodSeconds=0 and a resourceVersion precondition (<a href=https://github.com/kubernetes/kubernetes/pull/85516>#85516</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>) [SIG API Machinery]</li><li>Kube-proxy no longer modifies shared EndpointSlices. (<a href=https://github.com/kubernetes/kubernetes/pull/86092>#86092</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Network]</li><li>Kube-proxy: on dual-stack mode, if it is not able to get the IP Family of an endpoint, logs it with level InfoV(4) instead of Warning, avoiding flooding the logs for endpoints without addresses (<a href=https://github.com/kubernetes/kubernetes/pull/88934>#88934</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li><li>Kubeadm allows to configure single-stack clusters if dual-stack is enabled (<a href=https://github.com/kubernetes/kubernetes/pull/87453>#87453</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG API Machinery, Cluster Lifecycle and Network]</li><li>Kubeadm now includes CoreDNS version 1.6.7 (<a href=https://github.com/kubernetes/kubernetes/pull/86260>#86260</a>, <a href=https://github.com/rajansandeep>@rajansandeep</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm upgrades always persist the etcd backup for stacked (<a href=https://github.com/kubernetes/kubernetes/pull/86861>#86861</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: 'kubeadm alpha kubelet config download' has been removed, please use 'kubeadm upgrade node phase kubelet-config' instead (<a href=https://github.com/kubernetes/kubernetes/pull/87944>#87944</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: Forward cluster name to the controller-manager arguments (<a href=https://github.com/kubernetes/kubernetes/pull/85817>#85817</a>, <a href=https://github.com/ereslibre>@ereslibre</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: add support for the "ci/k8s-master" version label as a replacement for "ci-cross/*", which no longer exists. (<a href=https://github.com/kubernetes/kubernetes/pull/86609>#86609</a>, <a href=https://github.com/Pensu>@Pensu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: apply further improvements to the tentative support for concurrent etcd member join. Fixes a bug where multiple members can receive the same hostname. Increase the etcd client dial timeout and retry timeout for add/remove/... operations. (<a href=https://github.com/kubernetes/kubernetes/pull/87505>#87505</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: don't write the kubelet environment file on "upgrade apply" (<a href=https://github.com/kubernetes/kubernetes/pull/85412>#85412</a>, <a href=https://github.com/boluisa>@boluisa</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: fix potential panic when executing "kubeadm reset" with a corrupted kubelet.conf file (<a href=https://github.com/kubernetes/kubernetes/pull/86216>#86216</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: fix the bug that 'kubeadm upgrade' hangs in single node cluster (<a href=https://github.com/kubernetes/kubernetes/pull/88434>#88434</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: make sure images are pre-pulled even if a tag did not change but their contents changed (<a href=https://github.com/kubernetes/kubernetes/pull/85603>#85603</a>, <a href=https://github.com/bart0sh>@bart0sh</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove 'kubeadm upgrade node config' command since it was deprecated in v1.15, please use 'kubeadm upgrade node phase kubelet-config' instead (<a href=https://github.com/kubernetes/kubernetes/pull/87975>#87975</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove the deprecated CoreDNS feature-gate. It was set to "true" since v1.11 when the feature went GA. In v1.13 it was marked as deprecated and hidden from the CLI. (<a href=https://github.com/kubernetes/kubernetes/pull/87400>#87400</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: retry <code>kubeadm-config</code> ConfigMap creation or mutation if the apiserver is not responding. This will improve resiliency when joining new control plane nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/85763>#85763</a>, <a href=https://github.com/ereslibre>@ereslibre</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: tolerate whitespace when validating certificate authority PEM data in kubeconfig files (<a href=https://github.com/kubernetes/kubernetes/pull/86705>#86705</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: use bind-address option to configure the kube-controller-manager and kube-scheduler http probes (<a href=https://github.com/kubernetes/kubernetes/pull/86493>#86493</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: uses the api-server AdvertiseAddress IP family to choose the etcd endpoint IP family for non external etcd clusters (<a href=https://github.com/kubernetes/kubernetes/pull/85745>#85745</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Cluster Lifecycle]</li><li>Kubectl cluster-info dump --output-directory=xxx now generates files with an extension depending on the output format. (<a href=https://github.com/kubernetes/kubernetes/pull/82070>#82070</a>, <a href=https://github.com/olivierlemasle>@olivierlemasle</a>) [SIG CLI]</li><li><code>Kubectl describe &lt;type></code> and <code>kubectl top pod</code> will return a message saying <code>"No resources found"</code> or <code>"No resources found in &lt;namespace> namespace"</code> if there are no results to display. (<a href=https://github.com/kubernetes/kubernetes/pull/87527>#87527</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li><code>Kubectl drain node --dry-run</code> will list pods that would be evicted or deleted (<a href=https://github.com/kubernetes/kubernetes/pull/82660>#82660</a>, <a href=https://github.com/sallyom>@sallyom</a>) [SIG CLI]</li><li><code>Kubectl set resources</code> will no longer return an error if passed an empty change for a resource. <code>kubectl set subject</code> will no longer return an error if passed an empty change for a resource. (<a href=https://github.com/kubernetes/kubernetes/pull/85490>#85490</a>, <a href=https://github.com/sallyom>@sallyom</a>) [SIG CLI]</li><li>Kubelet metrics gathered through metrics-server or prometheus should no longer timeout for Windows nodes running more than 3 pods. (<a href=https://github.com/kubernetes/kubernetes/pull/87730>#87730</a>, <a href=https://github.com/marosset>@marosset</a>) [SIG Node, Testing and Windows]</li><li>Kubelet metrics have been changed to buckets. For example the <code>exec/{podNamespace}/{podID}/{containerName}</code> is now just exec. (<a href=https://github.com/kubernetes/kubernetes/pull/87913>#87913</a>, <a href=https://github.com/cheftako>@cheftako</a>) [SIG Node]</li><li>Kubelets perform fewer unnecessary pod status update operations on the API server. (<a href=https://github.com/kubernetes/kubernetes/pull/88591>#88591</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node and Scalability]</li><li>Kubernetes will try to acquire the iptables lock every 100 msec during 5 seconds instead of every second. This is especially useful for environments using kube-proxy in iptables mode with a high churn rate of services. (<a href=https://github.com/kubernetes/kubernetes/pull/85771>#85771</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li><li>Limit number of instances in a single update to GCE target pool to 1000. (<a href=https://github.com/kubernetes/kubernetes/pull/87881>#87881</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG Cloud Provider, Network and Scalability]</li><li>Make Azure clients only retry on specified HTTP status codes (<a href=https://github.com/kubernetes/kubernetes/pull/88017>#88017</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Make error message and service event message more clear (<a href=https://github.com/kubernetes/kubernetes/pull/86078>#86078</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Minimize AWS NLB health check timeout when externalTrafficPolicy set to Local (<a href=https://github.com/kubernetes/kubernetes/pull/73363>#73363</a>, <a href=https://github.com/kellycampbell>@kellycampbell</a>) [SIG Cloud Provider]</li><li>Pause image contains "Architecture" in non-amd64 images (<a href=https://github.com/kubernetes/kubernetes/pull/87954>#87954</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG Release]</li><li>Pause image upgraded to 3.2 in kubelet and kubeadm. (<a href=https://github.com/kubernetes/kubernetes/pull/88173>#88173</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG CLI, Cluster Lifecycle, Node and Testing]</li><li>Plugin/PluginConfig and Policy APIs are mutually exclusive when running the scheduler (<a href=https://github.com/kubernetes/kubernetes/pull/88864>#88864</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Remove <code>FilteredNodesStatuses</code> argument from <code>PreScore</code>'s interface. (<a href=https://github.com/kubernetes/kubernetes/pull/88189>#88189</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG Scheduling and Testing]</li><li>Resolved a performance issue in the node authorizer index maintenance. (<a href=https://github.com/kubernetes/kubernetes/pull/87693>#87693</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Auth]</li><li>Resolved regression in admission, authentication, and authorization webhook performance in v1.17.0-rc.1 (<a href=https://github.com/kubernetes/kubernetes/pull/85810>#85810</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery and Testing]</li><li>Resolves performance regression in <code>kubectl get all</code> and in client-go discovery clients constructed using <code>NewDiscoveryClientForConfig</code> or <code>NewDiscoveryClientForConfigOrDie</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/86168>#86168</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</li><li>Reverted a kubectl azure auth module change where oidc claim spn: prefix was omitted resulting a breaking behavior with existing Azure AD OIDC enabled api-server (<a href=https://github.com/kubernetes/kubernetes/pull/87507>#87507</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG API Machinery, Auth and Cloud Provider]</li><li>Shared informers are now more reliable in the face of network disruption. (<a href=https://github.com/kubernetes/kubernetes/pull/86015>#86015</a>, <a href=https://github.com/squeed>@squeed</a>) [SIG API Machinery]</li><li>Specifying PluginConfig for the same plugin more than once fails scheduler startup.
Specifying extenders and configuring .ignoredResources for the NodeResourcesFit plugin fails (<a href=https://github.com/kubernetes/kubernetes/pull/88870>#88870</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Terminating a restartPolicy=Never pod no longer has a chance to report the pod succeeded when it actually failed. (<a href=https://github.com/kubernetes/kubernetes/pull/88440>#88440</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node and Testing]</li><li>The CSR signing cert/key pairs will be reloaded from disk like the kube-apiserver cert/key pairs (<a href=https://github.com/kubernetes/kubernetes/pull/86816>#86816</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Apps and Auth]</li><li>The EventRecorder from k8s.io/client-go/tools/events will now create events in the default namespace (instead of kube-system) when the related object does not have it set. (<a href=https://github.com/kubernetes/kubernetes/pull/88815>#88815</a>, <a href=https://github.com/enj>@enj</a>) [SIG API Machinery]</li><li>The audit event sourceIPs list will now always end with the IP that sent the request directly to the API server. (<a href=https://github.com/kubernetes/kubernetes/pull/87167>#87167</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG API Machinery and Auth]</li><li>The sample-apiserver aggregated conformance test has updated to use the Kubernetes v1.17.0 sample apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/84735>#84735</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Architecture, CLI and Testing]</li><li>To reduce chances of throttling, VM cache is set to nil when Azure node provisioning state is deleting (<a href=https://github.com/kubernetes/kubernetes/pull/87635>#87635</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>VMSS cache is added so that less chances of VMSS GET throttling (<a href=https://github.com/kubernetes/kubernetes/pull/85885>#85885</a>, <a href=https://github.com/nilo19>@nilo19</a>) [SIG Cloud Provider]</li><li>Wait for kubelet & kube-proxy to be ready on Windows node within 10s (<a href=https://github.com/kubernetes/kubernetes/pull/85228>#85228</a>, <a href=https://github.com/YangLu1031>@YangLu1031</a>) [SIG Cluster Lifecycle]</li><li><code>kubectl apply -f &lt;file> --prune -n &lt;namespace></code> should prune all resources not defined in the file in the cli specified namespace. (<a href=https://github.com/kubernetes/kubernetes/pull/85613>#85613</a>, <a href=https://github.com/MartinKaburu>@MartinKaburu</a>) [SIG CLI]</li><li><code>kubectl create clusterrolebinding</code> creates rbac.authorization.k8s.io/v1 object (<a href=https://github.com/kubernetes/kubernetes/pull/85889>#85889</a>, <a href=https://github.com/oke-py>@oke-py</a>) [SIG CLI]</li><li><code>kubectl diff</code> now returns 1 only on diff finding changes, and >1 on kubectl errors. The "exit status code 1" message has also been muted. (<a href=https://github.com/kubernetes/kubernetes/pull/87437>#87437</a>, <a href=https://github.com/apelisse>@apelisse</a>) [SIG CLI and Testing]</li></ul><h2 id=dependencies>Dependencies</h2><ul><li>Update Calico to v3.8.4 (<a href=https://github.com/kubernetes/kubernetes/pull/84163>#84163</a>, <a href=https://github.com/david-tigera>@david-tigera</a>)[SIG Cluster Lifecycle]</li><li>Update aws-sdk-go dependency to v1.28.2 (<a href=https://github.com/kubernetes/kubernetes/pull/87253>#87253</a>, <a href=https://github.com/SaranBalaji90>@SaranBalaji90</a>)[SIG API Machinery and Cloud Provider]</li><li>Update CNI version to v0.8.5 (<a href=https://github.com/kubernetes/kubernetes/pull/78819>#78819</a>, <a href=https://github.com/justaugustus>@justaugustus</a>)[SIG Release, Testing, Network, Cluster Lifecycle and API Machinery]</li><li>Update cri-tools to v1.17.0 (<a href=https://github.com/kubernetes/kubernetes/pull/86305>#86305</a>, <a href=https://github.com/saschagrunert>@saschagrunert</a>)[SIG Release and Cluster Lifecycle]</li><li>Pause image upgraded to 3.2 in kubelet and kubeadm (<a href=https://github.com/kubernetes/kubernetes/pull/88173>#88173</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>)[SIG CLI, Node, Testing and Cluster Lifecycle]</li><li>Update CoreDNS version to 1.6.7 in kubeadm (<a href=https://github.com/kubernetes/kubernetes/pull/86260>#86260</a>, <a href=https://github.com/rajansandeep>@rajansandeep</a>)[SIG Cluster Lifecycle]</li><li>Update golang.org/x/crypto to fix CVE-2020-9283 (<a href=https://github.com/kubernetes/kubernetes/pull/88381>#8838</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>)[SIG CLI, Instrumentation, API Machinery, CLuster Lifecycle and Cloud Provider]</li><li>Update Go to 1.13.8 (<a href=https://github.com/kubernetes/kubernetes/pull/87648>#87648</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>)[SIG Release and Testing]</li><li>Update Cluster-Autoscaler to 1.18.0 (<a href=https://github.com/kubernetes/kubernetes/pull/89095>#89095</a>, <a href=https://github.com/losipiuk>@losipiuk</a>)[SIG Autoscaling and Cluster Lifecycle]</li></ul><h1 id=v1-18-0-rc-1>v1.18.0-rc.1</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-rc-1>Downloads for v1.18.0-rc.1</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>c17231d5de2e0677e8af8259baa11a388625821c79b86362049f2edb366404d6f4b4587b8f13ccbceeb2f32c6a9fe98607f779c0f3e1caec438f002e3a2c8c21</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>e84ffad57c301f5d6e90f916b996d5abb0c987928c3ca6b1565f7b042588f839b994ca12c43fc36f0ffb63f9fabc15110eb08be253b8939f49cd951e956da618</code></td></tr></tbody></table><h3 id=client-binaries-1>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>1aea99923d492436b3eb91aaecffac94e5d0aa2b38a0930d266fda85c665bbc4569745c409aa302247df3b578ce60324e7a489eb26240e97d4e65a67428ea3d1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>07fa7340a959740bd52b83ff44438bbd988e235277dad1e43f125f08ac85230a24a3b755f4e4c8645743444fa2b66a3602fc445d7da6d2fc3770e8c21ba24b33</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>48cebd26448fdd47aa36257baa4c716a98fda055bbf6a05230f2a3fe3c1b99b4e483668661415392190f3eebb9cb6e15c784626b48bb2541d93a37902f0e3974</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>c3a5fedf263f07a07f59c01fea6c63c1e0b76ee8dc67c45b6c134255c28ed69171ccc2f91b6a45d6a8ec5570a0a7562e24c33b9d7b0d1a864f4dc04b178b3c04</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>a6b11a55bd38583bbaac14931a6862f8ce6493afe30947ba29e5556654a571593358278df59412bbeb6888fa127e9ae4c0047a9d46cb59394995010796df6b14</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>9e15331ac8010154a9b64f5488969fc8ee2f21059639896cb84c5cf4f05f4c9d1d8970cb6f9831de6b34013848227c1972c12a698d07aac1ecc056e972fe6f79</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>f828fe6252678de9d4822e482f5873309ae9139b2db87298ab3273ce45d38aa07b6b9b42b76c140705f27ba71e101d58b43e59ac7259d7c08dc647ea809e207c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>19da4b45f0666c063934af616f3e7ed3caa99d4ee1e46d53efadc7a8a4d38e43a36ced7249acd7ad3dcc4b4f60d8451b4f7ec7727e478ee2fadd14d353228bce</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>775c9afb6cb3e7c4ba53e9f48a5df2cf207234a33059bd74448bc9f177dd120fb3f9c58ab45048a566326acc43bc8a67e886e10ef99f20780c8f63bb17426ebd</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>208d2595a5b57ac97aac75b4a2a6130f0c937f781a030bde1a432daf4bc51f2fa523fca2eb84c38798489c4b536ee90aad22f7be8477985d9691d51ad8e1c4dc</code></td></tr></tbody></table><h3 id=server-binaries-1>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>dcf832eae04f9f52ff473754ef5cfe697b35f4dc1a282622c94fa10943c8c35f4a8777a0c58c7de871c3c428c8973bf72d6bcd8751416d4c682125268b8fcefe</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>a04e34bea28eb1c8b492e8b1dd3c0dd87ebee71a7dbbef72be10a335e553361af7e48296e504f9844496b04e66350871114d20cfac3f3b49550d8be60f324ba3</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>a6af086b07a8c2e498f32b43e6511bf6a5e6baf358c572c6910c8df17cd6cae94f562f459714fcead1595767cb14c7f639c5735f1411173bbd38d5604c082a77</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>5a960ef5ba0c255f587f2ac0b028cd03136dc91e4efc5d1becab46417852e5524d18572b6f66259531ec6fea997da3c4d162ac153a9439672154375053fec6c7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>0f32c7d9b14bc238b9a5764d8f00edc4d3bf36bcf06b340b81061424e6070768962425194a8c2025c3a7ffb97b1de551d3ad23d1591ae34dd4e3ba25ab364c33</code></td></tr></tbody></table><h3 id=node-binaries-1>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>27d8955d535d14f3f4dca501fd27e4f06fad84c6da878ea5332a5c83b6955667f6f731bfacaf5a3a23c09f14caa400f9bee927a0f269f5374de7f79cd1919b3b</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>0d56eccad63ba608335988e90b377fe8ae978b177dc836cdb803a5c99d99e8f3399a666d9477ca9cfe5964944993e85c416aec10a99323e3246141efc0b1cc9e</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>79bb9be66f9e892d866b28e5cc838245818edb9706981fab6ccbff493181b341c1fcf6fe5d2342120a112eb93af413f5ba191cfba1ab4c4a8b0546a5ad8ec220</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>3e9e2c6f9a2747d828069511dce8b4034c773c2d122f005f4508e22518055c1e055268d9d86773bbd26fbd2d887d783f408142c6c2f56ab2f2365236fd4d2635</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>4f96e018c336fa13bb6df6f7217fe46a2b5c47f806f786499c429604ccba2ebe558503ab2c72f63250aa25b61dae2d166e4b80ae10f6ab37d714f87c1dcf6691</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-rc.1/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>ab110d76d506746af345e5897ef4f6993d5f53ac818ba69a334f3641047351aa63bfb3582841a9afca51dd0baff8b9010077d9c8ec85d2d69e4172b8d4b338b0</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-beta-2>Changelog since v1.18.0-beta.2</h2><h2 id=changes-by-kind-1>Changes by Kind</h2><h3 id=api-change-1>API Change</h3><ul><li>Removes ConfigMap as suggestion for IngressClass parameters (<a href=https://github.com/kubernetes/kubernetes/pull/89093>#89093</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Network]</li></ul><h3 id=other-bug-cleanup-or-flake-1>Other (Bug, Cleanup or Flake)</h3><ul><li>EndpointSlice should not contain endpoints for terminating pods (<a href=https://github.com/kubernetes/kubernetes/pull/89056>#89056</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Apps and Network]</li><li>Fix a bug where ExternalTrafficPolicy is not applied to service ExternalIPs. (<a href=https://github.com/kubernetes/kubernetes/pull/88786>#88786</a>, <a href=https://github.com/freehan>@freehan</a>) [SIG Network]</li><li>Fix invalid VMSS updates due to incorrect cache (<a href=https://github.com/kubernetes/kubernetes/pull/89002>#89002</a>, <a href=https://github.com/ArchangelSDY>@ArchangelSDY</a>) [SIG Cloud Provider]</li><li>Fix isCurrentInstance for Windows by removing the dependency of hostname. (<a href=https://github.com/kubernetes/kubernetes/pull/89138>#89138</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fixed a data race in kubelet image manager that can cause static pod workers to silently stop working. (<a href=https://github.com/kubernetes/kubernetes/pull/88915>#88915</a>, <a href=https://github.com/roycaihw>@roycaihw</a>) [SIG Node]</li><li>Fixed an issue that could cause the kubelet to incorrectly run concurrent pod reconciliation loops and crash. (<a href=https://github.com/kubernetes/kubernetes/pull/89055>#89055</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node]</li><li>Kube-proxy: on dual-stack mode, if it is not able to get the IP Family of an endpoint, logs it with level InfoV(4) instead of Warning, avoiding flooding the logs for endpoints without addresses (<a href=https://github.com/kubernetes/kubernetes/pull/88934>#88934</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG Network]</li><li>Update Cluster Autoscaler to 1.18.0; changelog: <a href=https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.18.0>https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.18.0</a> (<a href=https://github.com/kubernetes/kubernetes/pull/89095>#89095</a>, <a href=https://github.com/losipiuk>@losipiuk</a>) [SIG Autoscaling and Cluster Lifecycle]</li></ul><h1 id=v1-18-0-beta-2>v1.18.0-beta.2</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-beta-2>Downloads for v1.18.0-beta.2</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>3017430ca17f8a3523669b4a02c39cedfc6c48b07281bc0a67a9fbe9d76547b76f09529172cc01984765353a6134a43733b7315e0dff370bba2635dd2a6289af</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>c5fd60601380a99efff4458b1c9cf4dc02195f6f756b36e590e54dff68f7064daf32cf63980dddee13ef9dec7a60ad4eeb47a288083fdbbeeef4bc038384e9ea</code></td></tr></tbody></table><h3 id=client-binaries-2>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>7e49ede167b9271d4171e477fa21d267b2fb35f80869337d5b323198dc12f71b61441975bf925ad6e6cd7b61cbf6372d386417dc1e5c9b3c87ae651021c37237</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>3f5cdf0e85eee7d0773e0ae2df1c61329dea90e0da92b02dae1ffd101008dc4bade1c4951fc09f0cad306f0bcb7d16da8654334ddee43d5015913cc4ac8f3eda</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>b67b41c11bfecb88017c33feee21735c56f24cf6f7851b63c752495fc0fb563cd417a67a81f46bca091f74dc00fca1f296e483d2e3dfe2004ea4b42e252d30b9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>1fef2197cb80003e3a5c26f05e889af9d85fbbc23e27747944d2997ace4bfa28f3670b13c08f5e26b7e274176b4e2df89c1162aebd8b9506e63b39b311b2d405</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>84e5f4d9776490219ee94a84adccd5dfc7c0362eb330709771afcde95ec83f03d96fe7399eec218e47af0a1e6445e24d95e6f9c66c0882ef8233a09ff2022420</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>ba613b114e0cca32fa21a3d10f845aa2f215d3af54e775f917ff93919f7dd7075efe254e4047a85a1f4b817fc2bd78006c2e8873885f1208cbc02db99e2e2e25</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>502a6938d8c4bbe04abbd19b59919d86765058ff72334848be4012cec493e0e7027c6cd950cf501367ac2026eea9f518110cb72d1c792322b396fc2f73d23217</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>c24700e0ed2ef5c1d2dd282d638c88d90392ae90ea420837b39fd8e1cfc19525017325ccda71d8472fdaea174762208c09e1bba9bbc77c89deef6fac5e847ba2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>0d4c5a741b052f790c8b0923c9586ee9906225e51cf4dc8a56fc303d4d61bb5bf77fba9e65151dec7be854ff31da8fc2dcd3214563e1b4b9951e6af4aa643da4</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>841ef2e306c0c9593f04d9528ee019bf3b667761227d9afc1d6ca8bf1aa5631dc25f5fe13ff329c4bf0c816b971fd0dec808f879721e0f3bf51ce49772b38010</code></td></tr></tbody></table><h3 id=server-binaries-2>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>b373df2e6ef55215e712315a5508e85a39126bd81b7b93c6b6305238919a88c740077828a6f19bcd97141951048ef7a19806ef6b1c3e1772dbc45715c5fcb3af</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>b8103cb743c23076ce8dd7c2da01c8dd5a542fbac8480e82dc673139c8ee5ec4495ca33695e7a18dd36412cf1e18ed84c8de05042525ddd8e869fbdfa2766569</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>8f8f05cf64fb9c8d80cdcb4935b2d3e3edc48bdd303231ae12f93e3f4d979237490744a11e24ba7f52dbb017ca321a8e31624dcffa391b8afda3d02078767fa0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>b313b911c46f2ec129537407af3f165f238e48caeb4b9e530783ffa3659304a544ed02bef8ece715c279373b9fb2c781bd4475560e02c4b98a6d79837bc81938</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>a1b6b06571141f507b12e5ef98efb88f4b6b9aba924722b2a74f11278d29a2972ab8290608360151d124608e6e24da0eb3516d484cb5fa12ff2987562f15964a</code></td></tr></tbody></table><h3 id=node-binaries-2>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>20e02ca327543cddb2568ead3d5de164cbfb2914ab6416106d906bf12fcfbc4e55b13bea4d6a515e8feab038e2c929d72c4d6909dfd7881ba69fd1e8c772ab99</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>ecd817ef05d6284f9c6592b84b0a48ea31cf4487030c9fb36518474b2a33dad11b9c852774682e60e4e8b074e6bea7016584ca281dddbe2994da5eaf909025c0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>0020d32b7908ffd5055c8b26a8b3033e4702f89efcfffe3f6fcdb8a9921fa8eaaed4193c85597c24afd8c523662454f233521bb7055841a54c182521217ccc9d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>e065411d66d486e7793449c1b2f5a412510b913bf7f4e728c0a20e275642b7668957050dc266952cdff09acc391369ae6ac5230184db89af6823ba400745f2fc</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>082ee90413beaaea41d6cbe9a18f7d783a95852607f3b94190e0ca12aacdd97d87e233b87117871bfb7d0a4b6302fbc7688549492a9bc50a2f43a5452504d3ce</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.2/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>fb5aca0cc36be703f9d4033eababd581bac5de8399c50594db087a99ed4cb56e4920e960eb81d0132d696d094729254eeda2a5c0cb6e65e3abca6c8d61da579e</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-beta-1>Changelog since v1.18.0-beta.1</h2><h2 id=urgent-upgrade-notes-1>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade-1>(No, really, you MUST read this before you upgrade)</h3><ul><li><code>kubectl</code> no longer defaults to <code>http://localhost:8080</code>. If you own one of these legacy clusters, you are *strongly- encouraged to secure your server. If you cannot secure your server, you can set <code>KUBERNETES_MASTER</code> if you were relying on that behavior and you're a client-go user. Set <code>--server</code>, <code>--kubeconfig</code> or <code>KUBECONFIG</code> to make it work in <code>kubectl</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/86173>#86173</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG API Machinery, CLI and Testing]</li></ul><h2 id=changes-by-kind-2>Changes by Kind</h2><h3 id=deprecation-1>Deprecation</h3><ul><li>AlgorithmSource is removed from v1alpha2 Scheduler ComponentConfig (<a href=https://github.com/kubernetes/kubernetes/pull/87999>#87999</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Scheduling]</li><li>Kube-proxy: deprecate <code>--healthz-port</code> and <code>--metrics-port</code> flag, please use <code>--healthz-bind-address</code> and <code>--metrics-bind-address</code> instead (<a href=https://github.com/kubernetes/kubernetes/pull/88512>#88512</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Network]</li><li>Kubeadm: deprecate the usage of the experimental flag '--use-api' under the 'kubeadm alpha certs renew' command. (<a href=https://github.com/kubernetes/kubernetes/pull/88827>#88827</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li></ul><h3 id=api-change-2>API Change</h3><ul><li>A new IngressClass resource has been added to enable better Ingress configuration. (<a href=https://github.com/kubernetes/kubernetes/pull/88509>#88509</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG API Machinery, Apps, CLI, Network, Node and Testing]</li><li>Added GenericPVCDataSource feature gate to enable using arbitrary custom resources as the data source for a PVC. (<a href=https://github.com/kubernetes/kubernetes/pull/88636>#88636</a>, <a href=https://github.com/bswartz>@bswartz</a>) [SIG Apps and Storage]</li><li>Allow user to specify fsgroup permission change policy for pods (<a href=https://github.com/kubernetes/kubernetes/pull/88488>#88488</a>, <a href=https://github.com/gnufied>@gnufied</a>) [SIG Apps and Storage]</li><li>BlockVolume and CSIBlockVolume features are now GA. (<a href=https://github.com/kubernetes/kubernetes/pull/88673>#88673</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Apps, Node and Storage]</li><li>CustomResourceDefinition schemas that use <code>x-kubernetes-list-map-keys</code> to specify properties that uniquely identify list items must make those properties required or have a default value, to ensure those properties are present for all list items. See <a href=https://kubernetes.io/docs/reference/using-api/api-concepts/&amp;#35;merge-strategy>https://kubernetes.io/docs/reference/using-api/api-concepts/&amp;#35;merge-strategy</a> for details. (<a href=https://github.com/kubernetes/kubernetes/pull/88076>#88076</a>, <a href=https://github.com/eloyekunle>@eloyekunle</a>) [SIG API Machinery and Testing]</li><li>Fixes a regression with clients prior to 1.15 not being able to update podIP in pod status, or podCIDR in node spec, against >= 1.16 API servers (<a href=https://github.com/kubernetes/kubernetes/pull/88505>#88505</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Apps and Network]</li><li>Ingress: Add Exact and Prefix maching to Ingress PathTypes (<a href=https://github.com/kubernetes/kubernetes/pull/88587>#88587</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Apps, Cluster Lifecycle and Network]</li><li>Ingress: Add alternate backends via TypedLocalObjectReference (<a href=https://github.com/kubernetes/kubernetes/pull/88775>#88775</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Apps and Network]</li><li>Ingress: allow wildcard hosts in IngressRule (<a href=https://github.com/kubernetes/kubernetes/pull/88858>#88858</a>, <a href=https://github.com/cmluciano>@cmluciano</a>) [SIG Network]</li><li>Kube-controller-manager and kube-scheduler expose profiling by default to match the kube-apiserver. Use <code>--enable-profiling=false</code> to disable. (<a href=https://github.com/kubernetes/kubernetes/pull/88663>#88663</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Cloud Provider and Scheduling]</li><li>Move TaintBasedEvictions feature gates to GA (<a href=https://github.com/kubernetes/kubernetes/pull/87487>#87487</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG API Machinery, Apps, Node, Scheduling and Testing]</li><li>New flag --endpointslice-updates-batch-period in kube-controller-manager can be used to reduce number of endpointslice updates generated by pod changes. (<a href=https://github.com/kubernetes/kubernetes/pull/88745>#88745</a>, <a href=https://github.com/mborsz>@mborsz</a>) [SIG API Machinery, Apps and Network]</li><li>Scheduler Extenders can now be configured in the v1alpha2 component config (<a href=https://github.com/kubernetes/kubernetes/pull/88768>#88768</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Release, Scheduling and Testing]</li><li>The apiserver/v1alph1#EgressSelectorConfiguration API is now beta. (<a href=https://github.com/kubernetes/kubernetes/pull/88502>#88502</a>, <a href=https://github.com/caesarxuchao>@caesarxuchao</a>) [SIG API Machinery]</li><li>The storage.k8s.io/CSIDriver has moved to GA, and is now available for use. (<a href=https://github.com/kubernetes/kubernetes/pull/84814>#84814</a>, <a href=https://github.com/huffmanca>@huffmanca</a>) [SIG API Machinery, Apps, Auth, Node, Scheduling, Storage and Testing]</li><li>VolumePVCDataSource moves to GA in 1.18 release (<a href=https://github.com/kubernetes/kubernetes/pull/88686>#88686</a>, <a href=https://github.com/j-griffith>@j-griffith</a>) [SIG Apps, CLI and Cluster Lifecycle]</li></ul><h3 id=feature-1>Feature</h3><ul><li>Add <code>rest_client_rate_limiter_duration_seconds</code> metric to component-base to track client side rate limiter latency in seconds. Broken down by verb and URL. (<a href=https://github.com/kubernetes/kubernetes/pull/88134>#88134</a>, <a href=https://github.com/jennybuckley>@jennybuckley</a>) [SIG API Machinery, Cluster Lifecycle and Instrumentation]</li><li>Allow user to specify resource using --filename flag when invoking kubectl exec (<a href=https://github.com/kubernetes/kubernetes/pull/88460>#88460</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI and Testing]</li><li>Apiserver add a new flag --goaway-chance which is the fraction of requests that will be closed gracefully(GOAWAY) to prevent HTTP/2 clients from getting stuck on a single apiserver.
After the connection closed(received GOAWAY), the client's other in-flight requests won't be affected, and the client will reconnect.
The flag min value is 0 (off), max is .02 (1/50 requests); .001 (1/1000) is a recommended starting point.
Clusters with single apiservers, or which don't use a load balancer, should NOT enable this. (<a href=https://github.com/kubernetes/kubernetes/pull/88567>#88567</a>, <a href=https://github.com/answer1991>@answer1991</a>) [SIG API Machinery]</li><li>Azure: add support for single stack IPv6 (<a href=https://github.com/kubernetes/kubernetes/pull/88448>#88448</a>, <a href=https://github.com/aramase>@aramase</a>) [SIG Cloud Provider]</li><li>DefaultConstraints can be specified for the PodTopologySpread plugin in the component config (<a href=https://github.com/kubernetes/kubernetes/pull/88671>#88671</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Kubeadm: support Windows specific kubelet flags in kubeadm-flags.env (<a href=https://github.com/kubernetes/kubernetes/pull/88287>#88287</a>, <a href=https://github.com/gab-satchi>@gab-satchi</a>) [SIG Cluster Lifecycle and Windows]</li><li>Kubectl cluster-info dump changed to only display a message telling you the location where the output was written when the output is not standard output. (<a href=https://github.com/kubernetes/kubernetes/pull/88765>#88765</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li>Print NotReady when pod is not ready based on its conditions. (<a href=https://github.com/kubernetes/kubernetes/pull/88240>#88240</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</li><li>Scheduler Extender API is now located under k8s.io/kube-scheduler/extender (<a href=https://github.com/kubernetes/kubernetes/pull/88540>#88540</a>, <a href=https://github.com/damemi>@damemi</a>) [SIG Release, Scheduling and Testing]</li><li>Signatures on scale client methods have been modified to accept <code>context.Context</code> as a first argument. Signatures of Get, Update, and Patch methods have been updated to accept GetOptions, UpdateOptions and PatchOptions respectively. (<a href=https://github.com/kubernetes/kubernetes/pull/88599>#88599</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG API Machinery, Apps, Autoscaling and CLI]</li><li>Signatures on the dynamic client methods have been modified to accept <code>context.Context</code> as a first argument. Signatures of Delete and DeleteCollection methods now accept DeleteOptions by value instead of by reference. (<a href=https://github.com/kubernetes/kubernetes/pull/88906>#88906</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps, CLI, Cluster Lifecycle, Storage and Testing]</li><li>Signatures on the metadata client methods have been modified to accept <code>context.Context</code> as a first argument. Signatures of Delete and DeleteCollection methods now accept DeleteOptions by value instead of by reference. (<a href=https://github.com/kubernetes/kubernetes/pull/88910>#88910</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery, Apps and Testing]</li><li>Webhooks will have alpha support for network proxy (<a href=https://github.com/kubernetes/kubernetes/pull/85870>#85870</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG API Machinery, Auth and Testing]</li><li>When client certificate files are provided, reload files for new connections, and close connections when a certificate changes. (<a href=https://github.com/kubernetes/kubernetes/pull/79083>#79083</a>, <a href=https://github.com/jackkleeman>@jackkleeman</a>) [SIG API Machinery, Auth, Node and Testing]</li><li>When deleting objects using kubectl with the --force flag, you are no longer required to also specify --grace-period=0. (<a href=https://github.com/kubernetes/kubernetes/pull/87776>#87776</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li><code>kubectl</code> now contains a <code>kubectl alpha debug</code> command. This command allows attaching an ephemeral container to a running pod for the purposes of debugging. (<a href=https://github.com/kubernetes/kubernetes/pull/88004>#88004</a>, <a href=https://github.com/verb>@verb</a>) [SIG CLI]</li></ul><h3 id=documentation>Documentation</h3><ul><li>Update Japanese translation for kubectl help (<a href=https://github.com/kubernetes/kubernetes/pull/86837>#86837</a>, <a href=https://github.com/inductor>@inductor</a>) [SIG CLI and Docs]</li><li><code>kubectl plugin</code> now prints a note how to install krew (<a href=https://github.com/kubernetes/kubernetes/pull/88577>#88577</a>, <a href=https://github.com/corneliusweig>@corneliusweig</a>) [SIG CLI]</li></ul><h3 id=other-bug-cleanup-or-flake-2>Other (Bug, Cleanup or Flake)</h3><ul><li><p>Azure VMSS LoadBalancerBackendAddressPools updating has been improved with squential-sync + concurrent-async requests. (<a href=https://github.com/kubernetes/kubernetes/pull/88699>#88699</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>AzureFile and CephFS use new Mount library that prevents logging of sensitive mount options. (<a href=https://github.com/kubernetes/kubernetes/pull/88684>#88684</a>, <a href=https://github.com/saad-ali>@saad-ali</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Storage]</p></li><li><p>Build: Enable kube-cross image-building on K8s Infra (<a href=https://github.com/kubernetes/kubernetes/pull/88562>#88562</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG Release and Testing]</p></li><li><p>Client-go certificate manager rotation gained the ability to preserve optional intermediate chains accompanying issued certificates (<a href=https://github.com/kubernetes/kubernetes/pull/88744>#88744</a>, <a href=https://github.com/jackkleeman>@jackkleeman</a>) [SIG API Machinery and Auth]</p></li><li><p>Conformance image now depends on stretch-slim instead of debian-hyperkube-base as that image is being deprecated and removed. (<a href=https://github.com/kubernetes/kubernetes/pull/88702>#88702</a>, <a href=https://github.com/dims>@dims</a>) [SIG Cluster Lifecycle, Release and Testing]</p></li><li><p>Deprecate --generator flag from kubectl create commands (<a href=https://github.com/kubernetes/kubernetes/pull/88655>#88655</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG CLI]</p></li><li><p>FIX: prevent apiserver from panicking when failing to load audit webhook config file (<a href=https://github.com/kubernetes/kubernetes/pull/88879>#88879</a>, <a href=https://github.com/JoshVanL>@JoshVanL</a>) [SIG API Machinery and Auth]</p></li><li><p>Fix /readyz to return error immediately after a shutdown is initiated, before the --shutdown-delay-duration has elapsed. (<a href=https://github.com/kubernetes/kubernetes/pull/88911>#88911</a>, <a href=https://github.com/tkashem>@tkashem</a>) [SIG API Machinery]</p></li><li><p>Fix a bug where kubenet fails to parse the tc output. (<a href=https://github.com/kubernetes/kubernetes/pull/83572>#83572</a>, <a href=https://github.com/chendotjs>@chendotjs</a>) [SIG Network]</p></li><li><p>Fix describe ingress annotations not sorted. (<a href=https://github.com/kubernetes/kubernetes/pull/88394>#88394</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</p></li><li><p>Fix handling of aws-load-balancer-security-groups annotation. Security-Groups assigned with this annotation are no longer modified by kubernetes which is the expected behaviour of most users. Also no unnecessary Security-Groups are created anymore if this annotation is used. (<a href=https://github.com/kubernetes/kubernetes/pull/83446>#83446</a>, <a href=https://github.com/Elias481>@Elias481</a>) [SIG Cloud Provider]</p></li><li><p>Fix kubectl create deployment image name (<a href=https://github.com/kubernetes/kubernetes/pull/86636>#86636</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</p></li><li><p>Fix missing "apiVersion" for "involvedObject" in Events for Nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/87537>#87537</a>, <a href=https://github.com/uthark>@uthark</a>) [SIG Apps and Node]</p></li><li><p>Fix that prevents repeated fetching of PVC/PV objects by kubelet when processing of pod volumes fails. While this prevents hammering API server in these error scenarios, it means that some errors in processing volume(s) for a pod could now take up to 2-3 minutes before retry. (<a href=https://github.com/kubernetes/kubernetes/pull/88141>#88141</a>, <a href=https://github.com/tedyu>@tedyu</a>) [SIG Node and Storage]</p></li><li><p>Fix: azure file mount timeout issue (<a href=https://github.com/kubernetes/kubernetes/pull/88610>#88610</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</p></li><li><p>Fix: corrupted mount point in csi driver (<a href=https://github.com/kubernetes/kubernetes/pull/88569>#88569</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Storage]</p></li><li><p>Fixed a bug in the TopologyManager. Previously, the TopologyManager would only guarantee alignment if container creation was serialized in some way. Alignment is now guaranteed under all scenarios of container creation. (<a href=https://github.com/kubernetes/kubernetes/pull/87759>#87759</a>, <a href=https://github.com/klueska>@klueska</a>) [SIG Node]</p></li><li><p>Fixed block CSI volume cleanup after timeouts. (<a href=https://github.com/kubernetes/kubernetes/pull/88660>#88660</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Node and Storage]</p></li><li><p>Fixes issue where you can't attach more than 15 GCE Persistent Disks to c2, n2, m1, m2 machine types. (<a href=https://github.com/kubernetes/kubernetes/pull/88602>#88602</a>, <a href=https://github.com/yuga711>@yuga711</a>) [SIG Storage]</p></li><li><p>For volumes that allow attaches across multiple nodes, attach and detach operations across different nodes are now executed in parallel. (<a href=https://github.com/kubernetes/kubernetes/pull/88678>#88678</a>, <a href=https://github.com/verult>@verult</a>) [SIG Apps, Node and Storage]</p></li><li><p>Hide kubectl.kubernetes.io/last-applied-configuration in describe command (<a href=https://github.com/kubernetes/kubernetes/pull/88758>#88758</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG Auth and CLI]</p></li><li><p>In GKE alpha clusters it will be possible to use the service annotation <code>cloud.google.com/network-tier: Standard</code> (<a href=https://github.com/kubernetes/kubernetes/pull/88487>#88487</a>, <a href=https://github.com/zioproto>@zioproto</a>) [SIG Cloud Provider]</p></li><li><p>Kubelets perform fewer unnecessary pod status update operations on the API server. (<a href=https://github.com/kubernetes/kubernetes/pull/88591>#88591</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node and Scalability]</p></li><li><p>Plugin/PluginConfig and Policy APIs are mutually exclusive when running the scheduler (<a href=https://github.com/kubernetes/kubernetes/pull/88864>#88864</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</p></li><li><p>Specifying PluginConfig for the same plugin more than once fails scheduler startup.</p><p>Specifying extenders and configuring .ignoredResources for the NodeResourcesFit plugin fails (<a href=https://github.com/kubernetes/kubernetes/pull/88870>#88870</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</p></li><li><p>Support TLS Server Name overrides in kubeconfig file and via --tls-server-name in kubectl (<a href=https://github.com/kubernetes/kubernetes/pull/88769>#88769</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, Auth and CLI]</p></li><li><p>Terminating a restartPolicy=Never pod no longer has a chance to report the pod succeeded when it actually failed. (<a href=https://github.com/kubernetes/kubernetes/pull/88440>#88440</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node and Testing]</p></li><li><p>The EventRecorder from k8s.io/client-go/tools/events will now create events in the default namespace (instead of kube-system) when the related object does not have it set. (<a href=https://github.com/kubernetes/kubernetes/pull/88815>#88815</a>, <a href=https://github.com/enj>@enj</a>) [SIG API Machinery]</p></li><li><p>The audit event sourceIPs list will now always end with the IP that sent the request directly to the API server. (<a href=https://github.com/kubernetes/kubernetes/pull/87167>#87167</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG API Machinery and Auth]</p></li><li><p>Update to use golang 1.13.8 (<a href=https://github.com/kubernetes/kubernetes/pull/87648>#87648</a>, <a href=https://github.com/ialidzhikov>@ialidzhikov</a>) [SIG Release and Testing]</p></li><li><p>Validate kube-proxy flags --ipvs-tcp-timeout, --ipvs-tcpfin-timeout, --ipvs-udp-timeout (<a href=https://github.com/kubernetes/kubernetes/pull/88657>#88657</a>, <a href=https://github.com/chendotjs>@chendotjs</a>) [SIG Network]</p></li></ul><h1 id=v1-18-0-beta-1>v1.18.0-beta.1</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-beta-1>Downloads for v1.18.0-beta.1</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>7c182ca905b3a31871c01ab5fdaf46f074547536c7975e069ff230af0d402dfc0346958b1d084bd2c108582ffc407484e6a15a1cd93e9affbe34b6e99409ef1f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>d104b8c792b1517bd730787678c71c8ee3b259de81449192a49a1c6e37a6576d28f69b05c2019cc4a4c40ddeb4d60b80138323df3f85db8682caabf28e67c2de</code></td></tr></tbody></table><h3 id=client-binaries-3>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>bc337bb8f200a789be4b97ce99b9d7be78d35ebd64746307c28339dc4628f56d9903e0818c0888aaa9364357a528d1ac6fd34f74377000f292ec502fbea3837e</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>38dfa5e0b0cfff39942c913a6bcb2ad8868ec43457d35cffba08217bb6e7531720e0731f8588505f4c81193ce5ec0e5fe6870031cf1403fbbde193acf7e53540</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>8e63ec7ce29c69241120c037372c6c779e3f16253eabd612c7cbe6aa89326f5160eb5798004d723c5cd72d458811e98dac3574842eb6a57b2798ecd2bbe5bcf9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>c1be9f184a7c3f896a785c41cd6ece9d90d8cb9b1f6088bdfb5557d8856c55e455f6688f5f54c2114396d5ae7adc0361e34ebf8e9c498d0187bd785646ccc1d0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>8eab02453cfd9e847632a774a0e0cf3a33c7619fb4ced7f1840e1f71444e8719b1c8e8cbfdd1f20bb909f3abe39cdcac74f14cb9c878c656d35871b7c37c7cbe</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>f7df0ec02d2e7e63278d5386e8153cfe2b691b864f17b6452cc824a5f328d688976c975b076e60f1c6b3c859e93e477134fbccc53bb49d9e846fb038b34eee48</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>36dd5b10addca678a518e6d052c9d6edf473e3f87388a2f03f714c93c5fbfe99ace16cf3b382a531be20a8fe6f4160f8d891800dd2cff5f23c9ca12c2f4a151b</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>5bdbb44b996ab4ccf3a383780270f5cfdbf174982c300723c8bddf0a48ae5e459476031c1d51b9d30ffd621d0a126c18a5de132ef1d92fca2f3e477665ea10cc</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>5dea3d4c4e91ef889850143b361974250e99a3c526f5efee23ff9ccdcd2ceca4a2247e7c4f236bdfa77d2150157da5d676ac9c3ba26cf3a2f1e06d8827556f77</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>db298e698391368703e6aea7f4345aec5a4b8c69f9d8ff6c99fb5804a6cea16d295fb01e70fe943ade3d4ce9200a081ad40da21bd331317ec9213f69b4d6c48f</code></td></tr></tbody></table><h3 id=server-binaries-3>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>c6284929dd5940e750b48db72ffbc09f73c5ec31ab3db283babb8e4e07cd8cbb27642f592009caae4717981c0db82c16312849ef4cbafe76acc4264c7d5864ac</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>6fc9552cf082c54cc0833b19876117c87ba7feb5a12c7e57f71b52208daf03eaef3ca56bd22b7bce2d6e81b5a23537cf6f5497a6eaa356c0aab1d3de26c309f9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>b794b9c399e548949b5bfb2fe71123e86c2034847b2c99aca34b6de718a35355bbecdae9dc2a81c49e3c82fb4b5862526a3f63c2862b438895e12c5ea884f22e</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>fddaed7a54f97046a91c29534645811c6346e973e22950b2607b8c119c2377e9ec2d32144f81626078cdaeca673129cc4016c1a3dbd3d43674aa777089fb56ac</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>65951a534bb55069c7419f41cbcdfe2fae31541d8a3f9eca11fc2489addf281c5ad2d13719212657da0be5b898f22b57ac39446d99072872fbacb0a7d59a4f74</code></td></tr></tbody></table><h3 id=node-binaries-3>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>992059efb5cae7ed0ef55820368d854bad1c6d13a70366162cd3b5111ce24c371c7c87ded2012f055e08b2ff1b4ef506e1f4e065daa3ac474fef50b5efa4fb07</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>c63ae0f8add5821ad267774314b8c8c1ffe3b785872bf278e721fd5dfdad1a5db1d4db3720bea0a36bf10d9c6dd93e247560162c0eac6e1b743246f587d3b27a</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>47adb9ddf6eaf8f475b89f59ee16fbd5df183149a11ad1574eaa645b47a6d58aec2ca70ba857ce9f1a5793d44cf7a61ebc6874793bb685edaf19410f4f76fd13</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>a3bc4a165567c7b76a3e45ab7b102d6eb3ecf373eb048173f921a4964cf9be8891d0d5b8dafbd88c3af7b0e21ef3d41c1e540c3347ddd84b929b3a3d02ceb7b2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>109ddf37c748f69584c829db57107c3518defe005c11fcd2a1471845c15aae0a3c89aafdd734229f4069ed18856cc650c80436684e1bdc43cfee3149b0324746</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-beta.1/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>a3a75d2696ad3136476ad7d811e8eabaff5111b90e592695e651d6111f819ebf0165b8b7f5adc05afb5f7f01d1e5fb64876cb696e492feb20a477a5800382b7a</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-beta-0>Changelog since v1.18.0-beta.0</h2><h2 id=urgent-upgrade-notes-2>Urgent Upgrade Notes</h2><h3 id=no-really-you-must-read-this-before-you-upgrade-2>(No, really, you MUST read this before you upgrade)</h3><ul><li><p>The StreamingProxyRedirects feature and <code>--redirect-container-streaming</code> flag are deprecated, and will be removed in a future release. The default behavior (proxy streaming requests through the kubelet) will be the only supported option.
If you are setting <code>--redirect-container-streaming=true</code>, then you must migrate off this configuration. The flag will no longer be able to be enabled starting in v1.20. If you are not setting the flag, no action is necessary. (<a href=https://github.com/kubernetes/kubernetes/pull/88290>#88290</a>, <a href=https://github.com/tallclair>@tallclair</a>) [SIG API Machinery and Node]</p></li><li><p>Yes.</p><p>Feature Name: Support using network resources (VNet, LB, IP, etc.) in different AAD Tenant and Subscription than those for the cluster.</p><p>Changes in Pull Request:</p><ol><li>Add properties <code>networkResourceTenantID</code> and <code>networkResourceSubscriptionID</code> in cloud provider auth config section, which indicates the location of network resources.</li><li>Add function <code>GetMultiTenantServicePrincipalToken</code> to fetch multi-tenant service principal token, which will be used by Azure VM/VMSS Clients in this feature.</li><li>Add function <code>GetNetworkResourceServicePrincipalToken</code> to fetch network resource service principal token, which will be used by Azure Network Resource (Load Balancer, Public IP, Route Table, Network Security Group and their sub level resources) Clients in this feature.</li><li>Related unit tests.</li></ol><p>None.</p><p>User Documentation: In PR <a href=https://github.com/kubernetes-sigs/cloud-provider-azure/pull/301>https://github.com/kubernetes-sigs/cloud-provider-azure/pull/301</a> (<a href=https://github.com/kubernetes/kubernetes/pull/88384>#88384</a>, <a href=https://github.com/bowen5>@bowen5</a>) [SIG Cloud Provider]</p></li></ul><h2 id=changes-by-kind-3>Changes by Kind</h2><h3 id=deprecation-2>Deprecation</h3><ul><li>Azure service annotation service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset has been deprecated. Its support would be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/88462>#88462</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li></ul><h3 id=api-change-3>API Change</h3><ul><li>API additions to apiserver types (<a href=https://github.com/kubernetes/kubernetes/pull/87179>#87179</a>, <a href=https://github.com/Jefftree>@Jefftree</a>) [SIG API Machinery, Cloud Provider and Cluster Lifecycle]</li><li>Add Scheduling Profiles to kubescheduler.config.k8s.io/v1alpha2 (<a href=https://github.com/kubernetes/kubernetes/pull/88087>#88087</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling and Testing]</li><li>Added support for multiple sizes huge pages on a container level (<a href=https://github.com/kubernetes/kubernetes/pull/84051>#84051</a>, <a href=https://github.com/bart0sh>@bart0sh</a>) [SIG Apps, Node and Storage]</li><li>AppProtocol is a new field on Service and Endpoints resources, enabled with the ServiceAppProtocol feature gate. (<a href=https://github.com/kubernetes/kubernetes/pull/88503>#88503</a>, <a href=https://github.com/robscott>@robscott</a>) [SIG Apps and Network]</li><li>Fixed missing validation of uniqueness of list items in lists with <code>x-kubernetes-list-type: map</code> or x-kubernetes-list-type: set` in CustomResources. (<a href=https://github.com/kubernetes/kubernetes/pull/84920>#84920</a>, <a href=https://github.com/sttts>@sttts</a>) [SIG API Machinery]</li><li>Introduces optional --detect-local flag to kube-proxy.
Currently the only supported value is "cluster-cidr",
which is the default if not specified. (<a href=https://github.com/kubernetes/kubernetes/pull/87748>#87748</a>, <a href=https://github.com/satyasm>@satyasm</a>) [SIG Cluster Lifecycle, Network and Scheduling]</li><li>Kube-scheduler can run more than one scheduling profile. Given a pod, the profile is selected by using its <code>.spec.SchedulerName</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/88285>#88285</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Apps, Scheduling and Testing]</li><li>Moving Windows RunAsUserName feature to GA (<a href=https://github.com/kubernetes/kubernetes/pull/87790>#87790</a>, <a href=https://github.com/marosset>@marosset</a>) [SIG Apps and Windows]</li></ul><h3 id=feature-2>Feature</h3><ul><li>Add --dry-run to kubectl delete, taint, replace (<a href=https://github.com/kubernetes/kubernetes/pull/88292>#88292</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI and Testing]</li><li>Add huge page stats to Allocated resources in "kubectl describe node" (<a href=https://github.com/kubernetes/kubernetes/pull/80605>#80605</a>, <a href=https://github.com/odinuge>@odinuge</a>) [SIG CLI]</li><li>Kubeadm: The ClusterStatus struct present in the kubeadm-config ConfigMap is deprecated and will be removed on a future version. It is going to be maintained by kubeadm until it gets removed. The same information can be found on <code>etcd</code> and <code>kube-apiserver</code> pod annotations, <code>kubeadm.kubernetes.io/etcd.advertise-client-urls</code> and <code>kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint</code> respectively. (<a href=https://github.com/kubernetes/kubernetes/pull/87656>#87656</a>, <a href=https://github.com/ereslibre>@ereslibre</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: add the experimental feature gate PublicKeysECDSA that can be used to create a
cluster with ECDSA certificates from "kubeadm init". Renewal of existing ECDSA certificates is
also supported using "kubeadm alpha certs renew", but not switching between the RSA and
ECDSA algorithms on the fly or during upgrades. (<a href=https://github.com/kubernetes/kubernetes/pull/86953>#86953</a>, <a href=https://github.com/rojkov>@rojkov</a>) [SIG API Machinery, Auth and Cluster Lifecycle]</li><li>Kubeadm: on kubeconfig certificate renewal, keep the embedded CA in sync with the one on disk (<a href=https://github.com/kubernetes/kubernetes/pull/88052>#88052</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: upgrade supports fallback to the nearest known etcd version if an unknown k8s version is passed (<a href=https://github.com/kubernetes/kubernetes/pull/88373>#88373</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>New flag <code>--show-hidden-metrics-for-version</code> in kube-scheduler can be used to show all hidden metrics that deprecated in the previous minor release. (<a href=https://github.com/kubernetes/kubernetes/pull/84913>#84913</a>, <a href=https://github.com/serathius>@serathius</a>) [SIG Instrumentation and Scheduling]</li><li>Scheduler framework permit plugins now run at the end of the scheduling cycle, after reserve plugins. Waiting on permit will remain in the beginning of the binding cycle. (<a href=https://github.com/kubernetes/kubernetes/pull/88199>#88199</a>, <a href=https://github.com/mateuszlitwin>@mateuszlitwin</a>) [SIG Scheduling]</li><li>The kubelet and the default docker runtime now support running ephemeral containers in the Linux process namespace of a target container. Other container runtimes must implement this feature before it will be available in that runtime. (<a href=https://github.com/kubernetes/kubernetes/pull/84731>#84731</a>, <a href=https://github.com/verb>@verb</a>) [SIG Node]</li></ul><h3 id=other-bug-cleanup-or-flake-3>Other (Bug, Cleanup or Flake)</h3><ul><li>Add delays between goroutines for vm instance update (<a href=https://github.com/kubernetes/kubernetes/pull/88094>#88094</a>, <a href=https://github.com/aramase>@aramase</a>) [SIG Cloud Provider]</li><li>Add init containers log to cluster dump info. (<a href=https://github.com/kubernetes/kubernetes/pull/88324>#88324</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>CPU limits are now respected for Windows containers. If a node is over-provisioned, no weighting is used - only limits are respected. (<a href=https://github.com/kubernetes/kubernetes/pull/86101>#86101</a>, <a href=https://github.com/PatrickLang>@PatrickLang</a>) [SIG Node, Testing and Windows]</li><li>Cloud provider config CloudProviderBackoffMode has been removed since it won't be used anymore. (<a href=https://github.com/kubernetes/kubernetes/pull/88463>#88463</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Evictions due to pods breaching their ephemeral storage limits are now recorded by the <code>kubelet_evictions</code> metric and can be alerted on. (<a href=https://github.com/kubernetes/kubernetes/pull/87906>#87906</a>, <a href=https://github.com/smarterclayton>@smarterclayton</a>) [SIG Node]</li><li>Fix: add remediation in azure disk attach/detach (<a href=https://github.com/kubernetes/kubernetes/pull/88444>#88444</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fix: check disk status before disk azure disk (<a href=https://github.com/kubernetes/kubernetes/pull/88360>#88360</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fixed cleaning of CSI raw block volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/87978>#87978</a>, <a href=https://github.com/jsafrane>@jsafrane</a>) [SIG Storage]</li><li>Get-kube.sh uses the gcloud's current local GCP service account for auth when the provider is GCE or GKE instead of the metadata server default (<a href=https://github.com/kubernetes/kubernetes/pull/88383>#88383</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG Cluster Lifecycle]</li><li>Golang/x/net has been updated to bring in fixes for CVE-2020-9283 (<a href=https://github.com/kubernetes/kubernetes/pull/88381>#88381</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle and Instrumentation]</li><li>Kubeadm now includes CoreDNS version 1.6.7 (<a href=https://github.com/kubernetes/kubernetes/pull/86260>#86260</a>, <a href=https://github.com/rajansandeep>@rajansandeep</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: fix the bug that 'kubeadm upgrade' hangs in single node cluster (<a href=https://github.com/kubernetes/kubernetes/pull/88434>#88434</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Optimize kubectl version help info (<a href=https://github.com/kubernetes/kubernetes/pull/88313>#88313</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Removes the deprecated command <code>kubectl rolling-update</code> (<a href=https://github.com/kubernetes/kubernetes/pull/88057>#88057</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG Architecture, CLI and Testing]</li></ul><h1 id=v1-18-0-alpha-5>v1.18.0-alpha.5</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-alpha-5>Downloads for v1.18.0-alpha.5</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>6452cac2b80721e9f577cb117c29b9ac6858812b4275c2becbf74312566f7d016e8b34019bd1bf7615131b191613bf9b973e40ad9ac8f6de9007d41ef2d7fd70</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>e41d9d4dd6910a42990051fcdca4bf5d3999df46375abd27ffc56aae9b455ae984872302d590da6aa85bba6079334fb5fe511596b415ee79843dee1c61c137da</code></td></tr></tbody></table><h3 id=client-binaries-4>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>5c95935863492b31d4aaa6be93260088dafea27663eb91edca980ca3a8485310e60441bc9050d4d577e9c3f7ffd96db516db8d64321124cec1b712e957c9fe1c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>868faa578b3738604d8be62fae599ccc556799f1ce54807f1fe72599f20f8a1f98ad8152fac14a08a463322530b696d375253ba3653325e74b587df6e0510da3</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>76a89d1d30b476b47f8fb808e342f89608e5c1c1787c4c06f2d7e763f9482e2ae8b31e6ad26541972e2b9a3a7c28327e3150cdd355e8b8d8b050a801bbf08d49</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>07ad96a09b44d1c707d7c68312c5d69b101a3424bf1e6e9400b2e7a3fba78df04302985d473ddd640d8f3f0257be34110dbe1304b9565dd9d7a4639b7b7b85fd</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>c04fed9fa370a75c1b8e18b2be0821943bb9befcc784d14762ea3278e73600332a9b324d5eeaa1801d20ad6be07a553c41dcf4fa7ab3eadd0730ab043d687c8c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>4199147dea9954333df26d34248a1cb7b02ebbd6380ffcd42d9f9ed5fdabae45a59215474dab3c11436c82e60bd27cbd03b3dde288bf611cd3e78b87c783c6a9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>4f6d4d61d1c52d3253ca19031ebcd4bad06d19b68bbaaab5c8e8c590774faea4a5ceab1f05f2706b61780927e1467815b3479342c84d45df965aba78414727c4</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>e2a454151ae5dd891230fb516a3f73f73ab97832db66fd3d12e7f1657a569f58a9fe2654d50ddd7d8ec88a5ff5094199323a4c6d7d44dcf7edb06cca11dd4de1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>14b262ba3b71c41f545db2a017cf1746075ada5745a858d2a62bc9df7c5dc10607220375db85e2c4cb85307b09709e58bc66a407488e0961191e3249dc7742b0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>26353c294755a917216664364b524982b7f5fc6aa832ce90134bb178df8a78604963c68873f121ea5f2626ff615bdbf2ffe54e00578739cde6df42ffae034732</code></td></tr></tbody></table><h3 id=server-binaries-4>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>ba77e0e7c610f59647c1b2601f82752964a0f54b7ad609a89b00fcfd553d0f0249f6662becbabaa755bb769b36a2000779f08022c40fb8cc61440337481317a1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>45e87b3e844ea26958b0b489e8c9b90900a3253000850f5ff9e87ffdcafba72ab8fd17b5ba092051a58a4bc277912c047a85940ec7f093dff6f9e8bf6fed3b42</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>155e136e3124ead69c594eead3398d6cfdbb8f823c324880e8a7bbd1b570b05d13a77a69abd0a6758cfcc7923971cc6da4d3e0c1680fd519b632803ece00d5ce</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>3fa0fb8221da19ad9d03278961172b7fa29a618b30abfa55e7243bb937dede8df56658acf02e6b61e7274fbc9395e237f49c62f2a83017eca2a69f67af31c01c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>db3199c3d7ba0b326d71dc8b80f50b195e79e662f71386a3b2976d47d13d7b0136887cc21df6f53e70a3d733da6eac7bbbf3bab2df8a1909a3cee4b44c32dd0b</code></td></tr></tbody></table><h3 id=node-binaries-4>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>addcdfbad7f12647e6babb8eadf853a374605c8f18bf63f416fa4d3bf1b903aa206679d840433206423a984bb925e7983366edcdf777cf5daef6ef88e53d6dfa</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>b2ac54e0396e153523d116a2aaa32c919d6243931e0104cd47a23f546d710e7abdaa9eae92d978ce63c92041e63a9b56f5dd8fd06c812a7018a10ecac440f768</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>7aab36f2735cba805e4fd109831a1af0f586a88db3f07581b6dc2a2aab90076b22c96b490b4f6461a8fb690bf78948b6d514274f0d6fb0664081de2d44dc48e1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>a579936f07ebf86f69f297ac50ba4c34caf2c0b903f73190eb581c78382b05ef36d41ade5bfd25d7b1b658cfcbee3d7125702a18e7480f9b09a62733a512a18a</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>58fa0359ddd48835192fab1136a2b9b45d1927b04411502c269cda07cb8a8106536973fb4c7fedf1d41893a524c9fe2e21078fdf27bfbeed778273d024f14449</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.5/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>9086c03cd92b440686cea6d8c4e48045cc46a43ab92ae0e70350b3f51804b9e2aaae7178142306768bae00d9ef6dd938167972bfa90b12223540093f735a45db</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-alpha-3>Changelog since v1.18.0-alpha.3</h2><h3 id=deprecation-3>Deprecation</h3><ul><li>Kubeadm: command line option "kubelet-version" for <code>kubeadm upgrade node</code> has been deprecated and will be removed in a future release. (<a href=https://github.com/kubernetes/kubernetes/pull/87942>#87942</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li></ul><h3 id=api-change-4>API Change</h3><ul><li>Kubelet podresources API now provides the information about active pods only. (<a href=https://github.com/kubernetes/kubernetes/pull/79409>#79409</a>, <a href=https://github.com/takmatsu>@takmatsu</a>) [SIG Node]</li><li>Remove deprecated fields from .leaderElection in kubescheduler.config.k8s.io/v1alpha2 (<a href=https://github.com/kubernetes/kubernetes/pull/87904>#87904</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Signatures on generated clientset methods have been modified to accept <code>context.Context</code> as a first argument. Signatures of generated Create, Update, and Patch methods have been updated to accept CreateOptions, UpdateOptions and PatchOptions respectively. Clientsets that with the previous interface have been added in new "deprecated" packages to allow incremental migration to the new APIs. The deprecated packages will be removed in the 1.21 release. (<a href=https://github.com/kubernetes/kubernetes/pull/87299>#87299</a>, <a href=https://github.com/mikedanese>@mikedanese</a>) [SIG API Machinery, Apps, Auth, Autoscaling, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Scheduling, Storage, Testing and Windows]</li><li>The k8s.io/node-api component is no longer updated. Instead, use the RuntimeClass types located within k8s.io/api, and the generated clients located within k8s.io/client-go (<a href=https://github.com/kubernetes/kubernetes/pull/87503>#87503</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Node and Release]</li></ul><h3 id=feature-3>Feature</h3><ul><li>Add indexer for storage cacher (<a href=https://github.com/kubernetes/kubernetes/pull/85445>#85445</a>, <a href=https://github.com/shaloulcy>@shaloulcy</a>) [SIG API Machinery]</li><li>Add support for mount options to the FC volume plugin (<a href=https://github.com/kubernetes/kubernetes/pull/87499>#87499</a>, <a href=https://github.com/ejweber>@ejweber</a>) [SIG Storage]</li><li>Added a config-mode flag in azure auth module to enable getting AAD token without spn: prefix in audience claim. When it's not specified, the default behavior doesn't change. (<a href=https://github.com/kubernetes/kubernetes/pull/87630>#87630</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG API Machinery, Auth, CLI and Cloud Provider]</li><li>Introduced BackoffManager interface for backoff management (<a href=https://github.com/kubernetes/kubernetes/pull/87829>#87829</a>, <a href=https://github.com/zhan849>@zhan849</a>) [SIG API Machinery]</li><li>PodTopologySpread plugin now excludes terminatingPods when making scheduling decisions. (<a href=https://github.com/kubernetes/kubernetes/pull/87845>#87845</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li><li>Promote CSIMigrationOpenStack to Beta (off by default since it requires installation of the OpenStack Cinder CSI Driver)
The in-tree AWS OpenStack Cinder "kubernetes.io/cinder" was already deprecated a while ago and will be removed in 1.20. Users should enable CSIMigration + CSIMigrationOpenStack features and install the OpenStack Cinder CSI Driver (<a href=https://github.com/kubernetes-sigs/cloud-provider-openstack>https://github.com/kubernetes-sigs/cloud-provider-openstack</a>) to avoid disruption to existing Pod and PVC objects at that time.
Users should start using the OpenStack Cinder CSI Driver directly for any new volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/85637>#85637</a>, <a href=https://github.com/dims>@dims</a>) [SIG Cloud Provider]</li></ul><h3 id=design>Design</h3><ul><li>The scheduler Permit extension point doesn't return a boolean value in its Allow() and Reject() functions. (<a href=https://github.com/kubernetes/kubernetes/pull/87936>#87936</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>) [SIG Scheduling]</li></ul><h3 id=other-bug-cleanup-or-flake-4>Other (Bug, Cleanup or Flake)</h3><ul><li>Adds "volume.beta.kubernetes.io/migrated-to" annotation to PV's and PVC's when they are migrated to signal external provisioners to pick up those objects for Provisioning and Deleting. (<a href=https://github.com/kubernetes/kubernetes/pull/87098>#87098</a>, <a href=https://github.com/davidz627>@davidz627</a>) [SIG Apps and Storage]</li><li>Fix a bug in the dual-stack IPVS proxier where stale IPv6 endpoints were not being cleaned up (<a href=https://github.com/kubernetes/kubernetes/pull/87695>#87695</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG Network]</li><li>Fix kubectl drain ignore daemonsets and others. (<a href=https://github.com/kubernetes/kubernetes/pull/87361>#87361</a>, <a href=https://github.com/zhouya0>@zhouya0</a>) [SIG CLI]</li><li>Fix: add azure disk migration support for CSINode (<a href=https://github.com/kubernetes/kubernetes/pull/88014>#88014</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider and Storage]</li><li>Fix: add non-retriable errors in azure clients (<a href=https://github.com/kubernetes/kubernetes/pull/87941>#87941</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>) [SIG Cloud Provider]</li><li>Fixed NetworkPolicy validation that Except values are accepted when they are outside the CIDR range. (<a href=https://github.com/kubernetes/kubernetes/pull/86578>#86578</a>, <a href=https://github.com/tnqn>@tnqn</a>) [SIG Network]</li><li>Improves performance of the node authorizer (<a href=https://github.com/kubernetes/kubernetes/pull/87696>#87696</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Auth]</li><li>Iptables/userspace proxy: improve performance by getting local addresses only once per sync loop, instead of for every external IP (<a href=https://github.com/kubernetes/kubernetes/pull/85617>#85617</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>) [SIG API Machinery, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation and Network]</li><li>Kube-aggregator: always sets unavailableGauge metric to reflect the current state of a service. (<a href=https://github.com/kubernetes/kubernetes/pull/87778>#87778</a>, <a href=https://github.com/p0lyn0mial>@p0lyn0mial</a>) [SIG API Machinery]</li><li>Kubeadm allows to configure single-stack clusters if dual-stack is enabled (<a href=https://github.com/kubernetes/kubernetes/pull/87453>#87453</a>, <a href=https://github.com/aojea>@aojea</a>) [SIG API Machinery, Cluster Lifecycle and Network]</li><li>Kubeadm: 'kubeadm alpha kubelet config download' has been removed, please use 'kubeadm upgrade node phase kubelet-config' instead (<a href=https://github.com/kubernetes/kubernetes/pull/87944>#87944</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubeadm: remove 'kubeadm upgrade node config' command since it was deprecated in v1.15, please use 'kubeadm upgrade node phase kubelet-config' instead (<a href=https://github.com/kubernetes/kubernetes/pull/87975>#87975</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li><li>Kubectl describe <type>and kubectl top pod will return a message saying "No resources found" or "No resources found in <namespace>namespace" if there are no results to display. (<a href=https://github.com/kubernetes/kubernetes/pull/87527>#87527</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li>Kubelet metrics gathered through metrics-server or prometheus should no longer timeout for Windows nodes running more than 3 pods. (<a href=https://github.com/kubernetes/kubernetes/pull/87730>#87730</a>, <a href=https://github.com/marosset>@marosset</a>) [SIG Node, Testing and Windows]</li><li>Kubelet metrics have been changed to buckets.
For example the exec/{podNamespace}/{podID}/{containerName} is now just exec. (<a href=https://github.com/kubernetes/kubernetes/pull/87913>#87913</a>, <a href=https://github.com/cheftako>@cheftako</a>) [SIG Node]</li><li>Limit number of instances in a single update to GCE target pool to 1000. (<a href=https://github.com/kubernetes/kubernetes/pull/87881>#87881</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG Cloud Provider, Network and Scalability]</li><li>Make Azure clients only retry on specified HTTP status codes (<a href=https://github.com/kubernetes/kubernetes/pull/88017>#88017</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Pause image contains "Architecture" in non-amd64 images (<a href=https://github.com/kubernetes/kubernetes/pull/87954>#87954</a>, <a href=https://github.com/BenTheElder>@BenTheElder</a>) [SIG Release]</li><li>Pods that are considered for preemption and haven't started don't produce an error log. (<a href=https://github.com/kubernetes/kubernetes/pull/87900>#87900</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>Prevent error message from being displayed when running kubectl plugin list and your path includes an empty string (<a href=https://github.com/kubernetes/kubernetes/pull/87633>#87633</a>, <a href=https://github.com/brianpursley>@brianpursley</a>) [SIG CLI]</li><li><code>kubectl create clusterrolebinding</code> creates rbac.authorization.k8s.io/v1 object (<a href=https://github.com/kubernetes/kubernetes/pull/85889>#85889</a>, <a href=https://github.com/oke-py>@oke-py</a>) [SIG CLI]</li></ul><h1 id=v1-18-0-alpha-4>v1.18.0-alpha.4</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=important-note-about-manual-tag>Important note about manual tag</h2><p>Due to a <a href=https://github.com/kubernetes/release/issues/1080>tagging bug in our Release Engineering tooling</a> during <code>v1.18.0-alpha.3</code>, we needed to push a manual tag (<code>v1.18.0-alpha.4</code>).</p><p><strong>No binaries have been produced or will be provided for <code>v1.18.0-alpha.4</code>.</strong></p><p>The changelog for <code>v1.18.0-alpha.4</code> is included as part of the [changelog since v1.18.0-alpha.3][#changelog-since-v1180-alpha3] section.</p><h1 id=v1-18-0-alpha-3>v1.18.0-alpha.3</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-alpha-3>Downloads for v1.18.0-alpha.3</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>60bf3bfc23b428f53fd853bac18a4a905b980fcc0bacd35ccd6357a89cfc26e47de60975ea6b712e65980e6b9df82a22331152d9f08ed4dba44558ba23a422d4</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>8adf1016565a7c93713ab6fa4293c2d13b4f6e4e1ec4dcba60bd71e218b4dbe9ef5eb7dbb469006743f498fc7ddeb21865cd12bec041af60b1c0edce8b7aecd5</code></td></tr></tbody></table><h3 id=client-binaries-5>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>abb32e894e8280c772e96227b574da81cd1eac374b8d29158b7f222ed550087c65482eef4a9817dfb5f2baf0d9b85fcdfa8feced0fbc1aacced7296853b57e1f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>5e4b1a993264e256ec1656305de7c306094cae9781af8f1382df4ce4eed48ce030827fde1a5e757d4ad57233d52075c9e4e93a69efbdc1102e4ba810705ccddc</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>68da39c2ae101d2b38f6137ceda07eb0c2124794982a62ef483245dbffb0611c1441ca085fa3127e7a9977f45646788832a783544ff06954114548ea0e526e46</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>dc236ffa8ad426620e50181419e9bebe3c161e953dbfb8a019f61b11286e1eb950b40d7cc03423bdf3e6974973bcded51300f98b55570c29732fa492dcde761d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>ab0a8bd6dc31ea160b731593cdc490b3cc03668b1141cf95310bd7060dcaf55c7ee9842e0acae81063fdacb043c3552ccdd12a94afd71d5310b3ce056fdaa06c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>159ea083c601710d0d6aea423eeb346c99ffaf2abd137d35a53e87a07f5caf12fca8790925f3196f67b768fa92a024f83b50325dbca9ccd4dde6c59acdce3509</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>16b0459adfa26575d13be49ab53ac7f0ffd05e184e4e13d2dfbfe725d46bb8ac891e1fd8aebe36ecd419781d4cc5cf3bd2aaaf5263cf283724618c4012408f40</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>d5aa1f5d89168995d2797eb839a04ce32560f405b38c1c0baaa0e313e4771ae7bb3b28e22433ad5897d36aadf95f73eb69d8d411d31c4115b6b0adf5fe041f85</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>374e16a1e52009be88c94786f80174d82dff66399bf294c9bee18a2159c42251c5debef1109a92570799148b08024960c6c50b8299a93fd66ebef94f198f34e9</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>5a94c1068c19271f810b994adad8e62fae03b3d4473c7c9e6d056995ff7757ea61dd8d140c9267dd41e48808876673ce117826d35a3c1bb5652752f11a044d57</code></td></tr></tbody></table><h3 id=server-binaries-5>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>a677bec81f0eba75114b92ff955bac74512b47e53959d56a685dae5edd527283d91485b1e86ad74ef389c5405863badf7eb22e2f0c9a568a4d0cb495c6a5c32f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>2fb696f86ff13ebeb5f3cf2b254bf41303644c5ea84a292782eac6123550702655284d957676d382698c091358e5c7fe73f32803699c19be7138d6530fe413b6</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>738e95da9cfb8f1309479078098de1c38cef5e1dd5ee1129b77651a936a412b7cd0cf15e652afc7421219646a98846ab31694970432e48dea9c9cafa03aa59cf</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>7a85bfcbb2aa636df60c41879e96e788742ecd72040cb0db2a93418439c125218c58a4cfa96d01b0296c295793e94c544e87c2d98d50b49bc4cb06b41f874376</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>1f1cdb2efa3e7cac857203d8845df2fdaa5cf1f20df764efffff29371945ec58f6deeba06f8fbf70b96faf81b0c955bf4cb84e30f9516cb2cc1ed27c2d2185a6</code></td></tr></tbody></table><h3 id=node-binaries-5>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>4ccfced3f5ba4adfa58f4a9d1b2c5bdb3e89f9203ab0e27d11eb1c325ac323ebe63c015d2c9d070b233f5d1da76cab5349da3528511c1cd243e66edc9af381c4</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>d695a69d18449062e4c129e54ec8384c573955f8108f4b78adc2ec929719f2196b995469c728dd6656c63c44cda24315543939f85131ebc773cfe0de689df55b</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>21df1da88c89000abc22f97e482c3aaa5ce53ec9628d83dda2e04a1d86c4d53be46c03ed6f1f211df3ee5071bce39d944ff7716b5b6ada3b9c4821d368b0a898</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>ff77e3aacb6ed9d89baed92ef542c8b5cec83151b6421948583cf608bca3b779dce41fc6852961e00225d5e1502f6a634bfa61a36efa90e1aee90dedb787c2d2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>57d75b7977ec1a0f6e7ed96a304dbb3b8664910f42ca19aab319a9ec33535ff5901dfca4abcb33bf5741cde6d152acd89a5f8178f0efe1dc24430e0c1af5b98f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.3/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>63fdbb71773cfd73a914c498e69bb9eea3fc314366c99ffb8bd42ec5b4dae807682c83c1eb5cfb1e2feb4d11d9e49cc85ba644e954241320a835798be7653d61</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-alpha-2>Changelog since v1.18.0-alpha.2</h2><h3 id=deprecation-4>Deprecation</h3><ul><li>Remove all the generators from kubectl run. It will now only create pods. Additionally, deprecates all the flags that are not relevant anymore. (<a href=https://github.com/kubernetes/kubernetes/pull/87077>#87077</a>, <a href=https://github.com/soltysh>@soltysh</a>) [SIG Architecture, SIG CLI, and SIG Testing]</li><li>kubeadm: kube-dns is deprecated and will not be supported in a future version (<a href=https://github.com/kubernetes/kubernetes/pull/86574>#86574</a>, <a href=https://github.com/SataQiu>@SataQiu</a>) [SIG Cluster Lifecycle]</li></ul><h3 id=api-change-5>API Change</h3><ul><li>Add kubescheduler.config.k8s.io/v1alpha2 (<a href=https://github.com/kubernetes/kubernetes/pull/87628>#87628</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling]</li><li>--enable-cadvisor-endpoints is now disabled by default. If you need access to the cAdvisor v1 Json API please enable it explicitly in the kubelet command line. Please note that this flag was deprecated in 1.15 and will be removed in 1.19. (<a href=https://github.com/kubernetes/kubernetes/pull/87440>#87440</a>, <a href=https://github.com/dims>@dims</a>) [SIG Instrumentation, SIG Node, and SIG Testing]</li><li>The following feature gates are removed, because the associated features were unconditionally enabled in previous releases: CustomResourceValidation, CustomResourceSubresources, CustomResourceWebhookConversion, CustomResourcePublishOpenAPI, CustomResourceDefaulting (<a href=https://github.com/kubernetes/kubernetes/pull/87475>#87475</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG API Machinery]</li></ul><h3 id=feature-4>Feature</h3><ul><li><p>aggragation api will have alpha support for network proxy (<a href=https://github.com/kubernetes/kubernetes/pull/87515>#87515</a>, <a href=https://github.com/Sh4d1>@Sh4d1</a>) [SIG API Machinery]</p></li><li><p>API request throttling (due to a high rate of requests) is now reported in client-go logs at log level 2. The messages are of the form</p><p>Throttling request took 1.50705208s, request: GET:<url></p><p>The presence of these messages, may indicate to the administrator the need to tune the cluster accordingly. (<a href=https://github.com/kubernetes/kubernetes/pull/87740>#87740</a>, <a href=https://github.com/jennybuckley>@jennybuckley</a>) [SIG API Machinery]</p></li><li><p>kubeadm: reject a node joining the cluster if a node with the same name already exists (<a href=https://github.com/kubernetes/kubernetes/pull/81056>#81056</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</p></li><li><p>disableAvailabilitySetNodes is added to avoid VM list for VMSS clusters. It should only be used when vmType is "vmss" and all the nodes (including masters) are VMSS virtual machines. (<a href=https://github.com/kubernetes/kubernetes/pull/87685>#87685</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</p></li><li><p>The kubectl --dry-run flag now accepts the values 'client', 'server', and 'none', to support client-side and server-side dry-run strategies. The boolean and unset values for the --dry-run flag are deprecated and a value will be required in a future version. (<a href=https://github.com/kubernetes/kubernetes/pull/87580>#87580</a>, <a href=https://github.com/julianvmodesto>@julianvmodesto</a>) [SIG CLI]</p></li><li><p>Add support for pre-allocated hugepages for more than one page size (<a href=https://github.com/kubernetes/kubernetes/pull/82820>#82820</a>, <a href=https://github.com/odinuge>@odinuge</a>) [SIG Apps]</p></li><li><p>Update CNI version to v0.8.5 (<a href=https://github.com/kubernetes/kubernetes/pull/78819>#78819</a>, <a href=https://github.com/justaugustus>@justaugustus</a>) [SIG API Machinery, SIG Cluster Lifecycle, SIG Network, SIG Release, and SIG Testing]</p></li><li><p>Skip default spreading scoring plugin for pods that define TopologySpreadConstraints (<a href=https://github.com/kubernetes/kubernetes/pull/87566>#87566</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>) [SIG Scheduling]</p></li><li><p>Added more details to taint toleration errors (<a href=https://github.com/kubernetes/kubernetes/pull/87250>#87250</a>, <a href=https://github.com/starizard>@starizard</a>) [SIG Apps, and SIG Scheduling]</p></li><li><p>Scheduler: Add DefaultBinder plugin (<a href=https://github.com/kubernetes/kubernetes/pull/87430>#87430</a>, <a href=https://github.com/alculquicondor>@alculquicondor</a>) [SIG Scheduling, and SIG Testing]</p></li><li><p>Kube-apiserver metrics will now include request counts, latencies, and response sizes for /healthz, /livez, and /readyz requests. (<a href=https://github.com/kubernetes/kubernetes/pull/83598>#83598</a>, <a href=https://github.com/jktomer>@jktomer</a>) [SIG API Machinery]</p></li></ul><h3 id=other-bug-cleanup-or-flake-5>Other (Bug, Cleanup or Flake)</h3><ul><li>Fix the masters rolling upgrade causing thundering herd of LISTs on etcd leading to control plane unavailability. (<a href=https://github.com/kubernetes/kubernetes/pull/86430>#86430</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>) [SIG API Machinery, SIG Node, and SIG Testing]</li><li><code>kubectl diff</code> now returns 1 only on diff finding changes, and >1 on kubectl errors. The "exit status code 1" message as also been muted. (<a href=https://github.com/kubernetes/kubernetes/pull/87437>#87437</a>, <a href=https://github.com/apelisse>@apelisse</a>) [SIG CLI, and SIG Testing]</li><li>To reduce chances of throttling, VM cache is set to nil when Azure node provisioning state is deleting (<a href=https://github.com/kubernetes/kubernetes/pull/87635>#87635</a>, <a href=https://github.com/feiskyer>@feiskyer</a>) [SIG Cloud Provider]</li><li>Fix regression in statefulset conversion which prevented applying a statefulset multiple times. (<a href=https://github.com/kubernetes/kubernetes/pull/87706>#87706</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Apps, and SIG Testing]</li><li>fixed two scheduler metrics (pending_pods and schedule_attempts_total) not being recorded (<a href=https://github.com/kubernetes/kubernetes/pull/87692>#87692</a>, <a href=https://github.com/everpeace>@everpeace</a>) [SIG Scheduling]</li><li>Resolved a performance issue in the node authorizer index maintenance. (<a href=https://github.com/kubernetes/kubernetes/pull/87693>#87693</a>, <a href=https://github.com/liggitt>@liggitt</a>) [SIG Auth]</li><li>Removed the 'client' label from apiserver_request_total. (<a href=https://github.com/kubernetes/kubernetes/pull/87669>#87669</a>, <a href=https://github.com/logicalhan>@logicalhan</a>) [SIG API Machinery, and SIG Instrumentation]</li><li><code>(*"k8s.io/client-go/rest".Request).{Do,DoRaw,Stream,Watch}</code> now require callers to pass a <code>context.Context</code> as an argument. The context is used for timeout and cancellation signaling and to pass supplementary information to round trippers in the wrapped transport chain. If you don't need any of this functionality, it is sufficient to pass a context created with <code>context.Background()</code> to these functions. The <code>(*"k8s.io/client-go/rest".Request).Context</code> method is removed now that all methods that execute a request accept a context directly. (<a href=https://github.com/kubernetes/kubernetes/pull/87597>#87597</a>, <a href=https://github.com/mikedanese>@mikedanese</a>) [SIG API Machinery, SIG Apps, SIG Auth, SIG Autoscaling, SIG CLI, SIG Cloud Provider, SIG Cluster Lifecycle, SIG Instrumentation, SIG Network, SIG Node, SIG Scheduling, SIG Storage, and SIG Testing]</li><li>For volumes that allow attaches across multiple nodes, attach and detach operations across different nodes are now executed in parallel. (<a href=https://github.com/kubernetes/kubernetes/pull/87258>#87258</a>, <a href=https://github.com/verult>@verult</a>) [SIG Apps, SIG Node, and SIG Storage]</li><li>kubeadm: apply further improvements to the tentative support for concurrent etcd member join. Fixes a bug where multiple members can receive the same hostname. Increase the etcd client dial timeout and retry timeout for add/remove/... operations. (<a href=https://github.com/kubernetes/kubernetes/pull/87505>#87505</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Reverted a kubectl azure auth module change where oidc claim spn: prefix was omitted resulting a breaking behavior with existing Azure AD OIDC enabled api-server (<a href=https://github.com/kubernetes/kubernetes/pull/87507>#87507</a>, <a href=https://github.com/weinong>@weinong</a>) [SIG API Machinery, SIG Auth, and SIG Cloud Provider]</li><li>Update cri-tools to v1.17.0 (<a href=https://github.com/kubernetes/kubernetes/pull/86305>#86305</a>, <a href=https://github.com/saschagrunert>@saschagrunert</a>) [SIG Cluster Lifecycle, and SIG Release]</li><li>kubeadm: remove the deprecated CoreDNS feature-gate. It was set to "true" since v1.11 when the feature went GA. In v1.13 it was marked as deprecated and hidden from the CLI. (<a href=https://github.com/kubernetes/kubernetes/pull/87400>#87400</a>, <a href=https://github.com/neolit123>@neolit123</a>) [SIG Cluster Lifecycle]</li><li>Shared informers are now more reliable in the face of network disruption. (<a href=https://github.com/kubernetes/kubernetes/pull/86015>#86015</a>, <a href=https://github.com/squeed>@squeed</a>) [SIG API Machinery]</li><li>the CSR signing cert/key pairs will be reloaded from disk like the kube-apiserver cert/key pairs (<a href=https://github.com/kubernetes/kubernetes/pull/86816>#86816</a>, <a href=https://github.com/deads2k>@deads2k</a>) [SIG API Machinery, SIG Apps, and SIG Auth]</li><li>"kubectl describe statefulsets.apps" prints garbage for rolling update partition (<a href=https://github.com/kubernetes/kubernetes/pull/85846>#85846</a>, <a href=https://github.com/phil9909>@phil9909</a>) [SIG CLI]</li></ul><h1 id=v1-18-0-alpha-2>v1.18.0-alpha.2</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-alpha-2>Downloads for v1.18.0-alpha.2</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>7af83386b4b35353f0aa1bdaf73599eb08b1d1ca11ecc2c606854aff754db69f3cd3dc761b6d7fc86f01052f615ca53185f33dbf9e53b2f926b0f02fc103fbd3</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>a14b02a0a0bde97795a836a8f5897b0ee6b43e010e13e43dd4cca80a5b962a1ef3704eedc7916fed1c38ec663a71db48c228c91e5daacba7d9370df98c7ddfb6</code></td></tr></tbody></table><h3 id=client-binaries-6>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>427f214d47ded44519007de2ae87160c56c2920358130e474b768299751a9affcbc1b1f0f936c39c6138837bca2a97792a6700896976e98c4beee8a1944cfde1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>861fd81ac3bd45765575bedf5e002a2294aba48ef9e15980fc7d6783985f7d7fcde990ea0aef34690977a88df758722ec0a2e170d5dcc3eb01372e64e5439192</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>7d59b05d6247e2606a8321c72cd239713373d876dbb43b0fb7f1cb857fa6c998038b41eeed78d9eb67ce77b0b71776ceed428cce0f8d2203c5181b473e0bd86c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>7cdefb4e32bad9d2df5bb8e7e0a6f4dab2ae6b7afef5d801ac5c342d4effdeacd799081fa2dec699ecf549200786c7623c3176252010f12494a95240dd63311d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>6212bbf0fa1d01ced77dcca2c4b76b73956cd3c6b70e0701c1fe0df5ff37160835f6b84fa2481e0e6979516551b14d8232d1c72764a559a3652bfe2a1e7488ff</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>1f0d9990700510165ee471acb2f88222f1b80e8f6deb351ce14cf50a70a9840fb99606781e416a13231c74b2bd7576981b5348171aa33b628d2666e366cd4629</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>77e00ba12a32db81e96f8de84609de93f32c61bb3f53875a57496d213aa6d1b92c09ad5a6de240a78e1a5bf77fac587ff92874f34a10f8909ae08ca32fda45d2</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>a39ec2044bed5a4570e9c83068e0fc0ce923ccffa44380f8bbc3247426beaff79c8a84613bcb58b05f0eb3afbc34c79fe3309aa2e0b81abcfd0aa04770e62e05</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>1a0ab88f9b7e34b60ab31d5538e97202a256ad8b7b7ed5070cae5f2f12d5d4edeae615db7a34ebbe254004b6393c6b2480100b09e30e59c9139492a3019a596a</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>1966eb5dfb78c1bc33aaa6389f32512e3aa92584250a0164182f3566c81d901b59ec78ee4e25df658bc1dd221b5a9527d6ce3b6c487ca3e3c0b319a077caa735</code></td></tr></tbody></table><h3 id=server-binaries-6>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>f814d6a3872e4572aa4da297c29def4c1fad8eba0903946780b6bf9788c72b99d71085c5aef9e12c01133b26fa4563c1766ba724ad2a8af2670a24397951a94d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>56aa08225e546c92c2ff88ac57d3db7dd5e63640772ea72a429f080f7069827138cbc206f6f5fe3a0c01bfca043a9eda305ecdc1dcb864649114893e46b6dc84</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>fb87128d905211ba097aa860244a376575ae2edbaca6e51402a24bc2964854b9b273e09df3d31a2bcffc91509f7eecb2118b183fb0e0eb544f33403fa235c274</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>6d21fbf39b9d3a0df9642407d6f698fabdc809aca83af197bceb58a81b25846072f407f8fb7caae2e02dc90912e3e0f5894f062f91bcb69f8c2329625d3dfeb7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>ddcda4dc360ca97705f71bf2a18ddacd7b7ddf77535b62e699e97a1b2dd24843751313351d0112e238afe69558e8271eba4d27ab77bb67b4b9e3fbde6eec85c9</code></td></tr></tbody></table><h3 id=node-binaries-6>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>78915a9bde35c70c67014f0cea8754849db4f6a84491a3ad9678fd3bc0203e43af5a63cfafe104ae1d56b05ce74893a87a6dcd008d7859e1af6b3bce65425b5d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>3218e811abcb0cb09d80742def339be3916db5e9bbc62c0dc8e6d87085f7e3d9eeed79dea081906f1de78ddd07b7e3acdbd7765fdb838d262bb35602fd1df106</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>fa22de9c4440b8fb27f4e77a5a63c5e1c8aa8aa30bb79eda843b0f40498c21b8c0ad79fff1d841bb9fef53fe20da272506de9a86f81a0b36d028dbeab2e482ce</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>bbda9b5cc66e8f13d235703b2a85e2c4f02fa16af047be4d27a3e198e11eb11706e4a0fbb6c20978c770b069cd4cd9894b661f09937df9d507411548c36576e0</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>b2ed1eda013069adce2aac00b86d75b84e006cfce9bafac0b5a2bafcb60f8f2cb346b5ea44eafa72d777871abef1ea890eb3a2a05de28968f9316fa88886a8ed</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.2/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>bd8eb23dba711f31b5148257076b1bbe9629f2a75de213b2c779bd5b29279e9bf22f8bde32f4bc814f4c0cc49e19671eb8b24f4105f0fe2c1490c4b78ec3c704</code></td></tr></tbody></table><h2 id=changelog-since-v1-18-0-alpha-1>Changelog since v1.18.0-alpha.1</h2><h3 id=other-notable-changes>Other notable changes</h3><ul><li>Bump golang/mock version to v1.3.1 (<a href=https://github.com/kubernetes/kubernetes/pull/87326>#87326</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>fix a bug that orphan revision cannot be adopted and statefulset cannot be synced (<a href=https://github.com/kubernetes/kubernetes/pull/86801>#86801</a>, <a href=https://github.com/likakuli>@likakuli</a>)</li><li>Azure storage clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/87306>#87306</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</li><li>Introduce Alpha field <code>Immutable</code> in both Secret and ConfigMap objects to mark their contents as immutable. The implementation is hidden behind feature gate <code>ImmutableEphemeralVolumes</code> (currently in Alpha stage). (<a href=https://github.com/kubernetes/kubernetes/pull/86377>#86377</a>, <a href=https://github.com/wojtek-t>@wojtek-t</a>)</li><li>EndpointSlices will now be enabled by default. A new <code>EndpointSliceProxying</code> feature gate determines if kube-proxy will use EndpointSlices, this is disabled by default. (<a href=https://github.com/kubernetes/kubernetes/pull/86137>#86137</a>, <a href=https://github.com/robscott>@robscott</a>)</li><li>kubeadm upgrades always persist the etcd backup for stacked (<a href=https://github.com/kubernetes/kubernetes/pull/86861>#86861</a>, <a href=https://github.com/SataQiu>@SataQiu</a>)</li><li>Fix the bug PIP's DNS is deleted if no DNS label service annotation isn't set. (<a href=https://github.com/kubernetes/kubernetes/pull/87246>#87246</a>, <a href=https://github.com/nilo19>@nilo19</a>)</li><li>New flag <code>--show-hidden-metrics-for-version</code> in kube-controller-manager can be used to show all hidden metrics that deprecated in the previous minor release. (<a href=https://github.com/kubernetes/kubernetes/pull/85281>#85281</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)</li><li>Azure network and VM clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/87122>#87122</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</li><li><code>kubectl apply -f &lt;file> --prune -n &lt;namespace></code> should prune all resources not defined in the file in the cli specified namespace. (<a href=https://github.com/kubernetes/kubernetes/pull/85613>#85613</a>, <a href=https://github.com/MartinKaburu>@MartinKaburu</a>)</li><li>Fixes service account token admission error in clusters that do not run the service account token controller (<a href=https://github.com/kubernetes/kubernetes/pull/87029>#87029</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>CustomResourceDefinition status fields are no longer required for client validation when submitting manifests. (<a href=https://github.com/kubernetes/kubernetes/pull/87213>#87213</a>, <a href=https://github.com/hasheddan>@hasheddan</a>)</li><li>All apiservers log request lines in a more greppable format. (<a href=https://github.com/kubernetes/kubernetes/pull/87203>#87203</a>, <a href=https://github.com/lavalamp>@lavalamp</a>)</li><li>provider/azure: Network security groups can now be in a separate resource group. (<a href=https://github.com/kubernetes/kubernetes/pull/87035>#87035</a>, <a href=https://github.com/CecileRobertMichon>@CecileRobertMichon</a>)</li><li>Cleaned up the output from <code>kubectl describe CSINode &lt;name></code>. (<a href=https://github.com/kubernetes/kubernetes/pull/85283>#85283</a>, <a href=https://github.com/huffmanca>@huffmanca</a>)</li><li>Fixed the following (<a href=https://github.com/kubernetes/kubernetes/pull/84265>#84265</a>, <a href=https://github.com/bhagwat070919>@bhagwat070919</a>)<ul><li><ul><li>AWS Cloud Provider attempts to delete LoadBalancer security group it didn’t provision</li></ul></li><li><ul><li>AWS Cloud Provider creates default LoadBalancer security group even if annotation [service.beta.kubernetes.io/aws-load-balancer-security-groups] is present</li></ul></li></ul></li><li>kubelet: resource metrics endpoint <code>/metrics/resource/v1alpha1</code> as well as all metrics under this endpoint have been deprecated. (<a href=https://github.com/kubernetes/kubernetes/pull/86282>#86282</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)<ul><li>Please convert to the following metrics emitted by endpoint <code>/metrics/resource</code>:</li><li><ul><li>scrape_error --> scrape_error</li></ul></li><li><ul><li>node_cpu_usage_seconds_total --> node_cpu_usage_seconds</li></ul></li><li><ul><li>node_memory_working_set_bytes --> node_memory_working_set_bytes</li></ul></li><li><ul><li>container_cpu_usage_seconds_total --> container_cpu_usage_seconds</li></ul></li><li><ul><li>container_memory_working_set_bytes --> container_memory_working_set_bytes</li></ul></li><li><ul><li>scrape_error --> scrape_error</li></ul></li></ul></li><li>You can now pass "--node-ip ::" to kubelet to indicate that it should autodetect an IPv6 address to use as the node's primary address. (<a href=https://github.com/kubernetes/kubernetes/pull/85850>#85850</a>, <a href=https://github.com/danwinship>@danwinship</a>)</li><li>kubeadm: support automatic retry after failing to pull image (<a href=https://github.com/kubernetes/kubernetes/pull/86899>#86899</a>, <a href=https://github.com/SataQiu>@SataQiu</a>)</li><li>TODO (<a href=https://github.com/kubernetes/kubernetes/pull/87044>#87044</a>, <a href=https://github.com/jennybuckley>@jennybuckley</a>)</li><li>Improved yaml parsing performance (<a href=https://github.com/kubernetes/kubernetes/pull/85458>#85458</a>, <a href=https://github.com/cjcullen>@cjcullen</a>)</li><li>Fixed a bug which could prevent a provider ID from ever being set for node if an error occurred determining the provider ID when the node was added. (<a href=https://github.com/kubernetes/kubernetes/pull/87043>#87043</a>, <a href=https://github.com/zjs>@zjs</a>)</li><li>fix a regression in kubenet that prevent pods to obtain ip addresses (<a href=https://github.com/kubernetes/kubernetes/pull/85993>#85993</a>, <a href=https://github.com/chendotjs>@chendotjs</a>)</li><li>Bind kube-dns containers to linux nodes to avoid Windows scheduling (<a href=https://github.com/kubernetes/kubernetes/pull/83358>#83358</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>The following features are unconditionally enabled and the corresponding <code>--feature-gates</code> flags have been removed: <code>PodPriority</code>, <code>TaintNodesByCondition</code>, <code>ResourceQuotaScopeSelectors</code> and <code>ScheduleDaemonSetPods</code> (<a href=https://github.com/kubernetes/kubernetes/pull/86210>#86210</a>, <a href=https://github.com/draveness>@draveness</a>)</li><li>Bind dns-horizontal containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83364>#83364</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>fix kubectl annotate error when local=true is set (<a href=https://github.com/kubernetes/kubernetes/pull/86952>#86952</a>, <a href=https://github.com/zhouya0>@zhouya0</a>)</li><li>Bug fixes: (<a href=https://github.com/kubernetes/kubernetes/pull/84163>#84163</a>, <a href=https://github.com/david-tigera>@david-tigera</a>)<ul><li>Make sure we include latest packages node #351 (<a href=https://github.com/caseydavenport>@caseydavenport</a>)</li></ul></li><li>fix kuebctl apply set-last-applied namespaces error (<a href=https://github.com/kubernetes/kubernetes/pull/86474>#86474</a>, <a href=https://github.com/zhouya0>@zhouya0</a>)</li><li>Add VolumeBinder method to FrameworkHandle interface, which allows user to get the volume binder when implementing scheduler framework plugins. (<a href=https://github.com/kubernetes/kubernetes/pull/86940>#86940</a>, <a href=https://github.com/skilxn-go>@skilxn-go</a>)</li><li>elasticsearch supports automatically setting the advertise address (<a href=https://github.com/kubernetes/kubernetes/pull/85944>#85944</a>, <a href=https://github.com/SataQiu>@SataQiu</a>)</li><li>If a serving certificates param specifies a name that is an IP for an SNI certificate, it will have priority for replying to server connections. (<a href=https://github.com/kubernetes/kubernetes/pull/85308>#85308</a>, <a href=https://github.com/deads2k>@deads2k</a>)</li><li>kube-proxy: Added dual-stack IPv4/IPv6 support to the iptables proxier. (<a href=https://github.com/kubernetes/kubernetes/pull/82462>#82462</a>, <a href=https://github.com/vllry>@vllry</a>)</li><li>Azure VMSS/VMSSVM clients now suppress requests on throttling (<a href=https://github.com/kubernetes/kubernetes/pull/86740>#86740</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</li><li>New metric kubelet_pleg_last_seen_seconds to aid diagnosis of PLEG not healthy issues. (<a href=https://github.com/kubernetes/kubernetes/pull/86251>#86251</a>, <a href=https://github.com/bboreham>@bboreham</a>)</li><li>For subprotocol negotiation, both client and server protocol is required now. (<a href=https://github.com/kubernetes/kubernetes/pull/86646>#86646</a>, <a href=https://github.com/tedyu>@tedyu</a>)</li><li>kubeadm: use bind-address option to configure the kube-controller-manager and kube-scheduler http probes (<a href=https://github.com/kubernetes/kubernetes/pull/86493>#86493</a>, <a href=https://github.com/aojea>@aojea</a>)</li><li>Marked scheduler's metrics scheduling_algorithm_predicate_evaluation_seconds and (<a href=https://github.com/kubernetes/kubernetes/pull/86584>#86584</a>, <a href=https://github.com/xiaoanyunfei>@xiaoanyunfei</a>)<ul><li>scheduling_algorithm_priority_evaluation_seconds as deprecated. Those are replaced by framework_extension_point_duration_seconds[extenstion_point="Filter"] and framework_extension_point_duration_seconds[extenstion_point="Score"] respectively.</li></ul></li><li>Marked scheduler's scheduling_duration_seconds Summary metric as deprecated (<a href=https://github.com/kubernetes/kubernetes/pull/86586>#86586</a>, <a href=https://github.com/xiaoanyunfei>@xiaoanyunfei</a>)</li><li>Add instructions about how to bring up e2e test cluster (<a href=https://github.com/kubernetes/kubernetes/pull/85836>#85836</a>, <a href=https://github.com/YangLu1031>@YangLu1031</a>)</li><li>If a required flag is not provided to a command, the user will only see the required flag error message, instead of the entire usage menu. (<a href=https://github.com/kubernetes/kubernetes/pull/86693>#86693</a>, <a href=https://github.com/sallyom>@sallyom</a>)</li><li>kubeadm: tolerate whitespace when validating certificate authority PEM data in kubeconfig files (<a href=https://github.com/kubernetes/kubernetes/pull/86705>#86705</a>, <a href=https://github.com/neolit123>@neolit123</a>)</li><li>kubeadm: add support for the "ci/k8s-master" version label as a replacement for "ci-cross/*", which no longer exists. (<a href=https://github.com/kubernetes/kubernetes/pull/86609>#86609</a>, <a href=https://github.com/Pensu>@Pensu</a>)</li><li>Fix EndpointSlice controller race condition and ensure that it handles external changes to EndpointSlices. (<a href=https://github.com/kubernetes/kubernetes/pull/85703>#85703</a>, <a href=https://github.com/robscott>@robscott</a>)</li><li>Fix nil pointer dereference in azure cloud provider (<a href=https://github.com/kubernetes/kubernetes/pull/85975>#85975</a>, <a href=https://github.com/ldx>@ldx</a>)</li><li>fix: azure disk could not mounted on Standard_DC4s/DC2s instances (<a href=https://github.com/kubernetes/kubernetes/pull/86612>#86612</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>)</li><li>Fixes v1.17.0 regression in --service-cluster-ip-range handling with IPv4 ranges larger than 65536 IP addresses (<a href=https://github.com/kubernetes/kubernetes/pull/86534>#86534</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>Adds back support for AlwaysCheckAllPredicates flag. (<a href=https://github.com/kubernetes/kubernetes/pull/86496>#86496</a>, <a href=https://github.com/ahg-g>@ahg-g</a>)</li><li>Azure global rate limit is switched to per-client. A set of new rate limit configure options are introduced, including routeRateLimit, SubnetsRateLimit, InterfaceRateLimit, RouteTableRateLimit, LoadBalancerRateLimit, PublicIPAddressRateLimit, SecurityGroupRateLimit, VirtualMachineRateLimit, StorageAccountRateLimit, DiskRateLimit, SnapshotRateLimit, VirtualMachineScaleSetRateLimit and VirtualMachineSizeRateLimit. (<a href=https://github.com/kubernetes/kubernetes/pull/86515>#86515</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)<ul><li>The original rate limit options would be default values for those new client's rate limiter.</li></ul></li><li>Fix issue <a href=https://github.com/kubernetes/kubernetes/pull/85805>#85805</a> about resource not found in azure cloud provider when lb specified in other resource group. (<a href=https://github.com/kubernetes/kubernetes/pull/86502>#86502</a>, <a href=https://github.com/levimm>@levimm</a>)</li><li><code>AlwaysCheckAllPredicates</code> is deprecated in scheduler Policy API. (<a href=https://github.com/kubernetes/kubernetes/pull/86369>#86369</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>)</li><li>Kubernetes KMS provider for data encryption now supports disabling the in-memory data encryption key (DEK) cache by setting cachesize to a negative value. (<a href=https://github.com/kubernetes/kubernetes/pull/86294>#86294</a>, <a href=https://github.com/enj>@enj</a>)</li><li>option <code>preConfiguredBackendPoolLoadBalancerTypes</code> is added to azure cloud provider for the pre-configured load balancers, possible values: <code>""</code>, <code>"internal"</code>, "external"<code>, </code>"all"` (<a href=https://github.com/kubernetes/kubernetes/pull/86338>#86338</a>, <a href=https://github.com/gossion>@gossion</a>)</li><li>Promote StartupProbe to beta for 1.18 release (<a href=https://github.com/kubernetes/kubernetes/pull/83437>#83437</a>, <a href=https://github.com/matthyx>@matthyx</a>)</li><li>Fixes issue where AAD token obtained by kubectl is incompatible with on-behalf-of flow and oidc. (<a href=https://github.com/kubernetes/kubernetes/pull/86412>#86412</a>, <a href=https://github.com/weinong>@weinong</a>)<ul><li>The audience claim before this fix has "spn:" prefix. After this fix, "spn:" prefix is omitted.</li></ul></li><li>change CounterVec to Counter about PLEGDiscardEvent (<a href=https://github.com/kubernetes/kubernetes/pull/86167>#86167</a>, <a href=https://github.com/yiyang5055>@yiyang5055</a>)</li><li>hollow-node do not use remote CRI anymore (<a href=https://github.com/kubernetes/kubernetes/pull/86425>#86425</a>, <a href=https://github.com/jkaniuk>@jkaniuk</a>)</li><li>hollow-node use fake CRI (<a href=https://github.com/kubernetes/kubernetes/pull/85879>#85879</a>, <a href=https://github.com/gongguan>@gongguan</a>)</li></ul><h1 id=v1-18-0-alpha-1>v1.18.0-alpha.1</h1><p><a href=https://docs.k8s.io>Documentation</a></p><h2 id=downloads-for-v1-18-0-alpha-1>Downloads for v1.18.0-alpha.1</h2><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes.tar.gz>kubernetes.tar.gz</a></td><td><code>0c4904efc7f4f1436119c91dc1b6c93b3bd9c7490362a394bff10099c18e1e7600c4f6e2fcbaeb2d342a36c4b20692715cf7aa8ada6dfac369f44cc9292529d7</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-src.tar.gz>kubernetes-src.tar.gz</a></td><td><code>0a50fc6816c730ca5ae4c4f26d5ad7b049607d29f6a782a4e5b4b05ac50e016486e269dafcc6a163bd15e1a192780a9a987f1bb959696993641c603ed1e841c8</code></td></tr></tbody></table><h3 id=client-binaries-7>Client Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-darwin-386.tar.gz>kubernetes-client-darwin-386.tar.gz</a></td><td><code>c6d75f7f3f20bef17fc7564a619b54e6f4a673d041b7c9ec93663763a1cc8dd16aecd7a2af70e8d54825a0eecb9762cf2edfdade840604c9a32ecd9cc2d5ac3c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-darwin-amd64.tar.gz>kubernetes-client-darwin-amd64.tar.gz</a></td><td><code>ca1f19db289933beace6daee6fc30af19b0e260634ef6e89f773464a05e24551c791be58b67da7a7e2a863e28b7cbcc7b24b6b9bf467113c26da76ac8f54fdb6</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-386.tar.gz>kubernetes-client-linux-386.tar.gz</a></td><td><code>af2e673653eb39c3f24a54efc68e1055f9258bdf6cf8fea42faf42c05abefc2da853f42faac3b166c37e2a7533020b8993b98c0d6d80a5b66f39e91d8ae0a3fb</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-amd64.tar.gz>kubernetes-client-linux-amd64.tar.gz</a></td><td><code>9009032c3f94ac8a78c1322a28e16644ce3b20989eb762685a1819148aed6e883ca8e1200e5ec37ec0853f115c67e09b5d697d6cf5d4c45f653788a2d3a2f84f</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-arm.tar.gz>kubernetes-client-linux-arm.tar.gz</a></td><td><code>afba9595b37a3f2eead6e3418573f7ce093b55467dce4da0b8de860028576b96b837a2fd942f9c276e965da694e31fbd523eeb39aefb902d7e7a2f169344d271</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-arm64.tar.gz>kubernetes-client-linux-arm64.tar.gz</a></td><td><code>04fc3b2fe3f271807f0bc6c61be52456f26a1af904964400be819b7914519edc72cbab9afab2bb2e2ba1a108963079367cedfb253c9364c0175d1fcc64d52f5c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-ppc64le.tar.gz>kubernetes-client-linux-ppc64le.tar.gz</a></td><td><code>04c7edab874b33175ff7bebfff5b3a032bc6eb088fcd7387ffcd5b3fa71395ca8c5f9427b7ddb496e92087dfdb09eaf14a46e9513071d3bd73df76c182922d38</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-linux-s390x.tar.gz>kubernetes-client-linux-s390x.tar.gz</a></td><td><code>499287dbbc33399a37b9f3b35e0124ff20b17b6619f25a207ee9c606ef261af61fa0c328dde18c7ce2d3dfb2eea2376623bc3425d16bc8515932a68b44f8bede</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-windows-386.tar.gz>kubernetes-client-windows-386.tar.gz</a></td><td><code>cf84aeddf00f126fb13c0436b116dd0464a625659e44c84bf863517db0406afb4eefd86807e7543c4f96006d275772fbf66214ae7d582db5865c84ac3545b3e6</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-client-windows-amd64.tar.gz>kubernetes-client-windows-amd64.tar.gz</a></td><td><code>69f20558ccd5cd6dbaccf29307210db4e687af21f6d71f68c69d3a39766862686ac1333ab8a5012010ca5c5e3c11676b45e498e3d4c38773da7d24bcefc46d95</code></td></tr></tbody></table><h3 id=server-binaries-7>Server Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-server-linux-amd64.tar.gz>kubernetes-server-linux-amd64.tar.gz</a></td><td><code>3f29df2ce904a0f10db4c1d7a425a36f420867b595da3fa158ae430bfead90def2f2139f51425b349faa8a9303dcf20ea01657cb6ea28eb6ad64f5bb32ce2ed1</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-server-linux-arm.tar.gz>kubernetes-server-linux-arm.tar.gz</a></td><td><code>4a21073b2273d721fbf062c254840be5c8471a010bcc0c731b101729e36e61f637cb7fcb521a22e8d24808510242f4fff8a6ca40f10e9acd849c2a47bf135f27</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-server-linux-arm64.tar.gz>kubernetes-server-linux-arm64.tar.gz</a></td><td><code>7f1cb6d721bedc90e28b16f99bea7e59f5ad6267c31ef39c14d34db6ad6aad87ee51d2acdd01b6903307c1c00b58ff6b785a03d5a491cc3f8a4df9a1d76d406c</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-server-linux-ppc64le.tar.gz>kubernetes-server-linux-ppc64le.tar.gz</a></td><td><code>8f2b552030b5274b1c2c7c166eacd5a14b0c6ca0f23042f4c52efe87e22a167ba4460dcd66615a5ecd26d9e88336be1fb555548392e70efe59070dd2c314da98</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-server-linux-s390x.tar.gz>kubernetes-server-linux-s390x.tar.gz</a></td><td><code>8d9f2c96f66edafb7c8b3aa90960d29b41471743842aede6b47b3b2e61f4306fb6fc60b9ebc18820c547ee200bfedfe254c1cde962d447c791097dd30e79abdb</code></td></tr></tbody></table><h3 id=node-binaries-7>Node Binaries</h3><table><thead><tr><th>filename</th><th>sha512 hash</th></tr></thead><tbody><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-linux-amd64.tar.gz>kubernetes-node-linux-amd64.tar.gz</a></td><td><code>84194cb081d1502f8ca68143569f9707d96f1a28fcf0c574ebd203321463a8b605f67bb2a365eaffb14fbeb8d55c8d3fa17431780b242fb9cba3a14426a0cd4a</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-linux-arm.tar.gz>kubernetes-node-linux-arm.tar.gz</a></td><td><code>0091e108ab94fd8683b89c597c4fdc2fbf4920b007cfcd5297072c44bc3a230dfe5ceed16473e15c3e6cf5edab866d7004b53edab95be0400cc60e009eee0d9d</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-linux-arm64.tar.gz>kubernetes-node-linux-arm64.tar.gz</a></td><td><code>b7e85682cc2848a35d52fd6f01c247f039ee1b5dd03345713821ea10a7fa9939b944f91087baae95eaa0665d11857c1b81c454f720add077287b091f9f19e5d3</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-linux-ppc64le.tar.gz>kubernetes-node-linux-ppc64le.tar.gz</a></td><td><code>cd1f0849e9c62b5d2c93ff0cebf58843e178d8a88317f45f76de0db5ae020b8027e9503a5fccc96445184e0d77ecdf6f57787176ac31dbcbd01323cd0a190cbb</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-linux-s390x.tar.gz>kubernetes-node-linux-s390x.tar.gz</a></td><td><code>e1e697a34424c75d75415b613b81c8af5f64384226c5152d869f12fd7db1a3e25724975b73fa3d89e56e4bf78d5fd07e68a709ba8566f53691ba6a88addc79ea</code></td></tr><tr><td><a href=https://dl.k8s.io/v1.18.0-alpha.1/kubernetes-node-windows-amd64.tar.gz>kubernetes-node-windows-amd64.tar.gz</a></td><td><code>c725a19a4013c74e22383ad3fb4cb799b3e161c4318fdad066daf806730a89bc3be3ff0f75678d02b3cbe52b2ef0c411c0639968e200b9df470be40bb2c015cc</code></td></tr></tbody></table><h2 id=changelog-since-v1-17-0-1>Changelog since v1.17.0</h2><h3 id=action-required>Action Required</h3><ul><li>action required (<a href=https://github.com/kubernetes/kubernetes/pull/85363>#85363</a>, <a href=https://github.com/immutableT>@immutableT</a>)<ul><li><ol><li>Currently, if users were to explicitly specify CacheSize of 0 for KMS provider, they would end-up with a provider that caches up to 1000 keys. This PR changes this behavior.</li></ol></li><li>Post this PR, when users supply 0 for CacheSize this will result in a validation error.</li><li><ol start=2><li>CacheSize type was changed from int32 to *int32. This allows defaulting logic to differentiate between cases where users explicitly supplied 0 vs. not supplied any value.</li></ol></li><li><ol start=3><li>KMS Provider's endpoint (path to Unix socket) is now validated when the EncryptionConfiguration files is loaded. This used to be handled by the GRPCService.</li></ol></li></ul></li></ul><h3 id=other-notable-changes-1>Other notable changes</h3><ul><li>fix: azure data disk should use same key as os disk by default (<a href=https://github.com/kubernetes/kubernetes/pull/86351>#86351</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>)</li><li>New flag <code>--show-hidden-metrics-for-version</code> in kube-proxy can be used to show all hidden metrics that deprecated in the previous minor release. (<a href=https://github.com/kubernetes/kubernetes/pull/85279>#85279</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)</li><li>Remove cluster-monitoring addon (<a href=https://github.com/kubernetes/kubernetes/pull/85512>#85512</a>, <a href=https://github.com/serathius>@serathius</a>)</li><li>Changed core_pattern on COS nodes to be an absolute path. (<a href=https://github.com/kubernetes/kubernetes/pull/86329>#86329</a>, <a href=https://github.com/mml>@mml</a>)</li><li>Track mount operations as uncertain if operation fails with non-final error (<a href=https://github.com/kubernetes/kubernetes/pull/82492>#82492</a>, <a href=https://github.com/gnufied>@gnufied</a>)</li><li>add kube-proxy flags --ipvs-tcp-timeout, --ipvs-tcpfin-timeout, --ipvs-udp-timeout to configure IPVS connection timeouts. (<a href=https://github.com/kubernetes/kubernetes/pull/85517>#85517</a>, <a href=https://github.com/andrewsykim>@andrewsykim</a>)</li><li>The sample-apiserver aggregated conformance test has updated to use the Kubernetes v1.17.0 sample apiserver (<a href=https://github.com/kubernetes/kubernetes/pull/84735>#84735</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>The underlying format of the <code>CPUManager</code> state file has changed. Upgrades should be seamless, but any third-party tools that rely on reading the previous format need to be updated. (<a href=https://github.com/kubernetes/kubernetes/pull/84462>#84462</a>, <a href=https://github.com/klueska>@klueska</a>)</li><li>kubernetes will try to acquire the iptables lock every 100 msec during 5 seconds instead of every second. This specially useful for environments using kube-proxy in iptables mode with a high churn rate of services. (<a href=https://github.com/kubernetes/kubernetes/pull/85771>#85771</a>, <a href=https://github.com/aojea>@aojea</a>)</li><li>Fixed a panic in the kubelet cleaning up pod volumes (<a href=https://github.com/kubernetes/kubernetes/pull/86277>#86277</a>, <a href=https://github.com/tedyu>@tedyu</a>)</li><li>azure cloud provider cache TTL is configurable, list of the azure cloud provider is as following: (<a href=https://github.com/kubernetes/kubernetes/pull/86266>#86266</a>, <a href=https://github.com/zqingqing1>@zqingqing1</a>)<ul><li><ul><li>"availabilitySetNodesCacheTTLInSeconds"</li></ul></li><li><ul><li>"vmssCacheTTLInSeconds"</li></ul></li><li><ul><li>"vmssVirtualMachinesCacheTTLInSeconds"</li></ul></li><li><ul><li>"vmCacheTTLInSeconds"</li></ul></li><li><ul><li>"loadBalancerCacheTTLInSeconds"</li></ul></li><li><ul><li>"nsgCacheTTLInSeconds"</li></ul></li><li><ul><li>"routeTableCacheTTLInSeconds"</li></ul></li></ul></li><li>Fixes kube-proxy when EndpointSlice feature gate is enabled on Windows. (<a href=https://github.com/kubernetes/kubernetes/pull/86016>#86016</a>, <a href=https://github.com/robscott>@robscott</a>)</li><li>Fixes wrong validation result of NetworkPolicy PolicyTypes (<a href=https://github.com/kubernetes/kubernetes/pull/85747>#85747</a>, <a href=https://github.com/tnqn>@tnqn</a>)</li><li>Fixes an issue with kubelet-reported pod status on deleted/recreated pods. (<a href=https://github.com/kubernetes/kubernetes/pull/86320>#86320</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>kube-apiserver no longer serves the following deprecated APIs: (<a href=https://github.com/kubernetes/kubernetes/pull/85903>#85903</a>, <a href=https://github.com/liggitt>@liggitt</a>)
* All resources under <code>apps/v1beta1</code> and <code>apps/v1beta2</code> - use <code>apps/v1</code> instead
* <code>daemonsets</code>, <code>deployments</code>, <code>replicasets</code> resources under <code>extensions/v1beta1</code> - use <code>apps/v1</code> instead
* <code>networkpolicies</code> resources under <code>extensions/v1beta1</code> - use <code>networking.k8s.io/v1</code> instead
* <code>podsecuritypolicies</code> resources under <code>extensions/v1beta1</code> - use <code>policy/v1beta1</code> instead</li><li>kubeadm: fix potential panic when executing "kubeadm reset" with a corrupted kubelet.conf file (<a href=https://github.com/kubernetes/kubernetes/pull/86216>#86216</a>, <a href=https://github.com/neolit123>@neolit123</a>)</li><li>Fix a bug in port-forward: named port not working with service (<a href=https://github.com/kubernetes/kubernetes/pull/85511>#85511</a>, <a href=https://github.com/oke-py>@oke-py</a>)</li><li>kube-proxy no longer modifies shared EndpointSlices. (<a href=https://github.com/kubernetes/kubernetes/pull/86092>#86092</a>, <a href=https://github.com/robscott>@robscott</a>)</li><li>allow for configuration of CoreDNS replica count (<a href=https://github.com/kubernetes/kubernetes/pull/85837>#85837</a>, <a href=https://github.com/pickledrick>@pickledrick</a>)</li><li>Fixed a regression where the kubelet would fail to update the ready status of pods. (<a href=https://github.com/kubernetes/kubernetes/pull/84951>#84951</a>, <a href=https://github.com/tedyu>@tedyu</a>)</li><li>Resolves performance regression in client-go discovery clients constructed using <code>NewDiscoveryClientForConfig</code> or <code>NewDiscoveryClientForConfigOrDie</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/86168>#86168</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>Make error message and service event message more clear (<a href=https://github.com/kubernetes/kubernetes/pull/86078>#86078</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</li><li>e2e-test-framework: add e2e test namespace dump if all tests succeed but the cleanup fails. (<a href=https://github.com/kubernetes/kubernetes/pull/85542>#85542</a>, <a href=https://github.com/schrodit>@schrodit</a>)</li><li>SafeSysctlWhitelist: add net.ipv4.ping_group_range (<a href=https://github.com/kubernetes/kubernetes/pull/85463>#85463</a>, <a href=https://github.com/AkihiroSuda>@AkihiroSuda</a>)</li><li>kubelet: the metric process_start_time_seconds be marked as with the ALPHA stability level. (<a href=https://github.com/kubernetes/kubernetes/pull/85446>#85446</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)</li><li>API request throttling (due to a high rate of requests) is now reported in the kubelet (and other component) logs by default. The messages are of the form (<a href=https://github.com/kubernetes/kubernetes/pull/80649>#80649</a>, <a href=https://github.com/RobertKrawitz>@RobertKrawitz</a>)<ul><li>Throttling request took 1.50705208s, request: GET:<url></li><li>The presence of large numbers of these messages, particularly with long delay times, may indicate to the administrator the need to tune the cluster accordingly.</li></ul></li><li>Fix API Server potential memory leak issue in processing watch request. (<a href=https://github.com/kubernetes/kubernetes/pull/85410>#85410</a>, <a href=https://github.com/answer1991>@answer1991</a>)</li><li>Verify kubelet & kube-proxy can recover after being killed on Windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/84886>#84886</a>, <a href=https://github.com/YangLu1031>@YangLu1031</a>)</li><li>Fixed an issue that the scheduler only returns the first failure reason. (<a href=https://github.com/kubernetes/kubernetes/pull/86022>#86022</a>, <a href=https://github.com/Huang-Wei>@Huang-Wei</a>)</li><li>kubectl/drain: add skip-wait-for-delete-timeout option. (<a href=https://github.com/kubernetes/kubernetes/pull/85577>#85577</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>)<ul><li>If pod DeletionTimestamp older than N seconds, skip waiting for the pod. Seconds must be greater than 0 to skip.</li></ul></li><li>Following metrics have been turned off: (<a href=https://github.com/kubernetes/kubernetes/pull/83841>#83841</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)<ul><li><ul><li>kubelet_pod_worker_latency_microseconds</li></ul></li><li><ul><li>kubelet_pod_start_latency_microseconds</li></ul></li><li><ul><li>kubelet_cgroup_manager_latency_microseconds</li></ul></li><li><ul><li>kubelet_pod_worker_start_latency_microseconds</li></ul></li><li><ul><li>kubelet_pleg_relist_latency_microseconds</li></ul></li><li><ul><li>kubelet_pleg_relist_interval_microseconds</li></ul></li><li><ul><li>kubelet_eviction_stats_age_microseconds</li></ul></li><li><ul><li>kubelet_runtime_operations</li></ul></li><li><ul><li>kubelet_runtime_operations_latency_microseconds</li></ul></li><li><ul><li>kubelet_runtime_operations_errors</li></ul></li><li><ul><li>kubelet_device_plugin_registration_count</li></ul></li><li><ul><li>kubelet_device_plugin_alloc_latency_microseconds</li></ul></li><li><ul><li>kubelet_docker_operations</li></ul></li><li><ul><li>kubelet_docker_operations_latency_microseconds</li></ul></li><li><ul><li>kubelet_docker_operations_errors</li></ul></li><li><ul><li>kubelet_docker_operations_timeout</li></ul></li><li><ul><li>network_plugin_operations_latency_microseconds</li></ul></li></ul></li><li><ul><li>Renamed Kubelet metric certificate_manager_server_expiration_seconds to certificate_manager_server_ttl_seconds and changed to report the second until expiration at read time rather than absolute time of expiry. (<a href=https://github.com/kubernetes/kubernetes/pull/85874>#85874</a>, <a href=https://github.com/sambdavidson>@sambdavidson</a>)<ul><li><ul><li>Improved accuracy of Kubelet metric rest_client_exec_plugin_ttl_seconds.</li></ul></li></ul></li></ul></li><li>Bind metadata-agent containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83363>#83363</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>Bind metrics-server containers to linux nodes to avoid Windows scheduling on kubernetes cluster includes linux nodes and windows nodes (<a href=https://github.com/kubernetes/kubernetes/pull/83362>#83362</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>During initialization phase (preflight), kubeadm now verifies the presence of the conntrack executable (<a href=https://github.com/kubernetes/kubernetes/pull/85857>#85857</a>, <a href=https://github.com/hnanni>@hnanni</a>)</li><li>VMSS cache is added so that less chances of VMSS GET throttling (<a href=https://github.com/kubernetes/kubernetes/pull/85885>#85885</a>, <a href=https://github.com/nilo19>@nilo19</a>)</li><li>Update go-winio module version from 0.4.11 to 0.4.14 (<a href=https://github.com/kubernetes/kubernetes/pull/85739>#85739</a>, <a href=https://github.com/wawa0210>@wawa0210</a>)</li><li>Fix LoadBalancer rule checking so that no unexpected LoadBalancer updates are made (<a href=https://github.com/kubernetes/kubernetes/pull/85990>#85990</a>, <a href=https://github.com/feiskyer>@feiskyer</a>)</li><li>kubectl drain node --dry-run will list pods that would be evicted or deleted (<a href=https://github.com/kubernetes/kubernetes/pull/82660>#82660</a>, <a href=https://github.com/sallyom>@sallyom</a>)</li><li>Windows nodes on GCE can use TPM-based authentication to the master. (<a href=https://github.com/kubernetes/kubernetes/pull/85466>#85466</a>, <a href=https://github.com/pjh>@pjh</a>)</li><li>kubectl/drain: add disable-eviction option. (<a href=https://github.com/kubernetes/kubernetes/pull/85571>#85571</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>)<ul><li>Force drain to use delete, even if eviction is supported. This will bypass checking PodDisruptionBudgets, and should be used with caution.</li></ul></li><li>kubeadm now errors out whenever a not supported component config version is supplied for the kubelet and kube-proxy (<a href=https://github.com/kubernetes/kubernetes/pull/85639>#85639</a>, <a href=https://github.com/rosti>@rosti</a>)</li><li>Fixed issue with addon-resizer using deprecated extensions APIs (<a href=https://github.com/kubernetes/kubernetes/pull/85793>#85793</a>, <a href=https://github.com/bskiba>@bskiba</a>)</li><li>Includes FSType when describing CSI persistent volumes. (<a href=https://github.com/kubernetes/kubernetes/pull/85293>#85293</a>, <a href=https://github.com/huffmanca>@huffmanca</a>)</li><li>kubelet now exports a "server_expiration_renew_failure" and "client_expiration_renew_failure" metric counter if the certificate rotations cannot be performed. (<a href=https://github.com/kubernetes/kubernetes/pull/84614>#84614</a>, <a href=https://github.com/rphillips>@rphillips</a>)</li><li>kubeadm: don't write the kubelet environment file on "upgrade apply" (<a href=https://github.com/kubernetes/kubernetes/pull/85412>#85412</a>, <a href=https://github.com/boluisa>@boluisa</a>)</li><li>fix azure file AuthorizationFailure (<a href=https://github.com/kubernetes/kubernetes/pull/85475>#85475</a>, <a href=https://github.com/andyzhangx>@andyzhangx</a>)</li><li>Resolved regression in admission, authentication, and authorization webhook performance in v1.17.0-rc.1 (<a href=https://github.com/kubernetes/kubernetes/pull/85810>#85810</a>, <a href=https://github.com/liggitt>@liggitt</a>)</li><li>kubeadm: uses the apiserver AdvertiseAddress IP family to choose the etcd endpoint IP family for non external etcd clusters (<a href=https://github.com/kubernetes/kubernetes/pull/85745>#85745</a>, <a href=https://github.com/aojea>@aojea</a>)</li><li>kubeadm: Forward cluster name to the controller-manager arguments (<a href=https://github.com/kubernetes/kubernetes/pull/85817>#85817</a>, <a href=https://github.com/ereslibre>@ereslibre</a>)</li><li>Fixed "requested device X but found Y" attach error on AWS. (<a href=https://github.com/kubernetes/kubernetes/pull/85675>#85675</a>, <a href=https://github.com/jsafrane>@jsafrane</a>)</li><li>addons: elasticsearch discovery supports IPv6 (<a href=https://github.com/kubernetes/kubernetes/pull/85543>#85543</a>, <a href=https://github.com/SataQiu>@SataQiu</a>)</li><li>kubeadm: retry <code>kubeadm-config</code> ConfigMap creation or mutation if the apiserver is not responding. This will improve resiliency when joining new control plane nodes. (<a href=https://github.com/kubernetes/kubernetes/pull/85763>#85763</a>, <a href=https://github.com/ereslibre>@ereslibre</a>)</li><li>Update Cluster Autoscaler to 1.17.0; changelog: <a href=https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.17.0>https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.17.0</a> (<a href=https://github.com/kubernetes/kubernetes/pull/85610>#85610</a>, <a href=https://github.com/losipiuk>@losipiuk</a>)</li><li>Filter published OpenAPI schema by making nullable, required fields non-required in order to avoid kubectl to wrongly reject null values. (<a href=https://github.com/kubernetes/kubernetes/pull/85722>#85722</a>, <a href=https://github.com/sttts>@sttts</a>)</li><li>kubectl set resources will no longer return an error if passed an empty change for a resource. (<a href=https://github.com/kubernetes/kubernetes/pull/85490>#85490</a>, <a href=https://github.com/sallyom>@sallyom</a>)<ul><li>kubectl set subject will no longer return an error if passed an empty change for a resource.</li></ul></li><li>kube-apiserver: fixed a conflict error encountered attempting to delete a pod with gracePeriodSeconds=0 and a resourceVersion precondition (<a href=https://github.com/kubernetes/kubernetes/pull/85516>#85516</a>, <a href=https://github.com/michaelgugino>@michaelgugino</a>)</li><li>kubeadm: add a upgrade health check that deploys a Job (<a href=https://github.com/kubernetes/kubernetes/pull/81319>#81319</a>, <a href=https://github.com/neolit123>@neolit123</a>)</li><li>kubeadm: make sure images are pre-pulled even if a tag did not change but their contents changed (<a href=https://github.com/kubernetes/kubernetes/pull/85603>#85603</a>, <a href=https://github.com/bart0sh>@bart0sh</a>)</li><li>kube-apiserver: Fixes a bug that hidden metrics can not be enabled by the command-line option <code>--show-hidden-metrics-for-version</code>. (<a href=https://github.com/kubernetes/kubernetes/pull/85444>#85444</a>, <a href=https://github.com/RainbowMango>@RainbowMango</a>)</li><li>kubeadm now supports automatic calculations of dual-stack node cidr masks to kube-controller-manager. (<a href=https://github.com/kubernetes/kubernetes/pull/85609>#85609</a>, <a href=https://github.com/Arvinderpal>@Arvinderpal</a>)</li><li>Fix bug where EndpointSlice controller would attempt to modify shared objects. (<a href=https://github.com/kubernetes/kubernetes/pull/85368>#85368</a>, <a href=https://github.com/robscott>@robscott</a>)</li><li>Use context to check client closed instead of http.CloseNotifier in processing watch request which will reduce 1 goroutine for each request if proto is HTTP/2.x . (<a href=https://github.com/kubernetes/kubernetes/pull/85408>#85408</a>, <a href=https://github.com/answer1991>@answer1991</a>)</li><li>kubeadm: reset raises warnings if it cannot delete folders (<a href=https://github.com/kubernetes/kubernetes/pull/85265>#85265</a>, <a href=https://github.com/SataQiu>@SataQiu</a>)</li><li>Wait for kubelet & kube-proxy to be ready on Windows node within 10s (<a href=https://github.com/kubernetes/kubernetes/pull/85228>#85228</a>, <a href=https://github.com/YangLu1031>@YangLu1031</a>)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-85b7e96ac42e5e28ec570ad43f0ef5cd>1.2 - Kubernetes 版本及版本偏差支持策略</h1><p>本文描述 Kubernetes 各组件之间版本偏差支持策略。
特定的集群部署工具可能会有额外的限制。</p><h2 id=版本支持策略>版本支持策略</h2><p>Kubernetes 版本号格式为 <strong>x.y.z</strong>，其中 <strong>x</strong> 为大版本号，<strong>y</strong> 为小版本号，<strong>z</strong> 为补丁版本号。
版本号格式遵循 <a href=https://semver.org/>Semantic Versioning</a> 规则。
更多信息，请参阅 <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#kubernetes-release-versioning>Kubernetes 发布版本</a>。</p><p>Kubernetes 项目会维护最近的三个小版本分支（1.24, 1.23, 1.22）。
Kubernetes 1.19 及更高的版本将获得大约1年的补丁支持。
Kubernetes 1.18 及更早的版本获得大约9个月的补丁支持。</p><p>一些 bug 修复，包括安全修复，取决于其严重性和可行性，有可能会反向合并到这三个发布分支。
补丁版本会<a href=https://git.k8s.io/sig-release/releases/patch-releases.md#cadence>定期</a>或根据需要从这些分支中发布。
最终是否发布是由<a href=https://github.com/kubernetes/sig-release/blob/master/release-managers.md>发布管理者</a>来决定的。
如需了解更多信息，请查看 Kubernetes <a href=https://github.com/kubernetes/sig-release/blob/master/releases/patch-releases.md>补丁发布</a>。</p><h2 id=版本偏差策略>版本偏差策略</h2><h3 id=kube-apiserver>kube-apiserver</h3><p>在 <a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用（HA）集群</a> 中，
多个 <code>kube-apiserver</code> 实例小版本号最多差1。</p><p>例如：</p><ul><li>最新的 <code>kube-apiserver</code> 版本号如果是 <strong>1.24</strong></li><li>则受支持的 <code>kube-apiserver</code> 版本号包括 <strong>1.24</strong> 和 <strong>1.23</strong></li></ul><h3 id=kubelet>kubelet</h3><p><code>kubelet</code> 版本号不能高于 <code>kube-apiserver</code>，最多可以比 <code>kube-apiserver</code> 低两个小版本。</p><p>例如：</p><ul><li><code>kube-apiserver</code> 版本号如果是 <strong>1.24</strong></li><li>受支持的的 <code>kubelet</code> 版本将包括 <strong>1.24</strong>、<strong>1.23</strong> 和 <strong>1.22</strong></li></ul><blockquote class="note callout"><div><strong>说明：</strong> 如果 HA 集群中多个 <code>kube-apiserver</code> 实例版本号不一致，相应的 <code>kubelet</code> 版本号可选范围也要减小。</div></blockquote><p>例如：</p><ul><li>如果 <code>kube-apiserver</code> 实例同时存在 <strong>1.24</strong> 和 <strong>1.23</strong></li><li><code>kubelet</code> 的受支持版本将是 <strong>1.23</strong> 和 <strong>1.22</strong>
（<strong>1.24</strong> 不再支持，因为它比 <strong>1.23</strong> 版本的 <code>kube-apiserver</code> 更新）</li></ul><h3 id=kube-controller-manager-kube-scheduler-和-cloud-controller-manager>kube-controller-manager、 kube-scheduler 和 cloud-controller-manager</h3><p><code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 版本不能高于 <code>kube-apiserver</code> 版本号。
最好它们的版本号与 <code>kube-apiserver</code> 保持一致，但允许比 <code>kube-apiserver</code> 低一个小版本（为了支持在线升级）。</p><p>例如：</p><ul><li>如果 <code>kube-apiserver</code> 版本号为 <strong>1.24</strong></li><li><code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 版本支持 <strong>1.24</strong> 和 <strong>1.23</strong></li></ul><blockquote class="note callout"><div><strong>说明：</strong> 如果在 HA 集群中，多个 <code>kube-apiserver</code> 实例版本号不一致，他们也可以跟任意一个 <code>kube-apiserver</code> 实例通信（例如，通过 load balancer），
但 <code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 版本可用范围会相应的减小。</div></blockquote><p>例如：</p><ul><li><code>kube-apiserver</code> 实例同时存在 <strong>1.24</strong> 和 <strong>1.23</strong> 版本</li><li><code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 可以通过 load balancer 与所有的 <code>kube-apiserver</code> 通信</li><li><code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 可选版本为 <strong>1.23</strong>
（<strong>1.24</strong> 不再支持，因为它比 <strong>1.23</strong> 版本的 <code>kube-apiserver</code> 更新）</li></ul><h3 id=kubectl>kubectl</h3><p><code>kubectl</code> 可以比 <code>kube-apiserver</code> 高一个小版本，也可以低一个小版本。</p><p>例如：</p><ul><li>如果 <code>kube-apiserver</code> 当前是 <strong>1.24</strong> 版本</li><li><code>kubectl</code> 则支持 <strong>1.25</strong>、<strong>1.24</strong> 和 <strong>1.23</strong></li></ul><blockquote class="note callout"><div><strong>说明：</strong> 如果 HA 集群中的多个 <code>kube-apiserver</code> 实例版本号不一致，相应的 <code>kubectl</code> 可用版本范围也会减小。</div></blockquote><p>例如：</p><ul><li><code>kube-apiserver</code> 多个实例同时存在 <strong>1.24</strong> 和 <strong>1.23</strong></li><li><code>kubectl</code> 可选的版本为 <strong>1.24</strong> 和 <strong>1.23</strong>（其他版本不再支持，因为它会比其中某个 <code>kube-apiserver</code> 实例高或低一个小版本）</li></ul><h2 id=支持的组件升级次序>支持的组件升级次序</h2><p>组件之间支持的版本偏差会影响组件升级的顺序。
本节描述组件从版本 <strong>1.23</strong> 到 <strong>1.24</strong> 的升级次序。</p><h3 id=kube-apiserver-1>kube-apiserver</h3><p>前提条件：</p><ul><li>单实例集群中，<code>kube-apiserver</code> 实例版本号须是 <strong>1.23</strong></li><li>高可用（HA）集群中，所有的 <code>kube-apiserver</code> 实例版本号必须是 <strong>1.23</strong> 或 <strong>1.24</strong>（确保满足最新和最旧的实例小版本号相差不大于1）</li><li><code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 版本号必须为 <strong>1.23</strong>（确保不高于 API server 的版本，且版本号相差不大于1）</li><li><code>kubelet</code> 实例版本号必须是 <strong>1.23</strong> 或 <strong>1.22</strong>（确保版本号不高于 API server，且版本号相差不大于2）</li><li>注册的 admission 插件必须能够处理新的 <code>kube-apiserver</code> 实例发送过来的数据：<ul><li><code>ValidatingWebhookConfiguration</code> 和 <code>MutatingWebhookConfiguration</code> 对象必须升级到可以处理
<strong>1.24</strong> 版本新加的 REST 资源（或使用 1.15 版本提供的
<a href=/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchpolicy><code>matchPolicy: Equivalent</code> 选项</a>）</li><li>插件可以处理任何 <strong>1.24</strong> 版本新的 REST 资源数据和新加的字段</li></ul></li></ul><p>升级 <code>kube-apiserver</code> 到 <strong>1.24</strong></p><blockquote class="note callout"><div><strong>说明：</strong><p>根据 <a href=/zh/docs/reference/using-api/deprecation-policy/>API 弃用策略</a> 和
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md>API 变更指南</a>，
<code>kube-apiserver</code> 不能跨小版本号升级，即使是单实例集群也不可以。</div></blockquote><h3 id=kube-controller-manager-kube-scheduler-和-cloud-controller-manager-1>kube-controller-manager、kube-scheduler 和 cloud-controller-manager</h3><p>前提条件：</p><ul><li><code>kube-apiserver</code> 实例必须为 <strong>1.24</strong> （HA 集群中，所有的<code>kube-apiserver</code> 实例必须在组件升级前完成升级）</li></ul><p>升级 <code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>cloud-controller-manager</code> 到 <strong>1.24</strong></p><h3 id=kubelet-1>kubelet</h3><p>前提条件：</p><ul><li><code>kube-apiserver</code> 实例必须为 <strong>1.24</strong> 版本</li></ul><p><code>kubelet</code> 可以升级到 <strong>1.24</strong>（或者停留在 <strong>1.23</strong> 或 <strong>1.22</strong>）</p><blockquote class="warning callout"><div><strong>警告：</strong><p>集群中 <code>kubelet</code> 版本号不建议比 <code>kube-apiserver</code> 低两个版本号：</p><ul><li>它们必须升级到与 <code>kube-apiserver</code> 相差不超过 1 个小版本，才可以升级其他控制面组件</li><li>有可能使用低于 3 个在维护的小版本</li></ul></div></blockquote><h3 id=kube-proxy>kube-proxy</h3><ul><li><code>kube-proxy</code> 必须与节点上的 <code>kubelet</code> 的小版本相同</li><li><code>kube-proxy</code> 一定不能比 <code>kube-apiserver</code> 小版本更新</li><li><code>kube-proxy</code> 最多只能比 <code>kube-apiserver</code> 早两个小版本</li></ul><p>例如：</p><p>如果 <code>kube-proxy</code> 的版本是 <strong>1.22</strong>：</p><ul><li><code>kubelet</code> 版本必须相同，也是 <strong>1.22</strong></li><li><code>kube-apiserver</code> 版本必须在 <strong>1.22</strong> 到 <strong>1.24</strong> 之间（闭区间）</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0b597086a9d1382f86abadcfeab657d6>2 - 学习环境</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-4e14853fdaa3bd273f31a60112b9b5ac>3 - 生产环境</h1></div><div class=td-content><h1 id=pg-a77d3feb6e6d9978f32fa14622642e9a>3.1 - 容器运行时</h1><p>你需要在集群内每个节点上安装一个<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>
以使 Pod 可以运行在上面。本文概述了所涉及的内容并描述了与节点设置相关的任务。</p><p>本文列出了在 Linux 上结合 Kubernetes 使用的几种通用容器运行时的详细信息：</p><ul><li><a href=#containerd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li><li><a href=#docker>Docker</a></li></ul><p>提示：对于其他操作系统，请查阅特定于你所使用平台的相关文档。</p><h2 id=cgroup-驱动程序>Cgroup 驱动程序</h2><p>控制组用来约束分配给进程的资源。</p><p>当某个 Linux 系统发行版使用 <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组 (<code>cgroup</code>), 并充当 cgroup 管理器。
Systemd 与 cgroup 集成紧密，并将为每个 systemd 单元分配一个 cgroup。
你也可以配置容器运行时和 kubelet 使用 <code>cgroupfs</code>。
连同 systemd 一起使用 <code>cgroupfs</code> 意味着将有两个不同的 cgroup 管理器。</p><p>单个 cgroup 管理器将简化分配资源的视图，并且默认情况下将对可用资源和使用中的资源具有更一致的视图。
当有两个管理器共存于一个系统中时，最终将对这些资源产生两种视图。
在此领域人们已经报告过一些案例，某些节点配置让 kubelet 和 docker 使用 <code>cgroupfs</code>，而节点上运行的其余进程则使用 systemd; 这类节点在资源压力下会变得不稳定。</p><p>更改设置，令容器运行时和 kubelet 使用 <code>systemd</code> 作为 cgroup 驱动，以此使系统更为稳定。
对于 Docker, 设置 <code>native.cgroupdriver=systemd</code> 选项。</p><p>注意：非常 <em>不</em> 建议更改已加入集群的节点的 cgroup 驱动。
如果 kubelet 已经使用某 cgroup 驱动的语义创建了 pod，更改运行时以使用别的 cgroup 驱动，当为现有 Pods 重新创建 PodSandbox 时会产生错误。重启 kubelet 也可能无法解决此类问题。
如果你有切实可行的自动化方案，使用其他已更新配置的节点来替换该节点，或者使用自动化方案来重新安装。</p><h2 id=容器运行时>容器运行时</h2><blockquote class="callout caution" role=alert><strong>注意：</strong>
本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href=/docs/contribute/style/content-guide/#third-party-content>内容指南</a>。</blockquote><h3 id=containerd>containerd</h3><p>本节包含使用 containerd 作为 CRI 运行时的必要步骤。</p><p>使用以下命令在系统上安装容器：</p><p>安装和配置的先决条件：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style=color:#b44>overlay
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style=color:#080;font-style:italic># 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># 应用 sysctl 参数而无需重新启动</span>
sudo sysctl --system
</code></pre></div><p>安装 containerd:</p><ul class="nav nav-tabs" id=tab-cri-containerd-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-containerd-installation-0 role=tab aria-controls=tab-cri-containerd-installation-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-containerd-installation-1 role=tab aria-controls=tab-cri-containerd-installation-1>Windows (PowerShell)</a></li></ul><div class=tab-content id=tab-cri-containerd-installation><div id=tab-cri-containerd-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-containerd-installation-0><p><ol><li>从官方Docker仓库安装 <code>containerd.io</code> 软件包。可以在 <a href=https://docs.docker.com/engine/install/#server>安装 Docker 引擎</a> 中找到有关为各自的 Linux 发行版设置 Docker 存储库和安装 <code>containerd.io</code> 软件包的说明。</li></ol><ol start=2><li><p>配置 containerd：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre></div></li></ol><ol start=3><li><p>重新启动 containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart containerd
</code></pre></div></li></ol></div><div id=tab-cri-containerd-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-containerd-installation-1><p><p>启动 Powershell 会话，将 <code>$Version</code> 设置为所需的版本（例如：<code>$ Version=1.4.3</code>），然后运行以下命令：</p><ol><li><p>下载 containerd：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>curl.exe -L https<span>:</span>//github.com/containerd/containerd/releases/download/v<span style=color:#b8860b>$Version</span>/containerd-<span style=color:#b8860b>$Version</span>-windows-amd64.tar.gz -o containerd-windows-amd64.tar.gz
tar.exe xvf .\containerd-windows-amd64.tar.gz
</code></pre></div></li></ol><ol start=2><li><p>提取并配置：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Copy-Item</span> -Path <span style=color:#b44>&#34;.\bin\&#34;</span> -Destination <span style=color:#b44>&#34;$Env:ProgramFiles\containerd&#34;</span> -Recurse -Force
<span style=color:#a2f>cd </span><span style=color:#b8860b>$Env:ProgramFiles</span>\containerd\
.\containerd.exe config <span style=color:#a2f;font-weight:700>default</span> | <span style=color:#a2f>Out-File</span> config.toml -Encoding ascii

<span style=color:#080;font-style:italic># Review the configuration. Depending on setup you may want to adjust:</span>
<span style=color:#080;font-style:italic># - the sandbox_image (Kubernetes pause image)</span>
<span style=color:#080;font-style:italic># - cni bin_dir and conf_dir locations</span>
<span style=color:#a2f>Get-Content</span> config.toml

<span style=color:#080;font-style:italic># (Optional - but highly recommended) Exclude containerd from Windows Defender Scans</span>
<span style=color:#a2f>Add-MpPreference</span> -ExclusionProcess <span style=color:#b44>&#34;$Env:ProgramFiles\containerd\containerd.exe&#34;</span>
</code></pre></div></li></ol><ol start=3><li><p>启动 containerd:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>.\containerd.exe --register-service
<span style=color:#a2f>Start-Service</span> containerd
</code></pre></div></li></ol></div></div><h4 id=containerd-systemd>使用 <code>systemd</code> cgroup 驱动程序</h4><p>结合 <code>runc</code> 使用 <code>systemd</code> cgroup 驱动，在 <code>/etc/containerd/config.toml</code> 中设置</p><pre><code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
  ...
  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre><p>如果您应用此更改，请确保再次重新启动 containerd：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl restart containerd
</code></pre></div><p>当使用 kubeadm 时，请手动配置
<a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node>kubelet 的 cgroup 驱动</a>.</p><h3 id=cri-o>CRI-O</h3><p>本节包含安装 CRI-O 作为容器运行时的必要步骤。</p><p>使用以下命令在系统中安装 CRI-O：</p><p>提示：CRI-O 的主要以及次要版本必须与 Kubernetes 的主要和次要版本相匹配。
更多信息请查阅 <a href=https://github.com/cri-o/cri-o#compatibility-matrix-cri-o--kubernetes>CRI-O 兼容性列表</a>。</p><p>安装以及配置的先决条件：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>
<span style=color:#080;font-style:italic># 创建 .conf 文件，以便在系统启动时加载内核模块</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf
</span><span style=color:#b44>overlay
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style=color:#080;font-style:italic># 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>EOF</span>

sudo sysctl --system
</code></pre></div><ul class="nav nav-tabs" id=tab-cri-cri-o-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-cri-o-installation-0 role=tab aria-controls=tab-cri-cri-o-installation-0 aria-selected=true>Debian</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-1 role=tab aria-controls=tab-cri-cri-o-installation-1>Ubuntu</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-2 role=tab aria-controls=tab-cri-cri-o-installation-2>CentOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-3 role=tab aria-controls=tab-cri-cri-o-installation-3>openSUSE Tumbleweed</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-4 role=tab aria-controls=tab-cri-cri-o-installation-4>Fedora</a></li></ul><div class=tab-content id=tab-cri-cri-o-installation><div id=tab-cri-cri-o-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-cri-o-installation-0><p><p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p><table><thead><tr><th>操作系统</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Debian Unstable</td><td><code>Debian_Unstable</code></td></tr><tr><td>Debian Testing</td><td><code>Debian_Testing</code></td></tr></tbody></table><p><br>然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20.0:1.20.0</code>.<br></p><p>然后执行</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style=color:#b44>deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style=color:#b44>EOF</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style=color:#b44>deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style=color:#b44>EOF</span>

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div><div id=tab-cri-cri-o-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-1><p><p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p><table><thead><tr><th>操作系统</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Ubuntu 20.04</td><td><code>xUbuntu_20.04</code></td></tr><tr><td>Ubuntu 19.10</td><td><code>xUbuntu_19.10</code></td></tr><tr><td>Ubuntu 19.04</td><td><code>xUbuntu_19.04</code></td></tr><tr><td>Ubuntu 18.04</td><td><code>xUbuntu_18.04</code></td></tr></tbody></table><p><br>然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20:1.20.0</code>.<br></p><p>然后执行</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style=color:#b44>deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style=color:#b44>EOF</span>
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style=color:#b44>deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style=color:#b44>EOF</span>

curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers-cri-o.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div><div id=tab-cri-cri-o-installation-2 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-2><p><p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p><table><thead><tr><th>操作系统</th><th><code>$OS</code></th></tr></thead><tbody><tr><td>Centos 8</td><td><code>CentOS_8</code></td></tr><tr><td>Centos 8 Stream</td><td><code>CentOS_8_Stream</code></td></tr><tr><td>Centos 7</td><td><code>CentOS_7</code></td></tr></tbody></table><p><br>然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20:1.20.0</code>.<br></p><p>然后执行</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo
sudo yum install cri-o
</code></pre></div></div><div id=tab-cri-cri-o-installation-3 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-3><p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo zypper install cri-o
</code></pre></div></div><div id=tab-cri-cri-o-installation-4 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-4><p><p>将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果要安装 CRI-O 1.20，请设置 <code>VERSION=1.20</code>。
你可以用下列命令查找可用的版本：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo dnf module list cri-o
</code></pre></div><p>CRI-O 不支持在 Fedora 上固定到特定的版本。</p><p>然后执行</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo dnf module <span style=color:#a2f>enable</span> cri-o:<span style=color:#b8860b>$VERSION</span>
sudo dnf install cri-o --now
</code></pre></div></div></div><h4 id=cgroup-driver>cgroup driver</h4><p>默认情况下，CRI-O 使用 systemd cgroup 驱动程序。切换到<code></code>cgroupfs<code>cgroup 驱动程序，或者编辑 </code>/ etc / crio / crio.conf<code>或放置一个插件 在</code>/etc/crio/crio.conf.d/02-cgroup-manager.conf` 中的配置，例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml>[crio.runtime]
conmon_cgroup = <span style=color:#b44>&#34;pod&#34;</span>
cgroup_manager = <span style=color:#b44>&#34;cgroupfs&#34;</span>
</code></pre></div><p>另请注意更改后的 <code>conmon_cgroup</code> ，必须将其设置为
<code>pod</code>将 CRI-O 与 <code>cgroupfs</code> 一起使用时。通常有必要保持
kubelet 的 cgroup 驱动程序配置（通常透过 kubeadm 完成）和CRI-O 同步中。</p><h3 id=docker>Docker</h3><ol><li>在每个节点上，根据<a href=https://docs.docker.com/engine/install/#server>安装 Docker 引擎</a> 为你的 Linux 发行版安装 Docker。
你可以在此文件中找到最新的经过验证的 Docker 版本<a href=https://git.k8s.io/kubernetes/build/dependencies.yaml>依赖关系</a>。</li></ol><ol start=2><li><p>配置 Docker 守护程序，尤其是使用 systemd 来管理容器的cgroup。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo mkdir /etc/docker
cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/docker/daemon.json
</span><span style=color:#b44>{
</span><span style=color:#b44>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span style=color:#b44>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span style=color:#b44>  &#34;log-opts&#34;: {
</span><span style=color:#b44>    &#34;max-size&#34;: &#34;100m&#34;
</span><span style=color:#b44>  },
</span><span style=color:#b44>  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span><span style=color:#b44>}
</span><span style=color:#b44>EOF</span>
</code></pre></div><blockquote class="note callout"><div><strong>说明：</strong><p>对于运行 Linux 内核版本 4.0 或更高版本，或使用 3.10.0-51 及更高版本的 RHEL 或 CentOS 的系统，<code>overlay2</code>是首选的存储驱动程序。</div></blockquote></li></ol><ol start=3><li>重新启动 Docker 并在启动时启用：<div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo systemctl <span style=color:#a2f>enable</span> docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></div></li></ol><blockquote class="note callout"><div><strong>说明：</strong><blockquote></blockquote></div></blockquote><p>有关更多信息，请参阅</p><ul><li><a href=https://docs.docker.com/config/daemon/>配置 Docker 守护程序</a></li><li><a href=https://docs.docker.com/config/daemon/systemd/>使用 systemd 控制 Docker</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2f55eefe7222b7c637875af9c3ec199>3.2 - Turnkey 云解决方案</h1><p>本页列示 Kubernetes 认证解决方案供应商。
在每一个供应商分页，你可以学习如何安装和设置生产就绪的集群。</p><script>function updateLandscapeSource(a,b){console.log({button:a,shouldUpdateFragment:b});try{if(b)window.location.hash="#"+a.id;else{var c=document.querySelectorAll("#landscape");let b=a.dataset.landscapeTypes,d="https://landscape.cncf.io/card-mode?category="+encodeURIComponent(b)+"&grouping=category&embed=yes";c[0].src=d}}catch(a){console.log({message:"error handling Landscape switch",error:a})}}document.addEventListener("DOMContentLoaded",function(){var c,a;let b=()=>{if(window.location.hash){let a=document.querySelectorAll(".landscape-trigger"+window.location.hash);a.length==1&&(landscapeSource=a[0],console.log("Updating Landscape source based on fragment:",window.location.hash.substring(1)),updateLandscapeSource(landscapeSource,!1))}};if(c=document.querySelectorAll(".landscape-trigger"),c.forEach(a=>{a.onclick=function(){updateLandscapeSource(a,!0)}}),a=document.querySelectorAll(".landscape-trigger.landscape-default"),a.length==1){let b=a[0];updateLandscapeSource(b,!1)}window.addEventListener("hashchange",b,!1),b()})</script><div id=frameHolder><iframe frameborder=0 id=landscape scrolling=no src="https://landscape.cncf.io/card-mode?category=certified-kubernetes-hosted&grouping=category&embed=yes" style=width:1px;min-width:100%></iframe>
<script src=https://landscape.cncf.io/iframeResizer.js></script></div></div><div class=td-content style=page-break-before:always><h1 id=pg-00e1646f68aeb89f9722cf6f6cfcad94>3.3 - 使用部署工具安装 Kubernetes</h1></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>3.3.1 - 使用 kubeadm 引导集群</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>3.3.1.1 - 安装 kubeadm</h1><p><img src=https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png align=right width=150px>本页面显示如何安装 <code>kubeadm</code> 工具箱。
有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，请参见
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a> 页面。</p><h2 id=准备开始>准备开始</h2><ul><li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux
发行版以及一些不提供包管理器的发行版提供通用的指令</li><li>每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)</li><li>2 CPU 核或更多</li><li>集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)</li><li>节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见<a href=#verify-mac-address>这里</a>了解更多详细信息。</li><li>开启机器上的某些端口。请参见<a href=#check-required-ports>这里</a> 了解更多详细信息。</li><li>禁用交换分区。为了保证 kubelet 正常工作，你 <strong>必须</strong> 禁用交换分区。</li></ul><h2 id=verify-mac-address>确保每个节点上 MAC 地址和 product_uuid 的唯一性</h2><ul><li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li><li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li></ul><p>一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。
Kubernetes 使用这些值来唯一确定集群中的节点。
如果这些值在每个节点上不唯一，可能会导致安装
<a href=https://github.com/kubernetes/kubeadm/issues/31>失败</a>。</p><h2 id=检查网络适配器>检查网络适配器</h2><p>如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。</p><h2 id=允许-iptables-检查桥接流量>允许 iptables 检查桥接流量</h2><p>确保 <code>br_netfilter</code> 模块被加载。这一操作可以通过运行 <code>lsmod | grep br_netfilter</code>
来完成。若要显式加载该模块，可执行 <code>sudo modprobe br_netfilter</code>。</p><p>为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的
<code>sysctl</code> 配置中将 <code>net.bridge.bridge-nf-call-iptables</code> 设置为 1。例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span style=color:#b44>br_netfilter
</span><span style=color:#b44>EOF</span>

cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span><span style=color:#b44>EOF</span>
sudo sysctl --system
</code></pre></div><p>更多的相关细节可查看<a href=/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>网络插件需求</a>页面。</p><h2 id=check-required-ports>检查所需端口</h2><h3 id=控制平面节点>控制平面节点</h3><table><thead><tr><th>协议</th><th>方向</th><th>端口范围</th><th>作用</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>入站</td><td>6443</td><td>Kubernetes API 服务器</td><td>所有组件</td></tr><tr><td>TCP</td><td>入站</td><td>2379-2380</td><td>etcd 服务器客户端 API</td><td>kube-apiserver, etcd</td></tr><tr><td>TCP</td><td>入站</td><td>10250</td><td>Kubelet API</td><td>kubelet 自身、控制平面组件</td></tr><tr><td>TCP</td><td>入站</td><td>10251</td><td>kube-scheduler</td><td>kube-scheduler 自身</td></tr><tr><td>TCP</td><td>入站</td><td>10252</td><td>kube-controller-manager</td><td>kube-controller-manager 自身</td></tr></tbody></table><h3 id=工作节点>工作节点</h3><table><thead><tr><th>协议</th><th>方向</th><th>端口范围</th><th>作用</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>入站</td><td>10250</td><td>Kubelet API</td><td>kubelet 自身、控制平面组件</td></tr><tr><td>TCP</td><td>入站</td><td>30000-32767</td><td>NodePort 服务†</td><td>所有组件</td></tr></tbody></table><p>† <a href=/zh/docs/concepts/services-networking/service/>NodePort 服务</a> 的默认端口范围。</p><p>使用 * 标记的任意端口号都可以被覆盖，所以你需要保证所定制的端口是开放的。</p><p>虽然控制平面节点已经包含了 etcd 的端口，你也可以使用自定义的外部 etcd 集群，或是指定自定义端口。</p><p>你使用的 Pod 网络插件 (见下) 也可能需要某些特定端口开启。由于各个 Pod 网络插件都有所不同，
请参阅他们各自文档中对端口的要求。</p><h2 id=installing-runtime>安装 runtime</h2><p>为了在 Pod 中运行容器，Kubernetes 使用
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh/docs/setup/production-environment/container-runtimes target=_blank aria-label="容器运行时（Container Runtime）">容器运行时（Container Runtime）</a>。</p><ul class="nav nav-tabs" id=container-runtimes role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtimes-0 role=tab aria-controls=container-runtimes-0 aria-selected=true>Linux 节点</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtimes-1 role=tab aria-controls=container-runtimes-1>其它操作系统</a></li></ul><div class=tab-content id=container-runtimes><div id=container-runtimes-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtimes-0><p><p>默认情况下，Kubernetes 使用
<a class=glossary-tooltip title="一组与 kubelet 集成的容器运行时 API" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/overview/components/#container-runtime target=_blank aria-label="容器运行时接口（Container Runtime Interface，CRI）">容器运行时接口（Container Runtime Interface，CRI）</a>
来与你所选择的容器运行时交互。</p><p>如果你不指定运行时，则 kubeadm 会自动尝试检测到系统上已经安装的运行时，
方法是扫描一组众所周知的 Unix 域套接字。
下面的表格列举了一些容器运行时及其对应的套接字路径：</p><table><thead><tr><th>运行时</th><th>域套接字</th></tr></thead><tbody><tr><td>Docker</td><td>/var/run/dockershim.sock</td></tr><tr><td>containerd</td><td>/run/containerd/containerd.sock</td></tr><tr><td>CRI-O</td><td>/var/run/crio/crio.sock</td></tr></tbody></table><br>如果同时检测到 Docker 和 containerd，则优先选择 Docker。
这是必然的，因为 Docker 18.09 附带了 containerd 并且两者都是可以检测到的，
即使你仅安装了 Docker。
如果检测到其他两个或多个运行时，kubeadm 输出错误信息并退出。<p>kubelet 通过内置的 <code>dockershim</code> CRI 实现与 Docker 集成。</p><p>参阅<a href=/zh/docs/setup/production-environment/container-runtimes/>容器运行时</a>
以了解更多信息。</p></div><div id=container-runtimes-1 class=tab-pane role=tabpanel aria-labelledby=container-runtimes-1><p><p>默认情况下， kubeadm 使用 <a class=glossary-tooltip title="Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/kubectl/docker-cli-to-kubectl/ target=_blank aria-label=Docker>Docker</a> 作为容器运行时。
kubelet 通过内置的 <code>dockershim</code> CRI 实现与 Docker 集成。
参阅<a href=/zh/docs/setup/production-environment/container-runtimes/>容器运行时</a>
以了解更多信息。</p></div></div><h2 id=安装-kubeadm-kubelet-和-kubectl>安装 kubeadm、kubelet 和 kubectl</h2><p>你需要在每台机器上安装以下的软件包：</p><ul><li><p><code>kubeadm</code>：用来初始化集群的指令。</p></li><li><p><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</p></li><li><p><code>kubectl</code>：用来与集群通信的命令行工具。</p></li></ul><p>kubeadm <strong>不能</strong> 帮你安装或者管理 <code>kubelet</code> 或 <code>kubectl</code>，所以你需要
确保它们与通过 kubeadm 安装的控制平面的版本相匹配。
如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。
然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet
的版本不可以超过 API 服务器的版本。
例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。</p><p>有关安装 <code>kubectl</code> 的信息，请参阅<a href=/zh/docs/tasks/tools/>安装和设置 kubectl</a>文档。</p><blockquote class="warning callout"><div><strong>警告：</strong><p>这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes
有<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>特殊的升级注意事项</a>。</div></blockquote><p>关于版本偏差的更多信息，请参阅以下文档：</p><ul><li>Kubernetes <a href=/zh/docs/setup/release/version-skew-policy/>版本与版本间的偏差策略</a></li><li>Kubeadm 特定的<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>版本偏差策略</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>基于 Debian 的发行版</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>基于 Red Hat 的发行版</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>无包管理器的情况</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><ol><li><p>更新 <code>apt</code> 包索引并安装使用 Kubernetes <code>apt</code> 仓库所需要的包：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre></div></li></ol><ol start=2><li><p>下载 Google Cloud 公开签名秘钥：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</code></pre></div></li></ol><ol start=3><li><p>添加 Kubernetes <code>apt</code> 仓库：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre></div></li></ol><ol start=4><li><p>更新 <code>apt</code> 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div></li></ol></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span><span style=color:#b44>[kubernetes]
</span><span style=color:#b44>name=Kubernetes
</span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span><span style=color:#b44>enabled=1
</span><span style=color:#b44>gpgcheck=1
</span><span style=color:#b44>repo_gpgcheck=1
</span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span><span style=color:#b44>EOF</span>

<span style=color:#080;font-style:italic># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span>
sudo setenforce <span style=color:#666>0</span>
sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes

sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div><p><strong>请注意：</strong></p><ul><li><p>通过运行命令 <code>setenforce 0</code> 和 <code>sed ...</code> 将 SELinux 设置为 permissive 模式
可以有效地将其禁用。
这是允许容器访问主机文件系统所必需的，而这些操作时为了例如 Pod 网络工作正常。</p><p>你必须这么做，直到 kubelet 做出对 SELinux 的支持进行升级为止。</p></li><li><p>如果你知道如何配置 SELinux 则可以将其保持启用状态，但可能需要设定 kubeadm 不支持的部分配置</p></li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>安装 CNI 插件（大多数 Pod 网络都需要）：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
sudo mkdir -p /opt/cni/bin
curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-amd64-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C /opt/cni/bin -xz
</code></pre></div><p>定义要下载命令文件的目录。</p><blockquote class="note callout"><div><strong>说明：</strong><p><code>DOWNLOAD_DIR</code> 变量必须被设置为一个可写入的目录。
如果你在运行 Flatcar Container Linux，可将 <code>DOWNLOAD_DIR</code> 设置为 <code>/opt/bin</code>。</div></blockquote><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span>/usr/local/bin
sudo mkdir -p <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</code></pre></div><p>安装 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.17.0&#34;</span>
curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-amd64.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</code></pre></div><p>安装 <code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code> 并添加 <code>kubelet</code> 系统服务：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
<span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/amd64/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>

<span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.4.0&#34;</span>
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre></div><p>激活并启动 <code>kubelet</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl <span style=color:#a2f>enable</span> --now kubelet
</code></pre></div><blockquote class="note callout"><div><strong>说明：</strong><p>Flatcar Container Linux 发行版会将 <code>/usr/</code> 目录挂载为一个只读文件系统。
在启动引导你的集群之前，你需要执行一些额外的操作来配置一个可写入的目录。
参见 <a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>kubeadm 故障排查指南</a>
以了解如何配置一个可写入的目录。</div></blockquote></div></div><p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。</p><h2 id=configure-cgroup-driver-used-by-kubelet-on-contol-plane-node>在控制平面节点上配置 kubelet 使用的 cgroup 驱动程序</h2><p>使用 Docker 时，kubeadm 会自动为其检测 cgroup 驱动并在运行时对
<code>/var/lib/kubelet/kubeadm-flags.env</code> 文件进行配置。</p><p>如果你在使用不同的 CRI，你必须为 <code>kubeadm init</code> 传递 <code>cgroupDriver</code>
值，像这样：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>&lt;value&gt;<span style=color:#bbb>
</span></code></pre></div><p>进一步的相关细节，可参阅
<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>使用配置文件来执行 kubeadm init</a> 以及 <a href=/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration</a>。</p><p>请注意，你只需要在你的 cgroup 驱动程序不是 <code>cgroupfs</code> 时这么做，
因为它已经是 kubelet 中的默认值。</p><blockquote class="note callout"><div><strong>说明：</strong><p>由于 kubelet 已经弃用了 <code>--cgroup-driver</code> 标志，如果你在配置文件
<code>/var/lib/kubelet/kubeadm-flags.env</code> 或者 <code>/etc/default/kubelet</code>
（对于 RPM 而言是 <code>/etc/sysconfig/kubelet</code>）包含此设置，请将其删除
并使用 KubeletConfiguration 作为替代（默认存储于
<code>/var/lib/kubelet/config.yaml</code> 文件中）。</div></blockquote><p>自动检测其他容器运行时（例如 CRI-O 和 containerd）的 cgroup 驱动的相关
工作扔在进行中。</p><h2 id=故障排查>故障排查</h2><p>如果你在使用 kubeadm 时遇到困难，请参阅我们的
<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排查文档</a>。</p><h2 id=接下来>接下来</h2><ul><li><a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>3.3.1.2 - 对 kubeadm 进行故障排查</h1><p>与任何程序一样，你可能会在安装或者运行 kubeadm 时遇到错误。
本文列举了一些常见的故障场景，并提供可帮助你理解和解决这些问题的步骤。</p><p>如果你的问题未在下面列出，请执行以下步骤：</p><ul><li><p>如果你认为问题是 kubeadm 的错误：</p><ul><li>转到 <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> 并搜索存在的问题。</li><li>如果没有问题，请 <a href=https://github.com/kubernetes/kubeadm/issues/new>打开</a> 并遵循问题模板。</li></ul></li><li><p>如果你对 kubeadm 的工作方式有疑问，可以在 <a href=https://slack.k8s.io/>Slack</a> 上的 #kubeadm 频道提问，
或者在 <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a> 上提问。
请加入相关标签，例如 <code>#kubernetes</code> 和 <code>#kubeadm</code>，这样其他人可以帮助你。</p></li></ul><h2 id=在安装过程中没有找到-ebtables-或者其他类似的可执行文件>在安装过程中没有找到 <code>ebtables</code> 或者其他类似的可执行文件</h2><p>如果在运行 <code>kubeadm init</code> 命令时，遇到以下的警告</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
<span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</code></pre></div><p>那么或许在你的节点上缺失 <code>ebtables</code>、<code>ethtool</code> 或者类似的可执行文件。
你可以使用以下命令安装它们：</p><ul><li>对于 Ubuntu/Debian 用户，运行 <code>apt install ebtables ethtool</code> 命令。</li><li>对于 CentOS/Fedora 用户，运行 <code>yum install ebtables ethtool</code> 命令。</li></ul><h2 id=在安装过程中-kubeadm-一直等待控制平面就绪>在安装过程中，kubeadm 一直等待控制平面就绪</h2><p>如果你注意到 <code>kubeadm init</code> 在打印以下行后挂起：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</code></pre></div><p>这可能是由许多问题引起的。最常见的是：</p><ul><li><p>网络连接问题。在继续之前，请检查你的计算机是否具有全部联通的网络连接。</p></li><li><p>kubelet 的默认 cgroup 驱动程序配置不同于 Docker 使用的配置。
检查系统日志文件 (例如 <code>/var/log/message</code>) 或检查 <code>journalctl -u kubelet</code> 的输出。 如果你看见以下内容：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>error: failed to run Kubelet: failed to create kubelet:
misconfiguration: kubelet cgroup driver: <span style=color:#b44>&#34;systemd&#34;</span> is different from docker cgroup driver: <span style=color:#b44>&#34;cgroupfs&#34;</span>
</code></pre></div><p>有两种常见方法可解决 cgroup 驱动程序问题：</p><ol><li><p>按照 <a href=/zh/docs/setup/production-environment/container-runtimes/#docker>此处</a> 的说明再次安装 Docker。</p></li><li><p>更改 kubelet 配置以手动匹配 Docker cgroup 驱动程序，你可以参考
<a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node>在主节点上配置 kubelet 要使用的 cgroup 驱动程序</a></p></li></ol></li><li><p>控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。你可以运行 <code>docker ps</code> 命令来检查以及 <code>docker logs</code> 命令来检视每个容器的运行日志。</p></li></ul><h2 id=当删除托管容器时-kubeadm-阻塞>当删除托管容器时 kubeadm 阻塞</h2><p>如果 Docker 停止并且不删除 Kubernetes 所管理的所有容器，可能发生以下情况：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo kubeadm reset
<span style=color:#666>[</span>preflight<span style=color:#666>]</span> Running pre-flight checks
<span style=color:#666>[</span>reset<span style=color:#666>]</span> Stopping the kubelet service
<span style=color:#666>[</span>reset<span style=color:#666>]</span> Unmounting mounted directories in <span style=color:#b44>&#34;/var/lib/kubelet&#34;</span>
<span style=color:#666>[</span>reset<span style=color:#666>]</span> Removing kubernetes-managed containers
<span style=color:#666>(</span>block<span style=color:#666>)</span>
</code></pre></div><p>一个可行的解决方案是重新启动 Docker 服务，然后重新运行 <code>kubeadm reset</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo systemctl restart docker.service
sudo kubeadm reset
</code></pre></div><p>检查 docker 的日志也可能有用：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>journalctl -ul docker
</code></pre></div><h2 id=pods-处于-runcontainererror-crashloopbackoff-或者-error-状态>Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2><p>在 <code>kubeadm init</code> 命令运行后，系统中不应该有 pods 处于这类状态。</p><ul><li><p>在 <code>kubeadm init</code> 命令执行完后，如果有 pods 处于这些状态之一，请在 kubeadm
仓库提起一个 issue。<code>coredns</code> (或者 <code>kube-dns</code>) 应该处于 <code>Pending</code> 状态，
直到你部署了网络解决方案为止。</p></li><li><p>如果在部署完网络解决方案之后，有 Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code>
或 <code>Error</code> 状态之一，并且<code>coredns</code> （或者 <code>kube-dns</code>）仍处于 <code>Pending</code> 状态，
那很可能是你安装的网络解决方案由于某种原因无法工作。你或许需要授予它更多的
RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题，
然后在此处分类问题。</p></li><li><p>如果你安装的 Docker 版本早于 1.12.1，请在使用 <code>systemd</code> 来启动 <code>dockerd</code> 和重启 <code>docker</code> 时，
删除 <code>MountFlags=slave</code> 选项。
你可以在 <code>/usr/lib/systemd/system/docker.service</code> 中看到 MountFlags。
MountFlags 可能会干扰 Kubernetes 挂载的卷， 并使 Pods 处于 <code>CrashLoopBackOff</code> 状态。
当 Kubernetes 不能找到 <code>var/run/secrets/kubernetes.io/serviceaccount</code> 文件时会发生错误。</p></li></ul><h2 id=coredns-或-kube-dns-停滞在-pending-状态><code>coredns</code> （或 <code>kube-dns</code>）停滞在 <code>Pending</code> 状态</h2><p>这一行为是 <strong>预期之中</strong> 的，因为系统就是这么设计的。
kubeadm 的网络供应商是中立的，因此管理员应该选择 <a href=/zh/docs/concepts/cluster-administration/addons/>安装 pod 的网络解决方案</a>。
你必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。
在网络被配置好之前，DNS 组件会一直处于 <code>Pending</code> 状态。</p><h2 id=hostport-服务无法工作><code>HostPort</code> 服务无法工作</h2><p>此 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用取决于你的 Pod 网络配置。请联系 Pod 解决方案的作者，
以确认 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用。</p><p>已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。</p><p>有关更多信息，请参考 <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap 文档</a>.</p><p>如果你的网络提供商不支持 portmap CNI 插件，你或许需要使用
<a href=/zh/docs/concepts/services-networking/service/#nodeport>NodePort 服务的功能</a>
或者使用 <code>HostNetwork=true</code>。</p><h2 id=无法通过其服务-ip-访问-pod>无法通过其服务 IP 访问 Pod</h2><ul><li><p>许多网络附加组件尚未启用 <a href=/zh/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip>hairpin 模式</a>
该模式允许 Pod 通过其服务 IP 进行访问。这是与 <a href=https://github.com/containernetworking/cni/issues/476>CNI</a> 有关的问题。
请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。</p></li><li><p>如果你正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，你需要
确保 <code>hostname -i</code> 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。
解决方法是修改 <code>/etc/hosts</code>，请参考示例 <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>。</p></li></ul><h2 id=tls-证书错误>TLS 证书错误</h2><p>以下错误指出证书可能不匹配。</p><pre><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;)
</code></pre><ul><li><p>验证 <code>$HOME/.kube/config</code> 文件是否包含有效证书，并
在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。
该 <code>base64 -d</code> 命令可以用来解码证书，<code>openssl x509 -text -noout</code> 命令
可以用于查看证书信息。</p></li><li><p>使用如下方法取消设置 <code>KUBECONFIG</code> 环境变量的值：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>unset</span> KUBECONFIG
</code></pre></div><p>或者将其设置为默认的 <code>KUBECONFIG</code> 位置：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div></li><li><p>另一个方法是覆盖 <code>kubeconfig</code> 的现有用户 "管理员" ：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
mkdir <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div></li></ul><h2 id=在-vagrant-中使用-flannel-作为-pod-网络时的默认-nic>在 Vagrant 中使用 flannel 作为 pod 网络时的默认 NIC</h2><p>以下错误可能表明 Pod 网络中出现问题：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</code></pre></div><ul><li><p>如果你正在 Vagrant 中使用 flannel 作为 pod 网络，则必须指定 flannel 的默认接口名称。</p><p>Vagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 <code>10.0.2.15</code>，用于获得 NATed 的外部流量。</p><p>这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有
相同的公共 IP 地址。为防止这种情况，传递 <code>--iface eth1</code> 标志给 flannel 以便选择第二个接口。</p></li></ul><h2 id=容器使用的非公共-ip>容器使用的非公共 IP</h2><p>在某些情况下 <code>kubectl logs</code> 和 <code>kubectl run</code> 命令或许会返回以下错误，即便除此之外集群一切功能正常：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</code></pre></div><ul><li><p>这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故，
可能是由机器提供商的政策所导致的。</p></li><li><p>Digital Ocean 既分配一个共有 IP 给 <code>eth0</code>，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点，
然而 <code>kubelet</code> 将选择后者作为节点的 <code>InternalIP</code> 而不是公共 IP</p><p>使用 <code>ip addr show</code> 命令代替 <code>ifconfig</code> 命令去检查这种情况，因为 <code>ifconfig</code> 命令
不会显示有问题的别名 IP 地址。或者指定的 Digital Ocean 的 API 端口允许从 droplet 中
查询 anchor IP：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</code></pre></div><p>解决方法是通知 <code>kubelet</code> 使用哪个 <code>--node-ip</code>。当使用 Digital Ocean 时，可以是公网IP（分配给 <code>eth0</code>的），
或者是私网IP（分配给 <code>eth1</code> 的）。私网 IP 是可选的。
<a href=https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go>kubadm <code>NodeRegistrationOptions</code> 结构的 <code>KubeletExtraArgs</code> 部分</a> 被用来处理这种情况。</p><p>然后重启 <code>kubelet</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>systemctl daemon-reload
systemctl restart kubelet
</code></pre></div></li></ul><h2 id=coredns-pods-有-crashloopbackoff-或者-error-状态><code>coredns</code> pods 有 <code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2><p>如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 <code>coredns</code> pods 无法启动的情况。
要解决此问题，你可以尝试以下选项之一：</p><ul><li><p>升级到 <a href=/zh/docs/setup/production-environment/container-runtimes/#docker>Docker 的较新版本</a>。</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>禁用 SELinux</a>.</p></li><li><p>修改 <code>coredns</code> 部署以设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code>：</p></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</code></pre></div><p>CoreDNS 处于 <code>CrashLoopBackOff</code> 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测
到环路时。<a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>有许多解决方法</a>
可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。</p><blockquote class="warning callout"><div><strong>警告：</strong> 禁用 SELinux 或设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code> 可能会损害集群的安全性。</div></blockquote><h2 id=etcd-pods-持续重启>etcd pods 持续重启</h2><p>如果你遇到以下错误：</p><pre><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &quot;process_linux.go:110: decoding init error from pipe caused \&quot;read parent: connection reset by peer\&quot;&quot;
</code></pre><p>如果你使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。
此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。</p><p>为解决此问题，请选择以下选项之一：</p><ul><li><p>回滚到早期版本的 Docker，例如 1.13.1-75</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre></div></li><li><p>安装较新的推荐版本之一，例如 18.06:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
</code></pre></div></li></ul><h2 id=无法将以逗号分隔的值列表传递给-component-extra-args-标志内的参数>无法将以逗号分隔的值列表传递给 <code>--component-extra-args</code> 标志内的参数</h2><p><code>kubeadm init</code> 标志例如 <code>--component-extra-args</code> 允许你将自定义参数传递给像
kube-apiserver 这样的控制平面组件。然而，由于解析 (<code>mapStringString</code>) 的基础类型值，此机制将受到限制。</p><p>如果你决定传递一个支持多个逗号分隔值（例如
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"</code>）参数，
将出现 <code>flag: malformed pair, expect string=string</code> 错误。
发生这种问题是因为参数列表 <code>--apiserver-extra-args</code> 预期的是 <code>key=value</code> 形式，
而这里的 <code>NamespacesExists</code> 被误认为是缺少取值的键名。</p><p>一种解决方法是尝试分离 <code>key=value</code> 对，像这样：
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>
但这将导致键 <code>enable-admission-plugins</code> 仅有值 <code>NamespaceExists</code>。</p><p>已知的解决方法是使用 kubeadm
<a href=/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags>配置文件</a>。</p><h2 id=在节点被云控制管理器初始化之前-kube-proxy-就被调度了>在节点被云控制管理器初始化之前，kube-proxy 就被调度了</h2><p>在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。
这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。</p><p>在 kube-proxy Pod 中可以看到以下错误：</p><pre><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它，
而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kube-system patch ds kube-proxy -p<span style=color:#666>=</span><span style=color:#b44>&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/master&#34; } ] } } } }&#39;</span>
</code></pre></div><p>此问题的跟踪<a href=https://github.com/kubernetes/kubeadm/issues/1027>在这里</a>。</p><h2 id=noderegistration-taints-字段在编组-kubeadm-配置时丢失>NodeRegistration.Taints 字段在编组 kubeadm 配置时丢失</h2><p><em>注意：这个 <a href=https://github.com/kubernetes/kubeadm/issues/1358>问题</a>
仅适用于操控 kubeadm 数据类型的工具（例如，YAML 配置文件）。它将在 kubeadm API v1beta2 修复。</em></p><p>默认情况下，kubeadm 将 <code>node-role.kubernetes.io/master:NoSchedule</code> 污点应用于控制平面节点。
如果你希望 kubeadm 不污染控制平面节点，并将 <code>InitConfiguration.NodeRegistration.Taints</code> 设置成空切片，则应在编组时省略该字段。
如果省略该字段，则 kubeadm 将应用默认污点。</p><p>至少有两种解决方法：</p><ol><li><p>使用 <code>node-role.kubernetes.io/master:PreferNoSchedule</code> 污点代替空切片。
除非其他节点具有容量，<a href=/zh/docs/concepts/scheduling-eviction/taint-and-toleration/>否则将在主节点上调度 Pods</a>。</p></li><li><p>在 kubeadm init 退出后删除污点：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
</code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>3.3.1.3 - 使用 kubeadm 创建集群</h1><p><img src=https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png align=right width=150px>使用 <code>kubeadm</code>，你
能创建一个符合最佳实践的最小化 Kubernetes 集群。事实上，你可以使用 <code>kubeadm</code> 配置一个通过 <a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>Kubernetes 一致性测试</a> 的集群。
<code>kubeadm</code> 还支持其他集群生命周期功能，
例如 <a href=/zh/docs/reference/access-authn-authz/bootstrap-tokens/>启动引导令牌</a> 和集群升级。</p><p>kubeadm 工具很棒，如果你需要：</p><ul><li>一个尝试 Kubernetes 的简单方法。</li><li>一个现有用户可以自动设置集群并测试其应用程序的途径。</li><li>其他具有更大范围的生态系统和/或安装工具中的构建模块。</li></ul><p>你可以在各种机器上安装和使用 <code>kubeadm</code>：笔记本电脑，
一组云服务器，Raspberry Pi 等。无论是部署到云还是本地，
你都可以将 <code>kubeadm</code> 集成到预配置系统中，例如 Ansible 或 Terraform。</p><h2 id=准备开始>准备开始</h2><p>要遵循本指南，你需要：</p><ul><li>一台或多台运行兼容 deb/rpm 的 Linux 操作系统的计算机；例如：Ubuntu 或 CentOS。</li><li>每台机器 2 GB 以上的内存，内存不足时应用会受限制。</li><li>用作控制平面节点的计算机上至少有2个 CPU。</li><li>集群中所有计算机之间具有完全的网络连接。你可以使用公共网络或专用网络。</li></ul><p>你还需要使用可以在新集群中部署特定 Kubernetes 版本对应的 <code>kubeadm</code>。</p><p><a href=/zh/docs/setup/release/version-skew-policy/#supported-versions>Kubernetes 版本及版本倾斜支持策略</a> 适用于 <code>kubeadm</code> 以及整个 Kubernetes。
查阅该策略以了解支持哪些版本的 Kubernetes 和 <code>kubeadm</code>。
该页面是为 Kubernetes v1.20 编写的。</p><p><code>kubeadm</code> 工具的整体功能状态为一般可用性（GA）。一些子功能仍在积极开发中。
随着工具的发展，创建集群的实现可能会略有变化，但总体实现应相当稳定。</p><blockquote class="note callout"><div><strong>说明：</strong> 根据定义，在 <code>kubeadm alpha</code> 下的所有命令均在 alpha 级别上受支持。</div></blockquote><h2 id=目标>目标</h2><ul><li>安装单个控制平面的 Kubernetes 集群</li><li>在集群上安装 Pod 网络，以便你的 Pod 可以相互连通</li></ul><h2 id=操作指南>操作指南</h2><h3 id=在你的主机上安装-kubeadm>在你的主机上安装 kubeadm</h3><p>查看 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>"安装 kubeadm"</a>。</p><blockquote class="note callout"><div><strong>说明：</strong><p>如果你已经安装了kubeadm，执行 <code>apt-get update && apt-get upgrade</code> 或 <code>yum update</code> 以获取 kubeadm 的最新版本。</p><p>升级时，kubelet 每隔几秒钟重新启动一次，
在 crashloop 状态中等待 kubeadm 发布指令。crashloop 状态是正常现象。
初始化控制平面后，kubelet 将正常运行。</p></div></blockquote><h3 id=初始化控制平面节点>初始化控制平面节点</h3><p>控制平面节点是运行控制平面组件的机器，
包括 <a class=glossary-tooltip title="etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。" data-toggle=tooltip data-placement=top href=/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> （集群数据库）
和 <a class=glossary-tooltip title="提供 Kubernetes API 服务的控制面组件。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/command-line-tools-reference/kube-apiserver/ target=_blank aria-label="API Server">API Server</a>
（命令行工具 <a class=glossary-tooltip title="kubectl 是用来和 Kubernetes API 服务器进行通信的命令行工具。" data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a> 与之通信）。</p><ol><li>（推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用，
你应该指定 <code>--control-plane-endpoint</code> 为所有控制平面节点设置共享端点。
端点可以是负载均衡器的 DNS 名称或 IP 地址。</li><li>选择一个Pod网络插件，并验证是否需要为 <code>kubeadm init</code> 传递参数。
根据你选择的第三方网络插件，你可能需要设置 <code>--pod-network-cidr</code> 的值。
请参阅 <a href=#pod-network>安装Pod网络附加组件</a>。</li><li>（可选）从版本1.14开始，<code>kubeadm</code> 尝试使用一系列众所周知的域套接字路径来检测 Linux 上的容器运行时。
要使用不同的容器运行时，
或者如果在预配置的节点上安装了多个容器，请为 <code>kubeadm init</code> 指定 <code>--cri-socket</code> 参数。
请参阅<a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>安装运行时</a>。</li><li>（可选）除非另有说明，否则 <code>kubeadm</code> 使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。
要使用其他网络接口，请为 <code>kubeadm init</code> 设置 <code>--apiserver-advertise-address=&lt;ip-address></code> 参数。
要部署使用 IPv6 地址的 Kubernetes 集群，
必须指定一个 IPv6 地址，例如 <code>--apiserver-advertise-address=fd00::101</code></li><li>（可选）在 <code>kubeadm init</code> 之前运行 <code>kubeadm config images pull</code>，以验证与 gcr.io 容器镜像仓库的连通性。</li></ol><p>要初始化控制平面节点，请运行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init &lt;args&gt;
</code></pre></div><h3 id=关于-apiserver-advertise-address-和-controlplaneendpoint-的注意事项>关于 apiserver-advertise-address 和 ControlPlaneEndpoint 的注意事项</h3><p><code>--apiserver-advertise-address</code> 可用于为控制平面节点的 API server 设置广播地址，
<code>--control-plane-endpoint</code> 可用于为所有控制平面节点设置共享端点。</p><p><code>--control-plane-endpoint</code> 允许 IP 地址和可以映射到 IP 地址的 DNS 名称。
请与你的网络管理员联系，以评估有关此类映射的可能解决方案。</p><p>这是一个示例映射：</p><pre><code>192.168.0.102 cluster-endpoint
</code></pre><p>其中 <code>192.168.0.102</code> 是此节点的 IP 地址，<code>cluster-endpoint</code> 是映射到该 IP 的自定义 DNS 名称。
这将允许你将 <code>--control-plane-endpoint=cluster-endpoint</code> 传递给 <code>kubeadm init</code>，并将相同的 DNS 名称传递给 <code>kubeadm join</code>。
稍后你可以修改 <code>cluster-endpoint</code> 以指向高可用性方案中的负载均衡器的地址。</p><p>kubeadm 不支持将没有 <code>--control-plane-endpoint</code> 参数的单个控制平面集群转换为高可用性集群。</p><h3 id=更多信息>更多信息</h3><p>有关 <code>kubeadm init</code> 参数的更多信息，请参见 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm 参考指南</a>。</p><p>要使用配置文件配置 <code>kubeadm init</code> 命令，请参见<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>带配置文件使用 kubeadm init</a>。</p><p>要自定义控制平面组件，包括可选的对控制平面组件和 etcd 服务器的活动探针提供 IPv6 支持，请参阅<a href=/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>自定义参数</a>。</p><p>要再次运行 <code>kubeadm init</code>，你必须首先<a href=#tear-down>卸载集群</a>。</p><p>如果将具有不同架构的节点加入集群，
请确保已部署的 DaemonSet 对这种体系结构具有容器镜像支持。</p><p><code>kubeadm init</code> 首先运行一系列预检查以确保机器
准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 <code>kubeadm init</code>
下载并安装集群控制平面组件。这可能会需要几分钟。
完成之后你应该看到：</p><pre><code class=language-none data-lang=none>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>要使非 root 用户可以运行 kubectl，请运行以下命令，
它们也是 <code>kubeadm init</code> 输出的一部分：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</code></pre></div><p>或者，如果你是 <code>root</code> 用户，则可以运行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</code></pre></div><p>记录 <code>kubeadm init</code> 输出的 <code>kubeadm join</code> 命令。
你需要此命令<a href=#join-nodes>将节点加入集群</a>。</p><p>令牌用于控制平面节点和加入节点之间的相互身份验证。
这里包含的令牌是密钥。确保它的安全，
因为拥有此令牌的任何人都可以将经过身份验证的节点添加到你的集群中。
可以使用 <code>kubeadm token</code> 命令列出，创建和删除这些令牌。
请参阅 <a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadm 参考指南</a>。</p><h3 id=pod-network>安装 Pod 网络附加组件</h3><blockquote class="caution callout"><div><strong>注意：</strong><p>本节包含有关网络设置和部署顺序的重要信息。
在继续之前，请仔细阅读所有建议。</p><p><strong>你必须部署一个基于 Pod 网络插件的
<a class=glossary-tooltip title="容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni target=_blank aria-label=容器网络接口>容器网络接口</a>
(CNI)，以便你的 Pod 可以相互通信。
在安装网络之前，集群 DNS (CoreDNS) 将不会启动。</strong></p><ul><li>注意你的 Pod 网络不得与任何主机网络重叠：
如果有重叠，你很可能会遇到问题。
（如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突，
则应考虑使用一个合适的 CIDR 块来代替，
然后在执行 <code>kubeadm init</code> 时使用 <code>--pod-network-cidr</code> 参数并在你的网络插件的 YAML 中替换它）。</li></ul><ul><li>默认情况下，<code>kubeadm</code> 将集群设置为使用和强制使用 <a href=/zh/docs/reference/access-authn-authz/rbac/>RBAC</a>（基于角色的访问控制）。
确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。</li></ul><ul><li>如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络），
请确保你的Pod网络插件支持 IPv6。
IPv6 支持已在 CNI <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a> 版本中添加。</li></ul></div></blockquote><blockquote class="note callout"><div><strong>说明：</strong> 目前 Calico 是 kubeadm 项目中执行 e2e 测试的唯一 CNI 插件。
如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm 或 kubernetes 问题跟踪器中记录。</div></blockquote><p>一些外部项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些还支持<a href=/zh/docs/concepts/services-networking/network-policies/>网络策略</a>。</p><p>请参阅实现 <a href=/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes 网络模型</a> 的附加组件列表。</p><p>你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f &lt;add-on.yaml&gt;
</code></pre></div><p>每个集群只能安装一个 Pod 网络。</p><p>安装 Pod 网络后，您可以通过在 <code>kubectl get pods --all-namespaces</code> 输出中检查 CoreDNS Pod 是否 <code>Running</code> 来确认其是否正常运行。
一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。</p><p>如果您的网络无法正常工作或CoreDNS不在“运行中”状态，请查看 <code>kubeadm</code> 的<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除指南</a>。</p><h3 id=控制平面节点隔离>控制平面节点隔离</h3><p>默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。
如果你希望能够在控制平面节点上调度 Pod，
例如用于开发的单机 Kubernetes 集群，请运行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre></div><p>输出看起来像：</p><pre><code>node &quot;test-01&quot; untainted
taint &quot;node-role.kubernetes.io/master:&quot; not found
taint &quot;node-role.kubernetes.io/master:&quot; not found
</code></pre><p>这将从任何拥有 <code>node-role.kubernetes.io/master</code> taint 标记的节点中移除该标记，
包括控制平面节点，这意味着调度程序将能够在任何地方调度 Pods。</p><h3 id=join-nodes>加入节点</h3><p>节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作：</p><ul><li>SSH 到机器</li><li>成为 root （例如 <code>sudo su -</code>）</li><li>运行 <code>kubeadm init</code> 输出的命令。例如：</li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre></div><p>如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token list
</code></pre></div><p>输出类似于以下内容：</p><pre><code class=language-console data-lang=console>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
                                                   signing          token generated by     bootstrappers:
                                                                    'kubeadm init'.        kubeadm:
                                                                                           default-node-token
</code></pre><p>默认情况下，令牌会在24小时后过期。如果要在当前令牌过期后将节点加入集群，
则可以通过在控制平面节点上运行以下命令来创建新令牌：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm token create
</code></pre></div><p>输出类似于以下内容：</p><pre><code class=language-console data-lang=console>5didvk.d09sbcov8ph2amjw
</code></pre><p>如果你没有 <code>--discovery-token-ca-cert-hash</code> 的值，则可以通过在控制平面节点上执行以下命令链来获取它：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</code></pre></div><p>输出类似于以下内容：</p><pre><code class=language-console data-lang=console>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</code></pre><blockquote class="note callout"><div><strong>说明：</strong> 要为 <code>&lt;control-plane-host>:&lt;control-plane-port></code> 指定 IPv6 元组，必须将 IPv6 地址括在方括号中，例如：<code>[fd00::101]:2073</code></div></blockquote><p>输出应类似于：</p><pre><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><p>几秒钟后，当你在控制平面节点上执行 <code>kubectl get nodes</code>，你会注意到该节点出现在输出中。</p><h3 id=可选-从控制平面节点以外的计算机控制集群>（可选）从控制平面节点以外的计算机控制集群</h3><p>为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信，
你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
</code></pre></div><blockquote class="note callout"><div><strong>说明：</strong><p>上面的示例假定为 root 用户启用了SSH访问。如果不是这种情况，
你可以使用 <code>scp</code> 将 admin.conf 文件复制给其他允许访问的用户。</p><p>admin.conf 文件为用户提供了对集群的超级用户特权。
该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。
你可以使用 <code>kubeadm alpha kubeconfig user --client-name &lt;CN></code> 命令执行此操作。
该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。
之后，使用 <code>kubectl create (cluster)rolebinding</code> 授予特权。</p></div></blockquote><h3 id=可选-将api服务器代理到本地主机>（可选）将API服务器代理到本地主机</h3><p>如果要从集群外部连接到 API 服务器，则可以使用 <code>kubectl proxy</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
</code></pre></div><p>你现在可以在本地访问API服务器 http://localhost:8001/api/v1</p><h2 id=tear-down>清理</h2><p>如果你在集群中使用了一次性服务器进行测试，则可以关闭这些服务器，而无需进一步清理。你可以使用 <code>kubectl config delete-cluster</code> 删除对集群的本地引用。</p><p>但是，如果要更干净地取消配置群集，
则应首先<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>清空节点</a>并确保该节点为空，
然后取消配置该节点。</p><h3 id=删除节点>删除节点</h3><p>使用适当的凭证与控制平面节点通信，运行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
</code></pre></div><p>在删除节点之前，请重置 <code>kubeadm</code> 安装的状态：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm reset
</code></pre></div><p>重置过程不会重置或清除 iptables 规则或 IPVS 表。如果你希望重置 iptables，则必须手动进行：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</code></pre></div><p>如果要重置 IPVS 表，则必须运行以下命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ipvsadm -C
</code></pre></div><p>现在删除节点：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl delete node &lt;node name&gt;
</code></pre></div><p>如果你想重新开始，只需运行 <code>kubeadm init</code> 或 <code>kubeadm join</code> 并加上适当的参数。</p><h3 id=清理控制平面>清理控制平面</h3><p>你可以在控制平面主机上使用 <code>kubeadm reset</code> 来触发尽力而为的清理。</p><p>有关此子命令及其选项的更多信息，请参见<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>参考文档。</p><h2 id=whats-next>下一步</h2><ul><li>使用 <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a> 验证集群是否正常运行</li><li><a id=lifecycle>有关使用kubeadm升级集群的详细信息，请参阅<a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级 kubeadm 集群</a>。</li><li>在<a href=/zh/docs/reference/setup-tools/kubeadm>kubeadm 参考文档</a>中了解有关高级 <code>kubeadm</code> 用法的信息</li><li>了解有关Kubernetes<a href=/zh/docs/concepts/>概念</a>和<a href=/zh/docs/reference/kubectl/overview/><code>kubectl</code></a>的更多信息。</li><li>有关Pod网络附加组件的更多列表，请参见<a href=/zh/docs/concepts/cluster-administration/networking/>集群网络</a>页面。</li><li><a id=other-addons>请参阅<a href=/zh/docs/concepts/cluster-administration/addons/>附加组件列表</a>以探索其他附加组件，
包括用于 Kubernetes 集群的日志记录，监视，网络策略，可视化和控制的工具。</li><li>配置集群如何处理集群事件的日志以及
在Pods中运行的应用程序。
有关所涉及内容的概述，请参见<a href=/zh/docs/concepts/cluster-administration/logging/>日志架构</a>。</li></ul><h3 id=feedback>反馈</h3><ul><li>有关 bugs, 访问 <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li><li>有关支持, 访问
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a> Slack 频道</li><li>General SIG 集群生命周期开发 Slack 频道:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG 集群生命周期 <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG 集群生命周期邮件列表:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>版本倾斜政策</h2><p>版本 v1.24 的kubeadm 工具可以使用版本 v1.24 或 v1.23 的控制平面部署集群。kubeadm v1.24 还可以升级现有的 kubeadm 创建的 v1.23 版本的集群。</p><p>由于没有未来，kubeadm CLI v1.24 可能会或可能无法部署 v1.25 集群。</p><p>这些资源提供了有关 kubelet 与控制平面以及其他 Kubernetes 组件之间受支持的版本倾斜的更多信息：</p><ul><li>Kubernetes <a href=/zh/docs/setup/release/version-skew-policy/>版本和版本偏斜政策</a></li><li>Kubeadm-specific <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>安装指南</a></li></ul><h2 id=limitations>局限性</h2><h3 id=resilience>集群弹性</h3><p>此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。
这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。</p><p>解决方法:</p><ul><li>定期<a href=https://coreos.com/etcd/docs/latest/admin_guide.html>备份 etcd</a>。
kubeadm 配置的 etcd 数据目录位于控制平面节点上的 <code>/var/lib/etcd</code> 中。</li></ul><ul><li>使用多个控制平面节点。你可以阅读
<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/>可选的高可用性拓扑</a> 选择集群拓扑提供的
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>.</li></ul><h3 id=multi-platform>平台兼容性</h3><p>kubeadm deb/rpm 软件包和二进制文件是为 amd64，arm (32-bit)，arm64，ppc64le 和 s390x 构建的遵循<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>多平台提案</a>。</p><p>从 v1.12 开始还支持用于控制平面和附加组件的多平台容器镜像。</p><p>只有一些网络提供商为所有平台提供解决方案。请查阅上方的
网络提供商清单或每个提供商的文档以确定提供商是否
支持你选择的平台。</p><h2 id=troubleshooting>故障排除</h2><p>如果你在使用 kubeadm 时遇到困难，请查阅我们的<a href=/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除文档</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>3.3.1.4 - 使用 kubeadm 定制控制平面配置</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.12 [stable]</code></div><p>kubeadm <code>ClusterConfiguration</code> 对象公开了 <code>extraArgs</code> 字段，它可以覆盖传递给控制平面组件（如 APIServer、ControllerManager 和 Scheduler）的默认参数。各组件配置使用如下字段定义：</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li></ul><p><code>extraArgs</code> 字段由 <code>key: value</code> 对组成。
要覆盖控制平面组件的参数:</p><ol><li>将适当的字段添加到配置中。</li><li>向字段添加要覆盖的参数值。</li><li>用 <code>--config &lt;YOUR CONFIG YAML></code> 运行 <code>kubeadm init</code>。</li></ol><p>有关配置中的每个字段的详细信息，您可以导航到我们的 <a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2#ClusterConfiguration>API 参考页面</a>。</p><blockquote class="note callout"><div><strong>说明：</strong><p>您可以通过运行 <code>kubeadm config print init-defaults</code> 并将输出保存到您选择的文件中，以默认值形式生成 <code>ClusterConfiguration</code> 对象。</div></blockquote><h2 id=apiserver-参数>APIServer 参数</h2><p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertise-address</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.0.103</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></code></pre></div><h2 id=controllermanager-参数>ControllerManager 参数</h2><p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></code></pre></div><h2 id=scheduler-参数>Scheduler 参数</h2><p>有关详细信息，请参阅 <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubeconfig</span>:<span style=color:#bbb> </span>/home/johndoe/kubeconfig.yaml<span style=color:#bbb>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>3.3.1.5 - 高可用拓扑选项</h1><p>本页面介绍了配置高可用（HA） Kubernetes 集群拓扑的两个选项。</p><p>您可以设置 HA 集群：</p><ul><li>使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存</li><li>使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行</li></ul><p>在设置 HA 集群之前，您应该仔细考虑每种拓扑的优缺点。</p><blockquote class="note callout"><div><strong>说明：</strong> kubeadm 静态引导 etcd 集群。 阅读 etcd <a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>集群指南</a>以获得更多详细信息。</div></blockquote><h2 id=堆叠-stacked-etcd-拓扑>堆叠（Stacked） etcd 拓扑</h2><p>堆叠（Stacked） HA 集群是一种这样的<a href=https://en.wikipedia.org/wiki/Network_topology>拓扑</a>，其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。</p><p>每个控制平面节点运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。</p><p><code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</p><p>每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 <code>kube-apiserver</code> 通信。这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</p><p>这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，设置起来更简单，而且更易于副本管理。</p><p>然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，并且冗余会受到影响。您可以通过添加更多控制平面节点来降低此风险。</p><p>因此，您应该为 HA 集群运行至少三个堆叠的控制平面节点。</p><p>这是 kubeadm 中的默认拓扑。当使用 <code>kubeadm init</code> 和 <code>kubeadm join --control-plane</code> 时，在控制平面节点上会自动创建本地 etcd 成员。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt="堆叠的 etcd 拓扑"></p><h2 id=外部-etcd-拓扑>外部 etcd 拓扑</h2><p>具有外部 etcd 的 HA 集群是一种这样的<a href=https://en.wikipedia.org/wiki/Network_topology>拓扑</a>，其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。</p><p>就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。同样， <code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。但是，etcd 成员在不同的主机上运行，​​每个 etcd 主机与每个控制平面节点的 <code>kube-apiserver</code> 通信。</p><p>这种拓扑结构解耦了控制平面和 etcd 成员。因此，它提供了一种 HA 设置，其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。</p><p>但是，此拓扑需要两倍于堆叠 HA 拓扑的主机数量。</p><p>具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt="外部 etcd 拓扑"></p><h2 id=接下来>接下来</h2><ul><li><a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 设置高可用集群</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>3.3.1.6 - 利用 kubeadm 创建高可用集群</h1><p>本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：</p><ul><li>使用堆控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。</li><li>使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。</li></ul><p>在下一步之前，您应该仔细考虑哪种方法更好的满足您的应用程序和环境的需求。
<a href=/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/>这是对比文档</a> 讲述了每种方法的优缺点。</p><p>如果您在安装 HA 集群时遇到问题，请在 kubeadm <a href=https://github.com/kubernetes/kubeadm/issues/new>问题跟踪</a>里向我们提供反馈。</p><p>您也可以阅读 <a href=/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级文件</a></p><blockquote class="caution callout"><div><strong>注意：</strong> 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。</div></blockquote><h2 id=准备开始>准备开始</h2><p>对于这两种方法，您都需要以下基础设施：</p><ul><li>配置三台机器 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm 的最低要求</a> 给主节点</li><li>配置三台机器 <a href=/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm 的最低要求</a> 给工作节点</li><li>在集群中，所有计算机之间的完全网络连接（公网或私网）</li><li>所有机器上的 sudo 权限</li><li>每台设备对系统中所有节点的 SSH 访问</li><li>在所有机器上安装 <code>kubeadm</code> 和 <code>kubelet</code>，<code>kubectl</code> 是可选的。</li></ul><p>仅对于外部 etcd 集群来说，您还需要：</p><ul><li>给 etcd 成员使用的另外三台机器</li></ul><h2 id=这两种方法的第一步>这两种方法的第一步</h2><h3 id=为-kube-apiserver-创建负载均衡器>为 kube-apiserver 创建负载均衡器</h3><blockquote class="note callout"><div><strong>说明：</strong> 使用负载均衡器需要许多配置。您的集群搭建可能需要不同的配置。下面的例子只是其中的一方面配置。</div></blockquote><ol><li><p>创建一个名为 kube-apiserver 的负载均衡器解析 DNS。</p><ul><li><p>在云环境中，应该将控制平面节点放置在 TCP 后面转发负载平衡。 该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。健康检查 apiserver 是在 kube-apiserver 监听端口(默认值 <code>:6443</code>)上的一个 TCP 检查。</p></li><li><p>不建议在云环境中直接使用 IP 地址。</p></li><li><p>负载均衡器必须能够在 apiserver 端口上与所有控制平面节点通信。它还必须允许其监听端口的传入流量。</p></li><li><p>确保负载均衡器的地址始终匹配 kubeadm 的 <code>ControlPlaneEndpoint</code> 地址。</p></li><li><p>阅读<a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing>软件负载平衡选项指南</a>以获取更多详细信息。</p></li></ul></li></ol><ol start=2><li><p>添加第一个控制平面节点到负载均衡器并测试连接：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>nc -v LOAD_BALANCER_IP PORT
</code></pre></div><ul><li>由于 apiserver 尚未运行，预期会出现一个连接拒绝错误。然而超时意味着负载均衡器不能和控制平面节点通信。
如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。</li></ul></li><li><p>将其余控制平面节点添加到负载均衡器目标组。</p></li></ol><h2 id=使用堆控制平面和-etcd-节点>使用堆控制平面和 etcd 节点</h2><h3 id=控制平面节点的第一步>控制平面节点的第一步</h3><ol><li><p>初始化控制平面：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</code></pre></div><ul><li>您可以使用 <code>--kubernetes-version</code> 标志来设置要使用的 Kubernetes 版本。建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。</li><li>这个 <code>--control-plane-endpoint</code> 标志应该被设置成负载均衡器的地址或 DNS 和端口。</li><li>这个 <code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。如果正好相反，你更喜欢手动地通过控制平面节点或者使用自动化
工具复制证书，请删除此标志并参考如下部分<a href=#manual-certs>证书分配手册</a>。</li></ul></li></ol><blockquote class="note callout"><div><strong>说明：</strong> 标志 <code>kubeadm init</code>、<code>--config</code> 和 <code>--certificate-key</code> 不能混合使用，因此如果您要使用<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>kubeadm 配置</a>，您必须在相应的配置文件（位于 <code>InitConfiguration</code> 和 <code>JoinConfiguration: controlPlane</code>）添加 <code>certificateKey</code> 字段。</div></blockquote><blockquote class="note callout"><div><strong>说明：</strong> 一些 CNI 网络插件如 Calico 需要 CIDR 例如 <code>192.168.0.0/16</code> 和一些像 Weave 没有。参考
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNI 网络文档</a>。
通过传递 <code>--pod-network-cidr</code> 标志添加 pod CIDR，或者您可以使用 kubeadm 配置文件，在 <code>ClusterConfiguration</code> 的 <code>networking</code> 对象下设置 <code>podSubnet</code> 字段。</div></blockquote><ul><li><p>命令完成后，您应该会看到类似以下内容：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>...
现在，您可以通过在根目录上运行以下命令来加入任意数量的控制平面节点：
kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07

请注意，证书密钥可以访问集群内敏感数据，请保密！
为了安全起见，将在两个小时内删除上传的证书； 如有必要，您可以使用 kubeadm 初始化上传证书阶段，之后重新加载证书。

然后，您可以通过在根目录上运行以下命令来加入任意数量的工作节点：
kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div><ul><li><p>将此输出复制到文本文件。 稍后您将需要它来将控制平面节点和辅助节点加入集群。</p></li><li><p>当 <code>--upload-certs</code> 与 <code>kubeadm init</code> 一起使用时，主控制平面的证书被加密并上传到 <code>kubeadm-certs</code> 密钥中。</p></li><li><p>要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm init phase upload-certs --upload-certs
</code></pre></div></li><li><p>您还可以在 <code>init</code> 期间指定自定义的 <code>--certificate-key</code>，以后可以由 <code>join</code> 使用。
要生成这样的密钥，可以使用以下命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubeadm alpha certs certificate-key
</code></pre></div></li></ul></li></ul><blockquote class="note callout"><div><strong>说明：</strong> <code>kubeadm-certs</code> 密钥和解密密钥会在两个小时后失效。</div></blockquote><blockquote class="caution callout"><div><strong>注意：</strong> 正如命令输出中所述，证书密钥可访问群集敏感数据，并将其保密！</div></blockquote><ol><li><p>应用您选择的 CNI 插件：
<a href=/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>请遵循以下指示</a>
安装 CNI 提供程序。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod CIDR 相对应。</p><p>在此示例中，我们使用 Weave Net：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div></li><li><p>输入以下内容，并查看 pods 的控制平面组件启动：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl get pod -n kube-system -w
</code></pre></div></li></ol><h3 id=其余控制平面节点的步骤>其余控制平面节点的步骤</h3><blockquote class="note callout"><div><strong>说明：</strong> 从 kubeadm 1.15 版本开始，您可以并行加入多个控制平面节点。
在此版本之前，您必须在第一个节点初始化后才能依序的增加新的控制平面节点。</div></blockquote><p>对于每个其他控制平面节点，您应该：</p><ol><li><p>执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给您的 join 命令。
它看起来应该像这样：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</code></pre></div><ul><li>这个 <code>--control-plane</code> 命令通知 <code>kubeadm join</code> 创建一个新的控制平面。</li><li><code>--certificate-key ...</code> 将导致从集群中的 <code>kubeadm-certs</code> 秘钥下载控制平面证书并使用给定的密钥进行解密。</li></ul></li></ol><h2 id=外部-etcd-节点>外部 etcd 节点</h2><p>使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程，
不同之处在于您应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。</p><h3 id=设置-ectd-集群>设置 ectd 集群</h3><ol><li><p>按照 <a href=/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>这些指示</a> 去设置 etcd 集群。</p></li><li><p>设置 SSH 在 <a href=#manual-certs>这</a>描述。</p></li><li><p>将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</code></pre></div><ul><li>用第一台控制平面机的 <code>user@host</code> 替换 <code>CONTROL_PLANE</code> 的值。</li></ul></li></ol><h3 id=设置第一个控制平面节点>设置第一个控制平面节点</h3><ol><li><p>用以下内容创建一个名为 <code>kubeadm-config.yaml</code> 的文件：</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
</code></pre></li></ol><blockquote class="note callout"><div><strong>说明：</strong> 这里堆 etcd 和外部 etcd 之前的区别在于设置外部 etcd 需要一个 <code>etcd</code> 的 <code>external</code> 对象下带有 etcd 端点的配置文件。
如果是堆 etcd 技术，是自动管理的。</div></blockquote><ul><li><p>在您的集群中，将配置模板中的以下变量替换为适当值：</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul><p>以下的步骤与设置堆集群是相似的：</p><ol><li><p>在节点上运行 <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> 命令。</p></li><li><p>编写输出联接命令，这些命令将返回到文本文件以供以后使用。</p></li><li><p>应用您选择的 CNI 插件。 给定以下示例适用于 Weave Net：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</code></pre></div></li></ol><h3 id=其他控制平面节点的步骤>其他控制平面节点的步骤</h3><p>步骤与设置堆 etcd 相同：</p><ul><li>确保第一个控制平面节点已完全初始化。</li><li>使用保存到文本文件的连接命令将每个控制平面节点连接在一起。建议一次加入一个控制平面节点。</li><li>不要忘记默认情况下，<code>--certificate-key</code> 中的解密秘钥会在两个小时后过期。</li></ul><h2 id=列举控制平面之后的常见任务>列举控制平面之后的常见任务</h2><h3 id=安装工作节点>安装工作节点</h3><p>您可以使用之前存储的命令将工作节点加入集群中
作为 <code>kubeadm init</code> 命令的输出：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div><h2 id=manual-certs>手动证书分发</h2><p>如果您选择不将 <code>kubeadm init</code> 与 <code>--upload-certs</code> 命令一起使用，
则意味着您将必须手动将证书从主控制平面节点复制到
将要加入的控制平面节点上。</p><p>有许多方法可以实现这种操作。在下面的例子中我们使用 <code>ssh</code> 和 <code>scp</code>：</p><p>如果要在单独的一台计算机控制所有节点，则需要 SSH。</p><ol><li><p>在您的主设备上启动 ssh-agent，要求该设备能访问系统中的所有其他节点：</p><pre><code>eval $(ssh-agent)
</code></pre></li><li><p>将 SSH 身份添加到会话中：</p><pre><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>检查节点间的 SSH 以确保连接是正常运行的</p><ul><li><p>SSH 到任何节点时，请确保添加 <code>-A</code> 标志：</p><pre><code>ssh -A 10.0.0.7
</code></pre></li><li><p>当在任何节点上使用 sudo 时，请确保环境完善，以便使用 SSH
转发任务：</p><pre><code>sudo -E -s
</code></pre></li></ul></li></ol><ol><li><p>在所有节点上配置 SSH 之后，您应该在运行过 <code>kubeadm init</code> 命令的第一个控制平面节点上运行以下脚本。
该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：</p><p>在以下示例中，用其他控制平面节点的 IP 地址替换 <code>CONTROL_PLANE_IPS</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可自己设置</span>
<span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
<span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div></li></ol><blockquote class="caution callout"><div><strong>注意：</strong> 只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。
如果您错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。</div></blockquote><ol><li><p>然后，在每个连接控制平面节点上，您必须先运行以下脚本，然后再运行 <code>kubeadm join</code>。
该脚本会将先前复制的证书从主目录移动到 <code>/etc/kubernetes/pki</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可自己设置</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>3.3.1.7 - 使用 kubeadm 创建一个高可用 etcd 集群</h1><blockquote class="note callout"><div><strong>说明：</strong><p>在本指南中，当 kubeadm 用作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。对于长期规划是使用 <a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> 增强工具来管理这方面。</div></blockquote><p>默认情况下，kubeadm 运行单成员的 etcd 集群，该集群由控制面节点上的 kubelet 以静态 Pod 的方式进行管理。由于 etcd 集群只包含一个成员且不能在任一成员不可用时保持运行，所以这不是一种高可用设置。本任务，将告诉您如何在使用 kubeadm 创建一个 kubernetes 集群时创建一个外部 etcd：有三个成员的高可用 etcd 集群。</p><h2 id=准备开始>准备开始</h2><ul><li>三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。</li></ul><ul><li>每个主机必须 <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装有 docker、kubelet 和 kubeadm</a>。</li></ul><ul><li>一些可以用来在主机间复制文件的基础设施。例如 <code>ssh</code> 和 <code>scp</code> 就可以满足需求。</li></ul><h2 id=建立集群>建立集群</h2><p>一般来说，是在一个节点上生成所有证书并且只分发这些<em>必要</em>的文件到其它节点上。</p><blockquote class="note callout"><div><strong>说明：</strong><p>kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。</div></blockquote><ol><li><p>将 kubelet 配置为 etcd 的服务管理器。</p><p>由于 etcd 是首先创建的，因此您必须通过创建具有更高优先级的新文件来覆盖 kubeadm 提供的 kubelet 单元文件。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span><span style=color:#b44>[Service]
</span><span style=color:#b44>ExecStart=
</span><span style=color:#b44>#  Replace &#34;systemd&#34; with the cgroup driver of your container runtime. The default value in the kubelet is &#34;cgroupfs&#34;.
</span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span><span style=color:#b44>Restart=always
</span><span style=color:#b44>EOF</span>

systemctl daemon-reload
systemctl restart kubelet
</code></pre></div></li><li><p>为 kubeadm 创建配置文件。</p><p>使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#080;font-style:italic># 使用 IP 或可解析的主机名替换 HOST0、HOST1 和 HOST2</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
<span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8

<span style=color:#080;font-style:italic># 创建临时目录来存储将被分发到其它主机上的文件</span>
mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/

<span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
<span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>

<span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
<span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta2&#34;
</span><span style=color:#b44>kind: ClusterConfiguration
</span><span style=color:#b44>etcd:
</span><span style=color:#b44>    local:
</span><span style=color:#b44>        serverCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        peerCertSANs:
</span><span style=color:#b44>        - &#34;${HOST}&#34;
</span><span style=color:#b44>        extraArgs:
</span><span style=color:#b44>            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
</span><span style=color:#b44>            initial-cluster-state: new
</span><span style=color:#b44>            name: ${NAME}
</span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span><span style=color:#b44>EOF</span>
<span style=color:#a2f;font-weight:700>done</span>
</code></pre></div></li><li><p>生成证书颁发机构</p><p>如果您已经拥有 CA，那么唯一的操作是复制 CA 的 <code>crt</code> 和 <code>key</code> 文件到 <code>etc/kubernetes/pki/etcd/ca.crt</code> 和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。复制完这些文件后继续下一步，“为每个成员创建证书”。</p><p>如果您还没有 CA，则在 <code>$HOST0</code>（您为 kubeadm 生成配置文件的位置）上运行此命令。</p><pre><code>kubeadm init phase certs etcd-ca
</code></pre><p>创建了如下两个文件</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>为每个成员创建证书</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
<span style=color:#080;font-style:italic># 清理不可重复使用的证书</span>
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
<span style=color:#080;font-style:italic># 不需要移动 certs 因为它们是给 HOST0 使用的</span>

<span style=color:#080;font-style:italic># 清理不应从此主机复制的证书</span>
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</code></pre></div></li><li><p>复制证书和 kubeadm 配置</p><p>证书已生成，现在必须将它们移动到对应的主机。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
<span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/
</code></pre></div></li><li><p>确保已经所有预期的文件都存在</p><p><code>$HOST0</code> 所需文件的完整列表如下：</p><pre><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>在 <code>$HOST1</code>:</p><pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>在 <code>$HOST2</code></p><pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>创建静态 Pod 清单</p><p>既然证书和配置已经就绪，是时候去创建清单了。在每台主机上运行 <code>kubeadm</code> 命令来生成 etcd 使用的静态清单。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/home/ubuntu/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/home/ubuntu/kubeadmcfg.yaml
</code></pre></div></li><li><p>可选：检查群集运行状况</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
...
https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</code></pre></div><ul><li>将 <code>${ETCD_TAG}</code> 设置为你的 etcd 镜像的版本标签，例如 <code>3.4.3-0</code>。要查看 kubeadm 使用的 etcd 镜像和标签，请执行 <code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>，其中 <code>${K8S_VERSION}</code> 是 <code>v1.17.0</code> 作为例子。</li></ul><ul><li>将 <code>${HOST0}</code> 设置为要测试的主机的 IP 地址</li></ul></li></ol><h2 id=接下来>接下来</h2><p>一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 的外部 etcd 方法</a>，
继续部署一个高可用的控制平面。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>3.3.1.8 - 使用 kubeadm 配置集群中的每个 kubelet</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code></div><p>kubeadm CLI 工具的生命周期与 <a href=/zh/docs/reference/command-line-tools-reference/kubelet>kubelet</a>解耦，它是一个守护程序，在 Kubernetes 集群中的每个节点上运行。
当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。</p><p>由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。
当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。
你可以改用其他服务管理器，但需要手动地配置。</p><p>集群中涉及的所有 kubelet 的一些配置细节都必须相同，
而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性（例如操作系统、存储和网络）。
你可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 <code>KubeletConfiguration</code> API 类型
用于<a href=#configure-kubelets-using-kubeadm>集中管理 kubelet 的配置</a>。</p><h2 id=kubelet-配置模式>Kubelet 配置模式</h2><p>以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。</p><h3 id=将集群级配置传播到每个-kubelet-中>将集群级配置传播到每个 kubelet 中</h3><p>你可以通过使用 <code>kubeadm init</code> 和 <code>kubeadm join</code> 命令为 kubelet 提供默认值。
有趣的示例包括使用其他 CRI 运行时或通过服务器设置不同的默认子网。</p><p>如果你想使用子网 <code>10.96.0.0/12</code> 作为默认的服务，你可以给 kubeadm 传递 <code>--service-cidr</code> 参数：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubeadm init --service-cidr 10.96.0.0/12
</code></pre></div><p>现在，可以从该子网分配服务的虚拟 IP。
你还需要通过 kubelet 使用 <code>--cluster-dns</code> 标志设置 DNS 地址。
在集群中的每个管理器和节点上的 kubelet 的设置需要相同。
kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet 中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。
此对象被称为 <strong>kubelet 的配置组件</strong>。
该配置组件允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></code></pre></div><p>有关组件配置的更多详细信息，亲参阅 <a href=#configure-kubelets-using-kubeadm>本节</a>。</p><h3 id=提供指定实例的详细配置信息>提供指定实例的详细配置信息</h3><p>由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。
以下列表提供了一些示例。</p><ul><li><p>由 kubelet 配置标志 <code>--resolv-conf</code> 指定的 DNS 解析文件的路径在操作系统之间可能有所不同，
它取决于你是否使用 <code>systemd-resolved</code>。
如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。</p></li><li><p>除非你使用云驱动，否则默认情况下 Node API 对象的 <code>.metadata.name</code> 会被设置为计算机的主机名。
如果你需要指定一个与机器的主机名不同的节点名称，你可以使用 <code>--hostname-override</code> 标志覆盖默认值。</p></li><li><p>当前，kubelet 无法自动检测 CRI 运行时使用的 cgroup 驱动程序，
但是值 <code>--cgroup-driver</code> 必须与 CRI 运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。</p></li><li><p>取决于你的集群所使用的 CRI 运行时，你可能需要为 kubelet 指定不同的标志。
例如，当使用 Docker 时，你需要指定如 <code>--network-plugin=cni</code> 这类标志；但是如果你使用的是外部运行时，
则需要指定 <code>--container-runtime=remote</code> 并使用 <code>--container-runtime-endpoint=&lt;path></code> 指定 CRI 端点。</p></li></ul><p>你可以在服务管理器（例如 systemd）中设定某个 kubelet 的配置来指定这些参数。</p><h2 id=使用-kubeadm-配置-kubelet>使用 kubeadm 配置 kubelet</h2><p>如果自定义的 <code>KubeletConfiguration</code> API 对象使用像 <code>kubeadm ... --config some-config-file.yaml</code> 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。</p><p>通过调用 <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>，
你可以看到此结构中的所有默认值。</p><p>也可以阅读 <a href=https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration>kubelet 配置组件的 API 参考</a>来获取有关各个字段的更多信息。</p><h3 id=当使用-kubeadm-init-时的工作流程>当使用 <code>kubeadm init</code>时的工作流程</h3><p>当调用 <code>kubeadm init</code> 时，kubelet 配置被编组到磁盘上的 <code>/var/lib/kubelet/config.yaml</code> 中，
并且上传到集群中的 ConfigMap。
ConfigMap 名为 <code>kubelet-config-1.X</code>，其中 <code>X</code> 是你正在初始化的 kubernetes 版本的次版本。
在集群中所有 kubelet 的基准集群范围内配置，将 kubelet 配置文件写入 <code>/etc/kubernetes/kubelet.conf</code> 中。
此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。
这解决了 <a href=#propagating-cluster-level-configuration-to-each-kubelet>将集群级配置传播到每个 kubelet</a>的需求。</p><p>该文档 <a href=#providing-instance-specific-configuration-details>提供特定实例的配置详细信息</a> 是第二种解决模式，
kubeadm 将环境文件写入 <code>/var/lib/kubelet/kubeadm-flags.env</code>，其中包含了一个标志列表，
当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</code></pre></div><p>除了启动 kubelet 时使用该标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他 CRI 运行时 socket（<code>--cri-socket</code>）。</p><p>将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>如果重新加载和重新启动成功，则正常的 <code>kubeadm init</code> 工作流程将继续。</p><h3 id=当使用-kubeadm-join-时的工作流程>当使用 <code>kubeadm join</code>时的工作流程</h3><p>当运行 <code>kubeadm join</code> 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书，该证书需要下载 <code>kubelet-config-1.X</code> ConfigMap 并把它写入 <code>/var/lib/kubelet/config.yaml</code> 中。
动态环境文件的生成方式恰好与 <code>kubeadm init</code> 相同。</p><p>接下来，kubeadm 运行以下两个命令将新配置加载到 kubelet 中：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>在 kubelet 加载新配置后，kubeadm 将写入 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig 文件中，
该文件包含 CA 证书和引导程序令牌。
kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 <code>/etc/kubernetes/kubelet.conf</code> 中。
当此文件被写入后，kubelet 就完成了执行 TLS 引导程序。</p><h2 id=the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd 文件</h2><p><code>kubeadm</code> 中附带了有关系统如何运行 kubelet 的 systemd 配置文件。
请注意 kubeadm CLI 命令不会修改此文件。</p><p>通过 <code>kubeadm</code> <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEB</a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPM 包</a>
安装的配置文件被写入 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 并由系统使用。
它对原来的 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service>RPM 版本 <code>kubelet.service</code></a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service>DEB 版本 <code>kubelet.service</code></a>
作了增强：</p><pre><code class=language-none data-lang=none>[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;
# 这是 &quot;kubeadm init&quot; 和 &quot;kubeadm join&quot; 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。
# 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。
# KUBELET_EXTRA_ARGS 应该从此文件中获取。
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>该文件为 kubelet 指定由 kubeadm 管理的所有文件的默认位置。</p><ul><li>用于 TLS 引导程序的 KubeConfig 文件为 <code>/etc/kubernetes/bootstrap-kubelet.conf</code>，
但仅当 <code>/etc/kubernetes/kubelet.conf</code> 不存在时才能使用。</li><li>具有唯一 kubelet 标识的 KubeConfig 文件为 <code>/etc/kubernetes/kubelet.conf</code>。</li><li>包含 kubelet 的组件配置的文件为 <code>/var/lib/kubelet/config.yaml</code>。</li><li>包含的动态环境的文件 <code>KUBELET_KUBEADM_ARGS</code> 是来源于 <code>/var/lib/kubelet/kubeadm-flags.env</code>。</li><li>包含用户指定标志替代的文件 <code>KUBELET_EXTRA_ARGS</code> 是来源于
<code>/etc/default/kubelet</code>（对于 DEB），或者 <code>/etc/sysconfig/kubelet</code>（对于 RPM）。
<code>KUBELET_EXTRA_ARGS</code> 在标志链中排在最后，并且在设置冲突时具有最高优先级。</li></ul><h2 id=kubernetes-二进制文件和软件包内容>Kubernetes 二进制文件和软件包内容</h2><p>Kubernetes 版本对应的 DEB 和 RPM 软件包是：</p><table><thead><tr><th>Package name</th><th>Description</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td>给 kubelet 安装 <code>/usr/bin/kubeadm</code> CLI 工具和 <a href=#the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd 文件</a>。</td></tr><tr><td><code>kubelet</code></td><td>安装 kublet 可执行文件到 <code>/usr/bin</code> 路径，安装 CNI 可执行文件到 <code>/opt/cni/bin</code> 路径。</td></tr><tr><td><code>kubectl</code></td><td>安装 <code>/usr/bin/kubectl</code> 可执行文件。</td></tr><tr><td><code>cri-tools</code></td><td>从 <a href=https://github.com/kubernetes-sigs/cri-tools>cri-tools git 仓库</a>中安装 <code>/usr/bin/crictl</code> 可执行文件。</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ed857e09999827b013ee9062dc9c59bb>3.3.1.9 - 配置您的 kubernetes 集群以自托管控制平台</h1><h3 id=self-hosting>自托管 Kubernetes 控制平台</h3><p>kubeadm 允许您实验性地创建 <em>self-hosted</em> Kubernetes 控制平面。
这意味着 API 服务器，控制管理器和调度程序之类的关键组件将通过配置 Kubernetes API 以
<a href=/zh/docs/concepts/workloads/controllers/daemonset/>DaemonSet Pods</a> 的身份运行，
而不是通过静态文件在 kubelet 中配置<a href=/zh/docs/tasks/configure-pod-container/static-pod/>静态 Pods</a>。</p><p>要创建自托管集群，请参见
<a href=/zh/docs/reference/setup-tools/kubeadm/kubeadm-alpha/#cmd-selfhosting>kubeadm alpha selfhosting pivot</a>
命令。</p><h4 id=警告>警告</h4><blockquote class="caution callout"><div><strong>注意：</strong> 此功能将您的集群设置为不受支持的状态，从而使 kubeadm 无法再管理您的集群。
这包括 <code>kubeadm 升级</code> 。</div></blockquote><ol><li>1.8及更高版本中的自托管功能有一些重要限制。
特别是，自托管集群在没有人工干预的情况下_无法从控制平面节点的重新启动中恢复_ 。</li></ol><ol start=2><li>默认情况下，自托管的控制平面 Pod 依赖于从
<a href=/zh/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a> 卷加载的凭据。
除初始创建外，这些凭据不由 kubeadm 管理。</li></ol><ol start=3><li>控制平面的自托管部分不包括 etcd，后者仍作为静态 Pod 运行。</li></ol><h4 id=过程>过程</h4><p>自托管引导过程描述于 <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-self-hosting>kubeadm 设计文档</a> 中。</p><p>总体而言，<code>kubeadm alpha 自托管</code> 的工作原理如下：</p><ol><li>等待此引导静态控制平面运行且良好。
这与没有自我托管的 <code>kubeadm init</code> 过程相同。</li></ol><ol start=2><li>使用静态控制平面 Pod 清单来构造一组 DaemonSet 清单，这些清单将运行自托管的控制平面。
它还会在必要时修改这些清单，例如添加新的 secrets 卷。</li></ol><ol start=3><li>在 <code>kube-system</code> 名称空间中创建 DaemonSets ，并等待生成的 Pod 运行。</li></ol><ol start=4><li>自托管 Pod 运行后，将删除其关联的静态 Pod，然后 kubeadm 继续安装下一个组件。
这将触发 kubelet 停止那些静态 Pod 。</li></ol><ol start=5><li>当原始静态控制平面停止时，新的自托管控制平面能够绑定到侦听端口并变为活动状态。</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>3.3.2 - 使用 Kops 安装 Kubernetes</h1><p>本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。
本篇使用了一个名为 <a href=https://github.com/kubernetes/kops><code>kops</code></a> 的工具。</p><p>kops 是一个自用的供应系统：</p><ul><li>全自动安装流程</li><li>使用 DNS 识别集群</li><li>自我修复：一切都在自动扩展组中运行</li><li>支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a></li><li>支持高可用 - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/high_availability.md>high_availability.md</a></li><li>可以直接提供或者生成 terraform 清单 - 参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a></li></ul><p>如果你有不同的观点，你可能更喜欢使用 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a>
作为构建工具来构建自己的集群。kops 建立在 kubeadm 工作的基础上。</p><h2 id=创建集群>创建集群</h2><h3 id=1-5-安装-kops>(1/5) 安装 kops</h3><h4 id=前提条件>前提条件</h4><p>你必须安装 <a href=/zh/docs/tasks/tools/install-kubectl/>kubectl</a> 才能使 kops 工作。</p><h4 id=安装>安装</h4><p>从<a href=https://github.com/kubernetes/kops/releases>下载页面</a>下载 kops（从源代码构建也很容易）：</p><p>在 macOS 上：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -OL https://github.com/kubernetes/kops/releases/download/1.10.0/kops-darwin-amd64
chmod +x kops-darwin-amd64
mv kops-darwin-amd64 /usr/local/bin/kops
<span style=color:#080;font-style:italic># 你也可以使用 Homebrew 安装 kops</span>
brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</code></pre></div><p>在 Linux 上：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>wget https://github.com/kubernetes/kops/releases/download/1.10.0/kops-linux-amd64
chmod +x kops-linux-amd64
mv kops-linux-amd64 /usr/local/bin/kops
</code></pre></div><h3 id=2-5-为你的集群创建一个-route53-域名>(2/5) 为你的集群创建一个 route53 域名</h3><p>kops 在集群内部都使用 DNS 进行发现操作，因此你可以从客户端访问 kubernetes API 服务器。</p><p>kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱，
可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问群集。</p><p>你应该使用子域名来划分集群。作为示例，我们将使用域名 <code>useast1.dev.example.com</code>。
然后，API 服务器端点域名将为 <code>api.useast1.dev.example.com</code>。</p><p>Route53 托管区域可以服务子域名。你的托管区域可能是 <code>useast1.dev.example.com</code>，还有 <code>dev.example.com</code> 甚至 <code>example.com</code>。
kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。
例如，允许你在 <code>dev.example.com</code> 下创建记录，但不能在 <code>example.com</code> 下创建记录。</p><p>假设你使用 <code>dev.example.com</code> 作为托管区域。你可以使用
<a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>正常流程</a>
或者使用诸如 <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>
之类的命令来创建该托管区域。</p><p>然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。
在这里，你将在 <code>example.com</code> 中为 <code>dev</code> 创建 DNS 记录。
如果它是根域名，则可以在域名注册机构配置 DNS 记录。
例如，你需要在购买 <code>example.com</code> 的地方配置 <code>example.com</code>。</p><p>这一步很容易搞砸（这是问题的第一大原因！）
如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>dig DNS dev.example.com
</code></pre></div><p>你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。</p><h3 id=3-5-创建一个-s3-存储桶来存储集群状态>(3/5) 创建一个 S3 存储桶来存储集群状态</h3><p>kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。
此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。</p><p>多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个 S3 存储桶 - 这比传递 kubecfg 文件容易得多。
但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限，因此你不想在运营团队之外共享它。</p><p>因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）</p><p>在我们的示例中，我们选择 <code>dev.example.com</code> 作为托管区域，因此让我们选择 <code>clusters.dev.example.com</code> 作为 S3 存储桶名称。</p><ul><li>导出 <code>AWS_PROFILE</code> 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）</li><li>使用 <code>aws s3 mb s3://clusters.dev.example.com</code> 创建 S3 存储桶</li><li>你可以进行 <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> 操作，然后 kops 将默认使用此位置。
我们建议将其放入你的 bash profile 文件或类似文件中。</li></ul><h3 id=4-5-建立你的集群配置>(4/5) 建立你的集群配置</h3><p>运行 "kops create cluster" 以创建你的集群配置：</p><p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p><p>kops 将为你的集群创建配置。请注意，它_仅_创建配置，实际上并没有创建云资源 - 你将在下一步中使用 <code>kops update cluster</code> 进行配置。
这使你有机会查看配置或进行更改。</p><p>它打印出可用于进一步探索的命令：</p><ul><li>使用以下命令列出集群：<code>kops get cluster</code></li><li>使用以下命令编辑该集群：<code>kops edit cluster useast1.dev.example.com</code></li><li>使用以下命令编辑你的节点实例组：<code>kops edit ig --name = useast1.dev.example.com nodes</code></li><li>使用以下命令编辑你的主实例组：<code>kops edit ig --name = useast1.dev.example.com master-us-east-1c</code></li></ul><p>如果这是你第一次使用 kops，请花几分钟尝试一下！ 实例组是一组实例，将被注册为 kubernetes 节点。
在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。
例如，如果你想要的是混合实例和按需实例的节点，或者 GPU 和非 GPU 实例。</p><h3 id=5-5-在-aws-中创建集群>(5/5) 在 AWS 中创建集群</h3><p>运行 "kops update cluster" 以在 AWS 中创建集群：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kops update cluster useast1.dev.example.com --yes
</code></pre></div><p>这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。
每当更改集群配置时，都会使用 <code>kops update cluster</code> 工具。
它将对配置进行的更改应用于你的集群 - 根据需要重新配置 AWS 或者 kubernetes。</p><p>例如，在你运行 <code>kops edit ig nodes</code> 之后，然后运行 <code>kops update cluster --yes</code>
应用你的配置，有时你还必须运行 <code>kops rolling-update cluster</code> 立即回滚更新配置。</p><p>如果没有 <code>--yes</code> 参数，<code>kops update cluster</code> 操作将向你显示其操作的预览效果。这对于生产集群很方便！</p><h3 id=探索其他附加组件>探索其他附加组件</h3><p>请参阅<a href=/zh/docs/concepts/cluster-administration/addons/>附加组件列表</a>探索其他附加组件，
包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。</p><h2 id=清理>清理</h2><ul><li>删除集群：<code>kops delete cluster useast1.dev.example.com --yes</code></li></ul><h2 id=反馈>反馈</h2><ul><li>Slack 频道: <a href=https://kubernetes.slack.com/messages/kops-users/>#kops-users</a></li><li><a href=https://github.com/kubernetes/kops/issues>GitHub Issues</a></li></ul><h2 id=接下来>接下来</h2><ul><li>了解有关 Kubernetes 的<a href=/zh/docs/concepts/>概念</a> 和
<a href=/zh/docs/reference/kubectl/overview/><code>kubectl</code></a> 的更多信息。</li><li>了解 <code>kops</code> <a href=https://github.com/kubernetes/kops>高级用法</a>。</li><li>请参阅 <code>kops</code> <a href=https://github.com/kubernetes/kops>文档</a> 获取教程、最佳做法和高级配置选项。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b4964187fe973644e06ee629eff1de>3.3.3 - 使用 Kubespray 安装 Kubernetes</h1><p>此快速入门有助于使用 <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a> 安装在 GCE、Azure、OpenStack、AWS、vSphere、Packet（裸机）、Oracle Cloud Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。</p><p>Kubespray 是一个由 <a href=https://docs.ansible.com/>Ansible</a> playbooks、<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md>清单（inventory）</a>、供应工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。 Kubespray 提供：</p><ul><li>高可用性集群</li><li>可组合属性</li><li>支持大多数流行的 Linux 发行版<ul><li>Ubuntu 16.04、18.04、20.04</li><li>CentOS / RHEL / Oracle Linux 7、8</li><li>Debian Buster，Jessie，Stretch，Wheezy</li><li>Fedora 31、32</li><li>Fedora CoreOS</li><li>openSUSE Leap 15</li><li>Kinvolk 的 Flatcar Container Linux</li></ul></li><li>持续集成测试</li></ul><p>要选择最适合你的用例的工具，请阅读<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>此比较</a>以
<a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 和 <a href=/zh/docs/setup/production-environment/tools/kops/>kops</a> 。</p><h2 id=创建集群>创建集群</h2><h3 id=1-5-满足下层设施要求>（1/5）满足下层设施要求</h3><p>按以下<a href=https://github.com/kubernetes-sigs/kubespray#requirements>要求</a>来配置服务器：</p><ul><li>在将运行 Ansible 命令的计算机上安装 Ansible v2.9 和 python-netaddr</li><li><strong>运行 Ansible Playbook 需要 Jinja 2.11（或更高版本）</strong></li><li>目标服务器必须有权访问 Internet 才能拉取 Docker 镜像。否则，需要其他配置（<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>请参见离线环境</a>）</li><li>目标服务器配置为允许 IPv4 转发</li><li><strong>你的 SSH 密钥必须复制</strong>到清单中的所有服务器部分</li><li>防火墙不受管理，你将需要按照以前的方式实施自己的规则。为了避免在部署过程中出现任何问题，你应该禁用防火墙</li><li>如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法。然后应指定“ansible_become” 标志或命令参数 “--become” 或 “-b”</li></ul><p>Kubespray 提供以下实用程序来帮助你设置环境：</p><ul><li>为以下云驱动提供的 <a href=https://www.terraform.io/>Terraform</a> 脚本：</li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li><li><a href=http://sitebeskuethree/contrigetbernform/contribeskubernform/contribeskupernform/https/sitebesku/master/>OpenStack</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet>Packet</a></li></ul><h3 id=2-5-编写清单文件>（2/5）编写清单文件</h3><p>设置服务器后，请创建一个 <a href=https://docs.ansible.com/ansible/intro_inventory.html>Ansible 的清单文件</a>。你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>建立你自己的清单</a>”。</p><h3 id=3-5-规划集群部署>（3/5）规划集群部署</h3><p>Kubespray 能够自定义部署的许多方面：</p><ul><li>选择部署模式： kubeadm 或非 kubeadm</li><li>CNI（网络）插件</li><li>DNS 配置</li><li>控制平面的选择：本机/可执行文件或容器化</li><li>组件版本</li><li>Calico 路由反射器</li><li>组件运行时选项<ul><li><a class=glossary-tooltip title="Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。" data-toggle=tooltip data-placement=top href=/zh/docs/reference/kubectl/docker-cli-to-kubectl/ target=_blank aria-label=Docker>Docker</a></li><li><a class=glossary-tooltip title=强调简单性、健壮性和可移植性的一种容器运行时 data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li><li><a class=glossary-tooltip title="专用于 Kubernetes 的轻量级容器运行时软件" data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li></ul></li><li>证书生成方式</li></ul><p>可以修改<a href=https://docs.ansible.com/ansible/playbooks_variables.html>变量文件</a>以进行 Kubespray 定制。
如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群并探索 Kubernetes 。</p><h3 id=4-5-部署集群>（4/5）部署集群</h3><p>接下来，部署你的集群：</p><p>使用 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a> 进行j集群部署。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</code></pre></div><p>大型部署（超过 100 个节点）可能需要<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>特定的调整</a>，以获得最佳效果。</p><h3 id=5-5-验证部署>（5/5）验证部署</h3><p>Kubespray 提供了一种使用 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>
验证 Pod 间连接和 DNS 解析的方法。
Netchecker 确保 netchecker-agents pod 可以解析。
DNS 请求并在默认名称空间内对每个请求执行 ping 操作。
这些 Pods 模仿其余工作负载的类似行为，并用作集群运行状况指示器。</p><h2 id=集群操作>集群操作</h2><p>Kubespray 提供了其他 Playbooks 来管理集群： <em>scale</em> 和 <em>upgrade</em>。</p><h3 id=扩展集群>扩展集群</h3><p>你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>添加节点</a>”。
你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>删除节点</a>”。</p><h3 id=升级集群>升级集群</h3><p>你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见
“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>升级</a>”。</p><h2 id=清理>清理</h2><p>你可以通过 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset</a> Playbook
重置节点并清除所有与 Kubespray 一起安装的组件。</p><blockquote class="caution callout"><div><strong>注意：</strong> 运行 reset playbook 时，请确保不要意外地将生产集群作为目标！</div></blockquote><h2 id=反馈>反馈</h2><ul><li>Slack 频道：<a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a>（你可以在<a href=https://slack.k8s.io/>此处</a>获得邀请）</li><li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub 问题</a></li></ul><h2 id=接下来>接下来</h2><p>查看有关 Kubespray 的<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>路线图</a>的计划工作。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-acce7e24090fea04715a7a516ba3e69b>3.4 - Windows Kubernetes</h1></div><div class=td-content><h1 id=pg-a307d413f1f7430fced233023087e2a1>3.4.1 - Kubernetes 对 Windows 的支持</h1><p>在很多组织中，其服务和应用的很大比例是 Windows 应用。
<a href=https://aka.ms/windowscontainers>Windows 容器</a>提供了一种对进程和包依赖关系
进行封装的现代方式，这使得用户更容易采用 DevOps 实践，令 Windows 应用同样遵从
云原生模式。
Kubernetes 已经成为事实上的标准容器编排器，Kubernetes 1.14 发行版本中包含了将
Windows 容器调度到 Kubernetes 集群中 Windows 节点上的生产级支持，从而使得巨大
的 Windows 应用生态圈能够充分利用 Kubernetes 的能力。
对于同时投入基于 Windows 应用和 Linux 应用的组织而言，他们不必寻找不同的编排系统
来管理其工作负载，其跨部署的运维效率得以大幅提升，而不必关心所用操作系统。</p><h2 id=windows-containers-in-kubernetes>kubernetes 中的 Windows 容器</h2><p>若要在 Kubernetes 中启用对 Windows 容器的编排，只需在现有的 Linux 集群中
包含 Windows 节点。在 Kubernetes 上调度 <a class=glossary-tooltip title="Pod 表示您的集群上一组正在运行的容器。" data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>
中的 Windows 容器与调用基于 Linux 的容器一样简单、一样容易。</p><p>为了运行 Windows 容器，你的 Kubernetes 集群必须包含多个操作系统，控制面
节点运行 Linux，工作节点则可以根据负载需要运行 Windows 或 Linux。
Windows Server 2019 是唯一被支持的 Windows 操作系统，在 Windows 上启用
<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Kubernetes 节点</a>
支持（包括 kubelet, <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd>容器运行时</a>、
以及 kube-proxy）。关于 Windows 发行版渠道的详细讨论，可参见
<a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Microsoft 文档</a>。</p><blockquote class="note callout"><div><strong>说明：</strong> Kubernetes 控制面，包括<a href=/zh/docs/concepts/overview/components/>主控组件</a>，继续
在 Linux 上运行。目前没有支持完全是 Windows 节点的 Kubernetes 集群的计划。</div></blockquote><blockquote class="note callout"><div><strong>说明：</strong> 在本文中，当我们讨论 Windows 容器时，我们所指的是具有进程隔离能力的 Windows
容器。具有 <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container>Hyper-V 隔离能力</a>
的 Windows 容器计划在将来发行版本中推出。</div></blockquote><h2 id=supported-functionality-and-limitations>支持的功能与局限性</h2><h3 id=supported-functionality>支持的功能</h3><h4 id=windows-os-version-support>Windows 操作系统版本支持</h4><p>参考下面的表格，了解 Kubernetes 中支持的 Windows 操作系统。
同一个异构的 Kubernetes 集群中可以同时包含 Windows 和 Linux 工作节点。
Windows 容器仅能调度到 Windows 节点，Linux 容器则只能调度到 Linux 节点。</p><table><thead><tr><th>Kubernetes 版本</th><th>Windows Server LTSC 版本</th><th>Windows Server SAC 版本</th><th></th></tr></thead><tbody><tr><td><em>Kubernetes v1.14</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809</td><td></td></tr><tr><td><em>Kubernetes v1.15</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809</td><td></td></tr><tr><td><em>Kubernetes v1.16</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809</td><td></td></tr><tr><td><em>Kubernetes v1.17</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809</td><td></td></tr><tr><td><em>Kubernetes v1.18</em></td><td>Windows Server 2019</td><td>Windows Server ver 1809, Windows Server ver 1903, Windows Server ver 1909</td><td></td></tr><tr><td><em>Kubernetes v1.19</em></td><td>Windows Server 2019</td><td>Windows Server ver 1909, Windows Server ver 2004</td><td></td></tr></tbody></table><p>关于不同的 Windows Server 版本的服务渠道，包括其支持模式等相关信息可以在
<a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Windows Server servicing channels</a>
找到。</p><p>我们并不指望所有 Windows 客户都为其应用频繁地更新操作系统。
对应用的更新是向集群中引入新代码的根本原因。
对于想要更新运行于 Kubernetes 之上的容器中操作系统的客户，我们会在添加对新
操作系统版本的支持时提供指南和分步的操作指令。
该指南会包含与集群节点一起来升级用户应用的建议升级步骤。
Windows 节点遵从 Kubernetes
<a href=/zh/docs/setup/release/version-skew-policy/>版本偏差策略</a>（节点到控制面的
版本控制），与 Linux 节点的现行策略相同。</p><p>Windows Server 主机操作系统会受 <a href=https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing>Windows Server</a>
授权策略控制。Windows 容器镜像则遵从
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula>Windows 容器的补充授权条款</a>
约定。</p><p>带进程隔离的 Windows 容器受一些严格的兼容性规则约束，
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility>其中宿主 OS 版本必须与容器基准镜像的 OS 版本相同</a>。
一旦我们在 Kubernetes 中支持带 Hyper-V 隔离的 Windows 容器，
这一约束和兼容性规则也会发生改变。</p><h4 id=compute>计算</h4><p>从 API 和 kubectl 的角度，Windows 容器的表现在很大程度上与基于 Linux 的容器
是相同的。不过也有一些与关键功能相关的差别值得注意，这些差别列举于
<a href=#limitations>局限性</a>小节中。</p><p>关键性的 Kubernetes 元素在 Windows 下与其在 Linux 下工作方式相同。我们在本节中
讨论一些关键性的负载支撑组件及其在 Windows 中的映射。</p><ul><li><p><a href=/zh/docs/concepts/workloads/pods/>Pods</a></p><p>Pod 是 Kubernetes 中最基本的构造模块，是 Kubernetes 对象模型中你可以创建或部署的
最小、最简单元。你不可以在同一 Pod 中部署 Windows 和 Linux 容器。
Pod 中的所有容器都会被调度到同一节点（Node），而每个节点代表的是一种特定的平台
和体系结构。Windows 容器支持 Pod 的以下能力、属性和事件：</p><ul><li>在带进程隔离和卷共享支持的 Pod 中运行一个或多个容器</li><li>Pod 状态字段</li><li>就绪态（Readiness）和活跃性（Liveness）探针</li><li>postStart 和 preStop 容器生命周期事件</li><li>ConfigMap、Secrets：用作环境变量或卷</li><li>emptyDir 卷</li><li>从宿主系统挂载命名管道</li><li>资源限制</li></ul></li><li><p><a href=/zh/docs/concepts/workloads/controllers/>控制器（Controllers）</a></p><p>Kubernetes 控制器处理 Pod 的期望状态。Windows 容器支持以下负载控制器：</p><ul><li>ReplicaSet</li><li>ReplicationController</li><li>Deployment</li><li>StatefulSet</li><li>DaemonSet</li><li>Job</li><li>CronJob</li></ul></li><li><p><a href=/zh/docs/concepts/services-networking/service/>服务（Services）</a></p><p>Kubernetes Service 是一种抽象对象，用来定义 Pod 的一个逻辑集合及用来访问这些
Pod 的策略。Service 有时也称作微服务（Micro-service）。你可以使用服务来实现
跨操作系统的连接。在 Windows 系统中，服务可以使用下面的类型、属性和能力：</p><ul><li>Service 环境变量</li><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li><li>无头（Headless）服务</li></ul></li></ul><p>Pods、控制器和服务是在 Kubernetes 上管理 Windows 负载的关键元素。
不过，在一个动态的云原生环境中，这些元素本身还不足以用来正确管理
Windows 负载的生命周期。我们为此添加了如下功能特性：</p><ul><li>Pod 和容器的度量（Metrics）</li><li>对水平 Pod 自动扩展的支持</li><li>对 kubectl exec 命令的支持</li><li>资源配额</li><li>调度器抢占</li></ul><h4 id=container-runtime>容器运行时</h4><h5 id=docker-ee>Docker EE</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Docker EE-basic 19.03+ 是建议所有 Windows Server 版本采用的容器运行时。
该容器运行时能够与 kubelet 中的 dockershim 代码协同工作。</p><h5 id=cri-containerd>CRI-ContainerD</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><blockquote class="caution callout"><div><strong>注意：</strong> 在 ContainerD 上使用 GMSA 访问 Windows 网络共享资源时，有一个
<a href=/zh/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations>已知的局限</a>，
需要内核补丁来解决。
你可以在关注 <a href=https://github.com/microsoft/Windows-Containers/issues/44>Microsoft Windows Containers 问题跟踪</a>
来跟进相关的更新。</div></blockquote><p><a class=glossary-tooltip title=强调简单性、健壮性和可移植性的一种容器运行时 data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=ContainerD>ContainerD</a> 1.4.0-beta.2+
也可在 Windows Kubernetes 节点上用作容器运行时。</p><p>在 Windows 对 ContainerD 的最初支持是在 Kubernetes v1.18 加入的。
Windows 上 ContainerD 的进展可以在
<a href=https://github.com/kubernetes/enhancements/issues/1001>enhancements#1001</a>
跟进。</p><p>你可以进一步了解如何<a href=/zh/docs/setup/production-environment/container-runtimes/#install-containerd>在 Windows 上安装 ContainerD</a>.</p><h4 id=persistent-storage>持久性存储</h4><p>使用 Kubernetes <a href=/zh/docs/concepts/storage/volumes/>卷</a>，对数据持久性和 Pod 卷
共享有需求的复杂应用也可以部署到 Kubernetes 上。
管理与特定存储后端或协议相关的持久卷时，相关的操作包括：对卷的配备（Provisioning）、
去配（De-provisioning）和调整大小，将卷挂接到 Kubernetes 节点或从节点上解除挂接，
将卷挂载到需要持久数据的 Pod 中的某容器或从容器上卸载。
负责实现为特定存储后端或协议实现卷管理动作的代码以 Kubernetes 卷
<a href=/zh/docs/concepts/storage/volumes/#types-of-volumes>插件</a>的形式发布。
Windows 支持以下大类的 Kubernetes 卷插件：</p><h5 id=in-tree-volume-plugins>树内卷插件</h5><p>与树内卷插件（In-Tree Volume Plugin）相关的代码都作为核心 Kubernetes 代码基
的一部分发布。树内卷插件的部署不需要安装额外的脚本，也不需要额外部署独立的
容器化插件组件。这些插件可以处理：对应存储后端上存储卷的配备、去配和尺寸更改，
将卷挂接到 Kubernetes 或从其上解挂，以及将卷挂载到 Pod 中各个容器上或从其上
卸载。以下树内插件支持 Windows 节点：</p><ul><li><a href=/zh/docs/concepts/storage/volumes/#awselasticblockstore>awsElasticBlockStore</a></li><li><a href=/zh/docs/concepts/storage/volumes/#azuredisk>azureDisk</a></li><li><a href=/zh/docs/concepts/storage/volumes/#azurefile>azureFile</a></li><li><a href=/zh/docs/concepts/storage/volumes/#gcepersistentdisk>gcePersistentDisk</a></li><li><a href=/zh/docs/concepts/storage/volumes/#vspherevolume>vsphereVolume</a></li></ul><h5 id=flexvolume-plugins>FlexVolume 插件</h5><p>与 <a href=/docs/concepts/storage/volumes/#flexVolume>FlexVolume</a> 插件相关的代码是作为
树外（Out-of-tree）脚本或可执行文件来发布的，因此需要在宿主系统上直接部署。
FlexVolume 插件处理将卷挂接到 Kubernetes 节点或从其上解挂、将卷挂载到 Pod 中
各个容器上或从其上卸载等操作。对于与 FlexVolume 插件相关联的持久卷的配备和
去配操作，可以通过外部的配置程序来处理。这类配置程序通常与 FlexVolume 插件
相分离。下面的 FlexVolume
<a href=https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows>插件</a>
可以以 PowerShell 脚本的形式部署到宿主系统上，支持 Windows 节点：</p><ul><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd>SMB</a></li><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd>iSCSI</a></li></ul><h5 id=csi-plugins>CSI 插件</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [alpha]</code></div><p>与 <a class=glossary-tooltip title="容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。" data-toggle=tooltip data-placement=top href=/zh/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> 插件相关联的代码作为
树外脚本和可执行文件来发布且通常发布为容器镜像形式，并使用 DaemonSet 和
StatefulSet 这类标准的 Kubernetes 构造体来部署。
CSI 插件处理 Kubernetes 中的很多卷管理操作：对卷的配备、去配和调整大小，
将卷挂接到 Kubernetes 节点或从节点上解除挂接，将卷挂载到需要持久数据的 Pod
中的某容器或从容器上卸载，使用快照和克隆来备份或恢复持久数据。
CSI 插件通常包含节点插件（以 DaemonSet 形式运行于各节点上）和控制器插件。</p><p>CSI 节点插件（尤其是那些通过块设备或者共享文件系统形式来提供持久卷的插件）
需要执行很多特权级操作，例如扫描磁盘设备、挂载文件系统等等。
这些操作在不同的宿主操作系统上差别较大。对于 Linux 工作节点而言，容器化的
CSI 节点插件通常部署为特权级的容器。对于 Windows 工作节点而言，容器化的
CSI 节点插件的特权操作通过
<a href=https://github.com/kubernetes-csi/csi-proxy>csi-proxy</a>
来支持；csi-proxy 是一个社区管理的、独立的可执行文件，需要预安装在每个
Windows 节点之上。请参考你要部署的 CSI 插件的部署指南以进一步了解其细节。</p><h4 id=networking>联网</h4><p>Windows 容器的联网是通过
<a href=/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNI 插件</a>
来暴露出来的。Windows 容器的联网行为与虚拟机的联网行为类似。
每个容器有一块虚拟的网络适配器（vNIC）连接到 Hyper-V 的虚拟交换机（vSwitch）。
宿主的联网服务（Host Networking Service，HNS）和宿主计算服务（Host Compute
Service，HCS）协同工作，创建容器并将容器的虚拟网卡连接到网络上。
HCS 负责管理容器，HNS 则负责管理网络资源，例如：</p><ul><li>虚拟网络（包括创建 vSwitch）</li><li>端点（Endpoint）/ vNIC</li><li>名字空间（Namespace）</li><li>策略（报文封装、负载均衡规则、访问控制列表、网络地址转译规则等等）</li></ul><p>支持的服务规约类型如下：</p><ul><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li></ul><h5 id=network-modes>网络模式</h5><p>Windows 支持五种不同的网络驱动/模式：二层桥接（L2bridge）、二层隧道（L2tunnel）、
覆盖网络（Overlay）、透明网络（Transparent）和网络地址转译（NAT）。
在一个包含 Windows 和 Linux 工作节点的异构集群中，你需要选择一种对 Windows 和
Linux 兼容的联网方案。下面是 Windows 上支持的一些树外插件及何时使用某种
CNI 插件的建议：</p><table><thead><tr><th>网络驱动</th><th>描述</th><th>容器报文修改</th><th>网络插件</th><th>网络插件特点</th></tr></thead><tbody><tr><td>L2bridge</td><td>容器挂接到外部 vSwitch 上。容器挂接到下层网络之上，但由于容器的 MAC 地址在入站和出站时被重写，物理网络不需要这些地址。</td><td>MAC 地址被重写为宿主系统的 MAC 地址，IP 地址也可能依据 HNS OutboundNAT 策略重写为宿主的 IP 地址。</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge>win-bridge</a>、<a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a>；Flannel 宿主网关（host-gateway）使用 win-bridge</td><td>win-bridge 使用二层桥接（L2bridge）网络模式，将容器连接到下层宿主系统上，从而提供最佳性能。需要用户定义的路由（User-Defined Routes，UDR）才能实现节点间的连接。</td></tr><tr><td>L2Tunnel</td><td>这是二层桥接的一种特殊情形，但仅被用于 Azure 上。所有报文都被发送到虚拟化环境中的宿主机上并根据 SDN 策略进行处理。</td><td>MAC 地址被改写，IP 地址在下层网络上可见。</td><td><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a></td><td>Azure-CNI 使得容器能够与 Azure vNET 集成，并允许容器利用 <a href=https://azure.microsoft.com/en-us/services/virtual-network/>Azure 虚拟网络</a>所提供的功能特性集合。例如，可以安全地连接到 Azure 服务上或者使用 Azure NSG。你可以参考 <a href=https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking>azure-cni</a> 所提供的一些示例。</td></tr><tr><td>覆盖网络（Kubernetes 中为 Windows 提供的覆盖网络支持处于 <em>alpha</em> 阶段）</td><td>每个容器会获得一个连接到外部 vSwitch 的虚拟网卡（vNIC）。每个覆盖网络都有自己的、通过定制 IP 前缀来定义的 IP 子网。覆盖网络驱动使用 VXLAN 封装。</td><td>封装于外层包头内。</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay>Win-overlay</a>、Flannel VXLAN（使用 win-overlay）</td><td>当（比如出于安全原因）期望虚拟容器网络与下层宿主网络隔离时，应该使用 win-overlay。如果你的数据中心可用 IP 地址受限，覆盖网络允许你在不同的网络中复用 IP 地址（每个覆盖网络有不同的 VNID 标签）。这一选项要求在 Windows Server 2009 上安装 <a href=https://support.microsoft.com/help/4489899>KB4489899</a> 补丁。</td></tr><tr><td>透明网络（<a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a> 的特殊用例）</td><td>需要一个外部 vSwitch。容器挂接到某外部 vSwitch 上，该 vSwitch 通过逻辑网络（逻辑交换机和路由器）允许 Pod 间通信。</td><td>报文或者通过 <a href=https://datatracker.ietf.org/doc/draft-gross-geneve/>GENEVE</a> 来封装，或者通过 <a href=https://datatracker.ietf.org/doc/draft-davie-stt/>STT</a> 隧道来封装，以便能够到达不在同一宿主系统上的每个 Pod。<br>报文通过 OVN 网络控制器所提供的隧道元数据信息来判定是转发还是丢弃。<br>北-南向通信通过 NAT 网络地址转译来实现。</td><td><a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a></td><td><a href=https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib>通过 Ansible 来部署</a>。所发布的 ACL 可以通过 Kubernetes 策略来应用实施。支持 IPAM 。负载均衡能力不依赖 kube-proxy。网络地址转译（NAT）也不需要 iptables 或 netsh。</td></tr><tr><td>NAT（<em>未在 Kubernetes 中使用</em>）</td><td>容器获得一个连接到某内部 vSwitch 的 vNIC 接口。DNS/DHCP 服务通过名为 <a href=https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/>WinNAT</a> 的内部组件来提供。</td><td>MAC 地址和 IP 地址都被重写为宿主系统的 MAC 地址和 IP 地址。</td><td><a href=https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat>nat</a></td><td>列在此表中仅出于完整性考虑</td></tr></tbody></table><p>如前所述，<a href=https://github.com/coreos/flannel>Flannel</a> CNI
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>meta 插件</a>
在 Windows 上也是
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental>被支持</a>
的，方法是通过 <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLAN 网络后端</a>
（<strong>alpha 阶段</strong> ：委托给 win-overlay）和
<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw>主机-网关（host-gateway）网络后端</a>
（稳定版本；委托给 win-bridge 实现）。
此插件支持将操作委托给所引用的 CNI 插件（win-overlay、win-bridge）之一，
从而能够与 Windows 上的 Flannel 守护进程（Flanneld）一同工作，自动为节点
分配子网租期，创建 HNS 网络。
该插件读入其自身的配置文件（cni.conf），并将其与 FlannelD 所生成的 subnet.env
文件中的环境变量整合，之后将其操作委托给所引用的 CNI 插件之一以完成网络发现，
并将包含节点所被分配的子网信息的正确配置发送给 IPAM 插件（例如 host-local）。</p><p>对于节点、Pod 和服务对象，可针对 TCP/UDP 流量支持以下网络数据流：</p><ul><li>Pod -> Pod （IP 寻址）</li><li>Pod -> Pod （名字寻址）</li><li>Pod -> 服务（集群 IP）</li><li>Pod -> 服务（部分限定域名，仅适用于名称中不包含“.”的情形）</li><li>Pod -> 服务（全限定域名）</li><li>Pod -> 集群外部（IP 寻址）</li><li>Pod -> 集群外部（DNS 寻址）</li><li>节点 -> Pod</li><li>Pod -> 节点</li></ul><h5 id=ipam>IP 地址管理（IPAM）</h5><p>Windows 上支持以下 IPAM 选项：</p><ul><li><a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local>host-local</a></li><li>HNS IPAM (Inbox 平台 IPAM，未指定 IPAM 时的默认设置）</li><li><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md>Azure-vnet-ipam</a>（仅适用于 azure-cni ）</li></ul><h5 id=load-balancing-and-services>负载均衡与服务</h5><p>在 Windows 系统上，你可以使用以下配置来设定服务和负载均衡行为：</p><table><caption style=display:none>Windows 服务配置</caption><thead><tr><th>功能特性</th><th>描述</th><th>支持的 Kubernetes 版本</th><th>支持的 Windows OS 版本</th><th>如何启用</th></tr></thead><tbody><tr><td>会话亲和性</td><td>确保来自特定客户的连接每次都被交给同一 Pod。</td><td>v1.19+</td><td><a href=https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/>Windows Server vNext Insider Preview Build 19551</a> 或更高版本</td><td>将 <code>service.spec.sessionAffinity</code> 设置为 "ClientIP"</td></tr><tr><td>直接服务器返回</td><td>这是一种负载均衡模式，IP 地址的修正和负载均衡地址转译（LBNAT）直接在容器的 vSwitch 端口上处理；服务流量到达时，其源端 IP 地址设置为来源 Pod 的 IP。这种方案的延迟很低且可扩缩性好。</td><td>v1.15+</td><td>Windows Server 2004 版</td><td>为 kube-proxy 设置标志：<code>--feature-gates="WinDSR=true" --enable-dsr=true</code></td></tr><tr><td>保留目标地址</td><td>对服务流量略过 DNAT 步骤，这样就可以在到达后端 Pod 的报文中保留目标服务的虚拟 IP 地址。这一配置也会确保入站报文的客户端 IP 地址也被保留下来。</td><td>v1.15+</td><td>Windows Server 1903 或更高版本</td><td>在服务注解中设置 <code>"preserve-destination": "true"</code> 并启用 kube-proxy 中的 DSR 标志。</td></tr><tr><td>IPv4/IPv6 双栈网络</td><td>在集群内外同时支持原生的 IPv4-到-IPv4 和 IPv6-到-IPv6 通信。</td><td>v1.19+</td><td>Windows Server vNext Insider Preview Build 19603 或更高版本</td><td>参见 <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack</a></td></tr></tbody></table><h4 id=ipv4ipv6-dual-stack>IPv4/IPv6 双栈支持</h4><p>你可以通过使用 <code>IPv6DualStack</code>
<a href=/zh/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>
来为 <code>l2bridge</code> 网络启用 IPv4/IPv6 双栈联网支持。
进一步的细节可参见<a href=/zh/docs/concepts/services-networking/dual-stack#enable-ipv4ipv6-dual-stack>启用 IPv4/IPv6 双协议栈</a>。</p><p>对 Windows 而言，在 Kubernetes 中使用 IPv6 需要
Windows Server vNext Insider Preview Build 19603 或更高版本。</p><p>目前 Windows 上的覆盖网络（VXLAN）还不支持双协议栈联网。</p><h3 id=limitations>局限性</h3><h4 id=control-plane>控制面</h4><p>在 Kubernetes 架构和节点阵列中仅支持将 Windows 作为工作节点使用。
这意味着 Kubernetes 集群必须总是包含 Linux 主控节点，零个或者多个 Linux
工作节点以及零个或者多个 Windows 工作节点。</p><h4 id=compute>计算</h4><h5 id=resource-management-and-process-isolation>资源管理与进程隔离</h5><p>Linux 上使用 Linux 控制组（CGroups）作为 Pod 的边界，以实现资源控制。
容器都创建于这一边界之内，从而实现网络、进程和文件系统的隔离。
控制组 CGroups API 可用来收集 CPU、I/O 和内存的统计信息。
与此相比，Windows 为每个容器创建一个带有系统名字空间过滤设置的 Job 对象，
以容纳容器中的所有进程并提供其与宿主系统间的逻辑隔离。
没有现成的名字空间过滤设置是无法运行 Windows 容器的。
这也意味着，系统特权无法在宿主环境中评估，因而 Windows 上也就不存在特权容器。
归咎于独立存在的安全账号管理器（Security Account Manager，SAM），容器也不能
获得宿主系统上的任何身份标识。</p><h5 id=operating-system-restrictions>操作系统限制</h5><p>Windows 有着严格的兼容性规则，宿主 OS 的版本必须与容器基准镜像 OS 的版本匹配。
目前仅支持容器操作系统为 Windows Server 2019 的 Windows 容器。
对于容器的 Hyper-V 隔离、允许一定程度上的 Windows 容器镜像版本向后兼容性等等，
都是将来版本计划的一部分。</p><h5 id=feature-restrictions>功能特性限制</h5><ul><li>终止宽限期（Termination Grace Period）：未实现</li><li>单文件映射：将用 CRI-ContainerD 来实现</li><li>终止消息（Termination message）：将用 CRI-ContainerD 来实现</li><li>特权容器：Windows 容器当前不支持</li><li>巨页（Huge Pages）：Windows 容器当前不支持</li><li>现有的节点问题探测器（Node Problem Detector）仅适用于 Linux，且要求使用特权容器。
一般而言，我们不设想此探测器能用于 Windows 节点，因为 Windows 不支持特权容器。</li><li>并非支持共享名字空间的所有功能特性（参见 API 节以了解详细信息）</li></ul><h5 id=memory-reservations-and-handling>内存预留与处理</h5><p>Windows 不像 Linux 一样有一个内存耗尽（Out-of-memory）进程杀手（Process
Killer）机制。Windows 总是将用户态的内存分配视为虚拟请求，页面文件（Pagefile）
是必需的。这一差异的直接结果是 Windows 不会像 Linux 那样出现内存耗尽的状况，
系统会将进程内存页面写入磁盘而不会因内存耗尽而终止进程。
当内存被过量使用且所有物理内存都被用光时，系统的换页行为会导致性能下降。</p><p>通过一个两步的过程是有可能将内存用量限制在一个合理的范围的。
首先，使用 kubelet 参数 <code>--kubelet-reserve</code> 与/或 <code>--system-reserve</code>
来划分节点上的内存用量（各容器之外）。
这样做会减少节点可分配内存
（<a href=/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>）。
在你部署工作负载时，对容器使用资源限制（必须仅设置 limits 或者让 limits 等于
requests 值）。这也会从 NodeAllocatable 中耗掉部分内存量，从而避免在节点
负荷已满时调度器继续向节点添加 Pods。</p><p>避免过量分配的最佳实践是为 kubelet 配置至少 2 GB 的系统预留内存，以供
Windows、Docker 和 Kubernetes 进程使用。</p><p>参数的不同行为描述如下：</p><ul><li><code>--kubelet-reserve</code>、<code>--system-reserve</code> 和 <code>--eviction-hard</code> 标志会更新节点可分配内存量</li><li>未实现通过使用 <code>--enforce-node-allocable</code> 来完成的 Pod 驱逐</li><li>未实现通过使用 <code>--eviction-hard</code> 和 <code>--eviction-soft</code> 来完成的 Pod 驱逐</li><li><code>MemoryPressure</code> 状况未实现</li><li><code>kubelet</code> 不会采取措施来执行基于 OOM 的驱逐动作</li><li>Windows 节点上运行的 kubelet 没有内存约束。
<code>--kubelet-reserve</code> 和 <code>--system-reserve</code> 不会为 kubelet 或宿主系统上运行
的进程设限。这意味着 kubelet 或宿主系统上的进程可能导致内存资源紧张，
而这一情况既不受节点可分配量影响，也不会被调度器感知。</li></ul><h4 id=storage>存储</h4><p>Windows 上包含一个分层的文件系统来挂载容器的分层，并会基于 NTFS 来创建一个
拷贝文件系统。容器中的所有文件路径都仅在该容器的上下文内完成解析。</p><ul><li>卷挂载仅可针对容器中的目录进行，不可针对独立的文件</li><li>卷挂载无法将文件或目录投射回宿主文件系统</li><li>不支持只读文件系统，因为 Windows 注册表和 SAM 数据库总是需要写访问权限。
不过，Windows 支持只读的卷。</li><li>不支持卷的用户掩码和访问许可，因为宿主与容器之间并不共享 SAM，二者之间不存在
映射关系。所有访问许可都是在容器上下文中解析的。</li></ul><p>因此，Windows 节点上不支持以下存储功能：</p><ul><li>卷的子路径挂载；只能在 Windows 容器上挂载整个卷。</li><li>为 Secret 执行子路径挂载</li><li>宿主挂载投射</li><li>默认访问模式（因为该特性依赖 UID/GID）</li><li>只读的根文件系统；映射的卷仍然支持 <code>readOnly</code></li><li>块设备映射</li><li>将内存作为存储介质</li><li>类似 UUID/GUID、每用户不同的 Linux 文件系统访问许可等文件系统特性</li><li>基于 NFS 的存储和卷支持</li><li>扩充已挂载卷（resizefs）</li></ul><h4 id=networking>联网</h4><p>Windows 容器联网与 Linux 联网有着非常重要的差别。
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture>Microsoft documentation for Windows Container Networking</a>
中包含额外的细节和背景信息。</p><p>Windows 宿主联网服务和虚拟交换机实现了名字空间隔离，可以根据需要为 Pod 或容器
创建虚拟的网络接口（NICs）。不过，很多类似 DNS、路由、度量值之类的配置数据都
保存在 Windows 注册表数据库中而不是像 Linux 一样保存在 <code>/etc/...</code> 文件中。
Windows 为容器提供的注册表与宿主系统的注册表是分离的，因此类似于将 /etc/resolv.conf
文件从宿主系统映射到容器中的做法不会产生与 Linux 系统相同的效果。
这些信息必须在容器内部使用 Windows API 来配置。
因此，CNI 实现需要调用 HNS，而不是依赖文件映射来将网络细节传递到 Pod
或容器中。</p><p>Windows 节点不支持以下联网功能：</p><ul><li>Windows Pod 不能使用宿主网络模式</li><li>从节点本地访问 NodePort 会失败（但从其他节点或外部客户端可访问）</li><li>Windows Server 的未来版本中会支持从节点访问服务的 VIPs</li><li>kube-proxy 的覆盖网络支持是 Alpha 特性。此外，它要求在 Windows Server 2019 上安装
<a href=https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887>KB4482887</a> 补丁</li><li>本地流量策略和 DSR（保留目标地址）模式</li><li>连接到 l2bridge、l2tunnel 或覆盖网络的 Windows 容器不支持使用 IPv6 协议栈通信。
要使得这些网络驱动能够支持 IPv6 地址需要在 Windows 平台上开展大量的工作，
还需要在 Kubernetes 侧修改 kubelet、kube-proxy 以及 CNI 插件。</li><li>通过 win-overlay、win-bridge 和 Azure-CNI 插件使用 ICMP 协议向集群外通信。
尤其是，Windows 数据面（<a href=https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/>VFP</a>）
不支持转换 ICMP 报文。这意味着：<ul><li>指向同一网络内目标地址的 ICMP 报文（例如 Pod 之间的 ping 通信）是可以工作的，没有局限性</li><li>TCP/UDP 报文可以正常工作，没有局限性</li><li>指向远程网络的 ICMP 报文（例如，从 Pod 中 ping 外部互联网的通信）无法被转换，
因此也无法被路由回到其源点。</li><li>由于 TCP/UDP 包仍可被转换，用户可以将 <code>ping &lt;目标></code> 操作替换为 <code>curl &lt;目标></code>
以便能够调试与外部世界的网络连接。</li></ul></li></ul><p>Kubernetes v1.15 中添加了以下功能特性：</p><ul><li><code>kubectl port-forward</code></li></ul><h5 id=cni-plugins>CNI 插件</h5><ul><li><p>Windows 参考网络插件 win-bridge 和 win-overlay 当前未实现
<a href=https://github.com/containernetworking/cni/blob/master/SPEC.md>CNI spec</a> v0.4.0，
原因是缺少检查用（CHECK）的实现。</p></li><li><p>Windows 上的 Flannel VXLAN CNI 有以下局限性：</p><ol><li>其设计上不支持从节点到 Pod 的连接。
只有在 Flannel v0.12.0 或更高版本后才有可能访问本地 Pods。</li><li>我们被限制只能使用 VNI 4096 和 UDP 端口 4789。
VNI 的限制正在被解决，会在将来的版本中消失（开源的 Flannel 更改）。
参见官方的 <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>Flannel VXLAN</a>
后端文档以了解关于这些参数的详细信息。</li></ol></li></ul><h5 id=dns-limitations>DNS</h5><ul><li>不支持 DNS 的 ClusterFirstWithHostNet 配置。Windows 将所有包含 “.” 的名字
视为全限定域名（FQDN），因而不会对其执行部分限定域名（PQDN）解析。</li><li>在 Linux 上，你可以有一个 DNS 后缀列表供解析部分限定域名时使用。
在 Windows 上，我们只有一个 DNS 后缀，即与该 Pod 名字空间相关联的 DNS
后缀（例如 <code>mydns.svc.cluster.local</code>）。
Windows 可以解析全限定域名、或者恰好可用该后缀来解析的服务名称。
例如，在 default 名字空间中生成的 Pod 会获得 DNS 后缀
<code>default.svc.cluster.local</code>。在 Windows Pod 中，你可以解析
<code>kubernetes.default.svc.cluster.local</code> 和 <code>kubernetes</code>，但无法解析二者
之间的形式，如 <code>kubernetes.default</code> 或 <code>kubernetes.default.svc</code>。</li><li>在 Windows 上，可以使用的 DNS 解析程序有很多。由于这些解析程序彼此之间
会有轻微的行为差别，建议使用 <code>Resolve-DNSName</code> 工具来完成名字查询解析。</li></ul><h5 id=ipv6>IPv6</h5><p>Windows 上的 Kubernetes 不支持单协议栈的“只用 IPv6”联网选项。
不过，系统支持在 IPv4/IPv6 双协议栈的 Pod 和节点上运行单协议家族的服务。
更多细节可参阅 <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 双协议栈联网</a>一节。</p><h5 id=session-affinity>会话亲和性</h5><p>不支持使用 <code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> 来为
Windows 服务设置最大会话粘滞时间。</p><h5 id=security>安全性</h5><p>Secret 以明文形式写入节点的卷中（而不是像 Linux 那样写入内存或 tmpfs 中）。
这意味着客户必须做以下两件事：</p><ol><li>使用文件访问控制列表来保护 Secret 文件所在的位置</li><li>使用 <a href=https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a>
来执行卷层面的加密</li></ol><p>Windows 上目前不支持 <a href=/zh/docs/concepts/policy/pod-security-policy/#users-and-groups><code>RunAsUser</code></a>。
一种替代方案是在为容器打包时创建本地账号。
将来的版本中可能会添加对 <code>RunAsUser</code> 的支持。</p><p>不支持特定于 Linux 的 Pod 安全上下文特权，例如 SELinux、AppArmor、Seccomp、
权能字（POSIX 权能字）等等。</p><p>此外，如前所述，Windows 不支持特权容器。</p><h4 id=api>API</h4><p>对 Windows 而言，大多数 Kubernetes API 的工作方式没有变化。
一些不易察觉的差别通常体现在 OS 和容器运行时上的不同。
在某些场合，负载 API （如 Pod 或 Container）的某些属性在设计时假定其
在 Linux 上实现，因此会无法在 Windows 上运行。</p><p>在较高层面，不同的 OS 概念有：</p><ul><li>身份标识 - Linux 使用证书类型来表示用户 ID（UID）和组 ID（GID）。用户和组名
没有特定标准，它们仅是 <code>/etc/groups</code> 或 <code>/etc/passwd</code> 中的别名表项，会映射回
UID+GID。Windows 使用一个更大的二进制安全标识符（SID），保存在 Windows
安全访问管理器（Security Access Manager，SAM）数据库中。此数据库并不在宿主系统
与容器间，或者任意两个容器之间共享。</li><li>文件许可 - Windows 使用基于 SID 的访问控制列表，而不是基于 UID+GID 的访问权限位掩码。</li><li>文件路径 - Windows 上的习惯是使用 <code>\</code> 而非 <code>/</code>。Go 语言的 IO 库通常能够同时接受二者，
并做出正确判断。不过当你在指定要在容器内解析的路径或命令行时，可能需要使用 <code>\</code>。</li><li>信号 - Windows 交互式应用以不同方式来处理终止事件，并可实现以下方式之一或组合：<ul><li>UI 线程处理包含 WM_CLOSE 在内的良定的消息</li><li>控制台应用使用控制处理程序来处理 Ctrl-C 或 Ctrl-Break</li><li>服务会注册服务控制处理程序，接受 SERVICE_CONTROL_STOP 控制代码</li></ul></li></ul><p>退出代码遵从相同的习惯，0 表示成功，非 0 值表示失败。
特定的错误代码在 Windows 和 Linux 上可能会不同。不过，从 Kubernetes 组件
（kubelet、kube-proxy）所返回的退出代码是没有变化的。</p><h5 id=v1-container>V1.Container</h5><ul><li><p><code>v1.Container.ResourceRequirements.limits.cpu</code> 和 <code>v1.Container.ResourceRequirements.limits.memory</code> - Windows
不对 CPU 分配设置硬性的限制。与之相反，Windows 使用一个份额（share）系统。
基于毫核（millicores）的现有字段值会被缩放为相对的份额值，供 Windows 调度器使用。
参见 <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go>kuberuntime/helpers_windows.go</a> 和
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls>Microsoft 文档中关于资源控制的部分</a>。</p><ul><li>Windows 容器运行时中没有实现巨页支持，因此相关特性不可用。
巨页支持需要<a href=https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support>判定用户的特权</a>
而这一特性无法在容器级别配置。</li></ul></li><li><p><code>v1.Container.ResourceRequirements.requests.cpu</code> 和 <code>v1.Container.ResourceRequirements.requests.memory</code> - 请求
值会从节点可分配资源中扣除，从而可用来避免节点上的资源过量分配。
但是，它们无法用来在一个已经过量分配的节点上提供资源保障。
如果操作员希望彻底避免过量分配，作为最佳实践，他们就需要为所有容器设置资源请求值。</p></li><li><p><code>v1.Container.SecurityContext.allowPrivilegeEscalation</code> - 在 Windows 上无法实现，对应的权能
无一可在 Windows 上生效。</p></li><li><p><code>v1.Container.SecurityContext.Capabilities</code> - Windows 上未实现 POSIX 权能机制</p></li><li><p><code>v1.Container.SecurityContext.privileged</code> - Windows 不支持特权容器</p></li><li><p><code>v1.Container.SecurityContext.procMount</code> - Windows 不包含 <code>/proc</code> 文件系统</p></li><li><p><code>v1.Container.SecurityContext.readOnlyRootFilesystem</code> - 在 Windows 上无法实现，
要在容器内使用注册表或运行系统进程就必需写访问权限。</p></li><li><p><code>v1.Container.SecurityContext.runAsGroup</code> - 在 Windows 上无法实现，没有 GID 支持</p></li><li><p><code>v1.Container.SecurityContext.runAsNonRoot</code> - Windows 上没有 root 用户。
与之最接近的等价用户是 <code>ContainerAdministrator</code>，而该身份标识在节点上并不存在。</p></li><li><p><code>v1.Container.SecurityContext.runAsUser</code> - 在 Windows 上无法实现，因为没有作为整数支持的 GID。</p></li><li><p><code>v1.Container.SecurityContext.seLinuxOptions</code> - 在 Windows 上无法实现，因为没有 SELinux</p></li><li><p><code>V1.Container.terminationMessagePath</code> - 因为 Windows 不支持单个文件的映射，这一功能
在 Windows 上也受限。默认值 <code>/dev/termination-log</code> 在 Windows 上也无法使用因为
对应路径在 Windows 上不存在。</p></li></ul><h5 id=v1-pod>V1.Pod</h5><ul><li><code>v1.Pod.hostIPC</code>、<code>v1.Pod.hostPID</code> - Windows 不支持共享宿主系统的名字空间</li><li><code>v1.Pod.hostNetwork</code> - Windows 操作系统不支持共享宿主网络</li><li><code>v1.Pod.dnsPolicy</code> - 不支持 <code>ClusterFirstWithHostNet</code>，因为 Windows 不支持宿主网络</li><li><code>v1.Pod.podSecurityContext</code> - 参见下面的 <code>v1.PodSecurityContext</code></li><li><code>v1.Pod.shareProcessNamespace</code> - 此为 Beta 特性且依赖于 Windows 上未实现的 Linux
名字空间。Windows 无法共享进程名字空间或者容器的根文件系统。只能共享网络。</li><li><code>v1.Pod.terminationGracePeriodSeconds</code> - 这一特性未在 Windows 版本的 Docker 中完全实现。
参见<a href=https://github.com/moby/moby/issues/25982>问题报告</a>。
目前实现的行为是向 ENTRYPOINT 进程发送 CTRL_SHUTDOWN_EVENT 时间，之后 Windows 默认
等待 5 秒钟，并最终使用正常的 Windows 关机行为关闭所有进程。
这里的 5 秒钟默认值实际上保存在<a href=https://github.com/moby/moby/issues/25982#issuecomment-426441183>容器内</a>
的 Windows 注册表中，因此可以在构造容器时重载。</li><li><code>v1.Pod.volumeDevices</code> - 此为 Beta 特性且未在 Windows 上实现。Windows 无法挂接
原生的块设备到 Pod 中。</li><li><code>v1.Pod.volumes</code> - <code>emptyDir</code>、<code>secret</code>、<code>configMap</code> 和 <code>hostPath</code> 都可正常工作且在
TestGrid 中测试。<ul><li><code>v1.emptyDir.volumeSource</code> - Windows 上节点的默认介质是磁盘。不支持将内存作为介质，
因为 Windows 不支持内置的 RAM 磁盘。</li></ul></li><li><code>v1.VolumeMount.mountPropagation</code> - Windows 上不支持挂载传播。</li></ul><h5 id=v1-podsecuritycontext>V1.PodSecurityContext</h5><p>PodSecurityContext 的所有选项在 Windows 上都无法工作。这些选项列在下面仅供参考。</p><ul><li><code>v1.PodSecurityContext.seLinuxOptions</code> - Windows 上无 SELinux</li><li><code>v1.PodSecurityContext.runAsUser</code> - 提供 UID；Windows 不支持</li><li><code>v1.PodSecurityContext.runAsGroup</code> - 提供 GID；Windows 不支持</li><li><code>v1.PodSecurityContext.runAsNonRoot</code> - Windows 上没有 root 用户
最接近的等价账号是 <code>ContainerAdministrator</code>，而该身份标识在节点上不存在</li><li><code>v1.PodSecurityContext.supplementalGroups</code> - 提供 GID；Windows 不支持</li><li><code>v1.PodSecurityContext.sysctls</code> - 这些是 Linux sysctl 接口的一部分；Windows 上
没有等价机制。</li></ul><h2 id=troubleshooting>获取帮助和故障排查</h2><p>对你的 Kubernetes 集群进行排查的主要帮助信息来源应该是
<a href=/docs/tasks/debug-application-cluster/troubleshooting/>这份文档</a>。
该文档中包含了一些额外的、特定于 Windows 系统的故障排查帮助信息。
Kubernetes 中日志是故障排查的一个重要元素。确保你在尝试从其他贡献者那里获得
故障排查帮助时提供日志信息。
你可以按照 SIG-Windows <a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>贡献指南和收集日志</a>
所给的指令来操作。</p><ol><li><p>我怎样知道 <code>start.ps1</code> 是否已成功完成？</p><p>你应该能看到节点上运行的 kubelet、kube-proxy 和（如果你选择 Flannel
作为联网方案）flanneld 宿主代理进程，它们的运行日志显示在不同的
PowerShell 窗口中。此外，你的 Windows 节点应该在你的 Kubernetes 集群
列举为 "Ready" 节点。</p></li></ol><ol start=2><li><p>我可以将 Kubernetes 节点进程配置为服务运行在后台么？</p><p>kubelet 和 kube-proxy 都已经被配置为以本地 Windows 服务运行，
并且在出现失效事件（例如进程意外结束）时通过自动重启服务来提供一定的弹性。
你有两种办法将这些节点组件配置为服务。</p><ol><li><p>以本地 Windows 服务的形式</p><p>Kubelet 和 kube-proxy 可以用 <code>sc.exe</code> 以本地 Windows 服务的形式运行：</p></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># 用两个单独的命令为 kubelet 和 kube-proxy 创建服务</span>
sc.exe create &lt;组件名称&gt; binPath= <span style=color:#b44>&#34;&lt;可执行文件路径&gt; -service &lt;其它参数&gt;&#34;</span>

<span style=color:#080;font-style:italic># 请注意如果参数中包含空格，必须使用转义</span>
sc.exe create kubelet binPath= <span style=color:#b44>&#34;C:\kubelet.exe --service --hostname-override &#39;minion&#39; &lt;其它参数&gt;&#34;</span>

<span style=color:#080;font-style:italic># 启动服务</span>
<span style=color:#a2f>Start-Service</span> kubelet
<span style=color:#a2f>Start-Service</span> kube-proxy

<span style=color:#080;font-style:italic># 停止服务</span>
<span style=color:#a2f>Stop-Service</span> kubelet (-Force)
<span style=color:#a2f>Stop-Service</span> kube-proxy (-Force)

<span style=color:#080;font-style:italic># 查询服务状态</span>
<span style=color:#a2f>Get-Service</span> kubelet
<span style=color:#a2f>Get-Service</span> kube-proxy
</code></pre></div><ol start=2><li><p>使用 nssm.exe</p><p>你也总是可以使用替代的服务管理器，例如<a href=https://nssm.cc/>nssm.exe</a>，来为你在后台运行
这些进程（<code>flanneld</code>、<code>kubelet</code> 和 <code>kube-proxy</code>）。你可以使用这一
<a href=https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1>示例脚本</a>，
利用 <code>nssm.exe</code> 将 <code>kubelet</code>、<code>kube-proxy</code> 和 <code>flanneld.exe</code> 注册为要在后台运行的
Windows 服务。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>register-svc</span>.ps1 -NetworkMode &lt;网络模式&gt; -ManagementIP &lt;Windows 节点 IP&gt; -ClusterCIDR &lt;集群子网&gt; -KubeDnsServiceIP &lt;kube-dns 服务 IP&gt; -LogDir &lt;日志目录&gt;

<span style=color:#080;font-style:italic># NetworkMode      = 网络模式 l2bridge（flannel host-gw，也是默认值）或 overlay（flannel vxlan）选做网络方案</span>
<span style=color:#080;font-style:italic># ManagementIP     = 分配给 Windows 节点的 IP 地址。你可以使用 ipconfig 得到此值</span>
<span style=color:#080;font-style:italic># ClusterCIDR      = 集群子网范围（默认值为 10.244.0.0/16）</span>
<span style=color:#080;font-style:italic># KubeDnsServiceIP = Kubernetes DNS 服务 IP（默认值为 10.96.0.10）</span>
<span style=color:#080;font-style:italic># LogDir           = kubelet 和 kube-proxy 的日志会被重定向到这一目录中的对应输出文件，默认值为 `C:\k`。</span>
</code></pre></div><p>若以上所引用的脚本不适合，你可以使用下面的例子手动配置 <code>nssm.exe</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># 注册 flanneld.exe</span>
nssm install flanneld C:\flannel\flanneld.exe
nssm <span style=color:#a2f>set </span>flanneld AppParameters --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;ManagementIP&gt; --ip-masq=1 --kube-subnet-mgr=1
nssm <span style=color:#a2f>set </span>flanneld AppEnvironmentExtra NODE_NAME=&lt;主机名&gt;
nssm <span style=color:#a2f>set </span>flanneld AppDirectory C:\flannel
nssm <span style=color:#a2f>start </span>flanneld

<span style=color:#080;font-style:italic># 注册 kubelet.exe</span>
<span style=color:#080;font-style:italic># Microsoft 在 mcr.microsoft.com/k8s/core/pause:1.2.0 发布其 pause 基础设施容器</span>
nssm install kubelet C:\k\kubelet.exe
nssm <span style=color:#a2f>set </span>kubelet AppParameters --hostname-override=&lt;hostname&gt; --v=6 --pod-infra-container-image=mcr.microsoft.com/k8s/core/pause<span>:</span>1.2.0 --resolv-conf=<span style=color:#b44>&#34;&#34;</span> --allow-privileged=true --enable-debugging-handlers --cluster-dns=&lt;DNS 服务 IP&gt; --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=promiscuous-bridge --image-pull-progress-deadline=20m --cgroups-per-qos=false  --log-dir=&lt;log directory&gt; --logtostderr=false --enforce-node-allocatable=<span style=color:#b44>&#34;&#34;</span> --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config
nssm <span style=color:#a2f>set </span>kubelet AppDirectory C:\k
nssm <span style=color:#a2f>start </span>kubelet

<span style=color:#080;font-style:italic># 注册 kube-proxy.exe (l2bridge / host-gw)</span>
nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --hostname-override=&lt;主机名&gt;--kubeconfig=c:\k\config --enable-dsr=false --log-dir=&lt;日志目录&gt; --logtostderr=false
nssm.exe <span style=color:#a2f>set </span>kube-proxy AppEnvironmentExtra KUBE_NETWORK=cbr0
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy

<span style=color:#080;font-style:italic># 注册 kube-proxy.exe (overlay / vxlan)</span>
nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --feature-gates=<span style=color:#b44>&#34;WinOverlay=true&#34;</span> --hostname-override=&lt;主机名&gt; --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=&lt;源端 VIP&gt; --enable-dsr=false --log-dir=&lt;日志目录&gt; --logtostderr=false
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy
</code></pre></div><p>作为初始的故障排查操作，你可以使用在 <a href=https://nssm.cc/>nssm.exe</a> 中使用下面的标志
以便将标准输出和标准错误输出重定向到一个输出文件：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm <span style=color:#a2f>set </span>&lt;服务名称&gt; AppStdout C:\k\mysvc.log
nssm <span style=color:#a2f>set </span>&lt;服务名称&gt; AppStderr C:\k\mysvc.log
</code></pre></div><p>要了解更多的细节，可参见官方的 <a href=https://nssm.cc/usage>nssm 用法</a>文档。</p></li></ol></li></ol><ol start=3><li><p>我的 Windows Pods 无发连接网络</p><p>如果你在使用虚拟机，请确保 VM 网络适配器均已开启 MAC 侦听（Spoofing）。</p></li></ol><ol start=4><li><p>我的 Windows Pods 无法 ping 外部资源</p><p>Windows Pods 目前没有为 ICMP 协议提供出站规则。不过 TCP/UDP 是支持的。
尝试与集群外资源连接时，可以将 <code>ping &lt;IP></code> 命令替换为对应的 <code>curl &lt;IP></code> 命令。</p><p>如果你还遇到问题，很可能你在
<a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf>cni.conf</a>
中的网络配置值得额外的注意。你总是可以编辑这一静态文件。
配置的更新会应用到所有新创建的 Kubernetes 资源上。</p><p>Kubernetes 网络的需求之一（参见<a href=/zh/docs/concepts/cluster-administration/networking/>Kubernetes 模型</a>）
是集群内部无需网络地址转译（NAT）即可实现通信。为了符合这一要求，对所有我们不希望出站时发生 NAT
的通信都存在一个 <a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20>ExceptionList</a>。
然而这也意味着你需要将你要查询的外部 IP 从 ExceptionList 中移除。
只有这时，从你的 Windows Pod 发起的网络请求才会被正确地通过 SNAT 转换以接收到
来自外部世界的响应。
就此而言，你在 <code>cni.conf</code> 中的 <code>ExceptionList</code> 应该看起来像这样：</p><pre><code class=language-conf data-lang=conf>&quot;ExceptionList&quot;: [
    &quot;10.244.0.0/16&quot;,  # 集群子网
    &quot;10.96.0.0/12&quot;,   # 服务子网
    &quot;10.127.130.0/24&quot; # 管理（主机）子网
]
</code></pre></li></ol><ol start=5><li><p>我的 Windows 节点无法访问 NodePort 服务</p><p>从节点自身发起的本地 NodePort 请求会失败。这是一个已知的局限。
NodePort 服务的访问从其他节点或者外部客户端都可正常进行。</p></li></ol><ol start=6><li><p>容器的 vNICs 和 HNS 端点被删除了</p><p>这一问题可能因为 <code>hostname-override</code> 参数未能传递给
<a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a> 而导致。
解决这一问题时，用户需要按如下方式将主机名传递给 kube-proxy：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>C:\k\kube-proxy.exe --hostname-override=$(hostname)
</code></pre></div></li></ol><ol start=7><li><p>使用 Flannel 时，我的节点在重新加入集群时遇到问题</p><p>无论何时，当一个之前被删除的节点被重新添加到集群时，flannelD 都会将为节点分配
一个新的 Pod 子网。
用户需要将将下面路径中的老的 Pod 子网配置文件删除：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Remove-Item</span> C:\k\SourceVip.json
<span style=color:#a2f>Remove-Item</span> C:\k\SourceVipRequest.json
</code></pre></div></li></ol><ol start=8><li><p>在启动了 <code>start.ps1</code> 之后，flanneld 一直停滞在 "Waiting for the Network to be created" 状态</p><p>关于这一<a href=https://github.com/coreos/flannel/issues/1066>正在被分析的问题</a>有很多的报告；
最可能的一种原因是关于何时设置 Flannel 网络的管理 IP 的时间问题。
一种解决办法是重新启动 <code>start.ps1</code> 或者按如下方式手动重启之：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>PS </span>C:&gt; <span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;NODE_NAME&#34;</span>, <span style=color:#b44>&#34;&lt;Windows 工作节点主机名&gt;&#34;</span>)
<span style=color:#a2f>PS </span>C:&gt; C:\flannel\flanneld.exe --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;Windows 工作节点 IP&gt; --ip-masq=1 --kube-subnet-mgr=1
</code></pre></div></li></ol><ol start=9><li><p>我的 Windows Pods 无法启动，因为缺少 <code>/run/flannel/subnet.env</code> 文件</p><p>这表明 Flannel 网络未能正确启动。你可以尝试重启 flanneld.exe 或者将文件手动地
从 Kubernetes 主控节点的 <code>/run/flannel/subnet.env</code> 路径复制到 Windows 工作
节点的 <code>C:\run\flannel\subnet.env</code> 路径，并将 <code>FLANNEL_SUBNET</code> 行改为一个
不同的数值。例如，如果期望节点子网为 <code>10.244.4.1/24</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-env data-lang=env><span style=color:#b8860b>FLANNEL_NETWORK</span><span style=color:#666>=</span>10.244.0.0/16
<span style=color:#b8860b>FLANNEL_SUBNET</span><span style=color:#666>=</span>10.244.4.1/24
<span style=color:#b8860b>FLANNEL_MTU</span><span style=color:#666>=</span><span style=color:#666>1500</span>
<span style=color:#b8860b>FLANNEL_IPMASQ</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
</code></pre></div></li></ol><ol start=10><li><p>我的 Windows 节点无法使用服务 IP 访问我的服务</p><p>这是 Windows 上当前网络协议栈的一个已知的限制。
Windows Pods 能够访问服务 IP。</p></li></ol><ol start=11><li><p>启动 kubelet 时找不到网络适配器</p><p>Windows 网络堆栈需要一个虚拟的适配器，这样 Kubernetes 网络才能工作。
如果下面的命令（在管理员 Shell 中）没有任何返回结果，证明虚拟网络创建
（kubelet 正常工作的必要前提之一）失败了：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Get-HnsNetwork</span> | ? Name <span style=color:#666>-ieq</span> <span style=color:#b44>&#34;cbr0&#34;</span>
<span style=color:#a2f>Get-NetAdapter</span> | ? Name <span style=color:#666>-Like</span> <span style=color:#b44>&#34;vEthernet (Ethernet*&#34;</span>
</code></pre></div><p>当宿主系统的网络适配器名称不是 "Ethernet" 时，通常值得更改 <code>start.ps1</code> 脚本中的
<a href=https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7>InterfaceName</a>
参数来重试。否则可以查验 <code>start-kubelet.ps1</code> 的输出，看看是否在虚拟网络创建
过程中报告了其他错误。</p></li></ol><ol start=12><li><p>我的 Pods 停滞在 "Container Creating" 状态或者反复重启</p><p>检查你的 pause 镜像是与你的 OS 版本兼容的。
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources>这里的指令</a>
假定你的 OS 和容器版本都是 1803。如果你安装的是更新版本的 Windows，比如说
某个 Insider 构造版本，你需要相应地调整要使用的镜像。
请参照 Microsoft 的 <a href=https://hub.docker.com/u/microsoft/>Docker 仓库</a>
了解镜像。不管怎样，pause 镜像的 Dockerfile 和示例服务都期望镜像的标签
为 <code>:latest</code>。</p><p>从 Kubernetes v1.14 版本起，Microsoft 开始在 <code>mcr.microsoft.com/k8s/core/pause:1.2.0</code>
发布其 pause 基础设施容器。</p></li></ol><ol start=13><li><p>DNS 解析无法正常工作</p><p>参阅 Windows 上 <a href=#dns-limitations>DNS 相关的局限</a> 节。</p></li></ol><ol start=14><li><p><code>kubectl port-forward</code> 失败，错误信息为 "unable to do port forwarding: wincat not found"</p><p>此功能是在 Kubernetes v1.15 中实现的，pause 基础设施容器为 <code>mcr.microsoft.com/k8s/core/pause:1.2.0</code>。
请确保你使用的是这些版本或者更新版本。
如果你想要自行构造你自己的 pause 基础设施容器，要确保其中包含了
<a href=https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat>wincat</a></p></li></ol><ol start=15><li><p>我的 Kubernetes 安装失败，因为我的 Windows Server 节点在防火墙后面</p><p>如果你处于防火墙之后，那么必须定义如下 PowerShell 环境变量：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell><span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTP_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:80/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
<span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTPS_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:443/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
</code></pre></div></li></ol><ol start=15><li><p><code>pause</code> 容器是什么？</p><p>在一个 Kubernetes Pod 中，一个基础设施容器，或称 "pause" 容器，会被首先创建出来，
用以托管容器端点。属于同一 Pod 的容器，包括基础设施容器和工作容器，会共享相同的
网络名字空间和端点（相同的 IP 和端口空间）。我们需要 pause 容器来工作容器崩溃或
重启的状况，以确保不会丢失任何网络配置。</p><p>"pause" （基础设施）镜像托管在 Microsoft Container Registry (MCR) 上。
你可以使用 <code>docker pull mcr.microsoft.com/k8s/core/pause:1.2.0</code> 来访问它。
要了解进一步的细节，可参阅 <a href=https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat>DOCKERFILE</a>。</p></li></ol><h3 id=further-investigation>进一步探究</h3><p>如果以上步骤未能解决你遇到的问题，你可以通过以下方式获得在 Kubernetes
中的 Windows 节点上运行 Windows 容器的帮助：</p><ul><li>StackOverflow <a href=https://stackoverflow.com/questions/tagged/windows-server-container>Windows Server Container</a> 主题</li><li>Kubernetes 官方论坛 <a href=https://discuss.kubernetes.io/>discuss.kubernetes.io</a></li><li>Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-windows>#SIG-Windows 频道</a></li></ul><h2 id=reporting-issues-and-feature-requests>报告问题和功能需求</h2><p>如果你遇到看起来像是软件缺陷的问题，或者你想要提起某种功能需求，请使用
<a href=https://github.com/kubernetes/kubernetes/issues>GitHub 问题跟踪系统</a>。
你可以在 <a href=https://github.com/kubernetes/kubernetes/issues/new/choose>GitHub</a>
上发起 Issue 并将其指派给 SIG-Windows。你应该首先搜索 Issue 列表，看看是否
该 Issue 以前曾经被报告过，以评论形式将你在该 Issue 上的体验追加进去，并附上
额外的日志信息。SIG-Windows Slack 频道也是一个获得初步支持的好渠道，可以在
生成新的 Ticket 之前对一些想法进行故障分析。</p><p>在登记软件缺陷时，请给出如何重现该问题的详细信息，例如：</p><ul><li>Kubernetes 版本：kubectl 版本</li><li>环境细节：云平台、OS 版本、网络选型和配置情况以及 Docker 版本</li><li>重现该问题的详细步骤</li><li><a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>相关的日志</a></li><li>通过为该 Issue 添加 <code>/sig windows</code> 评论为其添加 <code>sig/windows</code> 标签，
进而引起 SIG-Windows 成员的注意。</li></ul><h2 id=接下来>接下来</h2><p>在我们的未来蓝图中包含很多功能特性（要实现）。下面是一个浓缩的简要列表，不过我们
鼓励你查看我们的 <a href=https://github.com/orgs/kubernetes/projects/8>roadmap 项目</a>并
通过<a href=https://github.com/kubernetes/community/blob/master/sig-windows/>贡献</a>的方式
帮助我们把 Windows 支持做得更好。</p><h3 id=hyper-v-isolation>Hyper-V 隔离</h3><p>要满足 Kubernetes 中 Windows 容器的如下用例，需要利用 Hyper-V 隔离：</p><ul><li>在 Pod 之间实施基于监管程序（Hypervisor）的隔离，以增强安全性</li><li>出于向后兼容需要，允许添加运行新 Windows Server 版本的节点时不必重新创建容器</li><li>为 Pod 设置特定的 CPU/NUMA 配置</li><li>实施内存隔离与预留</li></ul><p>现有的 Hyper-V 隔离支持是添加自 v1.10 版本的实验性功能特性，会在未来版本中弃用，
向前文所提到的 CRI-ContainerD 和 RuntimeClass 特性倾斜。
要使用当前的功能特性并创建 Hyper-V 隔离的容器，需要在启动 kubelet 时设置特性门控
<code>HyperVContainer=true</code>，同时为 Pod 添加注解
<code>experimental.windows.kubernetes.io/isolation-type=hyperv</code>。
在实验性实现版本中，此功能特性限制每个 Pod 中只能包含一个容器。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>experimental.windows.kubernetes.io/isolation-type</span>:<span style=color:#bbb> </span>hyperv<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>microsoft/iis<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></code></pre></div><h3 id=deployment-with-kubeadm-and-cluster-api>使用 kubeadm 和 Cluster API 来部署</h3><p>kubeadm 已经成为用户部署 Kubernetes 集群的事实标准。
kubeadm 对 Windows 节点的支持目前还在开发过程中，不过你可以阅读相关的
<a href=/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>指南</a>。
我们也在投入资源到 Cluster API，以确保 Windows 节点被正确配置。</p><h3 id=a-few-other-key-features>若干其他关键功能</h3><ul><li>为组管理的服务账号（Group Managed Service Accounts，GMSA）提供 Beta 支持</li><li>添加更多的 CNI 支持</li><li>实现更多的存储插件</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3a51e66c5de55f9093a8dc55742006d3>3.4.2 - Kubernetes 中调度 Windows 容器的指南</h1><p>Windows 应用程序构成了许多组织中运行的服务和应用程序的很大一部分。
本指南将引导您完成在 Kubernetes 中配置和部署 Windows 容器的步骤。</p><h2 id=目标>目标</h2><ul><li>配置一个示例 deployment 以在 Windows 节点上运行 Windows 容器</li><li>（可选）使用组托管服务帐户（GMSA）为您的 Pod 配置 Active Directory 身份</li></ul><h2 id=在你开始之前>在你开始之前</h2><ul><li>创建一个 Kubernetes 集群，其中包括一个
<a href=/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>运行 Windows 服务器的主节点和工作节点</a></li><li>重要的是要注意，对于 Linux 和 Windows 容器，在 Kubernetes 上创建和部署服务和工作负载的行为几乎相同。
与集群接口的 <a href=/zh/docs/reference/kubectl/overview/>Kubectl 命令</a>相同。提供以下部分中的示例只是为了快速启动 Windows 容器的使用体验。</li></ul><h2 id=入门-部署-windows-容器>入门：部署 Windows 容器</h2><p>要在 Kubernetes 上部署 Windows 容器，您必须首先创建一个示例应用程序。
下面的示例 YAML 文件创建了一个简单的 Web 服务器应用程序。
创建一个名为 <code>win-webserver.yaml</code> 的服务规约，其内容如下：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># the port that this service should serve on</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windowswebserver<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- powershell.exe<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- -command<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:#b44>&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString+=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></code></pre></div><blockquote class="note callout"><div><strong>说明：</strong> 端口映射也是支持的，但为简单起见，在此示例中容器端口 80 直接暴露给服务。</div></blockquote><ol><li><p>检查所有节点是否健康：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get nodes
</code></pre></div></li><li><p>部署服务并观察 pod 更新：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f win-webserver.yaml
kubectl get pods -o wide -w
</code></pre></div><p>正确部署服务后，两个 Pod 都标记为“Ready”。要退出 watch 命令，请按 Ctrl + C。</p></li><li><p>检查部署是否成功。验证：</p><ul><li>Windows 节点上每个 Pod 有两个容器，使用 <code>docker ps</code></li><li>Linux 主机列出两个 Pod，使用 <code>kubectl get pods</code></li><li>跨网络的节点到 Pod 通信，从 Linux 主服务器 <code>curl</code> 您的 pod IPs 的端口80，以检查 Web 服务器响应</li><li>Pod 到 Pod 的通信，使用 docker exec 或 kubectl exec 在 pod 之间（以及跨主机，如果您有多个 Windows 节点）进行 ping 操作</li><li>服务到 Pod 的通信，从 Linux 主服务器和各个 Pod 中 <code>curl</code> 虚拟服务 IP（在 <code>kubectl get services</code> 下可见）</li><li>服务发现，使用 Kubernetes <code>curl</code> 服务名称<a href=/zh/docs/concepts/services-networking/dns-pod-service/#services>默认 DNS 后缀</a></li><li>入站连接，从 Linux 主服务器或集群外部的计算机 <code>curl</code> NodePort</li><li>出站连接，使用 kubectl exec 从 Pod 内部 curl 外部 IP</li></ul></li></ol><blockquote class="note callout"><div><strong>说明：</strong> 由于当前平台对 Windows 网络堆栈的限制，Windows 容器主机无法访问在其上调度的服务的 IP。只有 Windows pods 才能访问服务 IP。</div></blockquote><h2 id=使用可配置的容器用户名>使用可配置的容器用户名</h2><p>从 Kubernetes v1.16 开始，可以为 Windows 容器配置与其镜像默认值不同的用户名来运行其入口点和进程。
此能力的实现方式和 Linux 容器有些不同。
在<a href=/zh/docs/tasks/configure-pod-container/configure-runasusername/>此处</a>可了解更多信息。</p><h2 id=使用组托管服务帐户管理工作负载身份>使用组托管服务帐户管理工作负载身份</h2><p>从 Kubernetes v1.14 开始，可以将 Windows 容器工作负载配置为使用组托管服务帐户（GMSA）。
组托管服务帐户是 Active Directory 帐户的一种特定类型，它提供自动密码管理，
简化的服务主体名称（SPN）管理以及将管理委派给跨多台服务器的其他管理员的功能。
配置了 GMSA 的容器可以访问外部 Active Directory 域资源，同时携带通过 GMSA 配置的身份。
在<a href=/zh/docs/tasks/configure-pod-container/configure-gmsa/>此处</a>了解有关为 Windows 容器配置和使用 GMSA 的更多信息。</p><h2 id=污点和容忍度>污点和容忍度</h2><p>目前，用户需要将 Linux 和 Windows 工作负载运行在各自特定的操作系统的节点上，
因而需要结合使用污点和节点选择算符。 这可能仅给 Windows 用户造成不便。
推荐的方法概述如下，其主要目标之一是该方法不应破坏与现有 Linux 工作负载的兼容性。</p><h3 id=确保特定操作系统的工作负载落在适当的容器主机上>确保特定操作系统的工作负载落在适当的容器主机上</h3><p>用户可以使用污点和容忍度确保 Windows 容器可以调度在适当的主机上。目前所有 Kubernetes 节点都具有以下默认标签：</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>如果 Pod 规范未指定诸如 <code>"kubernetes.io/os": windows</code> 之类的 nodeSelector，则该 Pod
可能会被调度到任何主机（Windows 或 Linux）上。
这是有问题的，因为 Windows 容器只能在 Windows 上运行，而 Linux 容器只能在 Linux 上运行。
最佳实践是使用 nodeSelector。</p><p>但是，我们了解到，在许多情况下，用户都有既存的大量的 Linux 容器部署，以及一个现成的配置生态系统，
例如社区 Helm charts，以及程序化 Pod 生成案例，例如 Operators。
在这些情况下，您可能会不愿意更改配置添加 nodeSelector。替代方法是使用污点。
由于 kubelet 可以在注册期间设置污点，因此可以轻松修改它，使其仅在 Windows 上运行时自动添加污点。</p><p>例如：<code>--register-with-taints='os=windows:NoSchedule'</code></p><p>向所有 Windows 节点添加污点后，Kubernetes 将不会在它们上调度任何负载（包括现有的 Linux Pod）。
为了使某 Windows Pod 调度到 Windows 节点上，该 Pod 既需要 nodeSelector 选择 Windows，
也需要合适的匹配的容忍度设置。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;os&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><h3 id=处理同一集群中的多个-windows-版本>处理同一集群中的多个 Windows 版本</h3><p>每个 Pod 使用的 Windows Server 版本必须与该节点的 Windows Server 版本相匹配。
如果要在同一集群中使用多个 Windows Server 版本，则应该设置其他节点标签和 nodeSelector。</p><p>Kubernetes 1.17 自动添加了一个新标签 <code>node.kubernetes.io/windows-build</code> 来简化此操作。
如果您运行的是旧版本，则建议手动将此标签添加到 Windows 节点。</p><p>此标签反映了需要兼容的 Windows 主要、次要和内部版本号。以下是当前每个 Windows Server 版本使用的值。</p><table><thead><tr><th>产品名称</th><th>内部编号</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1809</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1903</td><td>10.0.18362</td></tr></tbody></table><h3 id=使用-runtimeclass-简化>使用 RuntimeClass 简化</h3><p><a href=/zh/docs/concepts/containers/runtime-class/>RuntimeClass</a> 可用于简化使用污点和容忍度的过程。
集群管理员可以创建 <code>RuntimeClass</code> 对象，用于封装这些污点和容忍度。</p><ol><li><p>将此文件保存到 <code>runtimeClasses.yml</code> 文件。它包括适用于 Windows 操作系统、体系结构和版本的 <code>nodeSelector</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;docker&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduling</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;windows&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;amd64&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>os<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Equal<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></code></pre></div></li></ol><ol start=2><li>集群管理员运行 <code>kubectl create -f runtimeClasses.yml</code> 操作</li><li>根据需要向 Pod 规约中添加 <code>runtimeClassName: windows-2019</code></li></ol><p>例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>.1<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>300Mi<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-84b6491601d6a2b3da4cd5a105c866ba>4 - 最佳实践</h1></div><div class=td-content><h1 id=pg-970615c97499e3651fd3a98e0387cefc>4.1 - 运行于多区环境</h1><p>本页描述如何在多个区（Zone）中运行集群。</p><h2 id=介绍>介绍</h2><p>Kubernetes 1.2 添加了跨多个失效区（Failure Zone）运行同一集群的能力
（GCE 把它们称作“区（Zones）”，AWS 把它们称作“可用区（Availability Zones）”，
这里我们用“区（Zones）”指代它们）。
此能力是更广泛的集群联邦（Cluster Federation）特性的一个轻量级版本。
集群联邦之前有一个昵称
<a href=https://github.com/kubernetes/community/blob/v1.20.15/contributors/design-proposals/multicluster/federation.md>"Ubernetes"</a>)。
完全的集群联邦可以将运行在多个区域（Region）或云供应商（或本地数据中心）的多个
Kubernetes 集群组合起来。
不过，很多用户仅仅是希望在同一云厂商平台的多个区域运行一个可用性更好的集群，
而这恰恰是 1.2 引入的多区支持所带来的特性
（此特性之前有一个昵称 “Ubernetes Lite”）。</p><p>多区支持有意实现的有局限性：可以在跨多个区域运行同一 Kubernetes 集群，但只能
在同一区域（Region）和云厂商平台。目前仅自动支持 GCE 和 AWS，尽管为其他云平台
或裸金属平台添加支持页相对容易，只需要确保节点和卷上添加合适的标签即可。</p><h2 id=功能>功能</h2><p>节点启动时，<code>kubelet</code> 自动向其上添加区信息标签。</p><p>在单区（Single-Zone）集群中， Kubernetes 会自动将副本控制器或服务中的 Pod
分布到不同节点，以降低节点失效的影响。
在多区集群中，这一分布负载的行为被扩展到跨区分布，以降低区失效的影响，
跨区分布的能力是通过 <code>SelectorSpreadPriority</code> 实现的。此放置策略亦仅仅是
尽力而为，所以如果你的集群所跨区是异质的（例如，节点个数不同、节点类型
不同或者 Pod 资源需求不同），放置策略都可能无法完美地跨区完成 Pod 的
均衡分布。如果需要，你可以使用同质区（节点个数和类型相同）以降低不均衡
分布的可能性。</p><p>持久卷被创建时，<code>PersistentVolumeLabel</code> 准入控制器会自动为其添加区标签。
调度器使用 <code>VolumeZonePredicate</code> 断言确保申领某给定卷的 Pod 只会被放到
该卷所在的区。这是因为卷不可以跨区挂载。</p><h2 id=局限性>局限性</h2><p>多区支持有一些很重要的局限性：</p><ul><li>我们假定不同的区之间在网络上彼此距离很近，所以我们不执行可感知区的路由。
尤其是，即使某些负责提供该服务的 Pod 与客户端位于同一区，通过服务末端
进入的流量可能会跨区，因而会导致一些额外的延迟和开销。</li></ul><ul><li><p>卷与区之间的亲和性仅适用于 PV 持久卷。例如，如果你直接在 Pod 规约中指定某 EBS
卷，这种亲和性支持就无法工作。</p></li><li><p>集群无法跨多个云平台或者地理区域运行。这类功能需要完整的联邦特性支持。</p></li></ul><ul><li>尽管你的节点位于多个区中，<code>kube-up</code> 脚本目前默认只能构造一个主控节点。
尽管服务是高可用的，能够忍受失去某个区的问题，控制面位于某一个区中。
希望运行高可用控制面的用户应该遵照
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>
中的指令构建。</li></ul><h3 id=卷局限性>卷局限性</h3><p>以下局限性通过
<a href=/zh/docs/concepts/storage/storage-classes/#volume-binding-mode>拓扑感知的卷绑定</a>解决：</p><ul><li>使用动态卷供应时，StatefulSet 卷的跨区分布目前与 Pod
亲和性和反亲和性策略不兼容。</li></ul><ul><li><p>如果 StatefulSet 的名字中包含连字符（"-"），卷的跨区分布可能无法实现存储的
跨区同一分布。</p></li><li><p>当在一个 Deployment 或 Pod 规约中指定多个 PVC 申领时，则需要为某特定区域
配置 StorageClass，或者在某一特定区域中需要静态供应 PV 卷。
另一种解决方案是使用 StatefulSet，确保给定副本的所有卷都从同一区中供应。</p></li></ul><h2 id=演练>演练</h2><p>我们现在准备对在 GCE 和 AWS 上配置和使用多区集群进行演练。为了完成此演练，
你需要设置 <code>MULTIZONE=true</code> 来启动一个完整的集群，之后指定
<code>KUBE_USE_EXISTING_MASTER=true</code> 并再次运行 <code>kube-up</code> 添加其他区中的节点。</p><h3 id=建立集群>建立集群</h3><p>和往常一样创建集群，不过需要设置 MULTIZONE，以便告诉集群需要管理多个区。
这里我们在 <code>us-central1-a</code> 创建节点。</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</code></pre></div><p>这一步骤和往常一样启动一个集群，不过尽管 <code>MULTIZONE=true</code>
标志已经启用了多区功能特性支持，集群仍然运行在一个区内。</p><h3 id=节点已被打标签>节点已被打标签</h3><p>查看节点，你会看到节点上已经有了区信息标签。
目前这些节点都在 <code>us-central1-a</code> (GCE) 或 <code>us-west-2a</code> (AWS)。
对于区域（Region），标签为 <code>topology.kubernetes.io/region</code>，
对于区（Zone），标签为 <code>topology.kubernetes.io/zone</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME                     STATUS                     ROLES    AGE   VERSION          LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-1,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-master
kubernetes-minion-87j9   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-a12q
</code></pre><h3 id=添加第二个区中的节点>添加第二个区中的节点</h3><p>让我们向现有集群中添加另外一组节点，复用现有的主控节点，但运行在不同的区
（<code>us-central1-b</code> 或 <code>us-west-2b</code>）。
我们再次运行 <code>kube-up</code>，不过设置 <code>KUBE_USE_EXISTING_MASTER=true</code>。
<code>kube-up</code> 不会创建新的主控节点，而会复用之前创建的主控节点。</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</code></pre></div><p>在 AWS 上，我们还需要为额外的子网指定网络 CIDR，以及主控节点的内部 IP 地址：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.1.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div><p>再次查看节点，你会看到新启动了三个节点并且其标签表明运行在 <code>us-central1-b</code> 区：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME                     STATUS                     ROLES    AGE   VERSION           LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-1,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-master
kubernetes-minion-281d   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-87j9   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   17m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-a12q
kubernetes-minion-pp2f   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-pp2f
kubernetes-minion-wf8i   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-wf8i
</code></pre><h3 id=卷亲和性>卷亲和性</h3><p>通过动态卷供应创建一个卷（只有 PV 持久卷支持区亲和性）：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>{
</span><span style=color:#b44>  &#34;apiVersion&#34;: &#34;v1&#34;,
</span><span style=color:#b44>  &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
</span><span style=color:#b44>  &#34;metadata&#34;: {
</span><span style=color:#b44>    &#34;name&#34;: &#34;claim1&#34;,
</span><span style=color:#b44>    &#34;annotations&#34;: {
</span><span style=color:#b44>        &#34;volume.alpha.kubernetes.io/storage-class&#34;: &#34;foo&#34;
</span><span style=color:#b44>    }
</span><span style=color:#b44>  },
</span><span style=color:#b44>  &#34;spec&#34;: {
</span><span style=color:#b44>    &#34;accessModes&#34;: [
</span><span style=color:#b44>      &#34;ReadWriteOnce&#34;
</span><span style=color:#b44>    ],
</span><span style=color:#b44>    &#34;resources&#34;: {
</span><span style=color:#b44>      &#34;requests&#34;: {
</span><span style=color:#b44>        &#34;storage&#34;: &#34;5Gi&#34;
</span><span style=color:#b44>      }
</span><span style=color:#b44>    }
</span><span style=color:#b44>  }
</span><span style=color:#b44>}
</span><span style=color:#b44>EOF</span>
</code></pre></div><blockquote class="note callout"><div><strong>说明：</strong> Kubernetes 1.3 及以上版本会将动态 PV 申领散布到所配置的各个区。
在 1.2 版本中，动态持久卷总是在集群主控节点所在的区
（这里的 <code>us-central1-a</code> 或 <code>us-west-2a</code>），
对应的 Issue (<a href=https://github.com/kubernetes/kubernetes/issues/23330>#23330</a>)
在 1.3 及以上版本中已经解决。</div></blockquote><p>现在我们来验证 Kubernetes 自动为 PV 打上了所在区或区域的标签：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pv --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME           CAPACITY   ACCESSMODES   RECLAIM POLICY   STATUS    CLAIM            STORAGECLASS    REASON    AGE       LABELS
pv-gce-mj4gm   5Gi        RWO           Retain           Bound     default/claim1   manual                    46s       topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a
</code></pre><p>现在我们将创建一个使用 PVC 申领的 Pod。
由于 GCE PD 或 AWS EBS 卷都不能跨区挂载，这意味着 Pod 只能创建在卷所在的区：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>kubectl apply -f - &lt;&lt;EOF<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfrontend<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/www/html&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>claim1<span style=color:#bbb>
</span><span style=color:#bbb></span>EOF<span style=color:#bbb>
</span></code></pre></div><p>注意 Pod 自动创建在卷所在的区，因为云平台提供商一般不允许跨区挂接存储卷。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod mypod | grep Node
</code></pre></div><pre><code>Node:        kubernetes-minion-9vlv/10.240.0.5
</code></pre><p>检查节点标签：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get node kubernetes-minion-9vlv --show-labels
</code></pre></div><pre><code>NAME                     STATUS    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     22m    v1.6.0+fff5156   beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
</code></pre><h3 id=pod-跨区分布>Pod 跨区分布</h3><p>同一副本控制器或服务的多个 Pod 会自动完成跨区分布。
首先，我们现在第三个区启动一些节点：</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.2.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div><p>验证你现在有来自三个区的节点：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>创建 <code>guestbook-go</code> 示例，其中包含副本个数为 3 的 RC，运行一个简单的 Web 应用：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>find kubernetes/examples/guestbook-go/ -name <span style=color:#b44>&#39;*.json&#39;</span> | xargs -I <span style=color:#666>{}</span> kubectl apply -f <span style=color:#666>{}</span>
</code></pre></div><p>Pod 应该跨三个区分布：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook | grep Node
</code></pre></div><pre><code>Node:        kubernetes-minion-9vlv/10.240.0.5
Node:        kubernetes-minion-281d/10.240.0.8
Node:        kubernetes-minion-olsh/10.240.0.11
</code></pre><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d kubernetes-minion-olsh --show-labels
</code></pre></div><pre><code>NAME                     STATUS    ROLES    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     &lt;none&gt;   34m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-281d   Ready     &lt;none&gt;   20m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-olsh   Ready     &lt;none&gt;   3m     v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-f,kubernetes.io/hostname=kubernetes-minion-olsh
</code></pre><p>负载均衡器也会跨集群中的所有区；<code>guestbook-go</code> 示例中包含了一个负载均衡
服务的例子：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe service guestbook | grep LoadBalancer.Ingress
</code></pre></div><p>输出类似于：</p><pre><code>LoadBalancer Ingress:   130.211.126.21
</code></pre><p>设置上面的 IP 地址：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>IP</span><span style=color:#666>=</span>130.211.126.21
</code></pre></div><p>使用 curl 访问该 IP：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME
</code></pre></div><p>输出类似于：</p><pre><code>  &quot;HOSTNAME&quot;: &quot;guestbook-44sep&quot;,
</code></pre><p>如果多次尝试该命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>`</span>seq 20<span style=color:#b44>`</span>; <span style=color:#a2f;font-weight:700>do</span> curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME; <span style=color:#a2f;font-weight:700>done</span><span style=color:#666>)</span>  | sort | uniq
</code></pre></div><p>输出类似于：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-44sep&#34;</span>,
  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-hum5n&#34;</span>,
  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-ppm40&#34;</span>,
</code></pre></div><p>负载均衡器正确地选择不同的 Pod，即使它们跨了多个区。</p><h3 id=停止集群>停止集群</h3><p>当完成以上工作之后，清理任务现场：</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a kubernetes/cluster/kube-down.sh
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a kubernetes/cluster/kube-down.sh
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c797ee17120176c685455db89ae091a9>4.2 - 创建大型集群</h1><h2 id=支持>支持</h2><p>在 v1.20 版本中， Kubernetes 支持的最大节点数为 5000。更具体地说，我们支持满足以下<em>所有</em>条件的配置：</p><ul><li>节点数不超过 5000</li><li>Pod 总数不超过 150000</li><li>容器总数不超过 300000</li><li>每个节点的 pod 数量不超过 100</li></ul><br><nav id=TableOfContents><ul><li><a href=#支持>支持</a></li><li><a href=#设定>设定</a><ul><li><a href=#配额问题>配额问题</a></li><li><a href=#etcd-存储>Etcd 存储</a></li><li><a href=#主控节点大小和主控组件>主控节点大小和主控组件</a></li><li><a href=#addon-resources>插件资源</a></li><li><a href=#允许启动时次要节点失败>允许启动时次要节点失败</a></li></ul></li></ul></nav><h2 id=设定>设定</h2><p>集群是一组运行着 Kubernetes 代理的节点（物理机或者虚拟机），这些节点由主控节点（集群级控制面）控制。</p><p>通常，集群中的节点数由特定于云平台的配置文件 <code>config-default.sh</code>
（可以参考 <a href=https://releases.k8s.io/v1.20.15/cluster/gce/config-default.sh>GCE 平台的 <code>config-default.sh</code></a>）
中的 <code>NUM_NODES</code> 参数控制。</p><p>但是，在许多云供应商的平台上，仅将该值更改为非常大的值，可能会导致安装脚本运行失败。例如，在 GCE，由于配额问题，集群会启动失败。</p><p>因此，在创建大型 Kubernetes 集群时，必须考虑以下问题。</p><h3 id=配额问题>配额问题</h3><p>为了避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑：</p><ul><li>增加诸如 CPU，IP 等资源的配额。<ul><li>例如，在 <a href=https://cloud.google.com/compute/docs/resource-quotas>GCE</a>，您需要增加以下资源的配额：<ul><li>CPUs</li><li>VM 实例</li><li>永久磁盘总量</li><li>使用中的 IP 地址</li><li>防火墙规则</li><li>转发规则</li><li>路由</li><li>目标池</li></ul></li></ul></li><li>由于某些云供应商会对虚拟机的创建进行流控，因此需要对设置脚本进行更改，使其以较小的批次启动新的节点，并且之间有等待时间。</li></ul><h3 id=etcd-存储>Etcd 存储</h3><p>为了提高大规模集群的性能，我们将事件存储在专用的 etcd 实例中。</p><p>在创建集群时，现有 salt 脚本可以：</p><ul><li>启动并配置其它 etcd 实例</li><li>配置 API 服务器以使用 etcd 存储事件</li></ul><h3 id=主控节点大小和主控组件>主控节点大小和主控组件</h3><p>在 GCE/Google Kubernetes Engine 和 AWS 上，<code>kube-up</code> 会根据节点数量自动为您集群中的 master 节点配置适当的虚拟机大小。在其它云供应商的平台上，您将需要手动配置它。作为参考，我们在 GCE 上使用的规格为：</p><ul><li>1-5 个节点：n1-standard-1</li><li>6-10 个节点：n1-standard-2</li><li>11-100 个节点：n1-standard-4</li><li>101-250 个节点：n1-standard-8</li><li>251-500 个节点：n1-standard-16</li><li>超过 500 节点：n1-standard-32</li></ul><p>在 AWS 上使用的规格为</p><ul><li>1-5 个节点：m3.medium</li><li>6-10 个节点：m3.large</li><li>11-100 个节点：m3.xlarge</li><li>101-250 个节点：m3.2xlarge</li><li>251-500 个节点：c4.4xlarge</li><li>超过 500 节点：c4.8xlarge</li></ul><blockquote class="note callout"><div><strong>说明：</strong><p>在 Google Kubernetes Engine 上，主控节点的大小会根据集群的大小自动调整。更多有关信息，请参阅 <a href=https://cloudplatform.googleblog.com/2017/11/Cutting-Cluster-Management-Fees-on-Google-Kubernetes-Engine.html>此博客文章</a>。</p><p>在 AWS 上，主控节点的规格是在集群启动时设置的，并且，即使以后通过手动删除或添加节点的方式使集群缩容或扩容，主控节点的大小也不会更改。</p></div></blockquote><h3 id=addon-resources>插件资源</h3><p>为了防止内存泄漏或 <a href=https://releases.k8s.io/v1.20.15/cluster/addons>集群插件</a>
中的其它资源问题导致节点上所有可用资源被消耗，Kubernetes 限制了插件容器可以消耗的 CPU 和内存资源
（请参阅 PR <a href=http://pr.k8s.io/10653/files>#10653</a> 和 <a href=http://pr.k8s.io/10778/files>#10778</a>）。</p><p>例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.16<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></code></pre></div><p>除了 Heapster 之外，这些限制都是静态的，并且限制是基于 4 节点集群上运行的插件数据得出的（请参阅 <a href=http://issue.k8s.io/10335#issuecomment-117861225>#10335</a>）。在大规模集群上运行时，插件会消耗大量资源（请参阅 <a href=http://issue.k8s.io/5880#issuecomment-113984085>#5880</a>）。因此，如果在不调整这些值的情况下部署了大规模集群，插件容器可能会由于达到限制而不断被杀死。</p><p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p><ul><li>根据集群的规模，如果使用了以下插件，提高其内存和 CPU 上限（每个插件都有一个副本处理整个群集，因此内存和 CPU 使用率往往与集群的规模/负载成比例增长） ：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml>InfluxDB 和 Grafana</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/dns/kube-dns/kube-dns.yaml.in>kubedns、dnsmasq 和 sidecar</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml>Kibana</a></li></ul></li><li>根据集群的规模，如果使用了以下插件，调整其副本数量（每个插件都有多个副本，增加副本数量有助于处理增加的负载，但是，由于每个副本的负载也略有增加，因此也请考虑增加 CPU/内存限制）：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml>elasticsearch</a></li></ul></li><li>根据集群的规模，如果使用了以下插件，限制其内存和 CPU 上限（这些插件在每个节点上都有一个副本，但是 CPU/内存使用量也会随集群负载/规模而略有增加）：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml>FluentD 和 ElasticSearch 插件</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-gcp/fluentd-gcp-ds.yaml>FluentD 和 GCP 插件</a></li></ul></li></ul><p>Heapster 的资源限制与您集群的初始大小有关（请参阅 <a href=https://issue.k8s.io/16185>#16185</a>
和 <a href=http://issue.k8s.io/22940>#22940</a>）。如果您发现 Heapster 资源不足，您应该调整堆内存请求的计算公式（有关详细信息，请参阅相关 PR）。</p><p>关于如何检测插件容器是否达到资源限制，参见
<a href=/zh/docs/concepts/configuration/manage-resources-containers/#troubleshooting>计算资源的故障排除</a> 部分。</p><p><a href=https://issue.k8s.io/13048>未来</a>，我们期望根据集群规模大小来设置所有群集附加资源限制，并在集群扩缩容时动态调整它们。
我们欢迎您来实现这些功能。</p><h3 id=允许启动时次要节点失败>允许启动时次要节点失败</h3><p>出于各种原因（更多详细信息，请参见 <a href=https://github.com/kubernetes/kubernetes/issues/18969>#18969</a>），
在 <code>kube-up.sh</code> 中设置很大的 <code>NUM_NODES</code> 时，可能会由于少数节点无法正常启动而失败。
此时，您有两个选择：重新启动集群（运行 <code>kube-down.sh</code>，然后再运行 <code>kube-up.sh</code>），或者在运行 <code>kube-up.sh</code> 之前将环境变量 <code>ALLOWED_NOTREADY_NODES</code> 设置为您认为合适的任何值。采取后者时，即使运行成功的节点数量少于 <code>NUM_NODES</code>，<code>kube-up.sh</code> 仍可以运行成功。根据失败的原因，这些节点可能会稍后加入集群，又或者群集的大小保持在 <code>NUM_NODES-ALLOWED_NOTREADY_NODES</code>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>4.3 - 校验节点设置</h1><nav id=TableOfContents><ul><li><a href=#节点一致性测试>节点一致性测试</a></li><li><a href=#节点的前提条件>节点的前提条件</a></li><li><a href=#运行节点一致性测试>运行节点一致性测试</a></li><li><a href=#针对其他硬件体系结构运行节点一致性测试>针对其他硬件体系结构运行节点一致性测试</a></li><li><a href=#运行特定的测试>运行特定的测试</a></li><li><a href=#注意>注意</a></li></ul></nav><h2 id=节点一致性测试>节点一致性测试</h2><p><em>节点一致性测试</em> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。</p><p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p><h2 id=节点的前提条件>节点的前提条件</h2><p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p><ul><li>容器运行时 (Docker)</li><li>Kubelet</li></ul><h2 id=运行节点一致性测试>运行节点一致性测试</h2><p>要运行节点一致性测试，请执行以下步骤：</p><ol><li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
由于测试框架启动了本地控制平面来测试 kubelet， 因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：<ul><li><code>--pod-cidr</code>： 如果使用 <code>kubenet</code>， 需要为 Kubelet 任意指定一个 CIDR，
例如 <code>--pod-cidr=10.180.0.0/24</code>。</li><li><code>--cloud-provider</code>： 如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数
来运行测试。</li></ul></li></ol><ol start=2><li><p>使用以下命令运行节点一致性测试：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># $CONFIG_DIR 是您 Kubelet 的 pod manifest 路径。</span>
<span style=color:#080;font-style:italic># $LOG_DIR 是测试的输出路径。</span>
sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  k8s.gcr.io/node-test:0.2
</code></pre></div></li></ol><h2 id=针对其他硬件体系结构运行节点一致性测试>针对其他硬件体系结构运行节点一致性测试</h2><p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p><table><thead><tr><th>架构</th><th style=text-align:center>镜像</th><th></th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td><td></td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td><td></td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td><td></td></tr></tbody></table><h2 id=运行特定的测试>运行特定的测试</h2><p>要运行特定测试，请使用您希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><p>要跳过特定的测试，请使用您希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><p>节点一致性测试是<a href=https://github.com/kubernetes/community/blob/v1.20.15/contributors/devel/e2e-node-tests.md>节点端到端测试</a>的容器化版本。</p><p>默认情况下，它会运行所有一致性测试。</p><p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。 但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p><h2 id=注意>注意</h2><ul><li>测试会在节点上遗留一些 Docker 镜像， 包括节点一致性测试本身的镜像和功能测试相关的镜像。</li><li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>4.4 - PKI 证书和要求</h1><p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p><h2 id=集群是如何使用证书的>集群是如何使用证书的</h2><p>Kubernetes 需要 PKI 才能执行以下操作：</p><ul><li>Kubelet 的客户端证书，用于 API 服务器身份验证</li><li>API 服务器端点的证书</li><li>集群管理员的客户端证书，用于 API 服务器身份认证</li><li>API 服务器的客户端证书，用于和 Kubelet 的会话</li><li>API 服务器的客户端证书，用于和 etcd 的会话</li><li>控制器管理器的客户端证书/kubeconfig，用于和 API 服务器的会话</li><li>调度器的客户端证书/kubeconfig，用于和 API 服务器的会话</li><li><a href=/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a> 的客户端及服务端证书</li></ul><blockquote class="note callout"><div><strong>说明：</strong> 只有当你运行 kube-proxy 并要支持
<a href=/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/>扩展 API 服务器</a>
时，才需要 <code>front-proxy</code> 证书</div></blockquote><p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p><h2 id=证书存放的位置>证书存放的位置</h2><p>如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 <code>/etc/kubernetes/pki</code> 目录下。本文所有相关的路径都是基于该路径的相对路径。</p><h2 id=手动配置证书>手动配置证书</h2><p>如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。</p><h3 id=单根-ca>单根 CA</h3><p>你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p><p>需要这些 CA：</p><table><thead><tr><th>路径</th><th>默认 CN</th><th>描述</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes 通用 CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>与 etcd 相关的所有功能</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>用于 <a href=/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a></td></tr></tbody></table><p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p><h3 id=所有的证书>所有的证书</h3><p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p><p>需要这些证书：</p><table><thead><tr><th>默认 CN</th><th>父级 CA</th><th>O (位于 Subject 中)</th><th>类型</th><th>主机 (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名，<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p><p>其中，<code>kind</code> 对应一种或多种类型的 <a href=https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage>x509 密钥用途</a>：</p><table><thead><tr><th>kind</th><th>密钥用途</th></tr></thead><tbody><tr><td>server</td><td>数字签名、密钥加密、服务端认证</td></tr><tr><td>client</td><td>数字签名、密钥加密、客户端认证</td></tr></tbody></table><blockquote class="note callout"><div><strong>说明：</strong><p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。</div></blockquote><blockquote class="note callout"><div><strong>说明：</strong><p>对于 kubeadm 用户：</p><ul><li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li><li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 <code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li></ul></div></blockquote><h3 id=证书路径>证书路径</h3><p>证书应放置在建议的路径中（以便 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a>使用）。无论使用什么位置，都应使用给定的参数指定路径。</p><table><thead><tr><th>默认 CN</th><th>建议的密钥路径</th><th>建议的证书路径</th><th>命令</th><th>密钥参数</th><th>证书参数</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>注意事项同样适用于服务帐户密钥对：</p><table><thead><tr><th>私钥路径</th><th>公钥路径</th><th>命令</th><th>参数</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><h2 id=为用户帐户配置证书>为用户帐户配置证书</h2><p>你必须手动配置以下管理员帐户和服务帐户：</p><table><thead><tr><th>文件名</th><th>凭据名称</th><th>默认 CN</th><th>O (位于 Subject 中)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> （参阅注释）</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><blockquote class="note callout"><div><strong>说明：</strong> <code>kubelet.conf</code> 中 <code>&lt;nodeName></code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href=/zh/docs/reference/access-authn-authz/node/>节点授权</a>。</div></blockquote><ol><li><p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p></li><li><p>为每个配置运行下面的 <code>kubectl</code> 命令：</p></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</code></pre></div><p>这些文件用途如下：</p><table><thead><tr><th>文件名</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>配置集群的管理员</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>集群中的每个节点都需要一份</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td></tr></tbody></table></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh/docs/home/>主页</a>
<a class=text-white href=/zh/blog/>博客</a>
<a class=text-white href=/zh/training/>培训</a>
<a class=text-white href=/zh/partners/>合作伙伴</a>
<a class=text-white href=/zh/community/>社区</a>
<a class=text-white href=/zh/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>