<!doctype html><html lang=zh class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/best-practices/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/best-practices/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/best-practices/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/best-practices/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/best-practices/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh/docs/setup/best-practices/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>最佳实践 | Kubernetes</title><meta property="og:title" content="最佳实践"><meta property="og:description" content="生产级别的容器编排系统"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh/docs/setup/best-practices/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="最佳实践"><meta itemprop=description content="生产级别的容器编排系统"><meta name=twitter:card content="summary"><meta name=twitter:title content="最佳实践"><meta name=twitter:description content="生产级别的容器编排系统"><link rel=preload href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css as=style><link href=/scss/main.min.aeea2a074ae7ac3d467a0d6f52e45894b49452cbb3f0f410c268ec7280c5a653.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/zh/docs/setup/best-practices/"><meta property="og:title" content="最佳实践"><meta name=twitter:title content="最佳实践"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh/docs/>文档</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/blog/>Kubernetes 博客</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/training/>培训</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/partners/>合作伙伴</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/community/>社区</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh/case-studies/>案例分析</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://kubernetes.io/zh/docs/setup/best-practices/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh/docs/setup/best-practices/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh/docs/setup/best-practices/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh/docs/setup/best-practices/>v1.21</a>
<a class=dropdown-item href=https://v1-20.docs.kubernetes.io/zh/docs/setup/best-practices/>v1.20</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 Chinese</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/best-practices/>English</a>
<a class=dropdown-item href=/ko/docs/setup/best-practices/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/setup/best-practices/>日本語 Japanese</a>
<a class=dropdown-item href=/id/docs/setup/best-practices/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/setup/best-practices/>Українська</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh/docs/setup/best-practices/>返回本页常规视图</a>.</p></div><h1 class=title>最佳实践</h1><ul><li>1: <a href=#pg-970615c97499e3651fd3a98e0387cefc>运行于多区环境</a></li><li>2: <a href=#pg-c797ee17120176c685455db89ae091a9>创建大型集群</a></li><li>3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>校验节点设置</a></li><li>4: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI 证书和要求</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-970615c97499e3651fd3a98e0387cefc>1 - 运行于多区环境</h1><p>本页描述如何在多个区（Zone）中运行集群。</p><h2 id=介绍>介绍</h2><p>Kubernetes 1.2 添加了跨多个失效区（Failure Zone）运行同一集群的能力
（GCE 把它们称作“区（Zones）”，AWS 把它们称作“可用区（Availability Zones）”，
这里我们用“区（Zones）”指代它们）。
此能力是更广泛的集群联邦（Cluster Federation）特性的一个轻量级版本。
集群联邦之前有一个昵称
<a href=https://github.com/kubernetes/community/blob/v1.20.15/contributors/design-proposals/multicluster/federation.md>"Ubernetes"</a>)。
完全的集群联邦可以将运行在多个区域（Region）或云供应商（或本地数据中心）的多个
Kubernetes 集群组合起来。
不过，很多用户仅仅是希望在同一云厂商平台的多个区域运行一个可用性更好的集群，
而这恰恰是 1.2 引入的多区支持所带来的特性
（此特性之前有一个昵称 “Ubernetes Lite”）。</p><p>多区支持有意实现的有局限性：可以在跨多个区域运行同一 Kubernetes 集群，但只能
在同一区域（Region）和云厂商平台。目前仅自动支持 GCE 和 AWS，尽管为其他云平台
或裸金属平台添加支持页相对容易，只需要确保节点和卷上添加合适的标签即可。</p><h2 id=功能>功能</h2><p>节点启动时，<code>kubelet</code> 自动向其上添加区信息标签。</p><p>在单区（Single-Zone）集群中， Kubernetes 会自动将副本控制器或服务中的 Pod
分布到不同节点，以降低节点失效的影响。
在多区集群中，这一分布负载的行为被扩展到跨区分布，以降低区失效的影响，
跨区分布的能力是通过 <code>SelectorSpreadPriority</code> 实现的。此放置策略亦仅仅是
尽力而为，所以如果你的集群所跨区是异质的（例如，节点个数不同、节点类型
不同或者 Pod 资源需求不同），放置策略都可能无法完美地跨区完成 Pod 的
均衡分布。如果需要，你可以使用同质区（节点个数和类型相同）以降低不均衡
分布的可能性。</p><p>持久卷被创建时，<code>PersistentVolumeLabel</code> 准入控制器会自动为其添加区标签。
调度器使用 <code>VolumeZonePredicate</code> 断言确保申领某给定卷的 Pod 只会被放到
该卷所在的区。这是因为卷不可以跨区挂载。</p><h2 id=局限性>局限性</h2><p>多区支持有一些很重要的局限性：</p><ul><li>我们假定不同的区之间在网络上彼此距离很近，所以我们不执行可感知区的路由。
尤其是，即使某些负责提供该服务的 Pod 与客户端位于同一区，通过服务末端
进入的流量可能会跨区，因而会导致一些额外的延迟和开销。</li></ul><ul><li><p>卷与区之间的亲和性仅适用于 PV 持久卷。例如，如果你直接在 Pod 规约中指定某 EBS
卷，这种亲和性支持就无法工作。</p></li><li><p>集群无法跨多个云平台或者地理区域运行。这类功能需要完整的联邦特性支持。</p></li></ul><ul><li>尽管你的节点位于多个区中，<code>kube-up</code> 脚本目前默认只能构造一个主控节点。
尽管服务是高可用的，能够忍受失去某个区的问题，控制面位于某一个区中。
希望运行高可用控制面的用户应该遵照
<a href=/zh/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>
中的指令构建。</li></ul><h3 id=卷局限性>卷局限性</h3><p>以下局限性通过
<a href=/zh/docs/concepts/storage/storage-classes/#volume-binding-mode>拓扑感知的卷绑定</a>解决：</p><ul><li>使用动态卷供应时，StatefulSet 卷的跨区分布目前与 Pod
亲和性和反亲和性策略不兼容。</li></ul><ul><li><p>如果 StatefulSet 的名字中包含连字符（"-"），卷的跨区分布可能无法实现存储的
跨区同一分布。</p></li><li><p>当在一个 Deployment 或 Pod 规约中指定多个 PVC 申领时，则需要为某特定区域
配置 StorageClass，或者在某一特定区域中需要静态供应 PV 卷。
另一种解决方案是使用 StatefulSet，确保给定副本的所有卷都从同一区中供应。</p></li></ul><h2 id=演练>演练</h2><p>我们现在准备对在 GCE 和 AWS 上配置和使用多区集群进行演练。为了完成此演练，
你需要设置 <code>MULTIZONE=true</code> 来启动一个完整的集群，之后指定
<code>KUBE_USE_EXISTING_MASTER=true</code> 并再次运行 <code>kube-up</code> 添加其他区中的节点。</p><h3 id=建立集群>建立集群</h3><p>和往常一样创建集群，不过需要设置 MULTIZONE，以便告诉集群需要管理多个区。
这里我们在 <code>us-central1-a</code> 创建节点。</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</code></pre></div><p>这一步骤和往常一样启动一个集群，不过尽管 <code>MULTIZONE=true</code>
标志已经启用了多区功能特性支持，集群仍然运行在一个区内。</p><h3 id=节点已被打标签>节点已被打标签</h3><p>查看节点，你会看到节点上已经有了区信息标签。
目前这些节点都在 <code>us-central1-a</code> (GCE) 或 <code>us-west-2a</code> (AWS)。
对于区域（Region），标签为 <code>topology.kubernetes.io/region</code>，
对于区（Zone），标签为 <code>topology.kubernetes.io/zone</code>：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME                     STATUS                     ROLES    AGE   VERSION          LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-1,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-master
kubernetes-minion-87j9   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-a12q
</code></pre><h3 id=添加第二个区中的节点>添加第二个区中的节点</h3><p>让我们向现有集群中添加另外一组节点，复用现有的主控节点，但运行在不同的区
（<code>us-central1-b</code> 或 <code>us-west-2b</code>）。
我们再次运行 <code>kube-up</code>，不过设置 <code>KUBE_USE_EXISTING_MASTER=true</code>。
<code>kube-up</code> 不会创建新的主控节点，而会复用之前创建的主控节点。</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</code></pre></div><p>在 AWS 上，我们还需要为额外的子网指定网络 CIDR，以及主控节点的内部 IP 地址：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.1.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div><p>再次查看节点，你会看到新启动了三个节点并且其标签表明运行在 <code>us-central1-b</code> 区：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME                     STATUS                     ROLES    AGE   VERSION           LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-1,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-master
kubernetes-minion-281d   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-87j9   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   17m   v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-a12q
kubernetes-minion-pp2f   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-pp2f
kubernetes-minion-wf8i   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-wf8i
</code></pre><h3 id=卷亲和性>卷亲和性</h3><p>通过动态卷供应创建一个卷（只有 PV 持久卷支持区亲和性）：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span><span style=color:#b44>{
</span><span style=color:#b44>  &#34;apiVersion&#34;: &#34;v1&#34;,
</span><span style=color:#b44>  &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
</span><span style=color:#b44>  &#34;metadata&#34;: {
</span><span style=color:#b44>    &#34;name&#34;: &#34;claim1&#34;,
</span><span style=color:#b44>    &#34;annotations&#34;: {
</span><span style=color:#b44>        &#34;volume.alpha.kubernetes.io/storage-class&#34;: &#34;foo&#34;
</span><span style=color:#b44>    }
</span><span style=color:#b44>  },
</span><span style=color:#b44>  &#34;spec&#34;: {
</span><span style=color:#b44>    &#34;accessModes&#34;: [
</span><span style=color:#b44>      &#34;ReadWriteOnce&#34;
</span><span style=color:#b44>    ],
</span><span style=color:#b44>    &#34;resources&#34;: {
</span><span style=color:#b44>      &#34;requests&#34;: {
</span><span style=color:#b44>        &#34;storage&#34;: &#34;5Gi&#34;
</span><span style=color:#b44>      }
</span><span style=color:#b44>    }
</span><span style=color:#b44>  }
</span><span style=color:#b44>}
</span><span style=color:#b44>EOF</span>
</code></pre></div><blockquote class="note callout"><div><strong>说明：</strong> Kubernetes 1.3 及以上版本会将动态 PV 申领散布到所配置的各个区。
在 1.2 版本中，动态持久卷总是在集群主控节点所在的区
（这里的 <code>us-central1-a</code> 或 <code>us-west-2a</code>），
对应的 Issue (<a href=https://github.com/kubernetes/kubernetes/issues/23330>#23330</a>)
在 1.3 及以上版本中已经解决。</div></blockquote><p>现在我们来验证 Kubernetes 自动为 PV 打上了所在区或区域的标签：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pv --show-labels
</code></pre></div><p>输出类似于：</p><pre><code>NAME           CAPACITY   ACCESSMODES   RECLAIM POLICY   STATUS    CLAIM            STORAGECLASS    REASON    AGE       LABELS
pv-gce-mj4gm   5Gi        RWO           Retain           Bound     default/claim1   manual                    46s       topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a
</code></pre><p>现在我们将创建一个使用 PVC 申领的 Pod。
由于 GCE PD 或 AWS EBS 卷都不能跨区挂载，这意味着 Pod 只能创建在卷所在的区：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>kubectl apply -f - &lt;&lt;EOF<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfrontend<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/www/html&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>claim1<span style=color:#bbb>
</span><span style=color:#bbb></span>EOF<span style=color:#bbb>
</span></code></pre></div><p>注意 Pod 自动创建在卷所在的区，因为云平台提供商一般不允许跨区挂接存储卷。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod mypod | grep Node
</code></pre></div><pre><code>Node:        kubernetes-minion-9vlv/10.240.0.5
</code></pre><p>检查节点标签：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get node kubernetes-minion-9vlv --show-labels
</code></pre></div><pre><code>NAME                     STATUS    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     22m    v1.6.0+fff5156   beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
</code></pre><h3 id=pod-跨区分布>Pod 跨区分布</h3><p>同一副本控制器或服务的多个 Pod 会自动完成跨区分布。
首先，我们现在第三个区启动一些节点：</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.2.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</code></pre></div><p>验证你现在有来自三个区的节点：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get nodes --show-labels
</code></pre></div><p>创建 <code>guestbook-go</code> 示例，其中包含副本个数为 3 的 RC，运行一个简单的 Web 应用：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>find kubernetes/examples/guestbook-go/ -name <span style=color:#b44>&#39;*.json&#39;</span> | xargs -I <span style=color:#666>{}</span> kubectl apply -f <span style=color:#666>{}</span>
</code></pre></div><p>Pod 应该跨三个区分布：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe pod -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook | grep Node
</code></pre></div><pre><code>Node:        kubernetes-minion-9vlv/10.240.0.5
Node:        kubernetes-minion-281d/10.240.0.8
Node:        kubernetes-minion-olsh/10.240.0.11
</code></pre><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d kubernetes-minion-olsh --show-labels
</code></pre></div><pre><code>NAME                     STATUS    ROLES    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     &lt;none&gt;   34m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-a,kubernetes.io/hostname=kubernetes-minion-9vlv
kubernetes-minion-281d   Ready     &lt;none&gt;   20m    v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-b,kubernetes.io/hostname=kubernetes-minion-281d
kubernetes-minion-olsh   Ready     &lt;none&gt;   3m     v1.13.0          beta.kubernetes.io/instance-type=n1-standard-2,topology.kubernetes.io/region=us-central1,topology.kubernetes.io/zone=us-central1-f,kubernetes.io/hostname=kubernetes-minion-olsh
</code></pre><p>负载均衡器也会跨集群中的所有区；<code>guestbook-go</code> 示例中包含了一个负载均衡
服务的例子：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe service guestbook | grep LoadBalancer.Ingress
</code></pre></div><p>输出类似于：</p><pre><code>LoadBalancer Ingress:   130.211.126.21
</code></pre><p>设置上面的 IP 地址：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#a2f>export</span> <span style=color:#b8860b>IP</span><span style=color:#666>=</span>130.211.126.21
</code></pre></div><p>使用 curl 访问该 IP：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME
</code></pre></div><p>输出类似于：</p><pre><code>  &quot;HOSTNAME&quot;: &quot;guestbook-44sep&quot;,
</code></pre><p>如果多次尝试该命令：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>`</span>seq 20<span style=color:#b44>`</span>; <span style=color:#a2f;font-weight:700>do</span> curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME; <span style=color:#a2f;font-weight:700>done</span><span style=color:#666>)</span>  | sort | uniq
</code></pre></div><p>输出类似于：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-44sep&#34;</span>,
  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-hum5n&#34;</span>,
  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-ppm40&#34;</span>,
</code></pre></div><p>负载均衡器正确地选择不同的 Pod，即使它们跨了多个区。</p><h3 id=停止集群>停止集群</h3><p>当完成以上工作之后，清理任务现场：</p><p>GCE:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a kubernetes/cluster/kube-down.sh
</code></pre></div><p>AWS:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b kubernetes/cluster/kube-down.sh
<span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a kubernetes/cluster/kube-down.sh
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c797ee17120176c685455db89ae091a9>2 - 创建大型集群</h1><h2 id=支持>支持</h2><p>在 v1.20 版本中， Kubernetes 支持的最大节点数为 5000。更具体地说，我们支持满足以下<em>所有</em>条件的配置：</p><ul><li>节点数不超过 5000</li><li>Pod 总数不超过 150000</li><li>容器总数不超过 300000</li><li>每个节点的 pod 数量不超过 100</li></ul><br><nav id=TableOfContents><ul><li><a href=#支持>支持</a></li><li><a href=#设定>设定</a><ul><li><a href=#配额问题>配额问题</a></li><li><a href=#etcd-存储>Etcd 存储</a></li><li><a href=#主控节点大小和主控组件>主控节点大小和主控组件</a></li><li><a href=#addon-resources>插件资源</a></li><li><a href=#允许启动时次要节点失败>允许启动时次要节点失败</a></li></ul></li></ul></nav><h2 id=设定>设定</h2><p>集群是一组运行着 Kubernetes 代理的节点（物理机或者虚拟机），这些节点由主控节点（集群级控制面）控制。</p><p>通常，集群中的节点数由特定于云平台的配置文件 <code>config-default.sh</code>
（可以参考 <a href=https://releases.k8s.io/v1.20.15/cluster/gce/config-default.sh>GCE 平台的 <code>config-default.sh</code></a>）
中的 <code>NUM_NODES</code> 参数控制。</p><p>但是，在许多云供应商的平台上，仅将该值更改为非常大的值，可能会导致安装脚本运行失败。例如，在 GCE，由于配额问题，集群会启动失败。</p><p>因此，在创建大型 Kubernetes 集群时，必须考虑以下问题。</p><h3 id=配额问题>配额问题</h3><p>为了避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑：</p><ul><li>增加诸如 CPU，IP 等资源的配额。<ul><li>例如，在 <a href=https://cloud.google.com/compute/docs/resource-quotas>GCE</a>，您需要增加以下资源的配额：<ul><li>CPUs</li><li>VM 实例</li><li>永久磁盘总量</li><li>使用中的 IP 地址</li><li>防火墙规则</li><li>转发规则</li><li>路由</li><li>目标池</li></ul></li></ul></li><li>由于某些云供应商会对虚拟机的创建进行流控，因此需要对设置脚本进行更改，使其以较小的批次启动新的节点，并且之间有等待时间。</li></ul><h3 id=etcd-存储>Etcd 存储</h3><p>为了提高大规模集群的性能，我们将事件存储在专用的 etcd 实例中。</p><p>在创建集群时，现有 salt 脚本可以：</p><ul><li>启动并配置其它 etcd 实例</li><li>配置 API 服务器以使用 etcd 存储事件</li></ul><h3 id=主控节点大小和主控组件>主控节点大小和主控组件</h3><p>在 GCE/Google Kubernetes Engine 和 AWS 上，<code>kube-up</code> 会根据节点数量自动为您集群中的 master 节点配置适当的虚拟机大小。在其它云供应商的平台上，您将需要手动配置它。作为参考，我们在 GCE 上使用的规格为：</p><ul><li>1-5 个节点：n1-standard-1</li><li>6-10 个节点：n1-standard-2</li><li>11-100 个节点：n1-standard-4</li><li>101-250 个节点：n1-standard-8</li><li>251-500 个节点：n1-standard-16</li><li>超过 500 节点：n1-standard-32</li></ul><p>在 AWS 上使用的规格为</p><ul><li>1-5 个节点：m3.medium</li><li>6-10 个节点：m3.large</li><li>11-100 个节点：m3.xlarge</li><li>101-250 个节点：m3.2xlarge</li><li>251-500 个节点：c4.4xlarge</li><li>超过 500 节点：c4.8xlarge</li></ul><blockquote class="note callout"><div><strong>说明：</strong><p>在 Google Kubernetes Engine 上，主控节点的大小会根据集群的大小自动调整。更多有关信息，请参阅 <a href=https://cloudplatform.googleblog.com/2017/11/Cutting-Cluster-Management-Fees-on-Google-Kubernetes-Engine.html>此博客文章</a>。</p><p>在 AWS 上，主控节点的规格是在集群启动时设置的，并且，即使以后通过手动删除或添加节点的方式使集群缩容或扩容，主控节点的大小也不会更改。</p></div></blockquote><h3 id=addon-resources>插件资源</h3><p>为了防止内存泄漏或 <a href=https://releases.k8s.io/v1.20.15/cluster/addons>集群插件</a>
中的其它资源问题导致节点上所有可用资源被消耗，Kubernetes 限制了插件容器可以消耗的 CPU 和内存资源
（请参阅 PR <a href=http://pr.k8s.io/10653/files>#10653</a> 和 <a href=http://pr.k8s.io/10778/files>#10778</a>）。</p><p>例如：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.16<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></code></pre></div><p>除了 Heapster 之外，这些限制都是静态的，并且限制是基于 4 节点集群上运行的插件数据得出的（请参阅 <a href=http://issue.k8s.io/10335#issuecomment-117861225>#10335</a>）。在大规模集群上运行时，插件会消耗大量资源（请参阅 <a href=http://issue.k8s.io/5880#issuecomment-113984085>#5880</a>）。因此，如果在不调整这些值的情况下部署了大规模集群，插件容器可能会由于达到限制而不断被杀死。</p><p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p><ul><li>根据集群的规模，如果使用了以下插件，提高其内存和 CPU 上限（每个插件都有一个副本处理整个群集，因此内存和 CPU 使用率往往与集群的规模/负载成比例增长） ：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml>InfluxDB 和 Grafana</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/dns/kube-dns/kube-dns.yaml.in>kubedns、dnsmasq 和 sidecar</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml>Kibana</a></li></ul></li><li>根据集群的规模，如果使用了以下插件，调整其副本数量（每个插件都有多个副本，增加副本数量有助于处理增加的负载，但是，由于每个副本的负载也略有增加，因此也请考虑增加 CPU/内存限制）：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml>elasticsearch</a></li></ul></li><li>根据集群的规模，如果使用了以下插件，限制其内存和 CPU 上限（这些插件在每个节点上都有一个副本，但是 CPU/内存使用量也会随集群负载/规模而略有增加）：<ul><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml>FluentD 和 ElasticSearch 插件</a></li><li><a href=https://releases.k8s.io/v1.20.15/cluster/addons/fluentd-gcp/fluentd-gcp-ds.yaml>FluentD 和 GCP 插件</a></li></ul></li></ul><p>Heapster 的资源限制与您集群的初始大小有关（请参阅 <a href=https://issue.k8s.io/16185>#16185</a>
和 <a href=http://issue.k8s.io/22940>#22940</a>）。如果您发现 Heapster 资源不足，您应该调整堆内存请求的计算公式（有关详细信息，请参阅相关 PR）。</p><p>关于如何检测插件容器是否达到资源限制，参见
<a href=/zh/docs/concepts/configuration/manage-resources-containers/#troubleshooting>计算资源的故障排除</a> 部分。</p><p><a href=https://issue.k8s.io/13048>未来</a>，我们期望根据集群规模大小来设置所有群集附加资源限制，并在集群扩缩容时动态调整它们。
我们欢迎您来实现这些功能。</p><h3 id=允许启动时次要节点失败>允许启动时次要节点失败</h3><p>出于各种原因（更多详细信息，请参见 <a href=https://github.com/kubernetes/kubernetes/issues/18969>#18969</a>），
在 <code>kube-up.sh</code> 中设置很大的 <code>NUM_NODES</code> 时，可能会由于少数节点无法正常启动而失败。
此时，您有两个选择：重新启动集群（运行 <code>kube-down.sh</code>，然后再运行 <code>kube-up.sh</code>），或者在运行 <code>kube-up.sh</code> 之前将环境变量 <code>ALLOWED_NOTREADY_NODES</code> 设置为您认为合适的任何值。采取后者时，即使运行成功的节点数量少于 <code>NUM_NODES</code>，<code>kube-up.sh</code> 仍可以运行成功。根据失败的原因，这些节点可能会稍后加入集群，又或者群集的大小保持在 <code>NUM_NODES-ALLOWED_NOTREADY_NODES</code>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>3 - 校验节点设置</h1><nav id=TableOfContents><ul><li><a href=#节点一致性测试>节点一致性测试</a></li><li><a href=#节点的前提条件>节点的前提条件</a></li><li><a href=#运行节点一致性测试>运行节点一致性测试</a></li><li><a href=#针对其他硬件体系结构运行节点一致性测试>针对其他硬件体系结构运行节点一致性测试</a></li><li><a href=#运行特定的测试>运行特定的测试</a></li><li><a href=#注意>注意</a></li></ul></nav><h2 id=节点一致性测试>节点一致性测试</h2><p><em>节点一致性测试</em> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。</p><p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p><h2 id=节点的前提条件>节点的前提条件</h2><p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p><ul><li>容器运行时 (Docker)</li><li>Kubelet</li></ul><h2 id=运行节点一致性测试>运行节点一致性测试</h2><p>要运行节点一致性测试，请执行以下步骤：</p><ol><li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
由于测试框架启动了本地控制平面来测试 kubelet， 因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：<ul><li><code>--pod-cidr</code>： 如果使用 <code>kubenet</code>， 需要为 Kubelet 任意指定一个 CIDR，
例如 <code>--pod-cidr=10.180.0.0/24</code>。</li><li><code>--cloud-provider</code>： 如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数
来运行测试。</li></ul></li></ol><ol start=2><li><p>使用以下命令运行节点一致性测试：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#080;font-style:italic># $CONFIG_DIR 是您 Kubelet 的 pod manifest 路径。</span>
<span style=color:#080;font-style:italic># $LOG_DIR 是测试的输出路径。</span>
sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  k8s.gcr.io/node-test:0.2
</code></pre></div></li></ol><h2 id=针对其他硬件体系结构运行节点一致性测试>针对其他硬件体系结构运行节点一致性测试</h2><p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p><table><thead><tr><th>架构</th><th style=text-align:center>镜像</th><th></th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td><td></td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td><td></td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td><td></td></tr></tbody></table><h2 id=运行特定的测试>运行特定的测试</h2><p>要运行特定测试，请使用您希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>   -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><p>要跳过特定的测试，请使用您希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><p>节点一致性测试是<a href=https://github.com/kubernetes/community/blob/v1.20.15/contributors/devel/e2e-node-tests.md>节点端到端测试</a>的容器化版本。</p><p>默认情况下，它会运行所有一致性测试。</p><p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。 但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p><h2 id=注意>注意</h2><ul><li>测试会在节点上遗留一些 Docker 镜像， 包括节点一致性测试本身的镜像和功能测试相关的镜像。</li><li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>4 - PKI 证书和要求</h1><p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p><h2 id=集群是如何使用证书的>集群是如何使用证书的</h2><p>Kubernetes 需要 PKI 才能执行以下操作：</p><ul><li>Kubelet 的客户端证书，用于 API 服务器身份验证</li><li>API 服务器端点的证书</li><li>集群管理员的客户端证书，用于 API 服务器身份认证</li><li>API 服务器的客户端证书，用于和 Kubelet 的会话</li><li>API 服务器的客户端证书，用于和 etcd 的会话</li><li>控制器管理器的客户端证书/kubeconfig，用于和 API 服务器的会话</li><li>调度器的客户端证书/kubeconfig，用于和 API 服务器的会话</li><li><a href=/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a> 的客户端及服务端证书</li></ul><blockquote class="note callout"><div><strong>说明：</strong> 只有当你运行 kube-proxy 并要支持
<a href=/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/>扩展 API 服务器</a>
时，才需要 <code>front-proxy</code> 证书</div></blockquote><p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p><h2 id=证书存放的位置>证书存放的位置</h2><p>如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 <code>/etc/kubernetes/pki</code> 目录下。本文所有相关的路径都是基于该路径的相对路径。</p><h2 id=手动配置证书>手动配置证书</h2><p>如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。</p><h3 id=单根-ca>单根 CA</h3><p>你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p><p>需要这些 CA：</p><table><thead><tr><th>路径</th><th>默认 CN</th><th>描述</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes 通用 CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>与 etcd 相关的所有功能</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>用于 <a href=/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a></td></tr></tbody></table><p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p><h3 id=所有的证书>所有的证书</h3><p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p><p>需要这些证书：</p><table><thead><tr><th>默认 CN</th><th>父级 CA</th><th>O (位于 Subject 中)</th><th>类型</th><th>主机 (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名，<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p><p>其中，<code>kind</code> 对应一种或多种类型的 <a href=https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage>x509 密钥用途</a>：</p><table><thead><tr><th>kind</th><th>密钥用途</th></tr></thead><tbody><tr><td>server</td><td>数字签名、密钥加密、服务端认证</td></tr><tr><td>client</td><td>数字签名、密钥加密、客户端认证</td></tr></tbody></table><blockquote class="note callout"><div><strong>说明：</strong><p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。</div></blockquote><blockquote class="note callout"><div><strong>说明：</strong><p>对于 kubeadm 用户：</p><ul><li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li><li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 <code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li></ul></div></blockquote><h3 id=证书路径>证书路径</h3><p>证书应放置在建议的路径中（以便 <a href=/zh/docs/reference/setup-tools/kubeadm/>kubeadm</a>使用）。无论使用什么位置，都应使用给定的参数指定路径。</p><table><thead><tr><th>默认 CN</th><th>建议的密钥路径</th><th>建议的证书路径</th><th>命令</th><th>密钥参数</th><th>证书参数</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>注意事项同样适用于服务帐户密钥对：</p><table><thead><tr><th>私钥路径</th><th>公钥路径</th><th>命令</th><th>参数</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><h2 id=为用户帐户配置证书>为用户帐户配置证书</h2><p>你必须手动配置以下管理员帐户和服务帐户：</p><table><thead><tr><th>文件名</th><th>凭据名称</th><th>默认 CN</th><th>O (位于 Subject 中)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> （参阅注释）</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><blockquote class="note callout"><div><strong>说明：</strong> <code>kubelet.conf</code> 中 <code>&lt;nodeName></code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href=/zh/docs/reference/access-authn-authz/node/>节点授权</a>。</div></blockquote><ol><li><p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p></li><li><p>为每个配置运行下面的 <code>kubectl</code> 命令：</p></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
<span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</code></pre></div><p>这些文件用途如下：</p><table><thead><tr><th>文件名</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>配置集群的管理员</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>集群中的每个节点都需要一份</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td></tr></tbody></table></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh/docs/home/>主页</a>
<a class=text-white href=/zh/blog/>博客</a>
<a class=text-white href=/zh/training/>培训</a>
<a class=text-white href=/zh/partners/>合作伙伴</a>
<a class=text-white href=/zh/community/>社区</a>
<a class=text-white href=/zh/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>